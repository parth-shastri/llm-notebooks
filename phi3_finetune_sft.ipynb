{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install transformers datasets bitsandbytes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phi-3 family of models \n",
    " - Come in `phi-3-mini`(3B) & `phi-3-small`(7B) variants\n",
    " - The 'mini' version can be run on a Mobile phone.\n",
    " - The `phi-3-mini` version uses the transformer decoder architecture, using same layers like Llama model.\n",
    " - The tokenizer used is tiktokenizer with similar vocab like llama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "\n",
    "_ = torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('secrets/secrets.json', 'r') as f:\n",
    "    secrets = json.load(f)\n",
    "\n",
    "HF_TOKEN = secrets['HF_TOKEN']\n",
    "WANDB_TOKEN = secrets['WANDB_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609b423c286c4f7ab2339e7c74e1ac38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/phi-3-mini-128k-Instruct\",\n",
    "    quantization_config=config,\n",
    "    trust_remote_code=True,\n",
    "    low_cpu_mem_usage=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# phi tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/phi-3-mini-128k-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s><|system|>\\nYou are a helpful chat assistant.<|end|>\\n<|user|>\\nWhat are Large Language Models ?<|end|>\\n<|assistant|>\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful chat assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What are Large Language Models ?\"},\n",
    "]\n",
    "tokenizer.apply_chat_template(\n",
    "    message,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizers/microsoft/phi-3-mini-128k-Instruct/tokenizer_config.json',\n",
       " 'tokenizers/microsoft/phi-3-mini-128k-Instruct/special_tokens_map.json',\n",
       " 'tokenizers/microsoft/phi-3-mini-128k-Instruct/tokenizer.model',\n",
       " 'tokenizers/microsoft/phi-3-mini-128k-Instruct/added_tokens.json',\n",
       " 'tokenizers/microsoft/phi-3-mini-128k-Instruct/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('tokenizers/microsoft/phi-3-mini-128k-Instruct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    " - SFT Tuning the phi-3-mini-Instruct-128k model to follow finance instructions.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Decide on the dataset used, reference papers \"FinGPT & BloombergGPT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ostrich/anaconda3/envs/my_master/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, GenerationConfig\n",
    "\n",
    "pipe = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "\n",
    "prompts = [\n",
    "    \"A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\",\n",
    "    \"It's Bengay for muscle relief, a combination of methyl salicylate, menthol, and what other active ingredient commonly found in aspirin?\",\n",
    "    \"How can i get rid of llamas in my backyard?\",\n",
    "]\n",
    "\n",
    "messages = []\n",
    "for prompt in prompts:\n",
    "    messages.append(\n",
    "        [\n",
    "        {'role': 'system', 'content': 'You are a helpul chat assistant.'},\n",
    "        {'role': 'user', 'content': prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "generations = pipe(messages, **generation_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "PROMPT:\n",
      "A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\n",
      "=========================\n",
      "GENERATION:\n",
      "To find out how much fencing is needed, you need to calculate the perimeter of the rectangular garden. The formula for the perimeter of a rectangle is 2*(length + width). \n",
      "\n",
      "So, in this case, the calculation would be: 2 * (25 feet + 15 feet) = 2 * 40 feet = 80 feet.\n",
      "\n",
      "Therefore, you would need 80 feet of fencing to surround the entire garden.\n",
      "=========================\n",
      "=========================\n",
      "PROMPT:\n",
      "It's Bengay for muscle relief, a combination of methyl salicylate, menthol, and what other active ingredient commonly found in aspirin?\n",
      "=========================\n",
      "GENERATION:\n",
      "The active ingredient commonly found in aspirin that is also a component of Bengay for muscle relief is acetaminophen.\n",
      "=========================\n",
      "=========================\n",
      "PROMPT:\n",
      "How can i get rid of llamas in my backyard?\n",
      "=========================\n",
      "GENERATION:\n",
      "If you're finding llamas in your backyard, it's likely a misunderstanding as llamas are not common household pets and are native to South America. However, if you're dealing with an actual situation where llamas are present, here's what you can do:\n",
      "\n",
      "\n",
      "1. **Identify the Source**: First, try to find out how the llamas got into your yard. They might have escaped from a nearby farm or someone might be keeping them as a pet.\n",
      "\n",
      "\n",
      "2. **Contact Local Authorities**: If the llamas are not your own, it's best to contact local animal control or a professional animal rescue organization. They can help safely remove the animals and find them a suitable home.\n",
      "\n",
      "\n",
      "3. **Secure Your Property**: If the llamas are yours, ensure your property is secure. This might involve repairing any gaps in fences or enclosures where the llamas could have entered.\n",
      "\n",
      "\n",
      "4. **Provide Care**: If the llamas are yours and you need to keep them temporarily, make sure they have enough food, water, and shelter.\n",
      "\n",
      "\n",
      "5. **Consider Relocation**: If you cannot keep the llamas, look for a suitable environment where they can live comfortably. This could be a llama sanctuary or a farm that specializes in llamas.\n",
      "\n",
      "\n",
      "Remember, it's important to ensure the welfare of the animals and comply with local laws regarding animal ownership and care.\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "# display the results\n",
    "for i, generation in enumerate(generations):\n",
    "    print(\"=\" * 25)\n",
    "    print('PROMPT:')\n",
    "    print(f'{prompts[i]}')\n",
    "    print(\"=\" * 25)\n",
    "\n",
    "    print('GENERATION:')\n",
    "    print(f\"{generation[0]['generated_text']}\")\n",
    "    print(\"=\" * 25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
