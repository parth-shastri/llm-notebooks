/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.9616, 'grad_norm': 48.75383758544922, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
{'loss': 2.7373, 'grad_norm': 48.12888717651367, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}
{'loss': 2.665, 'grad_norm': 80.29344940185547, 'learning_rate': 6e-06, 'epoch': 0.0}
{'loss': 3.3419, 'grad_norm': 40.51714324951172, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}
{'loss': 2.8208, 'grad_norm': 87.94953155517578, 'learning_rate': 1e-05, 'epoch': 0.0}
{'loss': 3.6693, 'grad_norm': 49.99955749511719, 'learning_rate': 1.2e-05, 'epoch': 0.0}
{'loss': 1.9474, 'grad_norm': 130.48724365234375, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.0}
{'loss': 3.3926, 'grad_norm': 50.0963249206543, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.0}
{'loss': 2.6422, 'grad_norm': 23.17127799987793, 'learning_rate': 1.8e-05, 'epoch': 0.0}
{'loss': 3.2724, 'grad_norm': 32.814178466796875, 'learning_rate': 2e-05, 'epoch': 0.0}
{'loss': 3.1829, 'grad_norm': 35.379859924316406, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.0}
{'loss': 2.8973, 'grad_norm': 33.600563049316406, 'learning_rate': 2.4e-05, 'epoch': 0.0}
{'loss': 3.1357, 'grad_norm': 25.153202056884766, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.0}
{'loss': 2.7367, 'grad_norm': 25.749826431274414, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.0}
{'loss': 2.9013, 'grad_norm': 26.496593475341797, 'learning_rate': 3e-05, 'epoch': 0.0}
{'loss': 3.0777, 'grad_norm': 25.3740291595459, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.0}
{'loss': 3.0064, 'grad_norm': 31.318065643310547, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.0}
{'loss': 2.4549, 'grad_norm': 21.805526733398438, 'learning_rate': 3.6e-05, 'epoch': 0.0}
{'loss': 2.6314, 'grad_norm': 24.394723892211914, 'learning_rate': 3.8e-05, 'epoch': 0.0}
{'loss': 2.6635, 'grad_norm': 37.64463806152344, 'learning_rate': 4e-05, 'epoch': 0.0}
{'loss': 2.6901, 'grad_norm': 23.45008659362793, 'learning_rate': 4.2e-05, 'epoch': 0.0}
{'loss': 2.3259, 'grad_norm': 20.24517250061035, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.0}
{'loss': 1.8649, 'grad_norm': 13.754372596740723, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.0}
{'loss': 2.4683, 'grad_norm': 26.282751083374023, 'learning_rate': 4.8e-05, 'epoch': 0.0}
{'loss': 2.5658, 'grad_norm': 22.075651168823242, 'learning_rate': 5e-05, 'epoch': 0.0}
{'loss': 2.1778, 'grad_norm': 16.7067928314209, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.0}
{'loss': 2.3061, 'grad_norm': 16.092086791992188, 'learning_rate': 5.4000000000000005e-05, 'epoch': 0.0}
{'loss': 1.8689, 'grad_norm': 10.920830726623535, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.0}
{'loss': 2.0693, 'grad_norm': 19.844560623168945, 'learning_rate': 5.8e-05, 'epoch': 0.0}
{'loss': 1.9913, 'grad_norm': 38.808170318603516, 'learning_rate': 6e-05, 'epoch': 0.0}
{'loss': 2.0203, 'grad_norm': 23.240802764892578, 'learning_rate': 6.2e-05, 'epoch': 0.0}
{'loss': 1.8907, 'grad_norm': 29.105709075927734, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.0}
{'loss': 1.8074, 'grad_norm': 12.0838041305542, 'learning_rate': 6.6e-05, 'epoch': 0.01}
{'loss': 1.9671, 'grad_norm': 15.821662902832031, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.01}
{'loss': 1.7426, 'grad_norm': 9.595521926879883, 'learning_rate': 7e-05, 'epoch': 0.01}
{'loss': 1.7283, 'grad_norm': 13.047167778015137, 'learning_rate': 7.2e-05, 'epoch': 0.01}
{'loss': 1.6043, 'grad_norm': 14.239766120910645, 'learning_rate': 7.4e-05, 'epoch': 0.01}
{'loss': 1.6916, 'grad_norm': 16.12961196899414, 'learning_rate': 7.6e-05, 'epoch': 0.01}
{'loss': 1.527, 'grad_norm': 12.900609016418457, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.01}
{'loss': 1.5774, 'grad_norm': 19.13177490234375, 'learning_rate': 8e-05, 'epoch': 0.01}
{'loss': 1.4607, 'grad_norm': 10.547628402709961, 'learning_rate': 8.2e-05, 'epoch': 0.01}
{'loss': 1.215, 'grad_norm': 18.199481964111328, 'learning_rate': 8.4e-05, 'epoch': 0.01}
{'loss': 1.3497, 'grad_norm': 11.014484405517578, 'learning_rate': 8.6e-05, 'epoch': 0.01}
{'loss': 1.2661, 'grad_norm': 7.128716945648193, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.01}
{'loss': 1.2419, 'grad_norm': 6.545036315917969, 'learning_rate': 9e-05, 'epoch': 0.01}
{'loss': 1.2514, 'grad_norm': 9.345559120178223, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.01}
{'loss': 1.1455, 'grad_norm': 7.9706573486328125, 'learning_rate': 9.4e-05, 'epoch': 0.01}
{'loss': 1.2531, 'grad_norm': 13.870245933532715, 'learning_rate': 9.6e-05, 'epoch': 0.01}
{'loss': 1.0559, 'grad_norm': 7.433383941650391, 'learning_rate': 9.8e-05, 'epoch': 0.01}
{'loss': 1.1225, 'grad_norm': 7.4882025718688965, 'learning_rate': 0.0001, 'epoch': 0.01}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9939, 'grad_norm': 7.319204330444336, 'learning_rate': 0.00010200000000000001, 'epoch': 0.01}
{'loss': 1.2731, 'grad_norm': 8.173110961914062, 'learning_rate': 0.00010400000000000001, 'epoch': 0.01}
{'loss': 1.3409, 'grad_norm': 11.782841682434082, 'learning_rate': 0.00010600000000000002, 'epoch': 0.01}
{'loss': 0.9856, 'grad_norm': 3.9870846271514893, 'learning_rate': 0.00010800000000000001, 'epoch': 0.01}
{'loss': 1.4172, 'grad_norm': 10.721949577331543, 'learning_rate': 0.00011000000000000002, 'epoch': 0.01}
{'loss': 1.1372, 'grad_norm': 6.1739702224731445, 'learning_rate': 0.00011200000000000001, 'epoch': 0.01}
{'loss': 1.1235, 'grad_norm': 16.832887649536133, 'learning_rate': 0.00011399999999999999, 'epoch': 0.01}
{'loss': 1.0195, 'grad_norm': 4.225646018981934, 'learning_rate': 0.000116, 'epoch': 0.01}
{'loss': 1.0302, 'grad_norm': 6.188193321228027, 'learning_rate': 0.000118, 'epoch': 0.01}
{'loss': 1.2236, 'grad_norm': 9.238598823547363, 'learning_rate': 0.00012, 'epoch': 0.01}
{'loss': 1.0278, 'grad_norm': 5.806750297546387, 'learning_rate': 0.000122, 'epoch': 0.01}
{'loss': 0.9626, 'grad_norm': 3.2453811168670654, 'learning_rate': 0.000124, 'epoch': 0.01}
{'loss': 1.108, 'grad_norm': 5.470158100128174, 'learning_rate': 0.000126, 'epoch': 0.01}
{'loss': 0.772, 'grad_norm': 5.079631805419922, 'learning_rate': 0.00012800000000000002, 'epoch': 0.01}
{'loss': 0.7816, 'grad_norm': 8.08456802368164, 'learning_rate': 0.00013000000000000002, 'epoch': 0.01}
{'loss': 1.3639, 'grad_norm': 3.9692869186401367, 'learning_rate': 0.000132, 'epoch': 0.01}
{'loss': 1.0923, 'grad_norm': 4.805646896362305, 'learning_rate': 0.000134, 'epoch': 0.01}
{'loss': 1.3514, 'grad_norm': 10.331961631774902, 'learning_rate': 0.00013600000000000003, 'epoch': 0.01}
{'loss': 1.1554, 'grad_norm': 6.437063694000244, 'learning_rate': 0.000138, 'epoch': 0.01}
{'loss': 0.9409, 'grad_norm': 4.918188095092773, 'learning_rate': 0.00014, 'epoch': 0.01}
{'loss': 1.0393, 'grad_norm': 6.398371696472168, 'learning_rate': 0.000142, 'epoch': 0.01}
{'loss': 1.0395, 'grad_norm': 3.983691930770874, 'learning_rate': 0.000144, 'epoch': 0.01}
{'loss': 0.866, 'grad_norm': 2.139436721801758, 'learning_rate': 0.000146, 'epoch': 0.01}
{'loss': 1.306, 'grad_norm': 2.220828056335449, 'learning_rate': 0.000148, 'epoch': 0.01}
{'loss': 1.0188, 'grad_norm': 3.00917911529541, 'learning_rate': 0.00015000000000000001, 'epoch': 0.01}
{'loss': 0.7057, 'grad_norm': 3.417555809020996, 'learning_rate': 0.000152, 'epoch': 0.01}
{'loss': 1.2879, 'grad_norm': 6.506072044372559, 'learning_rate': 0.000154, 'epoch': 0.01}
{'loss': 1.1094, 'grad_norm': 4.781091213226318, 'learning_rate': 0.00015600000000000002, 'epoch': 0.01}
{'loss': 1.1069, 'grad_norm': 2.1367814540863037, 'learning_rate': 0.00015800000000000002, 'epoch': 0.01}
{'loss': 0.827, 'grad_norm': 4.627277851104736, 'learning_rate': 0.00016, 'epoch': 0.01}
{'loss': 0.8538, 'grad_norm': 3.5126402378082275, 'learning_rate': 0.000162, 'epoch': 0.01}
{'loss': 0.9627, 'grad_norm': 2.3389875888824463, 'learning_rate': 0.000164, 'epoch': 0.01}
{'loss': 1.165, 'grad_norm': 2.7904632091522217, 'learning_rate': 0.000166, 'epoch': 0.01}
{'loss': 1.3307, 'grad_norm': 3.917541265487671, 'learning_rate': 0.000168, 'epoch': 0.01}
{'loss': 1.1277, 'grad_norm': 3.367797613143921, 'learning_rate': 0.00017, 'epoch': 0.01}
{'loss': 1.0443, 'grad_norm': 3.196249008178711, 'learning_rate': 0.000172, 'epoch': 0.01}
{'loss': 1.172, 'grad_norm': 3.6042418479919434, 'learning_rate': 0.000174, 'epoch': 0.01}
{'loss': 0.9365, 'grad_norm': 5.6925506591796875, 'learning_rate': 0.00017600000000000002, 'epoch': 0.01}
{'loss': 0.7635, 'grad_norm': 4.485036373138428, 'learning_rate': 0.00017800000000000002, 'epoch': 0.01}
{'loss': 1.1185, 'grad_norm': 2.5997323989868164, 'learning_rate': 0.00018, 'epoch': 0.01}
{'loss': 0.8824, 'grad_norm': 2.7490596771240234, 'learning_rate': 0.000182, 'epoch': 0.01}
{'loss': 1.4158, 'grad_norm': 2.5031065940856934, 'learning_rate': 0.00018400000000000003, 'epoch': 0.01}
{'loss': 1.3157, 'grad_norm': 2.229755401611328, 'learning_rate': 0.00018600000000000002, 'epoch': 0.01}
{'loss': 0.9038, 'grad_norm': 6.136600017547607, 'learning_rate': 0.000188, 'epoch': 0.01}
{'loss': 0.8785, 'grad_norm': 2.103980541229248, 'learning_rate': 0.00019, 'epoch': 0.01}
{'loss': 0.8942, 'grad_norm': 2.5410537719726562, 'learning_rate': 0.000192, 'epoch': 0.01}
{'loss': 1.0735, 'grad_norm': 2.6304454803466797, 'learning_rate': 0.000194, 'epoch': 0.01}
{'loss': 1.1452, 'grad_norm': 5.16521692276001, 'learning_rate': 0.000196, 'epoch': 0.02}
{'loss': 0.9617, 'grad_norm': 5.625481128692627, 'learning_rate': 0.00019800000000000002, 'epoch': 0.02}
{'loss': 0.9065, 'grad_norm': 3.2933266162872314, 'learning_rate': 0.0002, 'epoch': 0.02}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.624, 'grad_norm': 10.378495216369629, 'learning_rate': 0.00019999939076577905, 'epoch': 0.02}
{'loss': 0.8826, 'grad_norm': 3.4779694080352783, 'learning_rate': 0.00019999756307053948, 'epoch': 0.02}
{'loss': 1.1737, 'grad_norm': 3.7999086380004883, 'learning_rate': 0.00019999451693655123, 'epoch': 0.02}
{'loss': 0.8835, 'grad_norm': 4.272115230560303, 'learning_rate': 0.00019999025240093044, 'epoch': 0.02}
{'loss': 1.0294, 'grad_norm': 2.2126495838165283, 'learning_rate': 0.00019998476951563915, 'epoch': 0.02}
{'loss': 1.0882, 'grad_norm': 3.029081106185913, 'learning_rate': 0.00019997806834748456, 'epoch': 0.02}
{'loss': 1.172, 'grad_norm': 2.635606050491333, 'learning_rate': 0.00019997014897811833, 'epoch': 0.02}
{'loss': 0.9414, 'grad_norm': 3.2828383445739746, 'learning_rate': 0.00019996101150403543, 'epoch': 0.02}
{'loss': 0.9499, 'grad_norm': 1.6906596422195435, 'learning_rate': 0.00019995065603657316, 'epoch': 0.02}
{'loss': 0.9009, 'grad_norm': 3.4072682857513428, 'learning_rate': 0.0001999390827019096, 'epoch': 0.02}
{'loss': 1.1114, 'grad_norm': 1.7944320440292358, 'learning_rate': 0.0001999262916410621, 'epoch': 0.02}
{'loss': 1.0874, 'grad_norm': 4.206650733947754, 'learning_rate': 0.00019991228300988585, 'epoch': 0.02}
{'loss': 1.0818, 'grad_norm': 2.833491802215576, 'learning_rate': 0.00019989705697907149, 'epoch': 0.02}
{'loss': 0.7895, 'grad_norm': 3.1116340160369873, 'learning_rate': 0.0001998806137341434, 'epoch': 0.02}
{'loss': 1.1218, 'grad_norm': 3.129791021347046, 'learning_rate': 0.0001998629534754574, 'epoch': 0.02}
{'loss': 1.3098, 'grad_norm': 1.8363866806030273, 'learning_rate': 0.00019984407641819812, 'epoch': 0.02}
{'loss': 0.8772, 'grad_norm': 3.0354228019714355, 'learning_rate': 0.00019982398279237655, 'epoch': 0.02}
{'loss': 1.042, 'grad_norm': 1.8520632982254028, 'learning_rate': 0.00019980267284282717, 'epoch': 0.02}
{'loss': 1.1068, 'grad_norm': 2.5367414951324463, 'learning_rate': 0.000199780146829205, 'epoch': 0.02}
{'loss': 1.0118, 'grad_norm': 2.5330066680908203, 'learning_rate': 0.00019975640502598244, 'epoch': 0.02}
{'loss': 1.1621, 'grad_norm': 2.2075719833374023, 'learning_rate': 0.00019973144772244582, 'epoch': 0.02}
{'loss': 1.0923, 'grad_norm': 2.3936495780944824, 'learning_rate': 0.00019970527522269205, 'epoch': 0.02}
{'loss': 0.8847, 'grad_norm': 1.9519410133361816, 'learning_rate': 0.00019967788784562473, 'epoch': 0.02}
{'loss': 1.1941, 'grad_norm': 1.8466858863830566, 'learning_rate': 0.00019964928592495045, 'epoch': 0.02}
{'loss': 1.0152, 'grad_norm': 1.7789713144302368, 'learning_rate': 0.00019961946980917456, 'epoch': 0.02}
{'loss': 1.1041, 'grad_norm': 3.044635057449341, 'learning_rate': 0.00019958843986159704, 'epoch': 0.02}
{'loss': 1.1753, 'grad_norm': 1.8071787357330322, 'learning_rate': 0.00019955619646030802, 'epoch': 0.02}
{'loss': 1.0445, 'grad_norm': 2.113219976425171, 'learning_rate': 0.0001995227399981831, 'epoch': 0.02}
{'loss': 0.9498, 'grad_norm': 1.616010308265686, 'learning_rate': 0.00019948807088287883, 'epoch': 0.02}
{'loss': 1.0719, 'grad_norm': 1.9962615966796875, 'learning_rate': 0.00019945218953682734, 'epoch': 0.02}
{'loss': 0.929, 'grad_norm': 2.835890054702759, 'learning_rate': 0.00019941509639723155, 'epoch': 0.02}
{'loss': 1.1243, 'grad_norm': 1.8564974069595337, 'learning_rate': 0.00019937679191605963, 'epoch': 0.02}
{'loss': 0.9269, 'grad_norm': 2.5963902473449707, 'learning_rate': 0.00019933727656003963, 'epoch': 0.02}
{'loss': 1.1281, 'grad_norm': 2.6453628540039062, 'learning_rate': 0.0001992965508106537, 'epoch': 0.02}
{'loss': 0.8205, 'grad_norm': 1.6855051517486572, 'learning_rate': 0.00019925461516413223, 'epoch': 0.02}
{'loss': 0.874, 'grad_norm': 2.5159194469451904, 'learning_rate': 0.0001992114701314478, 'epoch': 0.02}
{'loss': 1.2395, 'grad_norm': 1.8374736309051514, 'learning_rate': 0.00019916711623830903, 'epoch': 0.02}
{'loss': 1.0152, 'grad_norm': 1.931969404220581, 'learning_rate': 0.00019912155402515417, 'epoch': 0.02}
{'loss': 1.0397, 'grad_norm': 3.1111433506011963, 'learning_rate': 0.00019907478404714436, 'epoch': 0.02}
{'loss': 1.0331, 'grad_norm': 1.736452341079712, 'learning_rate': 0.00019902680687415705, 'epoch': 0.02}
{'loss': 1.0632, 'grad_norm': 1.6830532550811768, 'learning_rate': 0.0001989776230907789, 'epoch': 0.02}
{'loss': 0.8586, 'grad_norm': 1.6021769046783447, 'learning_rate': 0.00019892723329629887, 'epoch': 0.02}
{'loss': 0.8263, 'grad_norm': 2.4488842487335205, 'learning_rate': 0.0001988756381047006, 'epoch': 0.02}
{'loss': 1.3511, 'grad_norm': 2.0698320865631104, 'learning_rate': 0.0001988228381446553, 'epoch': 0.02}
{'loss': 1.1982, 'grad_norm': 1.7536542415618896, 'learning_rate': 0.00019876883405951377, 'epoch': 0.02}
{'loss': 1.0384, 'grad_norm': 1.51880943775177, 'learning_rate': 0.0001987136265072988, 'epoch': 0.02}
{'loss': 0.9834, 'grad_norm': 2.708439350128174, 'learning_rate': 0.00019865721616069696, 'epoch': 0.02}
{'loss': 1.117, 'grad_norm': 1.319016933441162, 'learning_rate': 0.0001985996037070505, 'epoch': 0.02}
{'loss': 0.8029, 'grad_norm': 1.9483500719070435, 'learning_rate': 0.00019854078984834903, 'epoch': 0.02}
{'loss': 1.2974, 'grad_norm': 1.7732080221176147, 'learning_rate': 0.00019848077530122083, 'epoch': 0.02}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0454, 'grad_norm': 2.494025945663452, 'learning_rate': 0.0001984195607969242, 'epoch': 0.02}
{'loss': 1.0792, 'grad_norm': 2.208735942840576, 'learning_rate': 0.00019835714708133862, 'epoch': 0.02}
{'loss': 1.2213, 'grad_norm': 2.2642383575439453, 'learning_rate': 0.00019829353491495545, 'epoch': 0.02}
{'loss': 1.1869, 'grad_norm': 2.0446696281433105, 'learning_rate': 0.0001982287250728689, 'epoch': 0.02}
{'loss': 0.949, 'grad_norm': 1.6007330417633057, 'learning_rate': 0.00019816271834476642, 'epoch': 0.02}
{'loss': 0.9235, 'grad_norm': 2.7916321754455566, 'learning_rate': 0.00019809551553491916, 'epoch': 0.02}
{'loss': 0.8162, 'grad_norm': 1.9780879020690918, 'learning_rate': 0.00019802711746217218, 'epoch': 0.02}
{'loss': 0.8305, 'grad_norm': 1.745807409286499, 'learning_rate': 0.0001979575249599344, 'epoch': 0.02}
{'loss': 0.9431, 'grad_norm': 2.149367094039917, 'learning_rate': 0.0001978867388761685, 'epoch': 0.02}
{'loss': 0.9609, 'grad_norm': 3.8719899654388428, 'learning_rate': 0.00019781476007338058, 'epoch': 0.02}
{'loss': 1.2902, 'grad_norm': 10.852639198303223, 'learning_rate': 0.0001977415894286096, 'epoch': 0.02}
{'loss': 0.9624, 'grad_norm': 2.0827829837799072, 'learning_rate': 0.0001976672278334168, 'epoch': 0.02}
{'loss': 1.0537, 'grad_norm': 2.3757543563842773, 'learning_rate': 0.00019759167619387476, 'epoch': 0.03}
{'loss': 1.3258, 'grad_norm': 1.7839853763580322, 'learning_rate': 0.00019751493543055632, 'epoch': 0.03}
{'loss': 0.7149, 'grad_norm': 2.568187713623047, 'learning_rate': 0.00019743700647852354, 'epoch': 0.03}
{'loss': 1.4272, 'grad_norm': 3.0249881744384766, 'learning_rate': 0.00019735789028731604, 'epoch': 0.03}
{'loss': 0.875, 'grad_norm': 2.224848747253418, 'learning_rate': 0.00019727758782093967, 'epoch': 0.03}
{'loss': 0.8369, 'grad_norm': 2.548943042755127, 'learning_rate': 0.00019719610005785465, 'epoch': 0.03}
{'loss': 1.1769, 'grad_norm': 1.9275528192520142, 'learning_rate': 0.00019711342799096361, 'epoch': 0.03}
{'loss': 1.3662, 'grad_norm': 2.496246814727783, 'learning_rate': 0.00019702957262759965, 'epoch': 0.03}
{'loss': 1.2427, 'grad_norm': 1.25537109375, 'learning_rate': 0.0001969445349895139, 'epoch': 0.03}
{'loss': 1.3532, 'grad_norm': 2.108187198638916, 'learning_rate': 0.0001968583161128631, 'epoch': 0.03}
{'loss': 1.1201, 'grad_norm': 1.4376120567321777, 'learning_rate': 0.00019677091704819715, 'epoch': 0.03}
{'loss': 1.0862, 'grad_norm': 3.5045037269592285, 'learning_rate': 0.00019668233886044597, 'epoch': 0.03}
{'loss': 0.6397, 'grad_norm': 1.8696306943893433, 'learning_rate': 0.00019659258262890683, 'epoch': 0.03}
{'loss': 1.1617, 'grad_norm': 1.6148029565811157, 'learning_rate': 0.00019650164944723115, 'epoch': 0.03}
{'loss': 1.1055, 'grad_norm': 2.6025230884552, 'learning_rate': 0.00019640954042341103, 'epoch': 0.03}
{'loss': 1.2516, 'grad_norm': 2.190619707107544, 'learning_rate': 0.00019631625667976583, 'epoch': 0.03}
{'loss': 0.9601, 'grad_norm': 2.3855512142181396, 'learning_rate': 0.00019622179935292855, 'epoch': 0.03}
{'loss': 1.0541, 'grad_norm': 2.142948865890503, 'learning_rate': 0.0001961261695938319, 'epoch': 0.03}
{'loss': 1.2317, 'grad_norm': 2.4248390197753906, 'learning_rate': 0.0001960293685676943, 'epoch': 0.03}
{'loss': 1.1234, 'grad_norm': 1.9522459506988525, 'learning_rate': 0.00019593139745400576, 'epoch': 0.03}
{'loss': 1.2229, 'grad_norm': 2.98726487159729, 'learning_rate': 0.00019583225744651333, 'epoch': 0.03}
{'loss': 1.0973, 'grad_norm': 1.6246086359024048, 'learning_rate': 0.00019573194975320673, 'epoch': 0.03}
{'loss': 1.4118, 'grad_norm': 1.7074573040008545, 'learning_rate': 0.00019563047559630357, 'epoch': 0.03}
{'loss': 1.1431, 'grad_norm': 1.3565336465835571, 'learning_rate': 0.00019552783621223436, 'epoch': 0.03}
{'loss': 0.7634, 'grad_norm': 1.7348833084106445, 'learning_rate': 0.0001954240328516277, 'epoch': 0.03}
{'loss': 1.0043, 'grad_norm': 1.6874006986618042, 'learning_rate': 0.0001953190667792947, 'epoch': 0.03}
{'loss': 1.2266, 'grad_norm': 1.4053459167480469, 'learning_rate': 0.00019521293927421388, 'epoch': 0.03}
{'loss': 0.985, 'grad_norm': 1.6474568843841553, 'learning_rate': 0.00019510565162951537, 'epoch': 0.03}
{'loss': 1.3013, 'grad_norm': 1.6176893711090088, 'learning_rate': 0.00019499720515246525, 'epoch': 0.03}
{'loss': 1.0875, 'grad_norm': 2.190934896469116, 'learning_rate': 0.00019488760116444966, 'epoch': 0.03}
{'loss': 1.0488, 'grad_norm': 1.6541539430618286, 'learning_rate': 0.0001947768410009586, 'epoch': 0.03}
{'loss': 0.938, 'grad_norm': 1.447292447090149, 'learning_rate': 0.00019466492601156966, 'epoch': 0.03}
{'loss': 1.1462, 'grad_norm': 1.6345462799072266, 'learning_rate': 0.0001945518575599317, 'epoch': 0.03}
{'loss': 1.1931, 'grad_norm': 1.2804937362670898, 'learning_rate': 0.00019443763702374812, 'epoch': 0.03}
{'loss': 1.0627, 'grad_norm': 1.9492806196212769, 'learning_rate': 0.0001943222657947601, 'epoch': 0.03}
{'loss': 1.1362, 'grad_norm': 2.3394899368286133, 'learning_rate': 0.00019420574527872968, 'epoch': 0.03}
{'loss': 1.0515, 'grad_norm': 2.3304498195648193, 'learning_rate': 0.00019408807689542257, 'epoch': 0.03}
{'loss': 1.0525, 'grad_norm': 1.4824594259262085, 'learning_rate': 0.00019396926207859084, 'epoch': 0.03}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2697, 'grad_norm': 1.705695629119873, 'learning_rate': 0.0001938493022759556, 'epoch': 0.03}
{'loss': 1.2623, 'grad_norm': 1.7406128644943237, 'learning_rate': 0.00019372819894918915, 'epoch': 0.03}
{'loss': 1.2221, 'grad_norm': 2.5139122009277344, 'learning_rate': 0.00019360595357389735, 'epoch': 0.03}
{'loss': 1.1148, 'grad_norm': 1.6577575206756592, 'learning_rate': 0.00019348256763960145, 'epoch': 0.03}
{'loss': 0.8862, 'grad_norm': 1.4568604230880737, 'learning_rate': 0.00019335804264972018, 'epoch': 0.03}
{'loss': 0.9563, 'grad_norm': 1.8776851892471313, 'learning_rate': 0.00019323238012155123, 'epoch': 0.03}
{'loss': 1.1305, 'grad_norm': 1.5532695055007935, 'learning_rate': 0.00019310558158625285, 'epoch': 0.03}
{'loss': 0.8386, 'grad_norm': 1.234458088874817, 'learning_rate': 0.00019297764858882514, 'epoch': 0.03}
{'loss': 1.0454, 'grad_norm': 2.4381773471832275, 'learning_rate': 0.00019284858268809137, 'epoch': 0.03}
{'loss': 0.9679, 'grad_norm': 1.746780514717102, 'learning_rate': 0.00019271838545667876, 'epoch': 0.03}
{'loss': 1.1485, 'grad_norm': 1.2878092527389526, 'learning_rate': 0.0001925870584809995, 'epoch': 0.03}
{'loss': 1.1874, 'grad_norm': 1.3721867799758911, 'learning_rate': 0.00019245460336123134, 'epoch': 0.03}
{'loss': 0.8537, 'grad_norm': 1.6284668445587158, 'learning_rate': 0.00019232102171129811, 'epoch': 0.03}
{'loss': 0.9258, 'grad_norm': 1.7024303674697876, 'learning_rate': 0.00019218631515885006, 'epoch': 0.03}
{'loss': 1.0861, 'grad_norm': 1.5344816446304321, 'learning_rate': 0.00019205048534524406, 'epoch': 0.03}
{'loss': 0.9373, 'grad_norm': 3.0205090045928955, 'learning_rate': 0.00019191353392552344, 'epoch': 0.03}
{'loss': 0.8337, 'grad_norm': 1.7276846170425415, 'learning_rate': 0.00019177546256839812, 'epoch': 0.03}
{'loss': 1.0258, 'grad_norm': 1.8464183807373047, 'learning_rate': 0.00019163627295622397, 'epoch': 0.03}
{'loss': 0.9396, 'grad_norm': 2.262805938720703, 'learning_rate': 0.0001914959667849825, 'epoch': 0.03}
{'loss': 0.8641, 'grad_norm': 1.2570462226867676, 'learning_rate': 0.0001913545457642601, 'epoch': 0.03}
{'loss': 1.1763, 'grad_norm': 1.5650533437728882, 'learning_rate': 0.0001912120116172273, 'epoch': 0.03}
{'loss': 0.923, 'grad_norm': 3.285475969314575, 'learning_rate': 0.00019106836608061772, 'epoch': 0.03}
{'loss': 0.9847, 'grad_norm': 2.8432624340057373, 'learning_rate': 0.00019092361090470688, 'epoch': 0.03}
{'loss': 1.1067, 'grad_norm': 1.7371970415115356, 'learning_rate': 0.00019077774785329087, 'epoch': 0.03}
{'loss': 0.9435, 'grad_norm': 2.5538973808288574, 'learning_rate': 0.000190630778703665, 'epoch': 0.03}
{'loss': 1.2099, 'grad_norm': 1.6142650842666626, 'learning_rate': 0.00019048270524660196, 'epoch': 0.03}
{'loss': 0.8823, 'grad_norm': 1.9549545049667358, 'learning_rate': 0.0001903335292863301, 'epoch': 0.03}
{'loss': 1.2985, 'grad_norm': 2.6861178874969482, 'learning_rate': 0.0001901832526405114, 'epoch': 0.04}
{'loss': 0.5996, 'grad_norm': 1.5954662561416626, 'learning_rate': 0.00019003187714021938, 'epoch': 0.04}
{'loss': 1.1202, 'grad_norm': 1.5617607831954956, 'learning_rate': 0.0001898794046299167, 'epoch': 0.04}
{'loss': 0.8336, 'grad_norm': 1.211726188659668, 'learning_rate': 0.00018972583696743285, 'epoch': 0.04}
{'loss': 1.0622, 'grad_norm': 2.298596143722534, 'learning_rate': 0.0001895711760239413, 'epoch': 0.04}
{'loss': 1.0966, 'grad_norm': 1.5816998481750488, 'learning_rate': 0.0001894154236839368, 'epoch': 0.04}
{'loss': 1.0772, 'grad_norm': 1.4042115211486816, 'learning_rate': 0.00018925858184521256, 'epoch': 0.04}
{'loss': 1.121, 'grad_norm': 1.6356760263442993, 'learning_rate': 0.0001891006524188368, 'epoch': 0.04}
{'loss': 1.2556, 'grad_norm': 1.516462802886963, 'learning_rate': 0.00018894163732912977, 'epoch': 0.04}
{'loss': 1.005, 'grad_norm': 2.6415553092956543, 'learning_rate': 0.00018878153851364013, 'epoch': 0.04}
{'loss': 1.1462, 'grad_norm': 1.803438425064087, 'learning_rate': 0.00018862035792312147, 'epoch': 0.04}
{'loss': 0.944, 'grad_norm': 2.5154693126678467, 'learning_rate': 0.0001884580975215084, 'epoch': 0.04}
{'loss': 1.088, 'grad_norm': 3.059285879135132, 'learning_rate': 0.00018829475928589271, 'epoch': 0.04}
{'loss': 0.8773, 'grad_norm': 1.6157985925674438, 'learning_rate': 0.0001881303452064992, 'epoch': 0.04}
{'loss': 1.0471, 'grad_norm': 1.3447016477584839, 'learning_rate': 0.00018796485728666165, 'epoch': 0.04}
{'loss': 0.9415, 'grad_norm': 1.8137413263320923, 'learning_rate': 0.00018779829754279805, 'epoch': 0.04}
{'loss': 1.0734, 'grad_norm': 1.5712027549743652, 'learning_rate': 0.00018763066800438636, 'epoch': 0.04}
{'loss': 0.9339, 'grad_norm': 2.4624290466308594, 'learning_rate': 0.00018746197071393958, 'epoch': 0.04}
{'loss': 0.9785, 'grad_norm': 2.2169790267944336, 'learning_rate': 0.00018729220772698097, 'epoch': 0.04}
{'loss': 1.1287, 'grad_norm': 2.676736354827881, 'learning_rate': 0.00018712138111201895, 'epoch': 0.04}
{'loss': 0.9137, 'grad_norm': 1.8683464527130127, 'learning_rate': 0.0001869494929505219, 'epoch': 0.04}
{'loss': 1.1675, 'grad_norm': 1.385538101196289, 'learning_rate': 0.00018677654533689287, 'epoch': 0.04}
{'loss': 1.1665, 'grad_norm': 1.5217955112457275, 'learning_rate': 0.00018660254037844388, 'epoch': 0.04}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1549, 'grad_norm': 1.5714198350906372, 'learning_rate': 0.0001864274801953705, 'epoch': 0.04}
{'loss': 1.0083, 'grad_norm': 2.5094289779663086, 'learning_rate': 0.00018625136692072575, 'epoch': 0.04}
{'loss': 1.0787, 'grad_norm': 1.6225827932357788, 'learning_rate': 0.0001860742027003944, 'epoch': 0.04}
{'loss': 1.0899, 'grad_norm': 2.05550479888916, 'learning_rate': 0.00018589598969306645, 'epoch': 0.04}
{'loss': 1.0114, 'grad_norm': 2.6861698627471924, 'learning_rate': 0.00018571673007021123, 'epoch': 0.04}
{'loss': 0.9477, 'grad_norm': 1.4068913459777832, 'learning_rate': 0.00018553642601605068, 'epoch': 0.04}
{'loss': 1.0783, 'grad_norm': 1.9087074995040894, 'learning_rate': 0.00018535507972753274, 'epoch': 0.04}
{'loss': 0.8684, 'grad_norm': 1.3959362506866455, 'learning_rate': 0.00018517269341430476, 'epoch': 0.04}
{'loss': 1.0248, 'grad_norm': 1.9457381963729858, 'learning_rate': 0.00018498926929868642, 'epoch': 0.04}
{'loss': 0.9817, 'grad_norm': 1.7680115699768066, 'learning_rate': 0.0001848048096156426, 'epoch': 0.04}
{'loss': 0.9094, 'grad_norm': 1.367344617843628, 'learning_rate': 0.00018461931661275643, 'epoch': 0.04}
{'loss': 0.7835, 'grad_norm': 1.5890529155731201, 'learning_rate': 0.00018443279255020152, 'epoch': 0.04}
{'loss': 1.156, 'grad_norm': 1.6720467805862427, 'learning_rate': 0.00018424523970071477, 'epoch': 0.04}
{'loss': 1.321, 'grad_norm': 1.6485193967819214, 'learning_rate': 0.00018405666034956844, 'epoch': 0.04}
{'loss': 1.0287, 'grad_norm': 2.5889954566955566, 'learning_rate': 0.00018386705679454242, 'epoch': 0.04}
{'loss': 1.0189, 'grad_norm': 2.8157618045806885, 'learning_rate': 0.00018367643134589617, 'epoch': 0.04}
{'loss': 1.2691, 'grad_norm': 2.9294989109039307, 'learning_rate': 0.00018348478632634066, 'epoch': 0.04}
{'loss': 1.2762, 'grad_norm': 1.2611345052719116, 'learning_rate': 0.00018329212407100994, 'epoch': 0.04}
{'loss': 1.1983, 'grad_norm': 2.0526578426361084, 'learning_rate': 0.00018309844692743283, 'epoch': 0.04}
{'loss': 1.131, 'grad_norm': 1.6767561435699463, 'learning_rate': 0.00018290375725550417, 'epoch': 0.04}
{'loss': 1.0427, 'grad_norm': 2.713543176651001, 'learning_rate': 0.00018270805742745617, 'epoch': 0.04}
{'loss': 1.2587, 'grad_norm': 1.9481064081192017, 'learning_rate': 0.00018251134982782952, 'epoch': 0.04}
{'loss': 1.3382, 'grad_norm': 1.6935287714004517, 'learning_rate': 0.0001823136368534442, 'epoch': 0.04}
{'loss': 0.9261, 'grad_norm': 1.7960838079452515, 'learning_rate': 0.00018211492091337042, 'epoch': 0.04}
{'loss': 1.0794, 'grad_norm': 1.6694248914718628, 'learning_rate': 0.0001819152044288992, 'epoch': 0.04}
{'loss': 1.3372, 'grad_norm': 58.41731262207031, 'learning_rate': 0.00018171448983351284, 'epoch': 0.04}
{'loss': 0.9163, 'grad_norm': 1.817958950996399, 'learning_rate': 0.00018151277957285543, 'epoch': 0.04}
{'loss': 0.6998, 'grad_norm': 1.5955791473388672, 'learning_rate': 0.00018131007610470276, 'epoch': 0.04}
{'loss': 0.9165, 'grad_norm': 1.579240083694458, 'learning_rate': 0.00018110638189893267, 'epoch': 0.04}
{'loss': 0.8101, 'grad_norm': 1.920757532119751, 'learning_rate': 0.00018090169943749476, 'epoch': 0.04}
{'loss': 1.1722, 'grad_norm': 2.0794625282287598, 'learning_rate': 0.00018069603121438022, 'epoch': 0.04}
{'loss': 0.802, 'grad_norm': 1.227332592010498, 'learning_rate': 0.0001804893797355914, 'epoch': 0.04}
{'loss': 1.0319, 'grad_norm': 1.9224636554718018, 'learning_rate': 0.00018028174751911146, 'epoch': 0.04}
{'loss': 0.9497, 'grad_norm': 1.8205664157867432, 'learning_rate': 0.00018007313709487334, 'epoch': 0.04}
{'loss': 0.9919, 'grad_norm': 3.9512407779693604, 'learning_rate': 0.00017986355100472928, 'epoch': 0.04}
{'loss': 1.1079, 'grad_norm': 1.3162572383880615, 'learning_rate': 0.00017965299180241963, 'epoch': 0.04}
{'loss': 0.9792, 'grad_norm': 1.4835344552993774, 'learning_rate': 0.00017944146205354182, 'epoch': 0.04}
{'loss': 0.9896, 'grad_norm': 1.3256880044937134, 'learning_rate': 0.00017922896433551907, 'epoch': 0.04}
{'loss': 1.23, 'grad_norm': 1.6493862867355347, 'learning_rate': 0.00017901550123756906, 'epoch': 0.04}
{'loss': 1.0872, 'grad_norm': 2.5037147998809814, 'learning_rate': 0.00017880107536067218, 'epoch': 0.04}
{'loss': 1.206, 'grad_norm': 1.7217190265655518, 'learning_rate': 0.0001785856893175402, 'epoch': 0.04}
{'loss': 1.099, 'grad_norm': 2.1520934104919434, 'learning_rate': 0.000178369345732584, 'epoch': 0.04}
{'loss': 0.9858, 'grad_norm': 1.3903250694274902, 'learning_rate': 0.00017815204724188187, 'epoch': 0.05}
{'loss': 1.0427, 'grad_norm': 1.9909472465515137, 'learning_rate': 0.00017793379649314744, 'epoch': 0.05}
{'loss': 0.9741, 'grad_norm': 1.4245860576629639, 'learning_rate': 0.0001777145961456971, 'epoch': 0.05}
{'loss': 1.2954, 'grad_norm': 2.6799044609069824, 'learning_rate': 0.00017749444887041799, 'epoch': 0.05}
{'loss': 1.0959, 'grad_norm': 1.6648132801055908, 'learning_rate': 0.00017727335734973512, 'epoch': 0.05}
{'loss': 1.1546, 'grad_norm': 2.135317087173462, 'learning_rate': 0.00017705132427757895, 'epoch': 0.05}
{'loss': 1.0892, 'grad_norm': 1.4815032482147217, 'learning_rate': 0.00017682835235935236, 'epoch': 0.05}
{'loss': 1.0748, 'grad_norm': 1.8517757654190063, 'learning_rate': 0.0001766044443118978, 'epoch': 0.05}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2032, 'grad_norm': 1.8978685140609741, 'learning_rate': 0.00017637960286346425, 'epoch': 0.05}
{'loss': 1.3829, 'grad_norm': 1.8516401052474976, 'learning_rate': 0.0001761538307536737, 'epoch': 0.05}
{'loss': 1.0602, 'grad_norm': 1.558038592338562, 'learning_rate': 0.00017592713073348807, 'epoch': 0.05}
{'loss': 0.8289, 'grad_norm': 1.4460119009017944, 'learning_rate': 0.00017569950556517566, 'epoch': 0.05}
{'loss': 0.9255, 'grad_norm': 1.4371588230133057, 'learning_rate': 0.00017547095802227723, 'epoch': 0.05}
{'loss': 1.1044, 'grad_norm': 1.4350427389144897, 'learning_rate': 0.00017524149088957245, 'epoch': 0.05}
{'loss': 0.948, 'grad_norm': 2.1882576942443848, 'learning_rate': 0.00017501110696304596, 'epoch': 0.05}
{'loss': 1.1755, 'grad_norm': 32.294281005859375, 'learning_rate': 0.0001747798090498532, 'epoch': 0.05}
{'loss': 0.899, 'grad_norm': 1.9782662391662598, 'learning_rate': 0.00017454759996828623, 'epoch': 0.05}
{'loss': 1.0439, 'grad_norm': 2.137211322784424, 'learning_rate': 0.00017431448254773944, 'epoch': 0.05}
{'loss': 1.2124, 'grad_norm': 1.4665164947509766, 'learning_rate': 0.000174080459628675, 'epoch': 0.05}
{'loss': 1.1554, 'grad_norm': 1.9006636142730713, 'learning_rate': 0.00017384553406258842, 'epoch': 0.05}
{'loss': 0.8409, 'grad_norm': 2.2672054767608643, 'learning_rate': 0.00017360970871197346, 'epoch': 0.05}
{'loss': 1.065, 'grad_norm': 1.3914810419082642, 'learning_rate': 0.00017337298645028764, 'epoch': 0.05}
{'loss': 1.0299, 'grad_norm': 1.6170779466629028, 'learning_rate': 0.00017313537016191706, 'epoch': 0.05}
{'loss': 1.0844, 'grad_norm': 1.6167305707931519, 'learning_rate': 0.00017289686274214118, 'epoch': 0.05}
{'loss': 0.9989, 'grad_norm': 1.7032264471054077, 'learning_rate': 0.0001726574670970976, 'epoch': 0.05}
{'loss': 1.165, 'grad_norm': 1.3117505311965942, 'learning_rate': 0.00017241718614374678, 'epoch': 0.05}
{'loss': 1.0182, 'grad_norm': 1.4831390380859375, 'learning_rate': 0.00017217602280983623, 'epoch': 0.05}
{'loss': 1.2547, 'grad_norm': 1.6144909858703613, 'learning_rate': 0.0001719339800338651, 'epoch': 0.05}
{'loss': 0.9271, 'grad_norm': 1.5006788969039917, 'learning_rate': 0.0001716910607650483, 'epoch': 0.05}
{'loss': 1.088, 'grad_norm': 1.4545420408248901, 'learning_rate': 0.00017144726796328034, 'epoch': 0.05}
{'loss': 1.2073, 'grad_norm': 1.944143295288086, 'learning_rate': 0.00017120260459909967, 'epoch': 0.05}
{'loss': 1.063, 'grad_norm': 1.650107502937317, 'learning_rate': 0.0001709570736536521, 'epoch': 0.05}
{'loss': 1.028, 'grad_norm': 1.4248805046081543, 'learning_rate': 0.00017071067811865476, 'epoch': 0.05}
{'loss': 0.9975, 'grad_norm': 5.26943826675415, 'learning_rate': 0.00017046342099635948, 'epoch': 0.05}
{'loss': 1.1111, 'grad_norm': 1.9025503396987915, 'learning_rate': 0.00017021530529951625, 'epoch': 0.05}
{'loss': 1.3725, 'grad_norm': 6.994993209838867, 'learning_rate': 0.00016996633405133655, 'epoch': 0.05}
{'loss': 0.9597, 'grad_norm': 1.6201038360595703, 'learning_rate': 0.00016971651028545648, 'epoch': 0.05}
{'loss': 1.0615, 'grad_norm': 1.8497105836868286, 'learning_rate': 0.00016946583704589973, 'epoch': 0.05}
{'loss': 1.3315, 'grad_norm': 3.609252691268921, 'learning_rate': 0.0001692143173870407, 'epoch': 0.05}
{'loss': 1.0361, 'grad_norm': 1.850328803062439, 'learning_rate': 0.000168961954373567, 'epoch': 0.05}
{'loss': 0.986, 'grad_norm': 1.9991285800933838, 'learning_rate': 0.0001687087510804423, 'epoch': 0.05}
{'loss': 0.9243, 'grad_norm': 1.5764394998550415, 'learning_rate': 0.00016845471059286887, 'epoch': 0.05}
{'loss': 0.9165, 'grad_norm': 2.205965995788574, 'learning_rate': 0.00016819983600624986, 'epoch': 0.05}
{'loss': 0.863, 'grad_norm': 2.178920269012451, 'learning_rate': 0.00016794413042615168, 'epoch': 0.05}
{'loss': 1.0391, 'grad_norm': 1.215433120727539, 'learning_rate': 0.00016768759696826608, 'epoch': 0.05}
{'loss': 1.2467, 'grad_norm': 1.5790956020355225, 'learning_rate': 0.00016743023875837233, 'epoch': 0.05}
{'loss': 0.9425, 'grad_norm': 2.041806221008301, 'learning_rate': 0.00016717205893229903, 'epoch': 0.05}
{'loss': 1.432, 'grad_norm': 1.6054879426956177, 'learning_rate': 0.00016691306063588583, 'epoch': 0.05}
{'loss': 1.0761, 'grad_norm': 2.4550392627716064, 'learning_rate': 0.00016665324702494524, 'epoch': 0.05}
{'loss': 0.9104, 'grad_norm': 1.2834562063217163, 'learning_rate': 0.00016639262126522418, 'epoch': 0.05}
{'loss': 0.8678, 'grad_norm': 1.765352725982666, 'learning_rate': 0.00016613118653236518, 'epoch': 0.05}
{'loss': 1.2603, 'grad_norm': 1.22605299949646, 'learning_rate': 0.00016586894601186805, 'epoch': 0.05}
{'loss': 1.0287, 'grad_norm': 1.8187143802642822, 'learning_rate': 0.00016560590289905073, 'epoch': 0.05}
{'loss': 1.2071, 'grad_norm': 1.494611144065857, 'learning_rate': 0.00016534206039901057, 'epoch': 0.05}
{'loss': 1.031, 'grad_norm': 2.500779390335083, 'learning_rate': 0.0001650774217265851, 'epoch': 0.05}
{'loss': 0.8717, 'grad_norm': 1.697754979133606, 'learning_rate': 0.0001648119901063131, 'epoch': 0.05}
{'loss': 1.0709, 'grad_norm': 1.5450464487075806, 'learning_rate': 0.00016454576877239507, 'epoch': 0.05}
{'loss': 0.9647, 'grad_norm': 1.3515571355819702, 'learning_rate': 0.00016427876096865394, 'epoch': 0.05}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9972, 'grad_norm': 2.254647970199585, 'learning_rate': 0.00016401096994849557, 'epoch': 0.05}
{'loss': 1.0072, 'grad_norm': 1.2143265008926392, 'learning_rate': 0.000163742398974869, 'epoch': 0.05}
{'loss': 1.1075, 'grad_norm': 1.9834504127502441, 'learning_rate': 0.00016347305132022677, 'epoch': 0.05}
{'loss': 1.2939, 'grad_norm': 1.9841302633285522, 'learning_rate': 0.0001632029302664851, 'epoch': 0.05}
{'loss': 1.0856, 'grad_norm': 1.2111955881118774, 'learning_rate': 0.00016293203910498376, 'epoch': 0.05}
{'loss': 1.033, 'grad_norm': 3.851386547088623, 'learning_rate': 0.00016266038113644607, 'epoch': 0.05}
{'loss': 1.1292, 'grad_norm': 1.3311231136322021, 'learning_rate': 0.00016238795967093864, 'epoch': 0.05}
{'loss': 1.3127, 'grad_norm': 1.9295170307159424, 'learning_rate': 0.00016211477802783103, 'epoch': 0.06}
{'loss': 1.3738, 'grad_norm': 2.8581490516662598, 'learning_rate': 0.0001618408395357554, 'epoch': 0.06}
{'loss': 1.2026, 'grad_norm': 1.7436988353729248, 'learning_rate': 0.0001615661475325658, 'epoch': 0.06}
{'loss': 0.8126, 'grad_norm': 1.7200268507003784, 'learning_rate': 0.00016129070536529766, 'epoch': 0.06}
{'loss': 0.7846, 'grad_norm': 2.0141329765319824, 'learning_rate': 0.0001610145163901268, 'epoch': 0.06}
{'loss': 1.1962, 'grad_norm': 1.5739662647247314, 'learning_rate': 0.00016073758397232868, 'epoch': 0.06}
{'loss': 1.1786, 'grad_norm': 1.5150139331817627, 'learning_rate': 0.0001604599114862375, 'epoch': 0.06}
{'loss': 1.2246, 'grad_norm': 1.4230318069458008, 'learning_rate': 0.00016018150231520486, 'epoch': 0.06}
{'loss': 1.2008, 'grad_norm': 1.4244929552078247, 'learning_rate': 0.0001599023598515586, 'epoch': 0.06}
{'loss': 0.9033, 'grad_norm': 1.7336056232452393, 'learning_rate': 0.0001596224874965616, 'epoch': 0.06}
{'loss': 0.9361, 'grad_norm': 1.343803882598877, 'learning_rate': 0.00015934188866037016, 'epoch': 0.06}
{'loss': 1.1438, 'grad_norm': 2.4675023555755615, 'learning_rate': 0.00015906056676199255, 'epoch': 0.06}
{'loss': 0.8632, 'grad_norm': 1.6959494352340698, 'learning_rate': 0.00015877852522924732, 'epoch': 0.06}
{'loss': 1.2555, 'grad_norm': 3.274556875228882, 'learning_rate': 0.00015849576749872157, 'epoch': 0.06}
{'loss': 1.3221, 'grad_norm': 1.748198390007019, 'learning_rate': 0.00015821229701572896, 'epoch': 0.06}
{'loss': 0.8405, 'grad_norm': 1.863791584968567, 'learning_rate': 0.0001579281172342679, 'epoch': 0.06}
{'loss': 1.1947, 'grad_norm': 1.4971526861190796, 'learning_rate': 0.00015764323161697935, 'epoch': 0.06}
{'loss': 1.0862, 'grad_norm': 2.1207878589630127, 'learning_rate': 0.0001573576436351046, 'epoch': 0.06}
{'loss': 0.9169, 'grad_norm': 1.8428503274917603, 'learning_rate': 0.0001570713567684432, 'epoch': 0.06}
{'loss': 0.8702, 'grad_norm': 1.1590688228607178, 'learning_rate': 0.00015678437450531013, 'epoch': 0.06}
{'loss': 0.9419, 'grad_norm': 1.5257703065872192, 'learning_rate': 0.0001564967003424938, 'epoch': 0.06}
{'loss': 1.4697, 'grad_norm': 1.4260010719299316, 'learning_rate': 0.00015620833778521307, 'epoch': 0.06}
{'loss': 1.1098, 'grad_norm': 5.62276029586792, 'learning_rate': 0.0001559192903470747, 'epoch': 0.06}
{'loss': 0.9699, 'grad_norm': 1.8980615139007568, 'learning_rate': 0.0001556295615500305, 'epoch': 0.06}
{'loss': 1.0214, 'grad_norm': 1.7475539445877075, 'learning_rate': 0.00015533915492433443, 'epoch': 0.06}
{'loss': 1.0327, 'grad_norm': 1.3826333284378052, 'learning_rate': 0.00015504807400849958, 'epoch': 0.06}
{'loss': 0.9613, 'grad_norm': 1.7315728664398193, 'learning_rate': 0.00015475632234925504, 'epoch': 0.06}
{'loss': 0.8126, 'grad_norm': 1.661173939704895, 'learning_rate': 0.00015446390350150273, 'epoch': 0.06}
{'loss': 0.9988, 'grad_norm': 1.5532922744750977, 'learning_rate': 0.000154170821028274, 'epoch': 0.06}
{'loss': 1.0345, 'grad_norm': 2.900963306427002, 'learning_rate': 0.0001538770785006863, 'epoch': 0.06}
{'loss': 0.8401, 'grad_norm': 1.3464365005493164, 'learning_rate': 0.00015358267949789966, 'epoch': 0.06}
{'loss': 0.9698, 'grad_norm': 1.3564544916152954, 'learning_rate': 0.000153287627607073, 'epoch': 0.06}
{'loss': 0.9498, 'grad_norm': 1.781101107597351, 'learning_rate': 0.0001529919264233205, 'epoch': 0.06}
{'loss': 1.0185, 'grad_norm': 1.6542836427688599, 'learning_rate': 0.00015269557954966778, 'epoch': 0.06}
{'loss': 1.0269, 'grad_norm': 1.3150138854980469, 'learning_rate': 0.00015239859059700794, 'epoch': 0.06}
{'loss': 1.1472, 'grad_norm': 1.2648570537567139, 'learning_rate': 0.00015210096318405767, 'epoch': 0.06}
{'loss': 1.3646, 'grad_norm': 1.5459959506988525, 'learning_rate': 0.00015180270093731303, 'epoch': 0.06}
{'loss': 0.917, 'grad_norm': 1.964820384979248, 'learning_rate': 0.00015150380749100545, 'epoch': 0.06}
{'loss': 1.1113, 'grad_norm': 1.4040906429290771, 'learning_rate': 0.00015120428648705717, 'epoch': 0.06}
{'loss': 0.8792, 'grad_norm': 1.3673354387283325, 'learning_rate': 0.00015090414157503714, 'epoch': 0.06}
{'loss': 0.9419, 'grad_norm': 1.7784712314605713, 'learning_rate': 0.00015060337641211637, 'epoch': 0.06}
{'loss': 1.2071, 'grad_norm': 2.247973918914795, 'learning_rate': 0.00015030199466302353, 'epoch': 0.06}
{'loss': 1.0664, 'grad_norm': 1.704643726348877, 'learning_rate': 0.00015000000000000001, 'epoch': 0.06}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0717, 'grad_norm': 1.3131392002105713, 'learning_rate': 0.00014969739610275556, 'epoch': 0.06}
{'loss': 1.0005, 'grad_norm': 1.7880486249923706, 'learning_rate': 0.0001493941866584231, 'epoch': 0.06}
{'loss': 0.8519, 'grad_norm': 1.5823981761932373, 'learning_rate': 0.00014909037536151409, 'epoch': 0.06}
{'loss': 1.2445, 'grad_norm': 1.8589566946029663, 'learning_rate': 0.0001487859659138733, 'epoch': 0.06}
{'loss': 0.9071, 'grad_norm': 2.026529550552368, 'learning_rate': 0.00014848096202463372, 'epoch': 0.06}
{'loss': 0.9226, 'grad_norm': 1.8937913179397583, 'learning_rate': 0.00014817536741017152, 'epoch': 0.06}
{'loss': 0.8828, 'grad_norm': 1.802884578704834, 'learning_rate': 0.0001478691857940607, 'epoch': 0.06}
{'loss': 1.0772, 'grad_norm': 1.5771042108535767, 'learning_rate': 0.00014756242090702756, 'epoch': 0.06}
{'loss': 0.9268, 'grad_norm': 1.4248199462890625, 'learning_rate': 0.00014725507648690543, 'epoch': 0.06}
{'loss': 1.2452, 'grad_norm': 1.4118378162384033, 'learning_rate': 0.00014694715627858908, 'epoch': 0.06}
{'loss': 0.8161, 'grad_norm': 1.5184366703033447, 'learning_rate': 0.00014663866403398913, 'epoch': 0.06}
{'loss': 0.8871, 'grad_norm': 1.5057417154312134, 'learning_rate': 0.00014632960351198618, 'epoch': 0.06}
{'loss': 1.4465, 'grad_norm': 1.4033215045928955, 'learning_rate': 0.00014601997847838518, 'epoch': 0.06}
{'loss': 0.752, 'grad_norm': 1.2305991649627686, 'learning_rate': 0.00014570979270586945, 'epoch': 0.06}
{'loss': 1.0794, 'grad_norm': 1.8694713115692139, 'learning_rate': 0.00014539904997395468, 'epoch': 0.06}
{'loss': 1.5344, 'grad_norm': 2.807382583618164, 'learning_rate': 0.00014508775406894307, 'epoch': 0.06}
{'loss': 1.0158, 'grad_norm': 1.9512163400650024, 'learning_rate': 0.00014477590878387696, 'epoch': 0.06}
{'loss': 0.877, 'grad_norm': 1.494410753250122, 'learning_rate': 0.00014446351791849276, 'epoch': 0.06}
{'loss': 0.8672, 'grad_norm': 1.2218669652938843, 'learning_rate': 0.00014415058527917452, 'epoch': 0.06}
{'loss': 1.0893, 'grad_norm': 1.4787089824676514, 'learning_rate': 0.00014383711467890774, 'epoch': 0.06}
{'loss': 1.1358, 'grad_norm': 1.251171350479126, 'learning_rate': 0.00014352310993723277, 'epoch': 0.06}
{'loss': 0.977, 'grad_norm': 1.3979532718658447, 'learning_rate': 0.00014320857488019824, 'epoch': 0.06}
{'loss': 1.0596, 'grad_norm': 1.4077138900756836, 'learning_rate': 0.0001428935133403146, 'epoch': 0.07}
{'loss': 1.0006, 'grad_norm': 1.487680196762085, 'learning_rate': 0.00014257792915650728, 'epoch': 0.07}
{'loss': 1.0233, 'grad_norm': 1.794657588005066, 'learning_rate': 0.00014226182617406996, 'epoch': 0.07}
{'loss': 0.9087, 'grad_norm': 1.5346897840499878, 'learning_rate': 0.00014194520824461771, 'epoch': 0.07}
{'loss': 0.8388, 'grad_norm': 1.45242440700531, 'learning_rate': 0.00014162807922604012, 'epoch': 0.07}
{'loss': 1.1117, 'grad_norm': 2.081449508666992, 'learning_rate': 0.0001413104429824542, 'epoch': 0.07}
{'loss': 1.5456, 'grad_norm': 2.055532932281494, 'learning_rate': 0.00014099230338415728, 'epoch': 0.07}
{'loss': 1.2037, 'grad_norm': 1.2752569913864136, 'learning_rate': 0.00014067366430758004, 'epoch': 0.07}
{'loss': 1.1499, 'grad_norm': 1.505993127822876, 'learning_rate': 0.00014035452963523902, 'epoch': 0.07}
{'loss': 1.0713, 'grad_norm': 1.7447104454040527, 'learning_rate': 0.00014003490325568954, 'epoch': 0.07}
{'loss': 0.9328, 'grad_norm': 2.4608123302459717, 'learning_rate': 0.00013971478906347806, 'epoch': 0.07}
{'loss': 1.1128, 'grad_norm': 1.3302911520004272, 'learning_rate': 0.00013939419095909512, 'epoch': 0.07}
{'loss': 1.0921, 'grad_norm': 1.5229880809783936, 'learning_rate': 0.00013907311284892736, 'epoch': 0.07}
{'loss': 0.9131, 'grad_norm': 1.8900902271270752, 'learning_rate': 0.0001387515586452103, 'epoch': 0.07}
{'loss': 1.0611, 'grad_norm': 1.251297950744629, 'learning_rate': 0.00013842953226598037, 'epoch': 0.07}
{'loss': 0.9869, 'grad_norm': 1.1322729587554932, 'learning_rate': 0.00013810703763502744, 'epoch': 0.07}
{'loss': 0.9399, 'grad_norm': 1.3473708629608154, 'learning_rate': 0.00013778407868184672, 'epoch': 0.07}
{'loss': 1.2762, 'grad_norm': 1.541083812713623, 'learning_rate': 0.00013746065934159123, 'epoch': 0.07}
{'loss': 1.1759, 'grad_norm': 1.354771375656128, 'learning_rate': 0.00013713678355502351, 'epoch': 0.07}
{'loss': 1.5282, 'grad_norm': 6.381112098693848, 'learning_rate': 0.00013681245526846783, 'epoch': 0.07}
{'loss': 0.9651, 'grad_norm': 1.8108474016189575, 'learning_rate': 0.00013648767843376196, 'epoch': 0.07}
{'loss': 1.0832, 'grad_norm': 1.4289720058441162, 'learning_rate': 0.00013616245700820922, 'epoch': 0.07}
{'loss': 1.4305, 'grad_norm': 2.2440977096557617, 'learning_rate': 0.00013583679495453, 'epoch': 0.07}
{'loss': 0.9397, 'grad_norm': 1.4569731950759888, 'learning_rate': 0.0001355106962408137, 'epoch': 0.07}
{'loss': 0.8167, 'grad_norm': 1.1164168119430542, 'learning_rate': 0.00013518416484047018, 'epoch': 0.07}
{'loss': 0.8246, 'grad_norm': 1.3772608041763306, 'learning_rate': 0.00013485720473218154, 'epoch': 0.07}
{'loss': 1.2416, 'grad_norm': 1.5045294761657715, 'learning_rate': 0.00013452981989985348, 'epoch': 0.07}
{'loss': 1.038, 'grad_norm': 1.3176333904266357, 'learning_rate': 0.00013420201433256689, 'epoch': 0.07}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2032, 'grad_norm': 1.2048368453979492, 'learning_rate': 0.00013387379202452917, 'epoch': 0.07}
{'loss': 0.8021, 'grad_norm': 2.4648303985595703, 'learning_rate': 0.00013354515697502553, 'epoch': 0.07}
{'loss': 1.0716, 'grad_norm': 1.7818678617477417, 'learning_rate': 0.00013321611318837032, 'epoch': 0.07}
{'loss': 0.9436, 'grad_norm': 1.6628907918930054, 'learning_rate': 0.00013288666467385833, 'epoch': 0.07}
{'loss': 1.0231, 'grad_norm': 1.28242826461792, 'learning_rate': 0.00013255681544571568, 'epoch': 0.07}
{'loss': 1.1937, 'grad_norm': 1.3828210830688477, 'learning_rate': 0.00013222656952305113, 'epoch': 0.07}
{'loss': 0.8626, 'grad_norm': 1.6308645009994507, 'learning_rate': 0.00013189593092980702, 'epoch': 0.07}
{'loss': 0.954, 'grad_norm': 1.1904256343841553, 'learning_rate': 0.00013156490369471027, 'epoch': 0.07}
{'loss': 0.7618, 'grad_norm': 1.0382797718048096, 'learning_rate': 0.00013123349185122327, 'epoch': 0.07}
{'loss': 0.8835, 'grad_norm': 3.6835556030273438, 'learning_rate': 0.00013090169943749476, 'epoch': 0.07}
{'loss': 0.8144, 'grad_norm': 1.868831753730774, 'learning_rate': 0.00013056953049631057, 'epoch': 0.07}
{'loss': 0.7546, 'grad_norm': 1.332947850227356, 'learning_rate': 0.00013023698907504446, 'epoch': 0.07}
{'loss': 1.0732, 'grad_norm': 1.4750202894210815, 'learning_rate': 0.00012990407922560868, 'epoch': 0.07}
{'loss': 1.1064, 'grad_norm': 1.2600778341293335, 'learning_rate': 0.00012957080500440468, 'epoch': 0.07}
{'loss': 1.2029, 'grad_norm': 1.2490025758743286, 'learning_rate': 0.00012923717047227368, 'epoch': 0.07}
{'loss': 1.0026, 'grad_norm': 1.7856284379959106, 'learning_rate': 0.00012890317969444716, 'epoch': 0.07}
{'loss': 0.9379, 'grad_norm': 1.1448631286621094, 'learning_rate': 0.00012856883674049736, 'epoch': 0.07}
{'loss': 0.9807, 'grad_norm': 1.3050943613052368, 'learning_rate': 0.00012823414568428768, 'epoch': 0.07}
{'loss': 1.1785, 'grad_norm': 1.5934914350509644, 'learning_rate': 0.00012789911060392294, 'epoch': 0.07}
{'loss': 0.7225, 'grad_norm': 1.8481721878051758, 'learning_rate': 0.0001275637355816999, 'epoch': 0.07}
{'loss': 0.971, 'grad_norm': 2.520782947540283, 'learning_rate': 0.00012722802470405744, 'epoch': 0.07}
{'loss': 1.0166, 'grad_norm': 1.300108790397644, 'learning_rate': 0.00012689198206152657, 'epoch': 0.07}
{'loss': 1.1368, 'grad_norm': 1.4008668661117554, 'learning_rate': 0.00012655561174868088, 'epoch': 0.07}
{'loss': 0.8966, 'grad_norm': 1.4255688190460205, 'learning_rate': 0.00012621891786408648, 'epoch': 0.07}
{'loss': 0.9199, 'grad_norm': 1.0256253480911255, 'learning_rate': 0.00012588190451025207, 'epoch': 0.07}
{'loss': 0.9666, 'grad_norm': 1.6062755584716797, 'learning_rate': 0.00012554457579357905, 'epoch': 0.07}
{'loss': 0.9367, 'grad_norm': 1.1767817735671997, 'learning_rate': 0.0001252069358243114, 'epoch': 0.07}
{'loss': 0.9666, 'grad_norm': 1.4013886451721191, 'learning_rate': 0.0001248689887164855, 'epoch': 0.07}
{'loss': 1.3675, 'grad_norm': 1.5140659809112549, 'learning_rate': 0.00012453073858788026, 'epoch': 0.07}
{'loss': 0.9851, 'grad_norm': 2.0699291229248047, 'learning_rate': 0.00012419218955996676, 'epoch': 0.07}
{'loss': 1.066, 'grad_norm': 1.200276255607605, 'learning_rate': 0.0001238533457578581, 'epoch': 0.07}
{'loss': 1.3015, 'grad_norm': 1.78734290599823, 'learning_rate': 0.000123514211310259, 'epoch': 0.07}
{'loss': 1.0941, 'grad_norm': 1.536349892616272, 'learning_rate': 0.00012317479034941573, 'epoch': 0.07}
{'loss': 0.7263, 'grad_norm': 1.5157045125961304, 'learning_rate': 0.00012283508701106557, 'epoch': 0.07}
{'loss': 1.0309, 'grad_norm': 1.4564510583877563, 'learning_rate': 0.0001224951054343865, 'epoch': 0.07}
{'loss': 1.0743, 'grad_norm': 1.176979422569275, 'learning_rate': 0.00012215484976194676, 'epoch': 0.07}
{'loss': 1.1427, 'grad_norm': 1.5912396907806396, 'learning_rate': 0.00012181432413965428, 'epoch': 0.07}
{'loss': 1.1214, 'grad_norm': 1.3072659969329834, 'learning_rate': 0.00012147353271670634, 'epoch': 0.08}
{'loss': 0.7643, 'grad_norm': 1.2931019067764282, 'learning_rate': 0.00012113247964553888, 'epoch': 0.08}
{'loss': 1.0868, 'grad_norm': 1.1821280717849731, 'learning_rate': 0.00012079116908177593, 'epoch': 0.08}
{'loss': 0.8651, 'grad_norm': 1.6590954065322876, 'learning_rate': 0.00012044960518417903, 'epoch': 0.08}
{'loss': 1.2335, 'grad_norm': 1.21429443359375, 'learning_rate': 0.00012010779211459648, 'epoch': 0.08}
{'loss': 1.0364, 'grad_norm': 2.0536141395568848, 'learning_rate': 0.00011976573403791262, 'epoch': 0.08}
{'loss': 1.0654, 'grad_norm': 1.1874048709869385, 'learning_rate': 0.0001194234351219972, 'epoch': 0.08}
{'loss': 0.8889, 'grad_norm': 1.2223622798919678, 'learning_rate': 0.00011908089953765449, 'epoch': 0.08}
{'loss': 0.9435, 'grad_norm': 1.4975932836532593, 'learning_rate': 0.00011873813145857249, 'epoch': 0.08}
{'loss': 1.1858, 'grad_norm': 2.2728993892669678, 'learning_rate': 0.00011839513506127203, 'epoch': 0.08}
{'loss': 1.1389, 'grad_norm': 1.3632550239562988, 'learning_rate': 0.00011805191452505602, 'epoch': 0.08}
{'loss': 1.2409, 'grad_norm': 1.688725233078003, 'learning_rate': 0.00011770847403195834, 'epoch': 0.08}
{'loss': 0.9965, 'grad_norm': 1.6035839319229126, 'learning_rate': 0.00011736481776669306, 'epoch': 0.08}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.6103, 'grad_norm': 7.693246364593506, 'learning_rate': 0.00011702094991660326, 'epoch': 0.08}
{'loss': 1.1719, 'grad_norm': 1.4661402702331543, 'learning_rate': 0.00011667687467161024, 'epoch': 0.08}
{'loss': 1.0976, 'grad_norm': 1.6435819864273071, 'learning_rate': 0.00011633259622416224, 'epoch': 0.08}
{'loss': 1.0771, 'grad_norm': 1.6230133771896362, 'learning_rate': 0.0001159881187691835, 'epoch': 0.08}
{'loss': 0.9452, 'grad_norm': 1.8426048755645752, 'learning_rate': 0.0001156434465040231, 'epoch': 0.08}
{'loss': 1.6046, 'grad_norm': 1.1250869035720825, 'learning_rate': 0.00011529858362840382, 'epoch': 0.08}
{'loss': 1.1097, 'grad_norm': 1.172540545463562, 'learning_rate': 0.00011495353434437098, 'epoch': 0.08}
{'loss': 1.0525, 'grad_norm': 1.2488536834716797, 'learning_rate': 0.00011460830285624118, 'epoch': 0.08}
{'loss': 0.9277, 'grad_norm': 1.3559259176254272, 'learning_rate': 0.00011426289337055119, 'epoch': 0.08}
{'loss': 0.9845, 'grad_norm': 1.626146674156189, 'learning_rate': 0.00011391731009600654, 'epoch': 0.08}
{'loss': 1.1508, 'grad_norm': 1.2907416820526123, 'learning_rate': 0.00011357155724343045, 'epoch': 0.08}
{'loss': 0.7488, 'grad_norm': 1.1530004739761353, 'learning_rate': 0.00011322563902571226, 'epoch': 0.08}
{'loss': 0.7519, 'grad_norm': 1.8151576519012451, 'learning_rate': 0.0001128795596577563, 'epoch': 0.08}
{'loss': 1.1753, 'grad_norm': 1.487994909286499, 'learning_rate': 0.00011253332335643043, 'epoch': 0.08}
{'loss': 0.9465, 'grad_norm': 2.1478748321533203, 'learning_rate': 0.00011218693434051475, 'epoch': 0.08}
{'loss': 1.0865, 'grad_norm': 1.368445873260498, 'learning_rate': 0.00011184039683065013, 'epoch': 0.08}
{'loss': 1.1198, 'grad_norm': 1.190157175064087, 'learning_rate': 0.00011149371504928668, 'epoch': 0.08}
{'loss': 0.9482, 'grad_norm': 1.1065969467163086, 'learning_rate': 0.00011114689322063255, 'epoch': 0.08}
{'loss': 1.1475, 'grad_norm': 1.2347438335418701, 'learning_rate': 0.0001107999355706023, 'epoch': 0.08}
{'loss': 0.8645, 'grad_norm': 1.2983462810516357, 'learning_rate': 0.00011045284632676536, 'epoch': 0.08}
{'loss': 1.237, 'grad_norm': 1.2734546661376953, 'learning_rate': 0.00011010562971829463, 'epoch': 0.08}
{'loss': 0.9609, 'grad_norm': 4.803282260894775, 'learning_rate': 0.00010975828997591495, 'epoch': 0.08}
{'loss': 0.9029, 'grad_norm': 1.506534218788147, 'learning_rate': 0.00010941083133185146, 'epoch': 0.08}
{'loss': 0.9031, 'grad_norm': 1.8940231800079346, 'learning_rate': 0.00010906325801977804, 'epoch': 0.08}
{'loss': 1.0969, 'grad_norm': 1.3610948324203491, 'learning_rate': 0.00010871557427476583, 'epoch': 0.08}
{'loss': 1.0255, 'grad_norm': 1.8789669275283813, 'learning_rate': 0.00010836778433323158, 'epoch': 0.08}
{'loss': 1.1334, 'grad_norm': 3.3445017337799072, 'learning_rate': 0.00010801989243288589, 'epoch': 0.08}
{'loss': 0.703, 'grad_norm': 1.3832570314407349, 'learning_rate': 0.00010767190281268187, 'epoch': 0.08}
{'loss': 1.0175, 'grad_norm': 1.573990821838379, 'learning_rate': 0.00010732381971276318, 'epoch': 0.08}
{'loss': 1.128, 'grad_norm': 1.4847724437713623, 'learning_rate': 0.00010697564737441252, 'epoch': 0.08}
{'loss': 0.9859, 'grad_norm': 1.599743127822876, 'learning_rate': 0.00010662739004000005, 'epoch': 0.08}
{'loss': 1.0275, 'grad_norm': 1.3263516426086426, 'learning_rate': 0.00010627905195293135, 'epoch': 0.08}
{'loss': 0.8288, 'grad_norm': 1.440066933631897, 'learning_rate': 0.00010593063735759618, 'epoch': 0.08}
{'loss': 0.9323, 'grad_norm': 2.0983011722564697, 'learning_rate': 0.00010558215049931638, 'epoch': 0.08}
{'loss': 1.2354, 'grad_norm': 1.924828052520752, 'learning_rate': 0.0001052335956242944, 'epoch': 0.08}
{'loss': 1.1053, 'grad_norm': 1.5655267238616943, 'learning_rate': 0.00010488497697956135, 'epoch': 0.08}
{'loss': 0.9491, 'grad_norm': 1.8753631114959717, 'learning_rate': 0.00010453629881292538, 'epoch': 0.08}
{'loss': 0.9025, 'grad_norm': 1.8827110528945923, 'learning_rate': 0.00010418756537291996, 'epoch': 0.08}
{'loss': 0.8688, 'grad_norm': 1.2216218709945679, 'learning_rate': 0.00010383878090875201, 'epoch': 0.08}
{'loss': 1.1831, 'grad_norm': 1.5508687496185303, 'learning_rate': 0.00010348994967025012, 'epoch': 0.08}
{'loss': 1.068, 'grad_norm': 1.7100257873535156, 'learning_rate': 0.00010314107590781284, 'epoch': 0.08}
{'loss': 1.0848, 'grad_norm': 1.2685192823410034, 'learning_rate': 0.0001027921638723569, 'epoch': 0.08}
{'loss': 1.2952, 'grad_norm': 1.6160534620285034, 'learning_rate': 0.00010244321781526533, 'epoch': 0.08}
{'loss': 0.7879, 'grad_norm': 1.741991400718689, 'learning_rate': 0.0001020942419883357, 'epoch': 0.08}
{'loss': 1.0028, 'grad_norm': 1.2319884300231934, 'learning_rate': 0.00010174524064372837, 'epoch': 0.08}
