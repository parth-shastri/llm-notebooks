/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 3.6079, 'grad_norm': 111.80207824707031, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
{'loss': 3.2496, 'grad_norm': 95.5153579711914, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}
{'loss': 3.2467, 'grad_norm': 49.70750045776367, 'learning_rate': 6e-06, 'epoch': 0.0}
{'loss': 4.1308, 'grad_norm': 48.23552322387695, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}
{'loss': 3.2967, 'grad_norm': 136.7300262451172, 'learning_rate': 1e-05, 'epoch': 0.0}
{'loss': 4.4645, 'grad_norm': 58.93779373168945, 'learning_rate': 1.2e-05, 'epoch': 0.0}
{'loss': 2.4382, 'grad_norm': 101.4372329711914, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.0}
{'loss': 4.1498, 'grad_norm': 56.03482437133789, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.0}
{'loss': 3.0713, 'grad_norm': 49.54450988769531, 'learning_rate': 1.8e-05, 'epoch': 0.0}
{'loss': 3.8883, 'grad_norm': 55.03109359741211, 'learning_rate': 2e-05, 'epoch': 0.0}
{'loss': 3.7949, 'grad_norm': 34.858036041259766, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.0}
{'loss': 3.4193, 'grad_norm': 33.72002410888672, 'learning_rate': 2.4e-05, 'epoch': 0.0}
{'loss': 3.683, 'grad_norm': 40.306182861328125, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.0}
{'loss': 3.2778, 'grad_norm': 27.225040435791016, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.0}
{'loss': 3.4986, 'grad_norm': 31.20496368408203, 'learning_rate': 3e-05, 'epoch': 0.0}
{'loss': 3.6425, 'grad_norm': 41.91122055053711, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.0}
{'loss': 3.6461, 'grad_norm': 35.16622543334961, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.0}
{'loss': 3.0449, 'grad_norm': 23.89719581604004, 'learning_rate': 3.6e-05, 'epoch': 0.0}
{'loss': 3.2439, 'grad_norm': 28.427934646606445, 'learning_rate': 3.8e-05, 'epoch': 0.0}
{'loss': 3.1696, 'grad_norm': 32.60505294799805, 'learning_rate': 4e-05, 'epoch': 0.0}
{'loss': 3.2977, 'grad_norm': 43.19951248168945, 'learning_rate': 4.2e-05, 'epoch': 0.0}
{'loss': 2.8501, 'grad_norm': 24.74259376525879, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.0}
trainable params: 277,923,840 || all params: 3,057,607,680 || trainable%: 9.089584704339831
None
{'loss': 3.6079, 'grad_norm': 111.80816650390625, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
{'loss': 3.2496, 'grad_norm': 95.51927185058594, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}
{'loss': 3.2397, 'grad_norm': 43.53374481201172, 'learning_rate': 6e-06, 'epoch': 0.0}
{'loss': 4.141, 'grad_norm': 63.276145935058594, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}
{'loss': 3.2886, 'grad_norm': 132.499267578125, 'learning_rate': 1e-05, 'epoch': 0.0}
{'loss': 4.4783, 'grad_norm': 118.49317169189453, 'learning_rate': 1.2e-05, 'epoch': 0.0}
{'loss': 2.4455, 'grad_norm': 52.1016960144043, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.0}
{'loss': 4.1255, 'grad_norm': 43.59172821044922, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.0}
{'loss': 3.0596, 'grad_norm': 33.83043670654297, 'learning_rate': 1.8e-05, 'epoch': 0.0}
{'loss': 3.8809, 'grad_norm': 113.40036010742188, 'learning_rate': 2e-05, 'epoch': 0.0}
{'loss': 3.8067, 'grad_norm': 33.21516418457031, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.0}
{'loss': 3.4172, 'grad_norm': 44.29437255859375, 'learning_rate': 2.4e-05, 'epoch': 0.0}
{'loss': 3.6903, 'grad_norm': 46.915809631347656, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.0}
{'loss': 3.2961, 'grad_norm': 26.623191833496094, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.0}
{'loss': 3.5126, 'grad_norm': 43.399349212646484, 'learning_rate': 3e-05, 'epoch': 0.0}
{'loss': 3.6617, 'grad_norm': 36.08379364013672, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.0}
{'loss': 3.6673, 'grad_norm': 34.23123550415039, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.0}
{'loss': 3.0549, 'grad_norm': 22.317338943481445, 'learning_rate': 3.6e-05, 'epoch': 0.0}
{'loss': 3.269, 'grad_norm': 29.20669174194336, 'learning_rate': 3.8e-05, 'epoch': 0.0}
{'loss': 3.18, 'grad_norm': 31.76542854309082, 'learning_rate': 4e-05, 'epoch': 0.0}
{'loss': 3.3199, 'grad_norm': 30.37599754333496, 'learning_rate': 4.2e-05, 'epoch': 0.0}
{'loss': 2.8835, 'grad_norm': 25.413728713989258, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.0}
{'loss': 2.2551, 'grad_norm': 17.49344825744629, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.0}
{'loss': 3.1192, 'grad_norm': 34.452545166015625, 'learning_rate': 4.8e-05, 'epoch': 0.0}
{'loss': 3.2037, 'grad_norm': 30.047897338867188, 'learning_rate': 5e-05, 'epoch': 0.0}
{'loss': 2.6147, 'grad_norm': 16.170242309570312, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.0}
{'loss': 2.8338, 'grad_norm': 20.869064331054688, 'learning_rate': 5.4000000000000005e-05, 'epoch': 0.0}
{'loss': 2.1989, 'grad_norm': 21.32379913330078, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.0}
{'loss': 2.6273, 'grad_norm': 25.856536865234375, 'learning_rate': 5.8e-05, 'epoch': 0.0}
{'loss': 2.3603, 'grad_norm': 20.326343536376953, 'learning_rate': 6e-05, 'epoch': 0.0}
{'loss': 2.4538, 'grad_norm': 14.603631019592285, 'learning_rate': 6.2e-05, 'epoch': 0.0}
{'loss': 2.2784, 'grad_norm': 22.504379272460938, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.0}
{'loss': 2.0882, 'grad_norm': 22.29538917541504, 'learning_rate': 6.6e-05, 'epoch': 0.01}
{'loss': 2.4028, 'grad_norm': 19.826921463012695, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.01}
{'loss': 2.0813, 'grad_norm': 13.667221069335938, 'learning_rate': 7e-05, 'epoch': 0.01}
{'loss': 2.0287, 'grad_norm': 14.569936752319336, 'learning_rate': 7.2e-05, 'epoch': 0.01}
{'loss': 1.884, 'grad_norm': 13.937232971191406, 'learning_rate': 7.4e-05, 'epoch': 0.01}
{'loss': 1.9271, 'grad_norm': 21.039243698120117, 'learning_rate': 7.6e-05, 'epoch': 0.01}
{'loss': 1.8206, 'grad_norm': 10.10046100616455, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.01}
{'loss': 1.8989, 'grad_norm': 20.66911506652832, 'learning_rate': 8e-05, 'epoch': 0.01}
{'loss': 1.7178, 'grad_norm': 12.556008338928223, 'learning_rate': 8.2e-05, 'epoch': 0.01}
{'loss': 1.4877, 'grad_norm': 12.634170532226562, 'learning_rate': 8.4e-05, 'epoch': 0.01}
{'loss': 1.476, 'grad_norm': 11.966015815734863, 'learning_rate': 8.6e-05, 'epoch': 0.01}
{'loss': 1.4235, 'grad_norm': 10.276918411254883, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.01}
{'loss': 1.3305, 'grad_norm': 8.95053768157959, 'learning_rate': 9e-05, 'epoch': 0.01}
{'loss': 1.4418, 'grad_norm': 10.167806625366211, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.01}
{'loss': 1.2362, 'grad_norm': 23.941192626953125, 'learning_rate': 9.4e-05, 'epoch': 0.01}
{'loss': 1.2634, 'grad_norm': 9.939820289611816, 'learning_rate': 9.6e-05, 'epoch': 0.01}
{'loss': 1.2011, 'grad_norm': 20.535396575927734, 'learning_rate': 9.8e-05, 'epoch': 0.01}
{'loss': 1.2886, 'grad_norm': 12.94841194152832, 'learning_rate': 0.0001, 'epoch': 0.01}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0592, 'grad_norm': 9.39468765258789, 'learning_rate': 0.00010200000000000001, 'epoch': 0.01}
{'loss': 1.2269, 'grad_norm': 12.071252822875977, 'learning_rate': 0.00010400000000000001, 'epoch': 0.01}
{'loss': 1.3183, 'grad_norm': 37.48760986328125, 'learning_rate': 0.00010600000000000002, 'epoch': 0.01}
{'loss': 0.9875, 'grad_norm': 5.317874908447266, 'learning_rate': 0.00010800000000000001, 'epoch': 0.01}
{'loss': 1.3991, 'grad_norm': 14.705487251281738, 'learning_rate': 0.00011000000000000002, 'epoch': 0.01}
{'loss': 1.1565, 'grad_norm': 6.660618782043457, 'learning_rate': 0.00011200000000000001, 'epoch': 0.01}
{'loss': 1.2007, 'grad_norm': 9.158034324645996, 'learning_rate': 0.00011399999999999999, 'epoch': 0.01}
{'loss': 1.0398, 'grad_norm': 5.196408271789551, 'learning_rate': 0.000116, 'epoch': 0.01}
{'loss': 1.0616, 'grad_norm': 6.330472946166992, 'learning_rate': 0.000118, 'epoch': 0.01}
{'loss': 1.2482, 'grad_norm': 9.415184020996094, 'learning_rate': 0.00012, 'epoch': 0.01}
{'loss': 1.0768, 'grad_norm': 7.1458048820495605, 'learning_rate': 0.000122, 'epoch': 0.01}
{'loss': 0.9598, 'grad_norm': 4.116945266723633, 'learning_rate': 0.000124, 'epoch': 0.01}
{'loss': 1.1397, 'grad_norm': 6.162032127380371, 'learning_rate': 0.000126, 'epoch': 0.01}
{'loss': 0.7701, 'grad_norm': 7.38576602935791, 'learning_rate': 0.00012800000000000002, 'epoch': 0.01}
{'loss': 0.8083, 'grad_norm': 6.162484645843506, 'learning_rate': 0.00013000000000000002, 'epoch': 0.01}
{'loss': 1.3589, 'grad_norm': 5.137710094451904, 'learning_rate': 0.000132, 'epoch': 0.01}
{'loss': 1.0884, 'grad_norm': 6.3188700675964355, 'learning_rate': 0.000134, 'epoch': 0.01}
{'loss': 1.1761, 'grad_norm': 7.320342063903809, 'learning_rate': 0.00013600000000000003, 'epoch': 0.01}
{'loss': 1.1539, 'grad_norm': 6.678949356079102, 'learning_rate': 0.000138, 'epoch': 0.01}
{'loss': 1.0488, 'grad_norm': 6.489222049713135, 'learning_rate': 0.00014, 'epoch': 0.01}
{'loss': 1.059, 'grad_norm': 8.017992973327637, 'learning_rate': 0.000142, 'epoch': 0.01}
{'loss': 1.0656, 'grad_norm': 7.0813984870910645, 'learning_rate': 0.000144, 'epoch': 0.01}
{'loss': 0.8762, 'grad_norm': 3.297234058380127, 'learning_rate': 0.000146, 'epoch': 0.01}
{'loss': 1.2937, 'grad_norm': 3.081303596496582, 'learning_rate': 0.000148, 'epoch': 0.01}
{'loss': 0.9938, 'grad_norm': 5.415705680847168, 'learning_rate': 0.00015000000000000001, 'epoch': 0.01}
{'loss': 0.701, 'grad_norm': 4.253382682800293, 'learning_rate': 0.000152, 'epoch': 0.01}
{'loss': 1.2485, 'grad_norm': 5.5853962898254395, 'learning_rate': 0.000154, 'epoch': 0.01}
{'loss': 1.073, 'grad_norm': 3.2626216411590576, 'learning_rate': 0.00015600000000000002, 'epoch': 0.01}
{'loss': 1.1398, 'grad_norm': 4.831854820251465, 'learning_rate': 0.00015800000000000002, 'epoch': 0.01}
{'loss': 0.8076, 'grad_norm': 6.726158142089844, 'learning_rate': 0.00016, 'epoch': 0.01}
{'loss': 0.85, 'grad_norm': 4.7299065589904785, 'learning_rate': 0.000162, 'epoch': 0.01}
{'loss': 0.9263, 'grad_norm': 5.493307590484619, 'learning_rate': 0.000164, 'epoch': 0.01}
{'loss': 1.1429, 'grad_norm': 5.451715469360352, 'learning_rate': 0.000166, 'epoch': 0.01}
{'loss': 1.3787, 'grad_norm': 5.437987327575684, 'learning_rate': 0.000168, 'epoch': 0.01}
{'loss': 1.1186, 'grad_norm': 3.6836822032928467, 'learning_rate': 0.00017, 'epoch': 0.01}
{'loss': 1.051, 'grad_norm': 2.847217082977295, 'learning_rate': 0.000172, 'epoch': 0.01}
{'loss': 1.1795, 'grad_norm': 3.8482229709625244, 'learning_rate': 0.000174, 'epoch': 0.01}
{'loss': 0.9644, 'grad_norm': 6.367372035980225, 'learning_rate': 0.00017600000000000002, 'epoch': 0.01}
{'loss': 0.7545, 'grad_norm': 4.579700469970703, 'learning_rate': 0.00017800000000000002, 'epoch': 0.01}
{'loss': 1.0727, 'grad_norm': 7.695255279541016, 'learning_rate': 0.00018, 'epoch': 0.01}
{'loss': 0.8783, 'grad_norm': 3.8988702297210693, 'learning_rate': 0.000182, 'epoch': 0.01}
{'loss': 1.3586, 'grad_norm': 2.194607734680176, 'learning_rate': 0.00018400000000000003, 'epoch': 0.01}
{'loss': 1.2845, 'grad_norm': 3.4247801303863525, 'learning_rate': 0.00018600000000000002, 'epoch': 0.01}
{'loss': 0.9228, 'grad_norm': 5.568692207336426, 'learning_rate': 0.000188, 'epoch': 0.01}
{'loss': 0.8829, 'grad_norm': 2.9530720710754395, 'learning_rate': 0.00019, 'epoch': 0.01}
{'loss': 0.9134, 'grad_norm': 3.8838887214660645, 'learning_rate': 0.000192, 'epoch': 0.01}
{'loss': 1.0851, 'grad_norm': 3.1458170413970947, 'learning_rate': 0.000194, 'epoch': 0.01}
{'loss': 1.1706, 'grad_norm': 6.199254035949707, 'learning_rate': 0.000196, 'epoch': 0.02}
{'loss': 0.9822, 'grad_norm': 2.7014224529266357, 'learning_rate': 0.00019800000000000002, 'epoch': 0.02}
{'loss': 0.8618, 'grad_norm': 3.9603779315948486, 'learning_rate': 0.0002, 'epoch': 0.02}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.6887, 'grad_norm': 12.550804138183594, 'learning_rate': 0.00019999939076577905, 'epoch': 0.02}
{'loss': 0.8333, 'grad_norm': 3.574453353881836, 'learning_rate': 0.00019999756307053948, 'epoch': 0.02}
{'loss': 1.1401, 'grad_norm': 4.211390018463135, 'learning_rate': 0.00019999451693655123, 'epoch': 0.02}
{'loss': 0.9172, 'grad_norm': 4.58246374130249, 'learning_rate': 0.00019999025240093044, 'epoch': 0.02}
{'loss': 1.016, 'grad_norm': 2.563632011413574, 'learning_rate': 0.00019998476951563915, 'epoch': 0.02}
{'loss': 1.083, 'grad_norm': 3.6702349185943604, 'learning_rate': 0.00019997806834748456, 'epoch': 0.02}
{'loss': 1.1301, 'grad_norm': 2.792571783065796, 'learning_rate': 0.00019997014897811833, 'epoch': 0.02}
{'loss': 0.8619, 'grad_norm': 3.3701934814453125, 'learning_rate': 0.00019996101150403543, 'epoch': 0.02}
{'loss': 0.9306, 'grad_norm': 2.1977698802948, 'learning_rate': 0.00019995065603657316, 'epoch': 0.02}
{'loss': 0.879, 'grad_norm': 3.4781665802001953, 'learning_rate': 0.0001999390827019096, 'epoch': 0.02}
{'loss': 1.1149, 'grad_norm': 2.12287974357605, 'learning_rate': 0.0001999262916410621, 'epoch': 0.02}
