/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 3.6374, 'grad_norm': 64.71869659423828, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
{'loss': 3.2221, 'grad_norm': 74.71148681640625, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}
{'loss': 3.2405, 'grad_norm': 85.88087463378906, 'learning_rate': 6e-06, 'epoch': 0.0}
{'loss': 4.0803, 'grad_norm': 108.76693725585938, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}
{'loss': 3.1998, 'grad_norm': 53.64974594116211, 'learning_rate': 1e-05, 'epoch': 0.0}
{'loss': 4.4058, 'grad_norm': 40.95015335083008, 'learning_rate': 1.2e-05, 'epoch': 0.0}
{'loss': 2.4477, 'grad_norm': 123.37677764892578, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.0}
{'loss': 4.1421, 'grad_norm': 56.64569091796875, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.0}
{'loss': 3.0008, 'grad_norm': 32.05800247192383, 'learning_rate': 1.8e-05, 'epoch': 0.0}
{'loss': 3.8308, 'grad_norm': 36.16508865356445, 'learning_rate': 2e-05, 'epoch': 0.0}
{'loss': 3.803, 'grad_norm': 35.51529312133789, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.0}
{'loss': 3.4454, 'grad_norm': 53.59978485107422, 'learning_rate': 2.4e-05, 'epoch': 0.0}
{'loss': 3.5986, 'grad_norm': 32.51306915283203, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.0}
{'loss': 3.2868, 'grad_norm': 35.99367141723633, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.0}
{'loss': 3.4863, 'grad_norm': 38.18941879272461, 'learning_rate': 3e-05, 'epoch': 0.0}
{'loss': 3.6521, 'grad_norm': 36.0058708190918, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.0}
trainable params: 277,923,840 || all params: 3,057,607,680 || trainable%: 9.089584704339831
None
{'loss': 3.6374, 'grad_norm': 64.71916198730469, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
{'loss': 3.2221, 'grad_norm': 74.71089172363281, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}
{'loss': 3.2293, 'grad_norm': 69.13931274414062, 'learning_rate': 6e-06, 'epoch': 0.0}
{'loss': 4.0828, 'grad_norm': 85.93672943115234, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}
{'loss': 3.1971, 'grad_norm': 46.37675094604492, 'learning_rate': 1e-05, 'epoch': 0.0}
{'loss': 4.4254, 'grad_norm': 67.60384368896484, 'learning_rate': 1.2e-05, 'epoch': 0.0}
{'loss': 2.4683, 'grad_norm': 55.2053108215332, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.0}
{'loss': 4.1314, 'grad_norm': 49.261634826660156, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.0}
{'loss': 3.0169, 'grad_norm': 27.118581771850586, 'learning_rate': 1.8e-05, 'epoch': 0.0}
{'loss': 3.8294, 'grad_norm': 46.625606536865234, 'learning_rate': 2e-05, 'epoch': 0.0}
{'loss': 3.801, 'grad_norm': 33.59780502319336, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.0}
{'loss': 3.416, 'grad_norm': 73.72120666503906, 'learning_rate': 2.4e-05, 'epoch': 0.0}
{'loss': 3.5971, 'grad_norm': 32.925758361816406, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.0}
{'loss': 3.2869, 'grad_norm': 29.408143997192383, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.0}
{'loss': 3.4911, 'grad_norm': 41.307334899902344, 'learning_rate': 3e-05, 'epoch': 0.0}
{'loss': 3.6558, 'grad_norm': 35.31870651245117, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.0}
{'loss': 3.6382, 'grad_norm': 37.28220748901367, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.0}
{'loss': 2.9913, 'grad_norm': 31.222129821777344, 'learning_rate': 3.6e-05, 'epoch': 0.0}
{'loss': 3.2535, 'grad_norm': 35.24452209472656, 'learning_rate': 3.8e-05, 'epoch': 0.0}
{'loss': 3.1185, 'grad_norm': 40.40635299682617, 'learning_rate': 4e-05, 'epoch': 0.0}
{'loss': 3.2552, 'grad_norm': 29.426755905151367, 'learning_rate': 4.2e-05, 'epoch': 0.0}
{'loss': 2.8664, 'grad_norm': 36.91421127319336, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.0}
{'loss': 2.252, 'grad_norm': 18.101247787475586, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.0}
{'loss': 3.0473, 'grad_norm': 34.69709396362305, 'learning_rate': 4.8e-05, 'epoch': 0.0}
{'loss': 3.2076, 'grad_norm': 31.20415496826172, 'learning_rate': 5e-05, 'epoch': 0.0}
{'loss': 2.5923, 'grad_norm': 22.112546920776367, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.0}
{'loss': 2.8613, 'grad_norm': 28.08216094970703, 'learning_rate': 5.4000000000000005e-05, 'epoch': 0.0}
{'loss': 2.1889, 'grad_norm': 15.773015022277832, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.0}
{'loss': 2.5928, 'grad_norm': 22.581398010253906, 'learning_rate': 5.8e-05, 'epoch': 0.0}
{'loss': 2.3586, 'grad_norm': 21.63423728942871, 'learning_rate': 6e-05, 'epoch': 0.0}
{'loss': 2.4803, 'grad_norm': 22.800912857055664, 'learning_rate': 6.2e-05, 'epoch': 0.0}
{'loss': 2.3428, 'grad_norm': 20.894655227661133, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.0}
{'loss': 2.0269, 'grad_norm': 9.702605247497559, 'learning_rate': 6.6e-05, 'epoch': 0.01}
{'loss': 2.3358, 'grad_norm': 25.34668731689453, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.01}
{'loss': 2.0567, 'grad_norm': 13.396093368530273, 'learning_rate': 7e-05, 'epoch': 0.01}
{'loss': 1.9691, 'grad_norm': 14.490662574768066, 'learning_rate': 7.2e-05, 'epoch': 0.01}
{'loss': 1.8497, 'grad_norm': 22.745588302612305, 'learning_rate': 7.4e-05, 'epoch': 0.01}
{'loss': 1.9415, 'grad_norm': 21.009307861328125, 'learning_rate': 7.6e-05, 'epoch': 0.01}
{'loss': 1.7848, 'grad_norm': 13.811460494995117, 'learning_rate': 7.800000000000001e-05, 'epoch': 0.01}
{'loss': 1.8784, 'grad_norm': 18.938419342041016, 'learning_rate': 8e-05, 'epoch': 0.01}
{'loss': 1.6852, 'grad_norm': 21.86087417602539, 'learning_rate': 8.2e-05, 'epoch': 0.01}
{'loss': 1.4925, 'grad_norm': 11.051284790039062, 'learning_rate': 8.4e-05, 'epoch': 0.01}
{'loss': 1.5169, 'grad_norm': 15.713969230651855, 'learning_rate': 8.6e-05, 'epoch': 0.01}
{'loss': 1.4114, 'grad_norm': 9.762426376342773, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.01}
{'loss': 1.351, 'grad_norm': 9.073782920837402, 'learning_rate': 9e-05, 'epoch': 0.01}
{'loss': 1.49, 'grad_norm': 9.996533393859863, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.01}
{'loss': 1.2588, 'grad_norm': 11.643044471740723, 'learning_rate': 9.4e-05, 'epoch': 0.01}
{'loss': 1.2516, 'grad_norm': 12.112081527709961, 'learning_rate': 9.6e-05, 'epoch': 0.01}
{'loss': 1.2069, 'grad_norm': 15.429759979248047, 'learning_rate': 9.8e-05, 'epoch': 0.01}
{'loss': 1.2486, 'grad_norm': 11.85441780090332, 'learning_rate': 0.0001, 'epoch': 0.01}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0386, 'grad_norm': 9.375616073608398, 'learning_rate': 0.00010200000000000001, 'epoch': 0.01}
{'loss': 1.2206, 'grad_norm': 5.819297790527344, 'learning_rate': 0.00010400000000000001, 'epoch': 0.01}
{'loss': 1.3168, 'grad_norm': 10.786847114562988, 'learning_rate': 0.00010600000000000002, 'epoch': 0.01}
{'loss': 0.9883, 'grad_norm': 5.672850608825684, 'learning_rate': 0.00010800000000000001, 'epoch': 0.01}
{'loss': 1.3592, 'grad_norm': 7.729985237121582, 'learning_rate': 0.00011000000000000002, 'epoch': 0.01}
{'loss': 1.1343, 'grad_norm': 6.600385665893555, 'learning_rate': 0.00011200000000000001, 'epoch': 0.01}
{'loss': 1.1812, 'grad_norm': 8.709474563598633, 'learning_rate': 0.00011399999999999999, 'epoch': 0.01}
{'loss': 1.0237, 'grad_norm': 3.647329568862915, 'learning_rate': 0.000116, 'epoch': 0.01}
{'loss': 1.0606, 'grad_norm': 6.345747470855713, 'learning_rate': 0.000118, 'epoch': 0.01}
{'loss': 1.1974, 'grad_norm': 7.777054786682129, 'learning_rate': 0.00012, 'epoch': 0.01}
{'loss': 1.0915, 'grad_norm': 6.7487077713012695, 'learning_rate': 0.000122, 'epoch': 0.01}
{'loss': 0.9525, 'grad_norm': 4.09079122543335, 'learning_rate': 0.000124, 'epoch': 0.01}
{'loss': 1.056, 'grad_norm': 5.5594987869262695, 'learning_rate': 0.000126, 'epoch': 0.01}
{'loss': 0.8238, 'grad_norm': 12.449872970581055, 'learning_rate': 0.00012800000000000002, 'epoch': 0.01}
{'loss': 0.8321, 'grad_norm': 7.995272159576416, 'learning_rate': 0.00013000000000000002, 'epoch': 0.01}
{'loss': 1.3497, 'grad_norm': 3.868250846862793, 'learning_rate': 0.000132, 'epoch': 0.01}
{'loss': 1.113, 'grad_norm': 7.409749984741211, 'learning_rate': 0.000134, 'epoch': 0.01}
{'loss': 1.2929, 'grad_norm': 9.105388641357422, 'learning_rate': 0.00013600000000000003, 'epoch': 0.01}
{'loss': 1.1352, 'grad_norm': 6.254110813140869, 'learning_rate': 0.000138, 'epoch': 0.01}
{'loss': 0.9648, 'grad_norm': 7.007051467895508, 'learning_rate': 0.00014, 'epoch': 0.01}
{'loss': 1.0201, 'grad_norm': 6.067365646362305, 'learning_rate': 0.000142, 'epoch': 0.01}
{'loss': 1.027, 'grad_norm': 6.2268385887146, 'learning_rate': 0.000144, 'epoch': 0.01}
{'loss': 0.8737, 'grad_norm': 2.651029109954834, 'learning_rate': 0.000146, 'epoch': 0.01}
{'loss': 1.277, 'grad_norm': 2.5401904582977295, 'learning_rate': 0.000148, 'epoch': 0.01}
{'loss': 1.0106, 'grad_norm': 3.9319348335266113, 'learning_rate': 0.00015000000000000001, 'epoch': 0.01}
{'loss': 0.6975, 'grad_norm': 3.7107551097869873, 'learning_rate': 0.000152, 'epoch': 0.01}
{'loss': 1.2079, 'grad_norm': 8.058069229125977, 'learning_rate': 0.000154, 'epoch': 0.01}
{'loss': 1.095, 'grad_norm': 8.750383377075195, 'learning_rate': 0.00015600000000000002, 'epoch': 0.01}
{'loss': 1.1182, 'grad_norm': 3.4182279109954834, 'learning_rate': 0.00015800000000000002, 'epoch': 0.01}
{'loss': 0.81, 'grad_norm': 5.853910446166992, 'learning_rate': 0.00016, 'epoch': 0.01}
{'loss': 0.824, 'grad_norm': 6.006849765777588, 'learning_rate': 0.000162, 'epoch': 0.01}
{'loss': 0.9084, 'grad_norm': 2.8050382137298584, 'learning_rate': 0.000164, 'epoch': 0.01}
{'loss': 1.1294, 'grad_norm': 4.69580078125, 'learning_rate': 0.000166, 'epoch': 0.01}
{'loss': 1.3333, 'grad_norm': 4.371888160705566, 'learning_rate': 0.000168, 'epoch': 0.01}
{'loss': 1.1517, 'grad_norm': 5.0664777755737305, 'learning_rate': 0.00017, 'epoch': 0.01}
{'loss': 1.0409, 'grad_norm': 2.7318115234375, 'learning_rate': 0.000172, 'epoch': 0.01}
{'loss': 1.1493, 'grad_norm': 4.146198749542236, 'learning_rate': 0.000174, 'epoch': 0.01}
{'loss': 0.9255, 'grad_norm': 7.85554838180542, 'learning_rate': 0.00017600000000000002, 'epoch': 0.01}
{'loss': 0.7656, 'grad_norm': 4.273538589477539, 'learning_rate': 0.00017800000000000002, 'epoch': 0.01}
{'loss': 1.0429, 'grad_norm': 3.0346219539642334, 'learning_rate': 0.00018, 'epoch': 0.01}
{'loss': 0.8843, 'grad_norm': 3.7713470458984375, 'learning_rate': 0.000182, 'epoch': 0.01}
{'loss': 1.3513, 'grad_norm': 2.0588717460632324, 'learning_rate': 0.00018400000000000003, 'epoch': 0.01}
{'loss': 1.254, 'grad_norm': 2.2245428562164307, 'learning_rate': 0.00018600000000000002, 'epoch': 0.01}
{'loss': 0.8668, 'grad_norm': 4.704864978790283, 'learning_rate': 0.000188, 'epoch': 0.01}
{'loss': 0.8792, 'grad_norm': 2.758233070373535, 'learning_rate': 0.00019, 'epoch': 0.01}
{'loss': 0.9355, 'grad_norm': 3.3491649627685547, 'learning_rate': 0.000192, 'epoch': 0.01}
{'loss': 1.0398, 'grad_norm': 2.8214502334594727, 'learning_rate': 0.000194, 'epoch': 0.01}
{'loss': 1.0905, 'grad_norm': 6.015566825866699, 'learning_rate': 0.000196, 'epoch': 0.02}
{'loss': 0.9872, 'grad_norm': 2.992018938064575, 'learning_rate': 0.00019800000000000002, 'epoch': 0.02}
{'loss': 0.8889, 'grad_norm': 6.375962257385254, 'learning_rate': 0.0002, 'epoch': 0.02}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.4488, 'grad_norm': 32.1489372253418, 'learning_rate': 0.00019999939076577905, 'epoch': 0.02}
{'loss': 0.8443, 'grad_norm': 4.857792377471924, 'learning_rate': 0.00019999756307053948, 'epoch': 0.02}
{'loss': 1.1093, 'grad_norm': 4.292560577392578, 'learning_rate': 0.00019999451693655123, 'epoch': 0.02}
{'loss': 0.8662, 'grad_norm': 4.425710201263428, 'learning_rate': 0.00019999025240093044, 'epoch': 0.02}
{'loss': 0.9783, 'grad_norm': 2.477126121520996, 'learning_rate': 0.00019998476951563915, 'epoch': 0.02}
{'loss': 1.0571, 'grad_norm': 3.453566074371338, 'learning_rate': 0.00019997806834748456, 'epoch': 0.02}
{'loss': 1.1022, 'grad_norm': 3.0399208068847656, 'learning_rate': 0.00019997014897811833, 'epoch': 0.02}
{'loss': 0.9455, 'grad_norm': 4.614914417266846, 'learning_rate': 0.00019996101150403543, 'epoch': 0.02}
{'loss': 0.9233, 'grad_norm': 1.7913377285003662, 'learning_rate': 0.00019995065603657316, 'epoch': 0.02}
{'loss': 0.8984, 'grad_norm': 3.3610615730285645, 'learning_rate': 0.0001999390827019096, 'epoch': 0.02}
{'loss': 1.1097, 'grad_norm': 2.1075992584228516, 'learning_rate': 0.0001999262916410621, 'epoch': 0.02}
{'loss': 1.1082, 'grad_norm': 7.251070499420166, 'learning_rate': 0.00019991228300988585, 'epoch': 0.02}
{'loss': 1.0702, 'grad_norm': 3.4315643310546875, 'learning_rate': 0.00019989705697907149, 'epoch': 0.02}
{'loss': 0.7967, 'grad_norm': 3.5899994373321533, 'learning_rate': 0.0001998806137341434, 'epoch': 0.02}
{'loss': 1.0679, 'grad_norm': 6.534547328948975, 'learning_rate': 0.0001998629534754574, 'epoch': 0.02}
{'loss': 1.3099, 'grad_norm': 2.7033355236053467, 'learning_rate': 0.00019984407641819812, 'epoch': 0.02}
{'loss': 0.8247, 'grad_norm': 3.4573352336883545, 'learning_rate': 0.00019982398279237655, 'epoch': 0.02}
{'loss': 1.0423, 'grad_norm': 2.3355655670166016, 'learning_rate': 0.00019980267284282717, 'epoch': 0.02}
{'loss': 1.0435, 'grad_norm': 2.389359712600708, 'learning_rate': 0.000199780146829205, 'epoch': 0.02}
{'loss': 0.9958, 'grad_norm': 2.990797281265259, 'learning_rate': 0.00019975640502598244, 'epoch': 0.02}
{'loss': 1.1462, 'grad_norm': 2.601487159729004, 'learning_rate': 0.00019973144772244582, 'epoch': 0.02}
{'loss': 1.0337, 'grad_norm': 2.4984302520751953, 'learning_rate': 0.00019970527522269205, 'epoch': 0.02}
{'loss': 0.8595, 'grad_norm': 1.818238377571106, 'learning_rate': 0.00019967788784562473, 'epoch': 0.02}
{'loss': 1.156, 'grad_norm': 2.0295066833496094, 'learning_rate': 0.00019964928592495045, 'epoch': 0.02}
{'loss': 0.9887, 'grad_norm': 2.304396152496338, 'learning_rate': 0.00019961946980917456, 'epoch': 0.02}
{'loss': 1.0606, 'grad_norm': 3.3151421546936035, 'learning_rate': 0.00019958843986159704, 'epoch': 0.02}
{'loss': 1.1673, 'grad_norm': 2.196587562561035, 'learning_rate': 0.00019955619646030802, 'epoch': 0.02}
{'loss': 1.0281, 'grad_norm': 2.6313724517822266, 'learning_rate': 0.0001995227399981831, 'epoch': 0.02}
{'loss': 0.93, 'grad_norm': 1.7124382257461548, 'learning_rate': 0.00019948807088287883, 'epoch': 0.02}
{'loss': 1.0375, 'grad_norm': 1.8231604099273682, 'learning_rate': 0.00019945218953682734, 'epoch': 0.02}
{'loss': 0.9266, 'grad_norm': 16.808826446533203, 'learning_rate': 0.00019941509639723155, 'epoch': 0.02}
{'loss': 1.115, 'grad_norm': 1.5783568620681763, 'learning_rate': 0.00019937679191605963, 'epoch': 0.02}
{'loss': 0.9513, 'grad_norm': 4.09857177734375, 'learning_rate': 0.00019933727656003963, 'epoch': 0.02}
{'loss': 1.0675, 'grad_norm': 2.653287649154663, 'learning_rate': 0.0001992965508106537, 'epoch': 0.02}
{'loss': 0.7573, 'grad_norm': 1.6730365753173828, 'learning_rate': 0.00019925461516413223, 'epoch': 0.02}
{'loss': 0.8488, 'grad_norm': 2.3372962474823, 'learning_rate': 0.0001992114701314478, 'epoch': 0.02}
{'loss': 1.1959, 'grad_norm': 2.2465572357177734, 'learning_rate': 0.00019916711623830903, 'epoch': 0.02}
{'loss': 1.0737, 'grad_norm': 2.9543631076812744, 'learning_rate': 0.00019912155402515417, 'epoch': 0.02}
{'loss': 1.0091, 'grad_norm': 3.4102087020874023, 'learning_rate': 0.00019907478404714436, 'epoch': 0.02}
{'loss': 1.049, 'grad_norm': 2.3720502853393555, 'learning_rate': 0.00019902680687415705, 'epoch': 0.02}
{'loss': 1.0361, 'grad_norm': 2.1688461303710938, 'learning_rate': 0.0001989776230907789, 'epoch': 0.02}
{'loss': 0.8217, 'grad_norm': 2.2023022174835205, 'learning_rate': 0.00019892723329629887, 'epoch': 0.02}
{'loss': 0.8174, 'grad_norm': 2.73020076751709, 'learning_rate': 0.0001988756381047006, 'epoch': 0.02}
{'loss': 1.2636, 'grad_norm': 2.112300157546997, 'learning_rate': 0.0001988228381446553, 'epoch': 0.02}
{'loss': 1.1746, 'grad_norm': 1.740698218345642, 'learning_rate': 0.00019876883405951377, 'epoch': 0.02}
{'loss': 0.9794, 'grad_norm': 1.5191012620925903, 'learning_rate': 0.0001987136265072988, 'epoch': 0.02}
{'loss': 1.005, 'grad_norm': 3.00753116607666, 'learning_rate': 0.00019865721616069696, 'epoch': 0.02}
{'loss': 1.1079, 'grad_norm': 1.366088628768921, 'learning_rate': 0.0001985996037070505, 'epoch': 0.02}
{'loss': 0.79, 'grad_norm': 1.9579325914382935, 'learning_rate': 0.00019854078984834903, 'epoch': 0.02}
{'loss': 1.2666, 'grad_norm': 1.944409966468811, 'learning_rate': 0.00019848077530122083, 'epoch': 0.02}
{'loss': 1.0129, 'grad_norm': 1.8155887126922607, 'learning_rate': 0.0001984195607969242, 'epoch': 0.02}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0594, 'grad_norm': 2.7333128452301025, 'learning_rate': 0.00019835714708133862, 'epoch': 0.02}
{'loss': 1.1789, 'grad_norm': 2.513442039489746, 'learning_rate': 0.00019829353491495545, 'epoch': 0.02}
{'loss': 1.1872, 'grad_norm': 2.4535973072052, 'learning_rate': 0.0001982287250728689, 'epoch': 0.02}
{'loss': 0.9132, 'grad_norm': 1.647295355796814, 'learning_rate': 0.00019816271834476642, 'epoch': 0.02}
{'loss': 0.9382, 'grad_norm': 3.9428484439849854, 'learning_rate': 0.00019809551553491916, 'epoch': 0.02}
{'loss': 0.8125, 'grad_norm': 2.052614688873291, 'learning_rate': 0.00019802711746217218, 'epoch': 0.02}
{'loss': 0.8029, 'grad_norm': 1.9226384162902832, 'learning_rate': 0.0001979575249599344, 'epoch': 0.02}
{'loss': 0.9151, 'grad_norm': 1.854574203491211, 'learning_rate': 0.0001978867388761685, 'epoch': 0.02}
{'loss': 0.9093, 'grad_norm': 3.4268829822540283, 'learning_rate': 0.00019781476007338058, 'epoch': 0.02}
{'loss': 1.1234, 'grad_norm': 3.5046095848083496, 'learning_rate': 0.0001977415894286096, 'epoch': 0.02}
{'loss': 0.9398, 'grad_norm': 2.2525346279144287, 'learning_rate': 0.0001976672278334168, 'epoch': 0.02}
{'loss': 1.0291, 'grad_norm': 2.582214593887329, 'learning_rate': 0.00019759167619387476, 'epoch': 0.03}
{'loss': 1.2923, 'grad_norm': 1.784165620803833, 'learning_rate': 0.00019751493543055632, 'epoch': 0.03}
{'loss': 0.7478, 'grad_norm': 3.117567777633667, 'learning_rate': 0.00019743700647852354, 'epoch': 0.03}
{'loss': 1.3875, 'grad_norm': 1.8401668071746826, 'learning_rate': 0.00019735789028731604, 'epoch': 0.03}
{'loss': 0.8748, 'grad_norm': 2.4511866569519043, 'learning_rate': 0.00019727758782093967, 'epoch': 0.03}
{'loss': 0.8149, 'grad_norm': 2.368396043777466, 'learning_rate': 0.00019719610005785465, 'epoch': 0.03}
{'loss': 1.1426, 'grad_norm': 1.7770663499832153, 'learning_rate': 0.00019711342799096361, 'epoch': 0.03}
{'loss': 1.322, 'grad_norm': 2.337559461593628, 'learning_rate': 0.00019702957262759965, 'epoch': 0.03}
{'loss': 1.2097, 'grad_norm': 1.2705581188201904, 'learning_rate': 0.0001969445349895139, 'epoch': 0.03}
{'loss': 1.3617, 'grad_norm': 2.8953442573547363, 'learning_rate': 0.0001968583161128631, 'epoch': 0.03}
{'loss': 1.0841, 'grad_norm': 2.295290946960449, 'learning_rate': 0.00019677091704819715, 'epoch': 0.03}
{'loss': 1.0398, 'grad_norm': 3.0351979732513428, 'learning_rate': 0.00019668233886044597, 'epoch': 0.03}
{'loss': 0.6412, 'grad_norm': 1.7438715696334839, 'learning_rate': 0.00019659258262890683, 'epoch': 0.03}
{'loss': 1.1336, 'grad_norm': 1.596426010131836, 'learning_rate': 0.00019650164944723115, 'epoch': 0.03}
{'loss': 1.0327, 'grad_norm': 2.0377964973449707, 'learning_rate': 0.00019640954042341103, 'epoch': 0.03}
{'loss': 1.2341, 'grad_norm': 2.3985676765441895, 'learning_rate': 0.00019631625667976583, 'epoch': 0.03}
{'loss': 0.9699, 'grad_norm': 2.583805799484253, 'learning_rate': 0.00019622179935292855, 'epoch': 0.03}
{'loss': 1.0179, 'grad_norm': 2.2276227474212646, 'learning_rate': 0.0001961261695938319, 'epoch': 0.03}
{'loss': 1.1929, 'grad_norm': 2.5999977588653564, 'learning_rate': 0.0001960293685676943, 'epoch': 0.03}
{'loss': 1.0658, 'grad_norm': 1.8833428621292114, 'learning_rate': 0.00019593139745400576, 'epoch': 0.03}
{'loss': 1.1702, 'grad_norm': 2.0182690620422363, 'learning_rate': 0.00019583225744651333, 'epoch': 0.03}
{'loss': 1.0712, 'grad_norm': 1.8057587146759033, 'learning_rate': 0.00019573194975320673, 'epoch': 0.03}
{'loss': 1.3645, 'grad_norm': 1.8934247493743896, 'learning_rate': 0.00019563047559630357, 'epoch': 0.03}
{'loss': 1.1426, 'grad_norm': 1.606839895248413, 'learning_rate': 0.00019552783621223436, 'epoch': 0.03}
{'loss': 0.7604, 'grad_norm': 2.442411422729492, 'learning_rate': 0.0001954240328516277, 'epoch': 0.03}
{'loss': 0.9618, 'grad_norm': 1.9505239725112915, 'learning_rate': 0.0001953190667792947, 'epoch': 0.03}
{'loss': 1.2102, 'grad_norm': 1.6956521272659302, 'learning_rate': 0.00019521293927421388, 'epoch': 0.03}
{'loss': 0.9505, 'grad_norm': 1.6546204090118408, 'learning_rate': 0.00019510565162951537, 'epoch': 0.03}
{'loss': 1.2723, 'grad_norm': 1.531240463256836, 'learning_rate': 0.00019499720515246525, 'epoch': 0.03}
{'loss': 1.0364, 'grad_norm': 2.1206212043762207, 'learning_rate': 0.00019488760116444966, 'epoch': 0.03}
{'loss': 1.0282, 'grad_norm': 1.7214266061782837, 'learning_rate': 0.0001947768410009586, 'epoch': 0.03}
{'loss': 0.9023, 'grad_norm': 1.8070690631866455, 'learning_rate': 0.00019466492601156966, 'epoch': 0.03}
{'loss': 1.1407, 'grad_norm': 1.7563461065292358, 'learning_rate': 0.0001945518575599317, 'epoch': 0.03}
{'loss': 1.175, 'grad_norm': 1.253926396369934, 'learning_rate': 0.00019443763702374812, 'epoch': 0.03}
{'loss': 1.0075, 'grad_norm': 2.178051233291626, 'learning_rate': 0.0001943222657947601, 'epoch': 0.03}
{'loss': 1.154, 'grad_norm': 2.7132279872894287, 'learning_rate': 0.00019420574527872968, 'epoch': 0.03}
{'loss': 1.0031, 'grad_norm': 2.14487886428833, 'learning_rate': 0.00019408807689542257, 'epoch': 0.03}
{'loss': 0.9966, 'grad_norm': 1.391657829284668, 'learning_rate': 0.00019396926207859084, 'epoch': 0.03}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2015, 'grad_norm': 2.08231258392334, 'learning_rate': 0.0001938493022759556, 'epoch': 0.03}
{'loss': 1.2066, 'grad_norm': 2.2091403007507324, 'learning_rate': 0.00019372819894918915, 'epoch': 0.03}
{'loss': 1.1889, 'grad_norm': 2.866828203201294, 'learning_rate': 0.00019360595357389735, 'epoch': 0.03}
{'loss': 1.098, 'grad_norm': 1.8346463441848755, 'learning_rate': 0.00019348256763960145, 'epoch': 0.03}
{'loss': 0.8604, 'grad_norm': 2.9217722415924072, 'learning_rate': 0.00019335804264972018, 'epoch': 0.03}
{'loss': 0.9369, 'grad_norm': 2.212592601776123, 'learning_rate': 0.00019323238012155123, 'epoch': 0.03}
{'loss': 1.0655, 'grad_norm': 1.486388087272644, 'learning_rate': 0.00019310558158625285, 'epoch': 0.03}
{'loss': 0.8132, 'grad_norm': 1.2025607824325562, 'learning_rate': 0.00019297764858882514, 'epoch': 0.03}
{'loss': 1.0203, 'grad_norm': 2.5008063316345215, 'learning_rate': 0.00019284858268809137, 'epoch': 0.03}
{'loss': 0.9629, 'grad_norm': 1.9601190090179443, 'learning_rate': 0.00019271838545667876, 'epoch': 0.03}
{'loss': 1.1331, 'grad_norm': 1.3424533605575562, 'learning_rate': 0.0001925870584809995, 'epoch': 0.03}
{'loss': 1.1397, 'grad_norm': 1.3592532873153687, 'learning_rate': 0.00019245460336123134, 'epoch': 0.03}
{'loss': 0.8316, 'grad_norm': 1.5767585039138794, 'learning_rate': 0.00019232102171129811, 'epoch': 0.03}
{'loss': 0.8865, 'grad_norm': 1.7891713380813599, 'learning_rate': 0.00019218631515885006, 'epoch': 0.03}
{'loss': 1.0764, 'grad_norm': 1.5573540925979614, 'learning_rate': 0.00019205048534524406, 'epoch': 0.03}
{'loss': 0.8869, 'grad_norm': 3.279165267944336, 'learning_rate': 0.00019191353392552344, 'epoch': 0.03}
{'loss': 0.82, 'grad_norm': 1.7353521585464478, 'learning_rate': 0.00019177546256839812, 'epoch': 0.03}
{'loss': 1.0128, 'grad_norm': 2.1802432537078857, 'learning_rate': 0.00019163627295622397, 'epoch': 0.03}
{'loss': 0.8651, 'grad_norm': 3.684743881225586, 'learning_rate': 0.0001914959667849825, 'epoch': 0.03}
{'loss': 0.8384, 'grad_norm': 1.3101986646652222, 'learning_rate': 0.0001913545457642601, 'epoch': 0.03}
{'loss': 1.1304, 'grad_norm': 1.7110499143600464, 'learning_rate': 0.0001912120116172273, 'epoch': 0.03}
{'loss': 0.8743, 'grad_norm': 2.133319139480591, 'learning_rate': 0.00019106836608061772, 'epoch': 0.03}
{'loss': 0.9034, 'grad_norm': 2.9454169273376465, 'learning_rate': 0.00019092361090470688, 'epoch': 0.03}
{'loss': 1.0726, 'grad_norm': 2.9705617427825928, 'learning_rate': 0.00019077774785329087, 'epoch': 0.03}
{'loss': 0.9113, 'grad_norm': 2.7442309856414795, 'learning_rate': 0.000190630778703665, 'epoch': 0.03}
{'loss': 1.1634, 'grad_norm': 2.1970767974853516, 'learning_rate': 0.00019048270524660196, 'epoch': 0.03}
{'loss': 0.8556, 'grad_norm': 2.2480416297912598, 'learning_rate': 0.0001903335292863301, 'epoch': 0.03}
{'loss': 1.3297, 'grad_norm': 3.529533863067627, 'learning_rate': 0.0001901832526405114, 'epoch': 0.04}
{'loss': 0.6326, 'grad_norm': 2.182546854019165, 'learning_rate': 0.00019003187714021938, 'epoch': 0.04}
{'loss': 1.1109, 'grad_norm': 1.7512229681015015, 'learning_rate': 0.0001898794046299167, 'epoch': 0.04}
{'loss': 0.798, 'grad_norm': 1.276776909828186, 'learning_rate': 0.00018972583696743285, 'epoch': 0.04}
{'loss': 0.9911, 'grad_norm': 2.096256732940674, 'learning_rate': 0.0001895711760239413, 'epoch': 0.04}
{'loss': 1.0462, 'grad_norm': 1.6411283016204834, 'learning_rate': 0.0001894154236839368, 'epoch': 0.04}
{'loss': 1.0507, 'grad_norm': 1.45045804977417, 'learning_rate': 0.00018925858184521256, 'epoch': 0.04}
{'loss': 1.1025, 'grad_norm': 1.7154028415679932, 'learning_rate': 0.0001891006524188368, 'epoch': 0.04}
{'loss': 1.2151, 'grad_norm': 3.3162009716033936, 'learning_rate': 0.00018894163732912977, 'epoch': 0.04}
{'loss': 0.955, 'grad_norm': 2.1751670837402344, 'learning_rate': 0.00018878153851364013, 'epoch': 0.04}
{'loss': 1.1055, 'grad_norm': 1.5024269819259644, 'learning_rate': 0.00018862035792312147, 'epoch': 0.04}
{'loss': 0.9294, 'grad_norm': 2.278486728668213, 'learning_rate': 0.0001884580975215084, 'epoch': 0.04}
{'loss': 1.0833, 'grad_norm': 2.6605989933013916, 'learning_rate': 0.00018829475928589271, 'epoch': 0.04}
{'loss': 0.8489, 'grad_norm': 1.6857272386550903, 'learning_rate': 0.0001881303452064992, 'epoch': 0.04}
{'loss': 1.0571, 'grad_norm': 1.5698803663253784, 'learning_rate': 0.00018796485728666165, 'epoch': 0.04}
{'loss': 0.9086, 'grad_norm': 1.6906808614730835, 'learning_rate': 0.00018779829754279805, 'epoch': 0.04}
{'loss': 1.0468, 'grad_norm': 1.561234712600708, 'learning_rate': 0.00018763066800438636, 'epoch': 0.04}
{'loss': 0.9078, 'grad_norm': 2.9298253059387207, 'learning_rate': 0.00018746197071393958, 'epoch': 0.04}
{'loss': 0.9032, 'grad_norm': 1.785441517829895, 'learning_rate': 0.00018729220772698097, 'epoch': 0.04}
{'loss': 1.0438, 'grad_norm': 1.8805736303329468, 'learning_rate': 0.00018712138111201895, 'epoch': 0.04}
{'loss': 0.8498, 'grad_norm': 1.8188425302505493, 'learning_rate': 0.0001869494929505219, 'epoch': 0.04}
{'loss': 1.1666, 'grad_norm': 1.4302455186843872, 'learning_rate': 0.00018677654533689287, 'epoch': 0.04}
{'loss': 1.1591, 'grad_norm': 1.793333888053894, 'learning_rate': 0.00018660254037844388, 'epoch': 0.04}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1435, 'grad_norm': 1.7435171604156494, 'learning_rate': 0.0001864274801953705, 'epoch': 0.04}
{'loss': 0.9708, 'grad_norm': 2.6601545810699463, 'learning_rate': 0.00018625136692072575, 'epoch': 0.04}
{'loss': 1.02, 'grad_norm': 1.4911409616470337, 'learning_rate': 0.0001860742027003944, 'epoch': 0.04}
{'loss': 1.0449, 'grad_norm': 2.076801300048828, 'learning_rate': 0.00018589598969306645, 'epoch': 0.04}
{'loss': 0.9403, 'grad_norm': 2.288090229034424, 'learning_rate': 0.00018571673007021123, 'epoch': 0.04}
{'loss': 0.9054, 'grad_norm': 1.399038553237915, 'learning_rate': 0.00018553642601605068, 'epoch': 0.04}
{'loss': 1.071, 'grad_norm': 1.8277004957199097, 'learning_rate': 0.00018535507972753274, 'epoch': 0.04}
{'loss': 0.8353, 'grad_norm': 1.4897091388702393, 'learning_rate': 0.00018517269341430476, 'epoch': 0.04}
{'loss': 0.936, 'grad_norm': 1.9612759351730347, 'learning_rate': 0.00018498926929868642, 'epoch': 0.04}
{'loss': 0.9007, 'grad_norm': 1.6821024417877197, 'learning_rate': 0.0001848048096156426, 'epoch': 0.04}
{'loss': 0.8953, 'grad_norm': 1.366422176361084, 'learning_rate': 0.00018461931661275643, 'epoch': 0.04}
{'loss': 0.7523, 'grad_norm': 1.5726561546325684, 'learning_rate': 0.00018443279255020152, 'epoch': 0.04}
{'loss': 1.1199, 'grad_norm': 1.7818036079406738, 'learning_rate': 0.00018424523970071477, 'epoch': 0.04}
{'loss': 1.2646, 'grad_norm': 1.65488862991333, 'learning_rate': 0.00018405666034956844, 'epoch': 0.04}
{'loss': 0.9525, 'grad_norm': 3.1161069869995117, 'learning_rate': 0.00018386705679454242, 'epoch': 0.04}
{'loss': 1.0321, 'grad_norm': 3.519791841506958, 'learning_rate': 0.00018367643134589617, 'epoch': 0.04}
{'loss': 1.2197, 'grad_norm': 2.0571727752685547, 'learning_rate': 0.00018348478632634066, 'epoch': 0.04}
{'loss': 1.2188, 'grad_norm': 1.2920459508895874, 'learning_rate': 0.00018329212407100994, 'epoch': 0.04}
{'loss': 1.1557, 'grad_norm': 1.378092885017395, 'learning_rate': 0.00018309844692743283, 'epoch': 0.04}
{'loss': 1.1018, 'grad_norm': 1.7226693630218506, 'learning_rate': 0.00018290375725550417, 'epoch': 0.04}
{'loss': 0.9774, 'grad_norm': 2.0780386924743652, 'learning_rate': 0.00018270805742745617, 'epoch': 0.04}
{'loss': 1.2703, 'grad_norm': 1.838732361793518, 'learning_rate': 0.00018251134982782952, 'epoch': 0.04}
{'loss': 1.3032, 'grad_norm': 1.3244273662567139, 'learning_rate': 0.0001823136368534442, 'epoch': 0.04}
{'loss': 0.9163, 'grad_norm': 1.6573282480239868, 'learning_rate': 0.00018211492091337042, 'epoch': 0.04}
{'loss': 1.0594, 'grad_norm': 1.626860499382019, 'learning_rate': 0.0001819152044288992, 'epoch': 0.04}
{'loss': 1.257, 'grad_norm': 3.194227933883667, 'learning_rate': 0.00018171448983351284, 'epoch': 0.04}
{'loss': 0.8603, 'grad_norm': 1.4303927421569824, 'learning_rate': 0.00018151277957285543, 'epoch': 0.04}
{'loss': 0.6964, 'grad_norm': 1.7665878534317017, 'learning_rate': 0.00018131007610470276, 'epoch': 0.04}
{'loss': 0.9043, 'grad_norm': 1.9151337146759033, 'learning_rate': 0.00018110638189893267, 'epoch': 0.04}
{'loss': 0.7621, 'grad_norm': 1.3531769514083862, 'learning_rate': 0.00018090169943749476, 'epoch': 0.04}
{'loss': 1.1036, 'grad_norm': 1.801775574684143, 'learning_rate': 0.00018069603121438022, 'epoch': 0.04}
{'loss': 0.7649, 'grad_norm': 1.1831400394439697, 'learning_rate': 0.0001804893797355914, 'epoch': 0.04}
{'loss': 0.9936, 'grad_norm': 1.759340763092041, 'learning_rate': 0.00018028174751911146, 'epoch': 0.04}
{'loss': 0.9454, 'grad_norm': 1.816333532333374, 'learning_rate': 0.00018007313709487334, 'epoch': 0.04}
{'loss': 0.9453, 'grad_norm': 1.8190699815750122, 'learning_rate': 0.00017986355100472928, 'epoch': 0.04}
{'loss': 1.0562, 'grad_norm': 1.3172532320022583, 'learning_rate': 0.00017965299180241963, 'epoch': 0.04}
{'loss': 0.9409, 'grad_norm': 1.339901328086853, 'learning_rate': 0.00017944146205354182, 'epoch': 0.04}
{'loss': 0.9755, 'grad_norm': 1.413244605064392, 'learning_rate': 0.00017922896433551907, 'epoch': 0.04}
{'loss': 1.1999, 'grad_norm': 1.7274034023284912, 'learning_rate': 0.00017901550123756906, 'epoch': 0.04}
{'loss': 1.0623, 'grad_norm': 2.767754077911377, 'learning_rate': 0.00017880107536067218, 'epoch': 0.04}
{'loss': 1.1338, 'grad_norm': 1.6856112480163574, 'learning_rate': 0.0001785856893175402, 'epoch': 0.04}
{'loss': 1.0155, 'grad_norm': 2.2025718688964844, 'learning_rate': 0.000178369345732584, 'epoch': 0.04}
{'loss': 0.9613, 'grad_norm': 1.2177222967147827, 'learning_rate': 0.00017815204724188187, 'epoch': 0.05}
{'loss': 1.0123, 'grad_norm': 1.9429545402526855, 'learning_rate': 0.00017793379649314744, 'epoch': 0.05}
{'loss': 0.9337, 'grad_norm': 1.3667292594909668, 'learning_rate': 0.0001777145961456971, 'epoch': 0.05}
{'loss': 1.2324, 'grad_norm': 2.659212589263916, 'learning_rate': 0.00017749444887041799, 'epoch': 0.05}
{'loss': 1.0337, 'grad_norm': 1.879251480102539, 'learning_rate': 0.00017727335734973512, 'epoch': 0.05}
{'loss': 1.0823, 'grad_norm': 1.8979499340057373, 'learning_rate': 0.00017705132427757895, 'epoch': 0.05}
{'loss': 1.0496, 'grad_norm': 1.380853295326233, 'learning_rate': 0.00017682835235935236, 'epoch': 0.05}
{'loss': 1.025, 'grad_norm': 1.4507852792739868, 'learning_rate': 0.0001766044443118978, 'epoch': 0.05}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1641, 'grad_norm': 1.4972988367080688, 'learning_rate': 0.00017637960286346425, 'epoch': 0.05}
{'loss': 1.3732, 'grad_norm': 1.8598816394805908, 'learning_rate': 0.0001761538307536737, 'epoch': 0.05}
{'loss': 1.0001, 'grad_norm': 1.520635962486267, 'learning_rate': 0.00017592713073348807, 'epoch': 0.05}
{'loss': 0.8148, 'grad_norm': 1.4086616039276123, 'learning_rate': 0.00017569950556517566, 'epoch': 0.05}
{'loss': 0.8981, 'grad_norm': 1.32364821434021, 'learning_rate': 0.00017547095802227723, 'epoch': 0.05}
{'loss': 1.0539, 'grad_norm': 1.3966597318649292, 'learning_rate': 0.00017524149088957245, 'epoch': 0.05}
{'loss': 0.9028, 'grad_norm': 1.7096118927001953, 'learning_rate': 0.00017501110696304596, 'epoch': 0.05}
{'loss': 1.1047, 'grad_norm': 2.015591859817505, 'learning_rate': 0.0001747798090498532, 'epoch': 0.05}
{'loss': 0.85, 'grad_norm': 1.8264870643615723, 'learning_rate': 0.00017454759996828623, 'epoch': 0.05}
{'loss': 0.977, 'grad_norm': 4.0221405029296875, 'learning_rate': 0.00017431448254773944, 'epoch': 0.05}
{'loss': 1.23, 'grad_norm': 4.101682662963867, 'learning_rate': 0.000174080459628675, 'epoch': 0.05}
{'loss': 1.0687, 'grad_norm': 1.909469723701477, 'learning_rate': 0.00017384553406258842, 'epoch': 0.05}
{'loss': 0.826, 'grad_norm': 1.7796272039413452, 'learning_rate': 0.00017360970871197346, 'epoch': 0.05}
{'loss': 1.0582, 'grad_norm': 1.428716778755188, 'learning_rate': 0.00017337298645028764, 'epoch': 0.05}
{'loss': 0.9923, 'grad_norm': 1.6545305252075195, 'learning_rate': 0.00017313537016191706, 'epoch': 0.05}
{'loss': 1.0599, 'grad_norm': 1.899398684501648, 'learning_rate': 0.00017289686274214118, 'epoch': 0.05}
{'loss': 0.977, 'grad_norm': 1.629506230354309, 'learning_rate': 0.0001726574670970976, 'epoch': 0.05}
{'loss': 1.1455, 'grad_norm': 1.5481468439102173, 'learning_rate': 0.00017241718614374678, 'epoch': 0.05}
{'loss': 0.9578, 'grad_norm': 1.2960084676742554, 'learning_rate': 0.00017217602280983623, 'epoch': 0.05}
{'loss': 1.1889, 'grad_norm': 1.4683287143707275, 'learning_rate': 0.0001719339800338651, 'epoch': 0.05}
{'loss': 0.9181, 'grad_norm': 1.4429240226745605, 'learning_rate': 0.0001716910607650483, 'epoch': 0.05}
{'loss': 1.0774, 'grad_norm': 1.422711968421936, 'learning_rate': 0.00017144726796328034, 'epoch': 0.05}
{'loss': 1.1474, 'grad_norm': 1.9845904111862183, 'learning_rate': 0.00017120260459909967, 'epoch': 0.05}
{'loss': 1.0156, 'grad_norm': 1.4484316110610962, 'learning_rate': 0.0001709570736536521, 'epoch': 0.05}
{'loss': 1.005, 'grad_norm': 1.3181140422821045, 'learning_rate': 0.00017071067811865476, 'epoch': 0.05}
{'loss': 0.9709, 'grad_norm': 2.9555530548095703, 'learning_rate': 0.00017046342099635948, 'epoch': 0.05}
{'loss': 1.0672, 'grad_norm': 2.516916513442993, 'learning_rate': 0.00017021530529951625, 'epoch': 0.05}
{'loss': 1.2621, 'grad_norm': 1.761709451675415, 'learning_rate': 0.00016996633405133655, 'epoch': 0.05}
{'loss': 0.9407, 'grad_norm': 1.9980181455612183, 'learning_rate': 0.00016971651028545648, 'epoch': 0.05}
{'loss': 1.0472, 'grad_norm': 2.158527135848999, 'learning_rate': 0.00016946583704589973, 'epoch': 0.05}
{'loss': 1.2532, 'grad_norm': 2.2058675289154053, 'learning_rate': 0.0001692143173870407, 'epoch': 0.05}
{'loss': 1.0257, 'grad_norm': 1.8256880044937134, 'learning_rate': 0.000168961954373567, 'epoch': 0.05}
{'loss': 0.985, 'grad_norm': 1.8545023202896118, 'learning_rate': 0.0001687087510804423, 'epoch': 0.05}
{'loss': 0.8753, 'grad_norm': 1.3300044536590576, 'learning_rate': 0.00016845471059286887, 'epoch': 0.05}
{'loss': 0.8686, 'grad_norm': 1.566195011138916, 'learning_rate': 0.00016819983600624986, 'epoch': 0.05}
{'loss': 0.8214, 'grad_norm': 1.8628350496292114, 'learning_rate': 0.00016794413042615168, 'epoch': 0.05}
{'loss': 1.0019, 'grad_norm': 1.133289098739624, 'learning_rate': 0.00016768759696826608, 'epoch': 0.05}
{'loss': 1.2037, 'grad_norm': 1.380951166152954, 'learning_rate': 0.00016743023875837233, 'epoch': 0.05}
{'loss': 0.9325, 'grad_norm': 1.8397133350372314, 'learning_rate': 0.00016717205893229903, 'epoch': 0.05}
{'loss': 1.3851, 'grad_norm': 1.3174102306365967, 'learning_rate': 0.00016691306063588583, 'epoch': 0.05}
{'loss': 1.0192, 'grad_norm': 1.8271499872207642, 'learning_rate': 0.00016665324702494524, 'epoch': 0.05}
{'loss': 0.863, 'grad_norm': 1.1961365938186646, 'learning_rate': 0.00016639262126522418, 'epoch': 0.05}
{'loss': 0.8009, 'grad_norm': 1.4247599840164185, 'learning_rate': 0.00016613118653236518, 'epoch': 0.05}
{'loss': 1.249, 'grad_norm': 1.1251659393310547, 'learning_rate': 0.00016586894601186805, 'epoch': 0.05}
{'loss': 0.9539, 'grad_norm': 1.3502954244613647, 'learning_rate': 0.00016560590289905073, 'epoch': 0.05}
{'loss': 1.1475, 'grad_norm': 1.2504254579544067, 'learning_rate': 0.00016534206039901057, 'epoch': 0.05}
{'loss': 1.0011, 'grad_norm': 2.943359851837158, 'learning_rate': 0.0001650774217265851, 'epoch': 0.05}
{'loss': 0.8481, 'grad_norm': 1.5723296403884888, 'learning_rate': 0.0001648119901063131, 'epoch': 0.05}
{'loss': 1.0584, 'grad_norm': 1.6956474781036377, 'learning_rate': 0.00016454576877239507, 'epoch': 0.05}
{'loss': 0.9316, 'grad_norm': 1.3757123947143555, 'learning_rate': 0.00016427876096865394, 'epoch': 0.05}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9659, 'grad_norm': 1.88265860080719, 'learning_rate': 0.00016401096994849557, 'epoch': 0.05}
{'loss': 0.9859, 'grad_norm': 1.130794644355774, 'learning_rate': 0.000163742398974869, 'epoch': 0.05}
{'loss': 1.0259, 'grad_norm': 1.8110582828521729, 'learning_rate': 0.00016347305132022677, 'epoch': 0.05}
{'loss': 1.2259, 'grad_norm': 1.4879194498062134, 'learning_rate': 0.0001632029302664851, 'epoch': 0.05}
{'loss': 1.0677, 'grad_norm': 1.2534379959106445, 'learning_rate': 0.00016293203910498376, 'epoch': 0.05}
{'loss': 0.9993, 'grad_norm': 1.7730885744094849, 'learning_rate': 0.00016266038113644607, 'epoch': 0.05}
{'loss': 1.0809, 'grad_norm': 1.2642309665679932, 'learning_rate': 0.00016238795967093864, 'epoch': 0.05}
{'loss': 1.2384, 'grad_norm': 1.7405592203140259, 'learning_rate': 0.00016211477802783103, 'epoch': 0.06}
{'loss': 1.2552, 'grad_norm': 2.4465930461883545, 'learning_rate': 0.0001618408395357554, 'epoch': 0.06}
{'loss': 1.1732, 'grad_norm': 1.4182072877883911, 'learning_rate': 0.0001615661475325658, 'epoch': 0.06}
{'loss': 0.8044, 'grad_norm': 1.4744722843170166, 'learning_rate': 0.00016129070536529766, 'epoch': 0.06}
{'loss': 0.7269, 'grad_norm': 1.5380911827087402, 'learning_rate': 0.0001610145163901268, 'epoch': 0.06}
{'loss': 1.1624, 'grad_norm': 1.5030540227890015, 'learning_rate': 0.00016073758397232868, 'epoch': 0.06}
{'loss': 1.1731, 'grad_norm': 1.4005964994430542, 'learning_rate': 0.0001604599114862375, 'epoch': 0.06}
{'loss': 1.1894, 'grad_norm': 1.3770403861999512, 'learning_rate': 0.00016018150231520486, 'epoch': 0.06}
{'loss': 1.1741, 'grad_norm': 1.5318670272827148, 'learning_rate': 0.0001599023598515586, 'epoch': 0.06}
{'loss': 0.8655, 'grad_norm': 1.4853174686431885, 'learning_rate': 0.0001596224874965616, 'epoch': 0.06}
{'loss': 0.8955, 'grad_norm': 1.3038275241851807, 'learning_rate': 0.00015934188866037016, 'epoch': 0.06}
{'loss': 1.1443, 'grad_norm': 2.2048356533050537, 'learning_rate': 0.00015906056676199255, 'epoch': 0.06}
{'loss': 0.8292, 'grad_norm': 1.577081561088562, 'learning_rate': 0.00015877852522924732, 'epoch': 0.06}
{'loss': 1.2622, 'grad_norm': 2.210615396499634, 'learning_rate': 0.00015849576749872157, 'epoch': 0.06}
{'loss': 1.2784, 'grad_norm': 1.5827924013137817, 'learning_rate': 0.00015821229701572896, 'epoch': 0.06}
{'loss': 0.8019, 'grad_norm': 1.542138695716858, 'learning_rate': 0.0001579281172342679, 'epoch': 0.06}
{'loss': 1.1488, 'grad_norm': 1.4871007204055786, 'learning_rate': 0.00015764323161697935, 'epoch': 0.06}
{'loss': 1.0304, 'grad_norm': 1.8357363939285278, 'learning_rate': 0.0001573576436351046, 'epoch': 0.06}
{'loss': 0.8748, 'grad_norm': 1.7080268859863281, 'learning_rate': 0.0001570713567684432, 'epoch': 0.06}
{'loss': 0.8565, 'grad_norm': 1.1403313875198364, 'learning_rate': 0.00015678437450531013, 'epoch': 0.06}
{'loss': 0.9226, 'grad_norm': 1.3723657131195068, 'learning_rate': 0.0001564967003424938, 'epoch': 0.06}
{'loss': 1.4447, 'grad_norm': 1.4157233238220215, 'learning_rate': 0.00015620833778521307, 'epoch': 0.06}
{'loss': 1.0511, 'grad_norm': 1.6576168537139893, 'learning_rate': 0.0001559192903470747, 'epoch': 0.06}
{'loss': 0.936, 'grad_norm': 1.7866239547729492, 'learning_rate': 0.0001556295615500305, 'epoch': 0.06}
{'loss': 0.9845, 'grad_norm': 1.3505440950393677, 'learning_rate': 0.00015533915492433443, 'epoch': 0.06}
{'loss': 0.9857, 'grad_norm': 1.2951548099517822, 'learning_rate': 0.00015504807400849958, 'epoch': 0.06}
{'loss': 0.944, 'grad_norm': 1.6125048398971558, 'learning_rate': 0.00015475632234925504, 'epoch': 0.06}
{'loss': 0.8159, 'grad_norm': 1.809000015258789, 'learning_rate': 0.00015446390350150273, 'epoch': 0.06}
{'loss': 0.9489, 'grad_norm': 1.5167032480239868, 'learning_rate': 0.000154170821028274, 'epoch': 0.06}
{'loss': 0.9872, 'grad_norm': 1.6411676406860352, 'learning_rate': 0.0001538770785006863, 'epoch': 0.06}
{'loss': 0.8073, 'grad_norm': 1.517822265625, 'learning_rate': 0.00015358267949789966, 'epoch': 0.06}
{'loss': 0.9791, 'grad_norm': 1.395959734916687, 'learning_rate': 0.000153287627607073, 'epoch': 0.06}
{'loss': 0.9139, 'grad_norm': 1.9365533590316772, 'learning_rate': 0.0001529919264233205, 'epoch': 0.06}
{'loss': 0.9741, 'grad_norm': 1.5970731973648071, 'learning_rate': 0.00015269557954966778, 'epoch': 0.06}
{'loss': 0.9775, 'grad_norm': 1.1449216604232788, 'learning_rate': 0.00015239859059700794, 'epoch': 0.06}
{'loss': 1.1299, 'grad_norm': 1.2213515043258667, 'learning_rate': 0.00015210096318405767, 'epoch': 0.06}
{'loss': 1.356, 'grad_norm': 1.414554476737976, 'learning_rate': 0.00015180270093731303, 'epoch': 0.06}
{'loss': 0.8992, 'grad_norm': 1.8053092956542969, 'learning_rate': 0.00015150380749100545, 'epoch': 0.06}
{'loss': 1.0562, 'grad_norm': 1.3373411893844604, 'learning_rate': 0.00015120428648705717, 'epoch': 0.06}
{'loss': 0.835, 'grad_norm': 1.2749733924865723, 'learning_rate': 0.00015090414157503714, 'epoch': 0.06}
{'loss': 0.9407, 'grad_norm': 1.5171788930892944, 'learning_rate': 0.00015060337641211637, 'epoch': 0.06}
{'loss': 1.1357, 'grad_norm': 1.835715651512146, 'learning_rate': 0.00015030199466302353, 'epoch': 0.06}
{'loss': 1.0008, 'grad_norm': 1.547739863395691, 'learning_rate': 0.00015000000000000001, 'epoch': 0.06}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0414, 'grad_norm': 1.1751898527145386, 'learning_rate': 0.00014969739610275556, 'epoch': 0.06}
{'loss': 0.9748, 'grad_norm': 1.6277745962142944, 'learning_rate': 0.0001493941866584231, 'epoch': 0.06}
{'loss': 0.8327, 'grad_norm': 1.423425316810608, 'learning_rate': 0.00014909037536151409, 'epoch': 0.06}
{'loss': 1.1734, 'grad_norm': 1.4977210760116577, 'learning_rate': 0.0001487859659138733, 'epoch': 0.06}
{'loss': 0.8846, 'grad_norm': 1.7138804197311401, 'learning_rate': 0.00014848096202463372, 'epoch': 0.06}
{'loss': 0.9103, 'grad_norm': 2.1036953926086426, 'learning_rate': 0.00014817536741017152, 'epoch': 0.06}
{'loss': 0.8535, 'grad_norm': 2.2773587703704834, 'learning_rate': 0.0001478691857940607, 'epoch': 0.06}
{'loss': 1.0433, 'grad_norm': 1.8890621662139893, 'learning_rate': 0.00014756242090702756, 'epoch': 0.06}
{'loss': 0.8808, 'grad_norm': 1.350174903869629, 'learning_rate': 0.00014725507648690543, 'epoch': 0.06}
{'loss': 1.2044, 'grad_norm': 1.6586300134658813, 'learning_rate': 0.00014694715627858908, 'epoch': 0.06}
{'loss': 0.8198, 'grad_norm': 1.5064598321914673, 'learning_rate': 0.00014663866403398913, 'epoch': 0.06}
{'loss': 0.8193, 'grad_norm': 1.2907294034957886, 'learning_rate': 0.00014632960351198618, 'epoch': 0.06}
{'loss': 1.3978, 'grad_norm': 1.3301597833633423, 'learning_rate': 0.00014601997847838518, 'epoch': 0.06}
{'loss': 0.7283, 'grad_norm': 1.0829880237579346, 'learning_rate': 0.00014570979270586945, 'epoch': 0.06}
{'loss': 1.0481, 'grad_norm': 1.6788253784179688, 'learning_rate': 0.00014539904997395468, 'epoch': 0.06}
{'loss': 1.4422, 'grad_norm': 2.2933974266052246, 'learning_rate': 0.00014508775406894307, 'epoch': 0.06}
{'loss': 1.0007, 'grad_norm': 1.7282419204711914, 'learning_rate': 0.00014477590878387696, 'epoch': 0.06}
{'loss': 0.8212, 'grad_norm': 1.4028772115707397, 'learning_rate': 0.00014446351791849276, 'epoch': 0.06}
{'loss': 0.8505, 'grad_norm': 1.1329972743988037, 'learning_rate': 0.00014415058527917452, 'epoch': 0.06}
{'loss': 1.0269, 'grad_norm': 1.1693284511566162, 'learning_rate': 0.00014383711467890774, 'epoch': 0.06}
{'loss': 1.0636, 'grad_norm': 1.1690528392791748, 'learning_rate': 0.00014352310993723277, 'epoch': 0.06}
{'loss': 0.9194, 'grad_norm': 1.3374254703521729, 'learning_rate': 0.00014320857488019824, 'epoch': 0.06}
{'loss': 1.0324, 'grad_norm': 1.3091275691986084, 'learning_rate': 0.0001428935133403146, 'epoch': 0.07}
{'loss': 0.9206, 'grad_norm': 1.2090526819229126, 'learning_rate': 0.00014257792915650728, 'epoch': 0.07}
{'loss': 0.9719, 'grad_norm': 1.271702766418457, 'learning_rate': 0.00014226182617406996, 'epoch': 0.07}
{'loss': 0.8893, 'grad_norm': 1.6388275623321533, 'learning_rate': 0.00014194520824461771, 'epoch': 0.07}
{'loss': 0.8232, 'grad_norm': 1.4927116632461548, 'learning_rate': 0.00014162807922604012, 'epoch': 0.07}
{'loss': 1.0671, 'grad_norm': 1.4356019496917725, 'learning_rate': 0.0001413104429824542, 'epoch': 0.07}
{'loss': 1.4171, 'grad_norm': 1.7701915502548218, 'learning_rate': 0.00014099230338415728, 'epoch': 0.07}
{'loss': 1.1981, 'grad_norm': 1.400489330291748, 'learning_rate': 0.00014067366430758004, 'epoch': 0.07}
{'loss': 1.1045, 'grad_norm': 1.4386781454086304, 'learning_rate': 0.00014035452963523902, 'epoch': 0.07}
{'loss': 1.025, 'grad_norm': 1.680646538734436, 'learning_rate': 0.00014003490325568954, 'epoch': 0.07}
{'loss': 0.8844, 'grad_norm': 1.8252732753753662, 'learning_rate': 0.00013971478906347806, 'epoch': 0.07}
{'loss': 1.0645, 'grad_norm': 1.3927485942840576, 'learning_rate': 0.00013939419095909512, 'epoch': 0.07}
{'loss': 1.0647, 'grad_norm': 1.56744384765625, 'learning_rate': 0.00013907311284892736, 'epoch': 0.07}
{'loss': 0.9074, 'grad_norm': 1.8944343328475952, 'learning_rate': 0.0001387515586452103, 'epoch': 0.07}
{'loss': 1.0443, 'grad_norm': 1.2155574560165405, 'learning_rate': 0.00013842953226598037, 'epoch': 0.07}
{'loss': 0.9891, 'grad_norm': 1.0912880897521973, 'learning_rate': 0.00013810703763502744, 'epoch': 0.07}
{'loss': 0.9389, 'grad_norm': 1.342526912689209, 'learning_rate': 0.00013778407868184672, 'epoch': 0.07}
{'loss': 1.2139, 'grad_norm': 1.5268638134002686, 'learning_rate': 0.00013746065934159123, 'epoch': 0.07}
{'loss': 1.1555, 'grad_norm': 1.3193426132202148, 'learning_rate': 0.00013713678355502351, 'epoch': 0.07}
{'loss': 1.4197, 'grad_norm': 4.921589374542236, 'learning_rate': 0.00013681245526846783, 'epoch': 0.07}
{'loss': 0.948, 'grad_norm': 1.4464232921600342, 'learning_rate': 0.00013648767843376196, 'epoch': 0.07}
{'loss': 1.0258, 'grad_norm': 1.4070743322372437, 'learning_rate': 0.00013616245700820922, 'epoch': 0.07}
{'loss': 1.4289, 'grad_norm': 1.775256872177124, 'learning_rate': 0.00013583679495453, 'epoch': 0.07}
{'loss': 0.9036, 'grad_norm': 1.5512571334838867, 'learning_rate': 0.0001355106962408137, 'epoch': 0.07}
{'loss': 0.8028, 'grad_norm': 1.154543161392212, 'learning_rate': 0.00013518416484047018, 'epoch': 0.07}
{'loss': 0.7674, 'grad_norm': 1.2431844472885132, 'learning_rate': 0.00013485720473218154, 'epoch': 0.07}
{'loss': 1.1944, 'grad_norm': 1.3078935146331787, 'learning_rate': 0.00013452981989985348, 'epoch': 0.07}
{'loss': 0.9869, 'grad_norm': 1.2797220945358276, 'learning_rate': 0.00013420201433256689, 'epoch': 0.07}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1887, 'grad_norm': 1.232506513595581, 'learning_rate': 0.00013387379202452917, 'epoch': 0.07}
{'loss': 0.8143, 'grad_norm': 2.4321985244750977, 'learning_rate': 0.00013354515697502553, 'epoch': 0.07}
{'loss': 1.0334, 'grad_norm': 1.7432456016540527, 'learning_rate': 0.00013321611318837032, 'epoch': 0.07}
{'loss': 0.8765, 'grad_norm': 1.4072178602218628, 'learning_rate': 0.00013288666467385833, 'epoch': 0.07}
{'loss': 1.004, 'grad_norm': 1.2292715311050415, 'learning_rate': 0.00013255681544571568, 'epoch': 0.07}
{'loss': 1.1553, 'grad_norm': 1.4150240421295166, 'learning_rate': 0.00013222656952305113, 'epoch': 0.07}
{'loss': 0.8138, 'grad_norm': 1.31541109085083, 'learning_rate': 0.00013189593092980702, 'epoch': 0.07}
{'loss': 0.9122, 'grad_norm': 1.1395381689071655, 'learning_rate': 0.00013156490369471027, 'epoch': 0.07}
{'loss': 0.7528, 'grad_norm': 0.9911668300628662, 'learning_rate': 0.00013123349185122327, 'epoch': 0.07}
{'loss': 0.8625, 'grad_norm': 1.3449429273605347, 'learning_rate': 0.00013090169943749476, 'epoch': 0.07}
{'loss': 0.791, 'grad_norm': 1.5571542978286743, 'learning_rate': 0.00013056953049631057, 'epoch': 0.07}
{'loss': 0.7345, 'grad_norm': 1.3045037984848022, 'learning_rate': 0.00013023698907504446, 'epoch': 0.07}
{'loss': 1.0362, 'grad_norm': 1.4182486534118652, 'learning_rate': 0.00012990407922560868, 'epoch': 0.07}
{'loss': 1.0765, 'grad_norm': 1.2173067331314087, 'learning_rate': 0.00012957080500440468, 'epoch': 0.07}
{'loss': 1.1592, 'grad_norm': 1.2264193296432495, 'learning_rate': 0.00012923717047227368, 'epoch': 0.07}
{'loss': 0.9284, 'grad_norm': 1.6153628826141357, 'learning_rate': 0.00012890317969444716, 'epoch': 0.07}
{'loss': 0.875, 'grad_norm': 1.0325373411178589, 'learning_rate': 0.00012856883674049736, 'epoch': 0.07}
{'loss': 0.9907, 'grad_norm': 1.4587386846542358, 'learning_rate': 0.00012823414568428768, 'epoch': 0.07}
{'loss': 1.1221, 'grad_norm': 1.5473488569259644, 'learning_rate': 0.00012789911060392294, 'epoch': 0.07}
{'loss': 0.7023, 'grad_norm': 1.9508103132247925, 'learning_rate': 0.0001275637355816999, 'epoch': 0.07}
{'loss': 0.9017, 'grad_norm': 1.2442095279693604, 'learning_rate': 0.00012722802470405744, 'epoch': 0.07}
{'loss': 0.9924, 'grad_norm': 1.3676159381866455, 'learning_rate': 0.00012689198206152657, 'epoch': 0.07}
{'loss': 1.0704, 'grad_norm': 1.3275189399719238, 'learning_rate': 0.00012655561174868088, 'epoch': 0.07}
{'loss': 0.8503, 'grad_norm': 1.2343168258666992, 'learning_rate': 0.00012621891786408648, 'epoch': 0.07}
{'loss': 0.8974, 'grad_norm': 0.9506278038024902, 'learning_rate': 0.00012588190451025207, 'epoch': 0.07}
{'loss': 0.93, 'grad_norm': 1.5451796054840088, 'learning_rate': 0.00012554457579357905, 'epoch': 0.07}
{'loss': 0.9388, 'grad_norm': 1.2703418731689453, 'learning_rate': 0.0001252069358243114, 'epoch': 0.07}
{'loss': 0.9127, 'grad_norm': 1.3819637298583984, 'learning_rate': 0.0001248689887164855, 'epoch': 0.07}
{'loss': 1.2961, 'grad_norm': 1.5890417098999023, 'learning_rate': 0.00012453073858788026, 'epoch': 0.07}
{'loss': 0.9547, 'grad_norm': 1.6333613395690918, 'learning_rate': 0.00012419218955996676, 'epoch': 0.07}
{'loss': 1.0174, 'grad_norm': 1.1924482583999634, 'learning_rate': 0.0001238533457578581, 'epoch': 0.07}
{'loss': 1.2343, 'grad_norm': 1.5506343841552734, 'learning_rate': 0.000123514211310259, 'epoch': 0.07}
{'loss': 1.0448, 'grad_norm': 1.5382070541381836, 'learning_rate': 0.00012317479034941573, 'epoch': 0.07}
{'loss': 0.7025, 'grad_norm': 1.653214931488037, 'learning_rate': 0.00012283508701106557, 'epoch': 0.07}
{'loss': 1.0177, 'grad_norm': 1.758887767791748, 'learning_rate': 0.0001224951054343865, 'epoch': 0.07}
{'loss': 1.0359, 'grad_norm': 1.104887843132019, 'learning_rate': 0.00012215484976194676, 'epoch': 0.07}
{'loss': 1.1361, 'grad_norm': 1.6831645965576172, 'learning_rate': 0.00012181432413965428, 'epoch': 0.07}
{'loss': 1.0879, 'grad_norm': 1.355843186378479, 'learning_rate': 0.00012147353271670634, 'epoch': 0.08}
{'loss': 0.76, 'grad_norm': 1.301714539527893, 'learning_rate': 0.00012113247964553888, 'epoch': 0.08}
{'loss': 1.0633, 'grad_norm': 1.1599302291870117, 'learning_rate': 0.00012079116908177593, 'epoch': 0.08}
{'loss': 0.8167, 'grad_norm': 1.5905804634094238, 'learning_rate': 0.00012044960518417903, 'epoch': 0.08}
{'loss': 1.2046, 'grad_norm': 1.1762582063674927, 'learning_rate': 0.00012010779211459648, 'epoch': 0.08}
{'loss': 1.0072, 'grad_norm': 2.037667989730835, 'learning_rate': 0.00011976573403791262, 'epoch': 0.08}
{'loss': 1.0646, 'grad_norm': 1.2355445623397827, 'learning_rate': 0.0001194234351219972, 'epoch': 0.08}
{'loss': 0.8853, 'grad_norm': 1.129347324371338, 'learning_rate': 0.00011908089953765449, 'epoch': 0.08}
{'loss': 0.887, 'grad_norm': 1.4175636768341064, 'learning_rate': 0.00011873813145857249, 'epoch': 0.08}
{'loss': 1.1298, 'grad_norm': 2.3113033771514893, 'learning_rate': 0.00011839513506127203, 'epoch': 0.08}
{'loss': 1.0657, 'grad_norm': 1.1695797443389893, 'learning_rate': 0.00011805191452505602, 'epoch': 0.08}
{'loss': 1.1893, 'grad_norm': 1.4832649230957031, 'learning_rate': 0.00011770847403195834, 'epoch': 0.08}
{'loss': 0.943, 'grad_norm': 1.4371100664138794, 'learning_rate': 0.00011736481776669306, 'epoch': 0.08}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.5583, 'grad_norm': 1.3130837678909302, 'learning_rate': 0.00011702094991660326, 'epoch': 0.08}
{'loss': 1.145, 'grad_norm': 1.4653370380401611, 'learning_rate': 0.00011667687467161024, 'epoch': 0.08}
{'loss': 1.1016, 'grad_norm': 1.6275455951690674, 'learning_rate': 0.00011633259622416224, 'epoch': 0.08}
{'loss': 1.0129, 'grad_norm': 1.480555534362793, 'learning_rate': 0.0001159881187691835, 'epoch': 0.08}
{'loss': 0.8889, 'grad_norm': 2.0308218002319336, 'learning_rate': 0.0001156434465040231, 'epoch': 0.08}
{'loss': 1.5612, 'grad_norm': 1.099379301071167, 'learning_rate': 0.00011529858362840382, 'epoch': 0.08}
{'loss': 1.0969, 'grad_norm': 1.139634609222412, 'learning_rate': 0.00011495353434437098, 'epoch': 0.08}
{'loss': 1.0232, 'grad_norm': 1.2196557521820068, 'learning_rate': 0.00011460830285624118, 'epoch': 0.08}
{'loss': 0.9092, 'grad_norm': 1.3609068393707275, 'learning_rate': 0.00011426289337055119, 'epoch': 0.08}
{'loss': 0.9506, 'grad_norm': 1.237328290939331, 'learning_rate': 0.00011391731009600654, 'epoch': 0.08}
{'loss': 1.1472, 'grad_norm': 1.3048192262649536, 'learning_rate': 0.00011357155724343045, 'epoch': 0.08}
{'loss': 0.7136, 'grad_norm': 1.1422134637832642, 'learning_rate': 0.00011322563902571226, 'epoch': 0.08}
{'loss': 0.6764, 'grad_norm': 1.5678613185882568, 'learning_rate': 0.0001128795596577563, 'epoch': 0.08}
{'loss': 1.1198, 'grad_norm': 1.318322777748108, 'learning_rate': 0.00011253332335643043, 'epoch': 0.08}
{'loss': 0.8664, 'grad_norm': 1.9906890392303467, 'learning_rate': 0.00011218693434051475, 'epoch': 0.08}
{'loss': 1.0593, 'grad_norm': 1.2784998416900635, 'learning_rate': 0.00011184039683065013, 'epoch': 0.08}
{'loss': 1.0901, 'grad_norm': 1.2030011415481567, 'learning_rate': 0.00011149371504928668, 'epoch': 0.08}
{'loss': 0.9276, 'grad_norm': 1.0391889810562134, 'learning_rate': 0.00011114689322063255, 'epoch': 0.08}
{'loss': 1.1304, 'grad_norm': 1.2452841997146606, 'learning_rate': 0.0001107999355706023, 'epoch': 0.08}
{'loss': 0.8176, 'grad_norm': 1.2677282094955444, 'learning_rate': 0.00011045284632676536, 'epoch': 0.08}
{'loss': 1.2098, 'grad_norm': 1.3464055061340332, 'learning_rate': 0.00011010562971829463, 'epoch': 0.08}
{'loss': 1.3481, 'grad_norm': 9.887103080749512, 'learning_rate': 0.00010975828997591495, 'epoch': 0.08}
{'loss': 0.8604, 'grad_norm': 1.407334327697754, 'learning_rate': 0.00010941083133185146, 'epoch': 0.08}
{'loss': 0.846, 'grad_norm': 1.6680643558502197, 'learning_rate': 0.00010906325801977804, 'epoch': 0.08}
{'loss': 1.0591, 'grad_norm': 1.333200216293335, 'learning_rate': 0.00010871557427476583, 'epoch': 0.08}
{'loss': 0.9598, 'grad_norm': 1.9059934616088867, 'learning_rate': 0.00010836778433323158, 'epoch': 0.08}
{'loss': 1.0452, 'grad_norm': 1.3286811113357544, 'learning_rate': 0.00010801989243288589, 'epoch': 0.08}
{'loss': 0.683, 'grad_norm': 1.4223864078521729, 'learning_rate': 0.00010767190281268187, 'epoch': 0.08}
{'loss': 0.9691, 'grad_norm': 1.3549538850784302, 'learning_rate': 0.00010732381971276318, 'epoch': 0.08}
{'loss': 1.0821, 'grad_norm': 1.3837981224060059, 'learning_rate': 0.00010697564737441252, 'epoch': 0.08}
{'loss': 0.9523, 'grad_norm': 1.6030298471450806, 'learning_rate': 0.00010662739004000005, 'epoch': 0.08}
{'loss': 0.9789, 'grad_norm': 1.253314733505249, 'learning_rate': 0.00010627905195293135, 'epoch': 0.08}
{'loss': 0.7884, 'grad_norm': 1.3431216478347778, 'learning_rate': 0.00010593063735759618, 'epoch': 0.08}
{'loss': 0.8821, 'grad_norm': 2.0028111934661865, 'learning_rate': 0.00010558215049931638, 'epoch': 0.08}
{'loss': 1.1885, 'grad_norm': 1.7404136657714844, 'learning_rate': 0.0001052335956242944, 'epoch': 0.08}
{'loss': 1.0432, 'grad_norm': 1.431028962135315, 'learning_rate': 0.00010488497697956135, 'epoch': 0.08}
{'loss': 0.9112, 'grad_norm': 1.8623038530349731, 'learning_rate': 0.00010453629881292538, 'epoch': 0.08}
{'loss': 0.8677, 'grad_norm': 1.8891583681106567, 'learning_rate': 0.00010418756537291996, 'epoch': 0.08}
{'loss': 0.8356, 'grad_norm': 1.173452615737915, 'learning_rate': 0.00010383878090875201, 'epoch': 0.08}
{'loss': 1.1046, 'grad_norm': 1.7737228870391846, 'learning_rate': 0.00010348994967025012, 'epoch': 0.08}
{'loss': 1.0393, 'grad_norm': 1.7105817794799805, 'learning_rate': 0.00010314107590781284, 'epoch': 0.08}
{'loss': 1.0758, 'grad_norm': 1.366982340812683, 'learning_rate': 0.0001027921638723569, 'epoch': 0.08}
{'loss': 1.2184, 'grad_norm': 1.673690915107727, 'learning_rate': 0.00010244321781526533, 'epoch': 0.08}
{'loss': 0.7343, 'grad_norm': 1.4857834577560425, 'learning_rate': 0.0001020942419883357, 'epoch': 0.08}
{'loss': 0.9534, 'grad_norm': 1.2383496761322021, 'learning_rate': 0.00010174524064372837, 'epoch': 0.08}
