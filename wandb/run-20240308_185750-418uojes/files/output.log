/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 3.6374, 'grad_norm': 64.722412109375, 'learning_rate': 3.076923076923077e-07, 'epoch': 0.0}
{'loss': 3.2221, 'grad_norm': 74.71922302246094, 'learning_rate': 6.153846153846154e-07, 'epoch': 0.0}
{'loss': 3.2464, 'grad_norm': 104.18841552734375, 'learning_rate': 9.230769230769232e-07, 'epoch': 0.0}
{'loss': 4.0972, 'grad_norm': 83.19877624511719, 'learning_rate': 1.2307692307692308e-06, 'epoch': 0.0}
{'loss': 3.2203, 'grad_norm': 136.02783203125, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.0}
{'loss': 4.4366, 'grad_norm': 58.39970016479492, 'learning_rate': 1.8461538461538465e-06, 'epoch': 0.0}
{'loss': 2.4497, 'grad_norm': 215.81268310546875, 'learning_rate': 2.1538461538461538e-06, 'epoch': 0.0}
{'loss': 4.2038, 'grad_norm': 52.645530700683594, 'learning_rate': 2.4615384615384615e-06, 'epoch': 0.0}
{'loss': 3.0625, 'grad_norm': 25.81698226928711, 'learning_rate': 2.7692307692307693e-06, 'epoch': 0.0}
{'loss': 3.978, 'grad_norm': 71.75670623779297, 'learning_rate': 3.0769230769230774e-06, 'epoch': 0.0}
{'loss': 3.921, 'grad_norm': 67.40164947509766, 'learning_rate': 3.3846153846153848e-06, 'epoch': 0.0}
{'loss': 3.6574, 'grad_norm': 77.10326385498047, 'learning_rate': 3.692307692307693e-06, 'epoch': 0.0}
{'loss': 3.848, 'grad_norm': 45.40496063232422, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}
{'loss': 3.5414, 'grad_norm': 58.61906051635742, 'learning_rate': 4.3076923076923076e-06, 'epoch': 0.0}
{'loss': 3.7807, 'grad_norm': 105.00968933105469, 'learning_rate': 4.615384615384616e-06, 'epoch': 0.0}
{'loss': 3.9693, 'grad_norm': 50.35712814331055, 'learning_rate': 4.923076923076923e-06, 'epoch': 0.0}
{'loss': 4.0873, 'grad_norm': 88.62633514404297, 'learning_rate': 5.230769230769231e-06, 'epoch': 0.0}
{'loss': 3.3444, 'grad_norm': 37.64897155761719, 'learning_rate': 5.5384615384615385e-06, 'epoch': 0.0}
{'loss': 3.7523, 'grad_norm': 38.4095573425293, 'learning_rate': 5.846153846153846e-06, 'epoch': 0.0}
{'loss': 3.5919, 'grad_norm': 43.175628662109375, 'learning_rate': 6.153846153846155e-06, 'epoch': 0.0}
{'loss': 3.8015, 'grad_norm': 45.091819763183594, 'learning_rate': 6.461538461538462e-06, 'epoch': 0.0}
{'loss': 3.3787, 'grad_norm': 38.57903289794922, 'learning_rate': 6.7692307692307695e-06, 'epoch': 0.0}
{'loss': 2.6602, 'grad_norm': 21.468870162963867, 'learning_rate': 7.076923076923076e-06, 'epoch': 0.0}
{'loss': 3.7142, 'grad_norm': 51.814205169677734, 'learning_rate': 7.384615384615386e-06, 'epoch': 0.0}
{'loss': 4.057, 'grad_norm': 46.91228485107422, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.0}
{'loss': 3.4206, 'grad_norm': 29.336477279663086, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}
{'loss': 3.6026, 'grad_norm': 44.78245162963867, 'learning_rate': 8.307692307692307e-06, 'epoch': 0.0}
{'loss': 2.8535, 'grad_norm': 18.5270938873291, 'learning_rate': 8.615384615384615e-06, 'epoch': 0.0}
{'loss': 3.5352, 'grad_norm': 34.03362274169922, 'learning_rate': 8.923076923076923e-06, 'epoch': 0.0}
{'loss': 3.1941, 'grad_norm': 33.687034606933594, 'learning_rate': 9.230769230769232e-06, 'epoch': 0.0}
{'loss': 3.483, 'grad_norm': 37.366146087646484, 'learning_rate': 9.538461538461538e-06, 'epoch': 0.0}
{'loss': 3.2691, 'grad_norm': 40.0904541015625, 'learning_rate': 9.846153846153846e-06, 'epoch': 0.0}
{'loss': 2.7721, 'grad_norm': 79.45584106445312, 'learning_rate': 1.0153846153846154e-05, 'epoch': 0.01}
{'loss': 3.5049, 'grad_norm': 34.66874313354492, 'learning_rate': 1.0461538461538462e-05, 'epoch': 0.01}
{'loss': 3.1165, 'grad_norm': 25.96940040588379, 'learning_rate': 1.0769230769230771e-05, 'epoch': 0.01}
{'loss': 3.0334, 'grad_norm': 31.18891716003418, 'learning_rate': 1.1076923076923077e-05, 'epoch': 0.01}
{'loss': 2.9499, 'grad_norm': 24.81770896911621, 'learning_rate': 1.1384615384615385e-05, 'epoch': 0.01}
{'loss': 2.8361, 'grad_norm': 53.7501335144043, 'learning_rate': 1.1692307692307693e-05, 'epoch': 0.01}
{'loss': 3.1502, 'grad_norm': 33.71907043457031, 'learning_rate': 1.2e-05, 'epoch': 0.01}
{'loss': 3.1992, 'grad_norm': 33.647308349609375, 'learning_rate': 1.230769230769231e-05, 'epoch': 0.01}
{'loss': 3.2196, 'grad_norm': 28.61025619506836, 'learning_rate': 1.2615384615384616e-05, 'epoch': 0.01}
{'loss': 3.062, 'grad_norm': 29.002872467041016, 'learning_rate': 1.2923076923076924e-05, 'epoch': 0.01}
{'loss': 2.9205, 'grad_norm': 27.48299217224121, 'learning_rate': 1.3230769230769233e-05, 'epoch': 0.01}
{'loss': 2.7264, 'grad_norm': 21.673370361328125, 'learning_rate': 1.3538461538461539e-05, 'epoch': 0.01}
{'loss': 2.6701, 'grad_norm': 19.958478927612305, 'learning_rate': 1.3846153846153847e-05, 'epoch': 0.01}
{'loss': 3.0995, 'grad_norm': 29.88567352294922, 'learning_rate': 1.4153846153846153e-05, 'epoch': 0.01}
{'loss': 2.4983, 'grad_norm': 27.991329193115234, 'learning_rate': 1.4461538461538462e-05, 'epoch': 0.01}
{'loss': 2.6171, 'grad_norm': 21.68141746520996, 'learning_rate': 1.4769230769230772e-05, 'epoch': 0.01}
{'loss': 2.7934, 'grad_norm': 28.335935592651367, 'learning_rate': 1.5076923076923078e-05, 'epoch': 0.01}
{'loss': 3.319, 'grad_norm': 32.3875846862793, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.01}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 2.5405, 'grad_norm': 22.8709659576416, 'learning_rate': 1.5692307692307693e-05, 'epoch': 0.01}
{'loss': 2.4185, 'grad_norm': 19.717382431030273, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.01}
{'loss': 3.2117, 'grad_norm': 37.63877487182617, 'learning_rate': 1.630769230769231e-05, 'epoch': 0.01}
{'loss': 2.7029, 'grad_norm': 20.378938674926758, 'learning_rate': 1.6615384615384615e-05, 'epoch': 0.01}
{'loss': 3.2616, 'grad_norm': 36.289363861083984, 'learning_rate': 1.6923076923076924e-05, 'epoch': 0.01}
{'loss': 2.9635, 'grad_norm': 27.211082458496094, 'learning_rate': 1.723076923076923e-05, 'epoch': 0.01}
{'loss': 2.0966, 'grad_norm': 24.97044563293457, 'learning_rate': 1.753846153846154e-05, 'epoch': 0.01}
{'loss': 2.0686, 'grad_norm': 18.750757217407227, 'learning_rate': 1.7846153846153846e-05, 'epoch': 0.01}
{'loss': 3.1343, 'grad_norm': 35.75571823120117, 'learning_rate': 1.8153846153846155e-05, 'epoch': 0.01}
{'loss': 2.5403, 'grad_norm': 23.11212921142578, 'learning_rate': 1.8461538461538465e-05, 'epoch': 0.01}
{'loss': 2.5949, 'grad_norm': 30.32206153869629, 'learning_rate': 1.876923076923077e-05, 'epoch': 0.01}
{'loss': 2.4066, 'grad_norm': 21.50684928894043, 'learning_rate': 1.9076923076923077e-05, 'epoch': 0.01}
{'loss': 2.823, 'grad_norm': 37.752052307128906, 'learning_rate': 1.9384615384615383e-05, 'epoch': 0.01}
{'loss': 2.4323, 'grad_norm': 113.26266479492188, 'learning_rate': 1.9692307692307692e-05, 'epoch': 0.01}
{'loss': 1.8677, 'grad_norm': 23.282453536987305, 'learning_rate': 2e-05, 'epoch': 0.01}
{'loss': 2.6554, 'grad_norm': 23.016345977783203, 'learning_rate': 2.0307692307692308e-05, 'epoch': 0.01}
{'loss': 2.7198, 'grad_norm': 32.921730041503906, 'learning_rate': 2.0615384615384617e-05, 'epoch': 0.01}
{'loss': 3.0308, 'grad_norm': 25.921083450317383, 'learning_rate': 2.0923076923076923e-05, 'epoch': 0.01}
{'loss': 2.7458, 'grad_norm': 23.441476821899414, 'learning_rate': 2.1230769230769233e-05, 'epoch': 0.01}
{'loss': 2.7545, 'grad_norm': 31.896930694580078, 'learning_rate': 2.1538461538461542e-05, 'epoch': 0.01}
{'loss': 2.732, 'grad_norm': 32.9717903137207, 'learning_rate': 2.1846153846153848e-05, 'epoch': 0.01}
{'loss': 2.143, 'grad_norm': 25.504196166992188, 'learning_rate': 2.2153846153846154e-05, 'epoch': 0.01}
{'loss': 1.766, 'grad_norm': 22.709440231323242, 'learning_rate': 2.246153846153846e-05, 'epoch': 0.01}
{'loss': 2.1604, 'grad_norm': 61.06761169433594, 'learning_rate': 2.276923076923077e-05, 'epoch': 0.01}
{'loss': 2.2054, 'grad_norm': 31.242259979248047, 'learning_rate': 2.307692307692308e-05, 'epoch': 0.01}
{'loss': 2.1698, 'grad_norm': 70.62879943847656, 'learning_rate': 2.3384615384615385e-05, 'epoch': 0.01}
{'loss': 2.4673, 'grad_norm': 71.82893371582031, 'learning_rate': 2.3692307692307695e-05, 'epoch': 0.01}
{'loss': 2.3015, 'grad_norm': 19.211013793945312, 'learning_rate': 2.4e-05, 'epoch': 0.01}
{'loss': 1.9285, 'grad_norm': 34.695552825927734, 'learning_rate': 2.430769230769231e-05, 'epoch': 0.01}
{'loss': 2.0505, 'grad_norm': 50.059791564941406, 'learning_rate': 2.461538461538462e-05, 'epoch': 0.01}
{'loss': 1.794, 'grad_norm': 19.26979637145996, 'learning_rate': 2.4923076923076926e-05, 'epoch': 0.01}
{'loss': 1.8318, 'grad_norm': 24.12272071838379, 'learning_rate': 2.523076923076923e-05, 'epoch': 0.01}
{'loss': 2.1681, 'grad_norm': 12.8773832321167, 'learning_rate': 2.5538461538461538e-05, 'epoch': 0.01}
{'loss': 2.2269, 'grad_norm': 19.028034210205078, 'learning_rate': 2.5846153846153847e-05, 'epoch': 0.01}
{'loss': 1.7857, 'grad_norm': 19.112804412841797, 'learning_rate': 2.6153846153846157e-05, 'epoch': 0.01}
{'loss': 1.7931, 'grad_norm': 14.338105201721191, 'learning_rate': 2.6461538461538466e-05, 'epoch': 0.01}
{'loss': 2.0989, 'grad_norm': 17.056093215942383, 'learning_rate': 2.676923076923077e-05, 'epoch': 0.01}
{'loss': 2.0887, 'grad_norm': 46.07561492919922, 'learning_rate': 2.7076923076923078e-05, 'epoch': 0.01}
{'loss': 2.008, 'grad_norm': 27.744827270507812, 'learning_rate': 2.7384615384615387e-05, 'epoch': 0.01}
{'loss': 1.9107, 'grad_norm': 16.50887680053711, 'learning_rate': 2.7692307692307694e-05, 'epoch': 0.01}
{'loss': 1.5092, 'grad_norm': 16.109773635864258, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.01}
{'loss': 1.8768, 'grad_norm': 10.224949836730957, 'learning_rate': 2.8307692307692306e-05, 'epoch': 0.01}
{'loss': 1.8697, 'grad_norm': 13.334637641906738, 'learning_rate': 2.8615384615384615e-05, 'epoch': 0.01}
{'loss': 1.6014, 'grad_norm': 21.377153396606445, 'learning_rate': 2.8923076923076925e-05, 'epoch': 0.01}
{'loss': 1.4002, 'grad_norm': 19.004114151000977, 'learning_rate': 2.9230769230769234e-05, 'epoch': 0.01}
{'loss': 1.5054, 'grad_norm': 29.49471664428711, 'learning_rate': 2.9538461538461543e-05, 'epoch': 0.01}
{'loss': 1.7093, 'grad_norm': 17.898571014404297, 'learning_rate': 2.9846153846153846e-05, 'epoch': 0.01}
{'loss': 2.0161, 'grad_norm': 21.269969940185547, 'learning_rate': 3.0153846153846155e-05, 'epoch': 0.02}
{'loss': 1.5695, 'grad_norm': 64.68475341796875, 'learning_rate': 3.0461538461538465e-05, 'epoch': 0.02}
{'loss': 1.703, 'grad_norm': 23.75892448425293, 'learning_rate': 3.0769230769230774e-05, 'epoch': 0.02}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.6763, 'grad_norm': 19.2735595703125, 'learning_rate': 3.107692307692308e-05, 'epoch': 0.02}
{'loss': 1.5342, 'grad_norm': 18.242942810058594, 'learning_rate': 3.1384615384615386e-05, 'epoch': 0.02}
{'loss': 1.5925, 'grad_norm': 19.510454177856445, 'learning_rate': 3.1692307692307696e-05, 'epoch': 0.02}
{'loss': 1.4337, 'grad_norm': 20.094541549682617, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.02}
{'loss': 1.3787, 'grad_norm': 10.889142990112305, 'learning_rate': 3.230769230769231e-05, 'epoch': 0.02}
{'loss': 1.644, 'grad_norm': 26.036142349243164, 'learning_rate': 3.261538461538462e-05, 'epoch': 0.02}
{'loss': 1.5528, 'grad_norm': 11.76601791381836, 'learning_rate': 3.292307692307692e-05, 'epoch': 0.02}
{'loss': 1.2773, 'grad_norm': 21.484384536743164, 'learning_rate': 3.323076923076923e-05, 'epoch': 0.02}
{'loss': 1.1513, 'grad_norm': 6.795480251312256, 'learning_rate': 3.353846153846154e-05, 'epoch': 0.02}
{'loss': 1.2896, 'grad_norm': 13.485112190246582, 'learning_rate': 3.384615384615385e-05, 'epoch': 0.02}
{'loss': 1.3113, 'grad_norm': 7.056479454040527, 'learning_rate': 3.415384615384615e-05, 'epoch': 0.02}
{'loss': 1.602, 'grad_norm': 20.29337501525879, 'learning_rate': 3.446153846153846e-05, 'epoch': 0.02}
{'loss': 1.3266, 'grad_norm': 10.962586402893066, 'learning_rate': 3.476923076923077e-05, 'epoch': 0.02}
{'loss': 1.2163, 'grad_norm': 24.379581451416016, 'learning_rate': 3.507692307692308e-05, 'epoch': 0.02}
{'loss': 1.3768, 'grad_norm': 13.013374328613281, 'learning_rate': 3.538461538461539e-05, 'epoch': 0.02}
{'loss': 1.3498, 'grad_norm': 7.699305534362793, 'learning_rate': 3.569230769230769e-05, 'epoch': 0.02}
{'loss': 1.2035, 'grad_norm': 17.611099243164062, 'learning_rate': 3.6e-05, 'epoch': 0.02}
{'loss': 1.2181, 'grad_norm': 12.68358325958252, 'learning_rate': 3.630769230769231e-05, 'epoch': 0.02}
{'loss': 1.273, 'grad_norm': 15.584039688110352, 'learning_rate': 3.661538461538462e-05, 'epoch': 0.02}
{'loss': 1.2276, 'grad_norm': 18.321012496948242, 'learning_rate': 3.692307692307693e-05, 'epoch': 0.02}
{'loss': 1.2745, 'grad_norm': 10.514573097229004, 'learning_rate': 3.723076923076923e-05, 'epoch': 0.02}
{'loss': 1.1391, 'grad_norm': 6.86998987197876, 'learning_rate': 3.753846153846154e-05, 'epoch': 0.02}
{'loss': 1.0086, 'grad_norm': 11.694137573242188, 'learning_rate': 3.784615384615385e-05, 'epoch': 0.02}
{'loss': 1.2578, 'grad_norm': 6.436767101287842, 'learning_rate': 3.8153846153846153e-05, 'epoch': 0.02}
{'loss': 1.0929, 'grad_norm': 5.153414249420166, 'learning_rate': 3.846153846153846e-05, 'epoch': 0.02}
{'loss': 1.2173, 'grad_norm': 14.023565292358398, 'learning_rate': 3.8769230769230766e-05, 'epoch': 0.02}
{'loss': 1.2389, 'grad_norm': 6.808423042297363, 'learning_rate': 3.9076923076923075e-05, 'epoch': 0.02}
{'loss': 1.1983, 'grad_norm': 8.696080207824707, 'learning_rate': 3.9384615384615384e-05, 'epoch': 0.02}
{'loss': 0.9673, 'grad_norm': 5.541778087615967, 'learning_rate': 3.9692307692307694e-05, 'epoch': 0.02}
{'loss': 1.0698, 'grad_norm': 4.620294570922852, 'learning_rate': 4e-05, 'epoch': 0.02}
{'loss': 1.0618, 'grad_norm': 8.191863059997559, 'learning_rate': 4.0307692307692306e-05, 'epoch': 0.02}
{'loss': 1.1339, 'grad_norm': 4.060777187347412, 'learning_rate': 4.0615384615384615e-05, 'epoch': 0.02}
{'loss': 1.0204, 'grad_norm': 12.11376953125, 'learning_rate': 4.0923076923076925e-05, 'epoch': 0.02}
{'loss': 1.2094, 'grad_norm': 8.407426834106445, 'learning_rate': 4.1230769230769234e-05, 'epoch': 0.02}
{'loss': 0.8399, 'grad_norm': 9.887214660644531, 'learning_rate': 4.1538461538461544e-05, 'epoch': 0.02}
{'loss': 0.9871, 'grad_norm': 8.854406356811523, 'learning_rate': 4.1846153846153846e-05, 'epoch': 0.02}
{'loss': 1.221, 'grad_norm': 4.5344319343566895, 'learning_rate': 4.2153846153846156e-05, 'epoch': 0.02}
{'loss': 1.1881, 'grad_norm': 8.506698608398438, 'learning_rate': 4.2461538461538465e-05, 'epoch': 0.02}
{'loss': 1.0581, 'grad_norm': 9.925725936889648, 'learning_rate': 4.2769230769230775e-05, 'epoch': 0.02}
{'loss': 1.166, 'grad_norm': 12.536063194274902, 'learning_rate': 4.3076923076923084e-05, 'epoch': 0.02}
{'loss': 1.0842, 'grad_norm': 5.232550621032715, 'learning_rate': 4.338461538461539e-05, 'epoch': 0.02}
{'loss': 0.8827, 'grad_norm': 4.3828887939453125, 'learning_rate': 4.3692307692307696e-05, 'epoch': 0.02}
{'loss': 0.8818, 'grad_norm': 8.95931625366211, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.02}
{'loss': 1.2634, 'grad_norm': 5.369802474975586, 'learning_rate': 4.430769230769231e-05, 'epoch': 0.02}
{'loss': 1.1489, 'grad_norm': 3.2761712074279785, 'learning_rate': 4.461538461538462e-05, 'epoch': 0.02}
{'loss': 0.9563, 'grad_norm': 2.841618061065674, 'learning_rate': 4.492307692307692e-05, 'epoch': 0.02}
{'loss': 1.0708, 'grad_norm': 7.722052574157715, 'learning_rate': 4.523076923076923e-05, 'epoch': 0.02}
{'loss': 1.0739, 'grad_norm': 7.7986321449279785, 'learning_rate': 4.553846153846154e-05, 'epoch': 0.02}
{'loss': 0.8526, 'grad_norm': 12.22533893585205, 'learning_rate': 4.584615384615385e-05, 'epoch': 0.02}
{'loss': 1.258, 'grad_norm': 5.789673805236816, 'learning_rate': 4.615384615384616e-05, 'epoch': 0.02}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0033, 'grad_norm': 5.639941215515137, 'learning_rate': 4.646153846153846e-05, 'epoch': 0.02}
{'loss': 1.0598, 'grad_norm': 5.875913619995117, 'learning_rate': 4.676923076923077e-05, 'epoch': 0.02}
{'loss': 1.2881, 'grad_norm': 22.934600830078125, 'learning_rate': 4.707692307692308e-05, 'epoch': 0.02}
{'loss': 1.2126, 'grad_norm': 4.977294445037842, 'learning_rate': 4.738461538461539e-05, 'epoch': 0.02}
{'loss': 0.9371, 'grad_norm': 4.896605014801025, 'learning_rate': 4.76923076923077e-05, 'epoch': 0.02}
{'loss': 1.0253, 'grad_norm': 7.3372721672058105, 'learning_rate': 4.8e-05, 'epoch': 0.02}
{'loss': 0.866, 'grad_norm': 7.317338466644287, 'learning_rate': 4.830769230769231e-05, 'epoch': 0.02}
{'loss': 0.8106, 'grad_norm': 6.2251200675964355, 'learning_rate': 4.861538461538462e-05, 'epoch': 0.02}
{'loss': 0.8798, 'grad_norm': 4.320887088775635, 'learning_rate': 4.892307692307693e-05, 'epoch': 0.02}
{'loss': 0.9005, 'grad_norm': 179.21006774902344, 'learning_rate': 4.923076923076924e-05, 'epoch': 0.02}
{'loss': 1.3234, 'grad_norm': 12.464905738830566, 'learning_rate': 4.953846153846154e-05, 'epoch': 0.02}
{'loss': 0.9327, 'grad_norm': 4.7549967765808105, 'learning_rate': 4.984615384615385e-05, 'epoch': 0.02}
{'loss': 0.9667, 'grad_norm': 5.083625316619873, 'learning_rate': 5.0153846153846154e-05, 'epoch': 0.03}
{'loss': 1.2662, 'grad_norm': 3.2943460941314697, 'learning_rate': 5.046153846153846e-05, 'epoch': 0.03}
{'loss': 0.7546, 'grad_norm': 6.832736492156982, 'learning_rate': 5.0769230769230766e-05, 'epoch': 0.03}
{'loss': 1.2594, 'grad_norm': 4.056657314300537, 'learning_rate': 5.1076923076923075e-05, 'epoch': 0.03}
{'loss': 0.9061, 'grad_norm': 6.2378058433532715, 'learning_rate': 5.1384615384615385e-05, 'epoch': 0.03}
{'loss': 0.8256, 'grad_norm': 6.737484455108643, 'learning_rate': 5.1692307692307694e-05, 'epoch': 0.03}
{'loss': 1.1362, 'grad_norm': 7.710320949554443, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.03}
{'loss': 1.3377, 'grad_norm': 5.540764808654785, 'learning_rate': 5.230769230769231e-05, 'epoch': 0.03}
{'loss': 1.1341, 'grad_norm': 2.4614241123199463, 'learning_rate': 5.261538461538462e-05, 'epoch': 0.03}
{'loss': 1.2622, 'grad_norm': 9.717498779296875, 'learning_rate': 5.292307692307693e-05, 'epoch': 0.03}
{'loss': 1.0293, 'grad_norm': 3.3136367797851562, 'learning_rate': 5.323076923076923e-05, 'epoch': 0.03}
{'loss': 1.0746, 'grad_norm': 7.543044090270996, 'learning_rate': 5.353846153846154e-05, 'epoch': 0.03}
{'loss': 0.6424, 'grad_norm': 8.032794952392578, 'learning_rate': 5.384615384615385e-05, 'epoch': 0.03}
{'loss': 1.1022, 'grad_norm': 4.607239723205566, 'learning_rate': 5.4153846153846156e-05, 'epoch': 0.03}
{'loss': 0.9936, 'grad_norm': 9.124320983886719, 'learning_rate': 5.4461538461538466e-05, 'epoch': 0.03}
{'loss': 1.1818, 'grad_norm': 6.3197760581970215, 'learning_rate': 5.4769230769230775e-05, 'epoch': 0.03}
{'loss': 0.923, 'grad_norm': 8.298884391784668, 'learning_rate': 5.5076923076923084e-05, 'epoch': 0.03}
{'loss': 1.0238, 'grad_norm': 5.481176853179932, 'learning_rate': 5.538461538461539e-05, 'epoch': 0.03}
{'loss': 1.1831, 'grad_norm': 4.331301689147949, 'learning_rate': 5.5692307692307696e-05, 'epoch': 0.03}
{'loss': 1.0331, 'grad_norm': 5.234412670135498, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.03}
{'loss': 1.1146, 'grad_norm': 5.029254913330078, 'learning_rate': 5.630769230769231e-05, 'epoch': 0.03}
{'loss': 1.0285, 'grad_norm': 4.439224720001221, 'learning_rate': 5.661538461538461e-05, 'epoch': 0.03}
{'loss': 1.2978, 'grad_norm': 3.8574905395507812, 'learning_rate': 5.692307692307692e-05, 'epoch': 0.03}
{'loss': 1.0963, 'grad_norm': 3.230579137802124, 'learning_rate': 5.723076923076923e-05, 'epoch': 0.03}
{'loss': 0.7482, 'grad_norm': 4.848361492156982, 'learning_rate': 5.753846153846154e-05, 'epoch': 0.03}
{'loss': 0.9645, 'grad_norm': 6.695972442626953, 'learning_rate': 5.784615384615385e-05, 'epoch': 0.03}
{'loss': 1.1226, 'grad_norm': 2.870889663696289, 'learning_rate': 5.815384615384616e-05, 'epoch': 0.03}
{'loss': 0.9093, 'grad_norm': 3.782792091369629, 'learning_rate': 5.846153846153847e-05, 'epoch': 0.03}
{'loss': 1.2013, 'grad_norm': 2.2720043659210205, 'learning_rate': 5.876923076923078e-05, 'epoch': 0.03}
{'loss': 0.9487, 'grad_norm': 4.475261211395264, 'learning_rate': 5.907692307692309e-05, 'epoch': 0.03}
{'loss': 0.9537, 'grad_norm': 3.8613526821136475, 'learning_rate': 5.938461538461538e-05, 'epoch': 0.03}
{'loss': 0.8765, 'grad_norm': 3.612821340560913, 'learning_rate': 5.969230769230769e-05, 'epoch': 0.03}
{'loss': 1.0805, 'grad_norm': 3.890805721282959, 'learning_rate': 6e-05, 'epoch': 0.03}
{'loss': 1.1042, 'grad_norm': 2.359055280685425, 'learning_rate': 6.030769230769231e-05, 'epoch': 0.03}
{'loss': 0.9779, 'grad_norm': 4.532672882080078, 'learning_rate': 6.061538461538462e-05, 'epoch': 0.03}
{'loss': 1.1085, 'grad_norm': 4.631585597991943, 'learning_rate': 6.092307692307693e-05, 'epoch': 0.03}
{'loss': 0.9253, 'grad_norm': 4.053799629211426, 'learning_rate': 6.123076923076923e-05, 'epoch': 0.03}
{'loss': 0.9789, 'grad_norm': 2.6742751598358154, 'learning_rate': 6.153846153846155e-05, 'epoch': 0.03}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1224, 'grad_norm': 3.910590410232544, 'learning_rate': 6.184615384615385e-05, 'epoch': 0.03}
{'loss': 1.1707, 'grad_norm': 5.590360641479492, 'learning_rate': 6.215384615384615e-05, 'epoch': 0.03}
{'loss': 1.1381, 'grad_norm': 5.088192462921143, 'learning_rate': 6.246153846153846e-05, 'epoch': 0.03}
{'loss': 1.0199, 'grad_norm': 3.551642656326294, 'learning_rate': 6.276923076923077e-05, 'epoch': 0.03}
{'loss': 0.8091, 'grad_norm': 3.1963319778442383, 'learning_rate': 6.307692307692308e-05, 'epoch': 0.03}
{'loss': 0.8772, 'grad_norm': 3.966951847076416, 'learning_rate': 6.338461538461539e-05, 'epoch': 0.03}
{'loss': 1.0086, 'grad_norm': 2.5346968173980713, 'learning_rate': 6.36923076923077e-05, 'epoch': 0.03}
{'loss': 0.7773, 'grad_norm': 2.486851453781128, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.03}
{'loss': 0.9489, 'grad_norm': 6.281452655792236, 'learning_rate': 6.430769230769231e-05, 'epoch': 0.03}
{'loss': 0.8902, 'grad_norm': 4.386714458465576, 'learning_rate': 6.461538461538462e-05, 'epoch': 0.03}
{'loss': 1.0705, 'grad_norm': 2.813239097595215, 'learning_rate': 6.492307692307693e-05, 'epoch': 0.03}
{'loss': 1.0755, 'grad_norm': 2.646092176437378, 'learning_rate': 6.523076923076923e-05, 'epoch': 0.03}
{'loss': 0.7408, 'grad_norm': 3.098686695098877, 'learning_rate': 6.553846153846154e-05, 'epoch': 0.03}
{'loss': 0.8805, 'grad_norm': 3.6457619667053223, 'learning_rate': 6.584615384615384e-05, 'epoch': 0.03}
{'loss': 1.0144, 'grad_norm': 2.718438148498535, 'learning_rate': 6.615384615384616e-05, 'epoch': 0.03}
{'loss': 0.8542, 'grad_norm': 4.529484748840332, 'learning_rate': 6.646153846153846e-05, 'epoch': 0.03}
{'loss': 0.829, 'grad_norm': 3.604356288909912, 'learning_rate': 6.676923076923078e-05, 'epoch': 0.03}
{'loss': 0.9745, 'grad_norm': 4.111674785614014, 'learning_rate': 6.707692307692308e-05, 'epoch': 0.03}
{'loss': 0.8023, 'grad_norm': 4.3220014572143555, 'learning_rate': 6.73846153846154e-05, 'epoch': 0.03}
{'loss': 0.7598, 'grad_norm': 1.934697151184082, 'learning_rate': 6.76923076923077e-05, 'epoch': 0.03}
{'loss': 1.0377, 'grad_norm': 2.734600305557251, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.03}
{'loss': 0.808, 'grad_norm': 3.7597761154174805, 'learning_rate': 6.83076923076923e-05, 'epoch': 0.03}
{'loss': 0.9167, 'grad_norm': 5.375489234924316, 'learning_rate': 6.861538461538462e-05, 'epoch': 0.03}
{'loss': 1.0104, 'grad_norm': 3.76338267326355, 'learning_rate': 6.892307692307692e-05, 'epoch': 0.03}
{'loss': 0.888, 'grad_norm': 6.107450008392334, 'learning_rate': 6.923076923076924e-05, 'epoch': 0.03}
{'loss': 1.0775, 'grad_norm': 6.024866580963135, 'learning_rate': 6.953846153846154e-05, 'epoch': 0.03}
{'loss': 0.8586, 'grad_norm': 4.904998779296875, 'learning_rate': 6.984615384615386e-05, 'epoch': 0.03}
{'loss': 1.2674, 'grad_norm': 6.129543781280518, 'learning_rate': 7.015384615384616e-05, 'epoch': 0.04}
{'loss': 0.5693, 'grad_norm': 10.6117525100708, 'learning_rate': 7.046153846153846e-05, 'epoch': 0.04}
{'loss': 1.0225, 'grad_norm': 2.9043993949890137, 'learning_rate': 7.076923076923078e-05, 'epoch': 0.04}
{'loss': 0.7487, 'grad_norm': 2.299596071243286, 'learning_rate': 7.107692307692308e-05, 'epoch': 0.04}
{'loss': 0.9649, 'grad_norm': 4.587498188018799, 'learning_rate': 7.138461538461538e-05, 'epoch': 0.04}
{'loss': 1.0349, 'grad_norm': 3.2930610179901123, 'learning_rate': 7.169230769230769e-05, 'epoch': 0.04}
{'loss': 0.976, 'grad_norm': 2.6167075634002686, 'learning_rate': 7.2e-05, 'epoch': 0.04}
{'loss': 1.0585, 'grad_norm': 3.1969168186187744, 'learning_rate': 7.23076923076923e-05, 'epoch': 0.04}
{'loss': 1.164, 'grad_norm': 2.9580931663513184, 'learning_rate': 7.261538461538462e-05, 'epoch': 0.04}
{'loss': 0.8878, 'grad_norm': 9.358880043029785, 'learning_rate': 7.292307692307692e-05, 'epoch': 0.04}
{'loss': 1.015, 'grad_norm': 2.7539005279541016, 'learning_rate': 7.323076923076924e-05, 'epoch': 0.04}
{'loss': 0.9203, 'grad_norm': 5.215342998504639, 'learning_rate': 7.353846153846154e-05, 'epoch': 0.04}
{'loss': 1.0137, 'grad_norm': 5.035269260406494, 'learning_rate': 7.384615384615386e-05, 'epoch': 0.04}
{'loss': 0.7868, 'grad_norm': 3.763012170791626, 'learning_rate': 7.415384615384616e-05, 'epoch': 0.04}
{'loss': 1.0007, 'grad_norm': 2.7721240520477295, 'learning_rate': 7.446153846153846e-05, 'epoch': 0.04}
{'loss': 0.8857, 'grad_norm': 3.9649386405944824, 'learning_rate': 7.476923076923077e-05, 'epoch': 0.04}
{'loss': 0.9693, 'grad_norm': 2.706023693084717, 'learning_rate': 7.507692307692308e-05, 'epoch': 0.04}
{'loss': 0.8764, 'grad_norm': 7.475767135620117, 'learning_rate': 7.538461538461539e-05, 'epoch': 0.04}
{'loss': 0.8801, 'grad_norm': 4.225054740905762, 'learning_rate': 7.56923076923077e-05, 'epoch': 0.04}
{'loss': 1.005, 'grad_norm': 3.8538856506347656, 'learning_rate': 7.6e-05, 'epoch': 0.04}
{'loss': 0.7959, 'grad_norm': 3.3787662982940674, 'learning_rate': 7.630769230769231e-05, 'epoch': 0.04}
{'loss': 1.0981, 'grad_norm': 3.5512049198150635, 'learning_rate': 7.661538461538462e-05, 'epoch': 0.04}
{'loss': 1.0739, 'grad_norm': 3.085447072982788, 'learning_rate': 7.692307692307693e-05, 'epoch': 0.04}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0885, 'grad_norm': 3.3453011512756348, 'learning_rate': 7.723076923076924e-05, 'epoch': 0.04}
{'loss': 0.951, 'grad_norm': 14.009800910949707, 'learning_rate': 7.753846153846153e-05, 'epoch': 0.04}
{'loss': 1.019, 'grad_norm': 3.4315154552459717, 'learning_rate': 7.784615384615385e-05, 'epoch': 0.04}
{'loss': 1.0089, 'grad_norm': 4.749821662902832, 'learning_rate': 7.815384615384615e-05, 'epoch': 0.04}
{'loss': 0.9419, 'grad_norm': 5.180674076080322, 'learning_rate': 7.846153846153847e-05, 'epoch': 0.04}
{'loss': 0.8475, 'grad_norm': 2.3523974418640137, 'learning_rate': 7.876923076923077e-05, 'epoch': 0.04}
{'loss': 1.0115, 'grad_norm': 3.5286734104156494, 'learning_rate': 7.907692307692309e-05, 'epoch': 0.04}
{'loss': 0.7708, 'grad_norm': 2.268716812133789, 'learning_rate': 7.938461538461539e-05, 'epoch': 0.04}
{'loss': 0.9078, 'grad_norm': 5.437304973602295, 'learning_rate': 7.96923076923077e-05, 'epoch': 0.04}
{'loss': 0.8641, 'grad_norm': 3.2058887481689453, 'learning_rate': 8e-05, 'epoch': 0.04}
{'loss': 0.8274, 'grad_norm': 2.19484543800354, 'learning_rate': 8.030769230769231e-05, 'epoch': 0.04}
{'loss': 0.7263, 'grad_norm': 3.2937633991241455, 'learning_rate': 8.061538461538461e-05, 'epoch': 0.04}
{'loss': 1.0531, 'grad_norm': 2.613617420196533, 'learning_rate': 8.092307692307693e-05, 'epoch': 0.04}
{'loss': 1.2229, 'grad_norm': 3.603651523590088, 'learning_rate': 8.123076923076923e-05, 'epoch': 0.04}
{'loss': 0.9376, 'grad_norm': 4.684399127960205, 'learning_rate': 8.153846153846155e-05, 'epoch': 0.04}
{'loss': 1.0052, 'grad_norm': 4.988842964172363, 'learning_rate': 8.184615384615385e-05, 'epoch': 0.04}
{'loss': 1.1946, 'grad_norm': 4.488469123840332, 'learning_rate': 8.215384615384615e-05, 'epoch': 0.04}
{'loss': 1.1526, 'grad_norm': 1.9729937314987183, 'learning_rate': 8.246153846153847e-05, 'epoch': 0.04}
{'loss': 1.099, 'grad_norm': 3.0178158283233643, 'learning_rate': 8.276923076923077e-05, 'epoch': 0.04}
{'loss': 1.0571, 'grad_norm': 3.8612558841705322, 'learning_rate': 8.307692307692309e-05, 'epoch': 0.04}
{'loss': 0.9758, 'grad_norm': 4.660201072692871, 'learning_rate': 8.338461538461538e-05, 'epoch': 0.04}
{'loss': 1.1356, 'grad_norm': 2.644397258758545, 'learning_rate': 8.369230769230769e-05, 'epoch': 0.04}
{'loss': 1.2331, 'grad_norm': 2.7040977478027344, 'learning_rate': 8.4e-05, 'epoch': 0.04}
{'loss': 0.8557, 'grad_norm': 2.586627244949341, 'learning_rate': 8.430769230769231e-05, 'epoch': 0.04}
{'loss': 0.9939, 'grad_norm': 2.958721160888672, 'learning_rate': 8.461538461538461e-05, 'epoch': 0.04}
{'loss': 1.2097, 'grad_norm': 11.450345993041992, 'learning_rate': 8.492307692307693e-05, 'epoch': 0.04}
{'loss': 0.8577, 'grad_norm': 4.215157508850098, 'learning_rate': 8.523076923076923e-05, 'epoch': 0.04}
{'loss': 0.6736, 'grad_norm': 2.7945029735565186, 'learning_rate': 8.553846153846155e-05, 'epoch': 0.04}
{'loss': 0.8879, 'grad_norm': 3.6187853813171387, 'learning_rate': 8.584615384615385e-05, 'epoch': 0.04}
{'loss': 0.7285, 'grad_norm': 2.43741774559021, 'learning_rate': 8.615384615384617e-05, 'epoch': 0.04}
{'loss': 1.0324, 'grad_norm': 3.724083185195923, 'learning_rate': 8.646153846153846e-05, 'epoch': 0.04}
{'loss': 0.7348, 'grad_norm': 2.3236958980560303, 'learning_rate': 8.676923076923077e-05, 'epoch': 0.04}
{'loss': 0.913, 'grad_norm': 3.9003264904022217, 'learning_rate': 8.707692307692308e-05, 'epoch': 0.04}
{'loss': 0.8885, 'grad_norm': 4.11837100982666, 'learning_rate': 8.738461538461539e-05, 'epoch': 0.04}
{'loss': 0.8821, 'grad_norm': 2.5803563594818115, 'learning_rate': 8.76923076923077e-05, 'epoch': 0.04}
{'loss': 1.0268, 'grad_norm': 9.36653995513916, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.04}
{'loss': 0.8944, 'grad_norm': 2.251706123352051, 'learning_rate': 8.830769230769231e-05, 'epoch': 0.04}
{'loss': 0.9, 'grad_norm': 2.6363816261291504, 'learning_rate': 8.861538461538462e-05, 'epoch': 0.04}
{'loss': 1.1449, 'grad_norm': 2.8751561641693115, 'learning_rate': 8.892307692307693e-05, 'epoch': 0.04}
{'loss': 0.9961, 'grad_norm': 5.146117687225342, 'learning_rate': 8.923076923076924e-05, 'epoch': 0.04}
{'loss': 1.0542, 'grad_norm': 2.9707798957824707, 'learning_rate': 8.953846153846154e-05, 'epoch': 0.04}
{'loss': 0.9682, 'grad_norm': 4.708358287811279, 'learning_rate': 8.984615384615384e-05, 'epoch': 0.04}
{'loss': 0.8625, 'grad_norm': 2.044022798538208, 'learning_rate': 9.015384615384616e-05, 'epoch': 0.05}
{'loss': 0.9293, 'grad_norm': 3.784229278564453, 'learning_rate': 9.046153846153846e-05, 'epoch': 0.05}
{'loss': 0.9205, 'grad_norm': 2.7881970405578613, 'learning_rate': 9.076923076923078e-05, 'epoch': 0.05}
{'loss': 1.2624, 'grad_norm': 6.206418991088867, 'learning_rate': 9.107692307692308e-05, 'epoch': 0.05}
{'loss': 1.0418, 'grad_norm': 4.066438674926758, 'learning_rate': 9.13846153846154e-05, 'epoch': 0.05}
{'loss': 1.0343, 'grad_norm': 3.6095619201660156, 'learning_rate': 9.16923076923077e-05, 'epoch': 0.05}
{'loss': 0.9574, 'grad_norm': 2.0965373516082764, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.05}
{'loss': 0.999, 'grad_norm': 8.245369911193848, 'learning_rate': 9.230769230769232e-05, 'epoch': 0.05}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1388, 'grad_norm': 3.0987555980682373, 'learning_rate': 9.261538461538462e-05, 'epoch': 0.05}
{'loss': 1.2941, 'grad_norm': 1.95473051071167, 'learning_rate': 9.292307692307692e-05, 'epoch': 0.05}
{'loss': 0.9148, 'grad_norm': 2.3420183658599854, 'learning_rate': 9.323076923076924e-05, 'epoch': 0.05}
{'loss': 0.7634, 'grad_norm': 2.025991201400757, 'learning_rate': 9.353846153846154e-05, 'epoch': 0.05}
{'loss': 0.8513, 'grad_norm': 2.3301525115966797, 'learning_rate': 9.384615384615386e-05, 'epoch': 0.05}
{'loss': 1.0278, 'grad_norm': 1.9575624465942383, 'learning_rate': 9.415384615384616e-05, 'epoch': 0.05}
{'loss': 0.8219, 'grad_norm': 2.3749754428863525, 'learning_rate': 9.446153846153846e-05, 'epoch': 0.05}
{'loss': 1.0682, 'grad_norm': 3.713651180267334, 'learning_rate': 9.476923076923078e-05, 'epoch': 0.05}
{'loss': 0.806, 'grad_norm': 2.8542613983154297, 'learning_rate': 9.507692307692308e-05, 'epoch': 0.05}
{'loss': 1.0442, 'grad_norm': 24.425931930541992, 'learning_rate': 9.53846153846154e-05, 'epoch': 0.05}
{'loss': 1.1596, 'grad_norm': 3.1369643211364746, 'learning_rate': 9.569230769230769e-05, 'epoch': 0.05}
{'loss': 1.0598, 'grad_norm': 4.723794460296631, 'learning_rate': 9.6e-05, 'epoch': 0.05}
{'loss': 0.7577, 'grad_norm': 3.499819278717041, 'learning_rate': 9.63076923076923e-05, 'epoch': 0.05}
{'loss': 1.0228, 'grad_norm': 3.1017680168151855, 'learning_rate': 9.661538461538462e-05, 'epoch': 0.05}
{'loss': 0.9281, 'grad_norm': 2.594332218170166, 'learning_rate': 9.692307692307692e-05, 'epoch': 0.05}
{'loss': 0.9814, 'grad_norm': 3.411790609359741, 'learning_rate': 9.723076923076924e-05, 'epoch': 0.05}
{'loss': 0.913, 'grad_norm': 2.383404493331909, 'learning_rate': 9.753846153846154e-05, 'epoch': 0.05}
{'loss': 1.0875, 'grad_norm': 2.648702383041382, 'learning_rate': 9.784615384615386e-05, 'epoch': 0.05}
{'loss': 0.9163, 'grad_norm': 1.9920560121536255, 'learning_rate': 9.815384615384616e-05, 'epoch': 0.05}
{'loss': 1.1103, 'grad_norm': 2.577516794204712, 'learning_rate': 9.846153846153848e-05, 'epoch': 0.05}
{'loss': 0.8703, 'grad_norm': 2.573888063430786, 'learning_rate': 9.876923076923077e-05, 'epoch': 0.05}
{'loss': 0.9728, 'grad_norm': 2.3830394744873047, 'learning_rate': 9.907692307692308e-05, 'epoch': 0.05}
{'loss': 1.1038, 'grad_norm': 4.451659679412842, 'learning_rate': 9.938461538461539e-05, 'epoch': 0.05}
{'loss': 0.9429, 'grad_norm': 2.2581331729888916, 'learning_rate': 9.96923076923077e-05, 'epoch': 0.05}
{'loss': 0.9493, 'grad_norm': 2.0423121452331543, 'learning_rate': 0.0001, 'epoch': 0.05}
{'loss': 0.9049, 'grad_norm': 2.720677375793457, 'learning_rate': 0.00010030769230769231, 'epoch': 0.05}
{'loss': 0.9595, 'grad_norm': 3.311129331588745, 'learning_rate': 0.00010061538461538462, 'epoch': 0.05}
{'loss': 1.1812, 'grad_norm': 3.306393623352051, 'learning_rate': 0.00010092307692307693, 'epoch': 0.05}
{'loss': 0.9014, 'grad_norm': 2.845184326171875, 'learning_rate': 0.00010123076923076924, 'epoch': 0.05}
{'loss': 0.9593, 'grad_norm': 5.042331695556641, 'learning_rate': 0.00010153846153846153, 'epoch': 0.05}
{'loss': 1.1686, 'grad_norm': 4.624892234802246, 'learning_rate': 0.00010184615384615386, 'epoch': 0.05}
{'loss': 0.9749, 'grad_norm': 4.33228874206543, 'learning_rate': 0.00010215384615384615, 'epoch': 0.05}
{'loss': 0.9114, 'grad_norm': 2.739119052886963, 'learning_rate': 0.00010246153846153848, 'epoch': 0.05}
{'loss': 0.8615, 'grad_norm': 2.5713422298431396, 'learning_rate': 0.00010276923076923077, 'epoch': 0.05}
{'loss': 0.829, 'grad_norm': 3.255424976348877, 'learning_rate': 0.00010307692307692307, 'epoch': 0.05}
{'loss': 0.8043, 'grad_norm': 2.959717035293579, 'learning_rate': 0.00010338461538461539, 'epoch': 0.05}
{'loss': 0.9317, 'grad_norm': 2.034050703048706, 'learning_rate': 0.00010369230769230769, 'epoch': 0.05}
{'loss': 1.121, 'grad_norm': 2.1937732696533203, 'learning_rate': 0.00010400000000000001, 'epoch': 0.05}
{'loss': 0.8819, 'grad_norm': 2.994358539581299, 'learning_rate': 0.00010430769230769231, 'epoch': 0.05}
{'loss': 1.3078, 'grad_norm': 2.066873073577881, 'learning_rate': 0.00010461538461538463, 'epoch': 0.05}
{'loss': 0.9942, 'grad_norm': 3.397670030593872, 'learning_rate': 0.00010492307692307693, 'epoch': 0.05}
{'loss': 0.8201, 'grad_norm': 1.6963127851486206, 'learning_rate': 0.00010523076923076924, 'epoch': 0.05}
{'loss': 0.7533, 'grad_norm': 2.0622036457061768, 'learning_rate': 0.00010553846153846155, 'epoch': 0.05}
{'loss': 1.1632, 'grad_norm': 1.630811333656311, 'learning_rate': 0.00010584615384615386, 'epoch': 0.05}
{'loss': 0.9053, 'grad_norm': 2.4019601345062256, 'learning_rate': 0.00010615384615384615, 'epoch': 0.05}
{'loss': 1.0795, 'grad_norm': 1.769011378288269, 'learning_rate': 0.00010646153846153846, 'epoch': 0.05}
{'loss': 0.9304, 'grad_norm': 2.7933249473571777, 'learning_rate': 0.00010676923076923077, 'epoch': 0.05}
{'loss': 0.7832, 'grad_norm': 2.4144418239593506, 'learning_rate': 0.00010707692307692307, 'epoch': 0.05}
{'loss': 0.9862, 'grad_norm': 2.4963746070861816, 'learning_rate': 0.00010738461538461539, 'epoch': 0.05}
{'loss': 0.8777, 'grad_norm': 1.7993124723434448, 'learning_rate': 0.0001076923076923077, 'epoch': 0.05}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9654, 'grad_norm': 3.191753387451172, 'learning_rate': 0.00010800000000000001, 'epoch': 0.05}
{'loss': 0.9295, 'grad_norm': 1.6322187185287476, 'learning_rate': 0.00010830769230769231, 'epoch': 0.05}
{'loss': 0.9669, 'grad_norm': 3.2789981365203857, 'learning_rate': 0.00010861538461538463, 'epoch': 0.05}
{'loss': 1.153, 'grad_norm': 2.2883033752441406, 'learning_rate': 0.00010892307692307693, 'epoch': 0.05}
{'loss': 1.0068, 'grad_norm': 1.8679766654968262, 'learning_rate': 0.00010923076923076922, 'epoch': 0.05}
{'loss': 0.8824, 'grad_norm': 3.534550905227661, 'learning_rate': 0.00010953846153846155, 'epoch': 0.05}
{'loss': 1.0195, 'grad_norm': 2.0052103996276855, 'learning_rate': 0.00010984615384615384, 'epoch': 0.05}
{'loss': 1.1403, 'grad_norm': 2.7780797481536865, 'learning_rate': 0.00011015384615384617, 'epoch': 0.06}
{'loss': 1.1302, 'grad_norm': 3.8763673305511475, 'learning_rate': 0.00011046153846153846, 'epoch': 0.06}
{'loss': 1.0938, 'grad_norm': 2.287867307662964, 'learning_rate': 0.00011076923076923077, 'epoch': 0.06}
{'loss': 0.7503, 'grad_norm': 2.141937017440796, 'learning_rate': 0.00011107692307692308, 'epoch': 0.06}
{'loss': 0.7243, 'grad_norm': 3.7995688915252686, 'learning_rate': 0.00011138461538461539, 'epoch': 0.06}
{'loss': 1.0856, 'grad_norm': 2.3222086429595947, 'learning_rate': 0.0001116923076923077, 'epoch': 0.06}
{'loss': 1.0757, 'grad_norm': 1.8978358507156372, 'learning_rate': 0.00011200000000000001, 'epoch': 0.06}
{'loss': 1.1098, 'grad_norm': 1.8237189054489136, 'learning_rate': 0.00011230769230769231, 'epoch': 0.06}
{'loss': 1.1161, 'grad_norm': 2.376523494720459, 'learning_rate': 0.00011261538461538462, 'epoch': 0.06}
{'loss': 0.825, 'grad_norm': 2.4482016563415527, 'learning_rate': 0.00011292307692307693, 'epoch': 0.06}
{'loss': 0.87, 'grad_norm': 2.161010980606079, 'learning_rate': 0.00011323076923076922, 'epoch': 0.06}
{'loss': 1.0223, 'grad_norm': 2.692012310028076, 'learning_rate': 0.00011353846153846155, 'epoch': 0.06}
{'loss': 0.7755, 'grad_norm': 1.9827401638031006, 'learning_rate': 0.00011384615384615384, 'epoch': 0.06}
{'loss': 1.1907, 'grad_norm': 2.46354341506958, 'learning_rate': 0.00011415384615384617, 'epoch': 0.06}
{'loss': 1.2347, 'grad_norm': 2.8788750171661377, 'learning_rate': 0.00011446153846153846, 'epoch': 0.06}
{'loss': 0.6957, 'grad_norm': 2.1540184020996094, 'learning_rate': 0.00011476923076923079, 'epoch': 0.06}
{'loss': 1.1193, 'grad_norm': 1.9482039213180542, 'learning_rate': 0.00011507692307692308, 'epoch': 0.06}
{'loss': 0.9742, 'grad_norm': 3.122997760772705, 'learning_rate': 0.00011538461538461538, 'epoch': 0.06}
{'loss': 0.8494, 'grad_norm': 3.183759927749634, 'learning_rate': 0.0001156923076923077, 'epoch': 0.06}
{'loss': 0.8005, 'grad_norm': 1.5314607620239258, 'learning_rate': 0.000116, 'epoch': 0.06}
{'loss': 0.8527, 'grad_norm': 2.25311279296875, 'learning_rate': 0.00011630769230769232, 'epoch': 0.06}
{'loss': 1.3488, 'grad_norm': 1.8917443752288818, 'learning_rate': 0.00011661538461538462, 'epoch': 0.06}
{'loss': 1.0277, 'grad_norm': 1.801393985748291, 'learning_rate': 0.00011692307692307694, 'epoch': 0.06}
{'loss': 0.9032, 'grad_norm': 2.611473321914673, 'learning_rate': 0.00011723076923076924, 'epoch': 0.06}
{'loss': 0.9465, 'grad_norm': 2.4551711082458496, 'learning_rate': 0.00011753846153846155, 'epoch': 0.06}
{'loss': 0.9512, 'grad_norm': 1.8263441324234009, 'learning_rate': 0.00011784615384615386, 'epoch': 0.06}
{'loss': 0.9056, 'grad_norm': 2.570873975753784, 'learning_rate': 0.00011815384615384617, 'epoch': 0.06}
{'loss': 0.7648, 'grad_norm': 2.8559772968292236, 'learning_rate': 0.00011846153846153846, 'epoch': 0.06}
{'loss': 0.9152, 'grad_norm': 2.975130558013916, 'learning_rate': 0.00011876923076923077, 'epoch': 0.06}
{'loss': 0.9294, 'grad_norm': 2.6044273376464844, 'learning_rate': 0.00011907692307692308, 'epoch': 0.06}
{'loss': 0.7638, 'grad_norm': 1.7723044157028198, 'learning_rate': 0.00011938461538461538, 'epoch': 0.06}
{'loss': 0.9065, 'grad_norm': 1.9116560220718384, 'learning_rate': 0.0001196923076923077, 'epoch': 0.06}
{'loss': 0.8521, 'grad_norm': 2.420919895172119, 'learning_rate': 0.00012, 'epoch': 0.06}
{'loss': 0.8902, 'grad_norm': 1.997434139251709, 'learning_rate': 0.00012030769230769232, 'epoch': 0.06}
{'loss': 0.9128, 'grad_norm': 1.6072814464569092, 'learning_rate': 0.00012061538461538462, 'epoch': 0.06}
{'loss': 1.0791, 'grad_norm': 1.5743712186813354, 'learning_rate': 0.00012092307692307694, 'epoch': 0.06}
{'loss': 1.2536, 'grad_norm': 1.724971055984497, 'learning_rate': 0.00012123076923076924, 'epoch': 0.06}
{'loss': 0.8488, 'grad_norm': 2.741129159927368, 'learning_rate': 0.00012153846153846153, 'epoch': 0.06}
{'loss': 1.0261, 'grad_norm': 1.9132978916168213, 'learning_rate': 0.00012184615384615386, 'epoch': 0.06}
{'loss': 0.7747, 'grad_norm': 1.5773462057113647, 'learning_rate': 0.00012215384615384616, 'epoch': 0.06}
{'loss': 0.8837, 'grad_norm': 2.318063974380493, 'learning_rate': 0.00012246153846153846, 'epoch': 0.06}
{'loss': 1.0342, 'grad_norm': 2.2526276111602783, 'learning_rate': 0.00012276923076923077, 'epoch': 0.06}
{'loss': 0.9671, 'grad_norm': 2.111923933029175, 'learning_rate': 0.0001230769230769231, 'epoch': 0.06}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0051, 'grad_norm': 1.7993967533111572, 'learning_rate': 0.0001233846153846154, 'epoch': 0.06}
{'loss': 0.9674, 'grad_norm': 2.3765604496002197, 'learning_rate': 0.0001236923076923077, 'epoch': 0.06}
{'loss': 0.7927, 'grad_norm': 2.1806437969207764, 'learning_rate': 0.000124, 'epoch': 0.06}
{'loss': 1.116, 'grad_norm': 2.259033441543579, 'learning_rate': 0.0001243076923076923, 'epoch': 0.06}
{'loss': 0.8894, 'grad_norm': 2.972794532775879, 'learning_rate': 0.0001246153846153846, 'epoch': 0.06}
{'loss': 0.8036, 'grad_norm': 2.710792064666748, 'learning_rate': 0.0001249230769230769, 'epoch': 0.06}
{'loss': 0.7796, 'grad_norm': 2.348531484603882, 'learning_rate': 0.00012523076923076924, 'epoch': 0.06}
{'loss': 1.0184, 'grad_norm': 2.24460506439209, 'learning_rate': 0.00012553846153846155, 'epoch': 0.06}
{'loss': 0.8587, 'grad_norm': 2.1444449424743652, 'learning_rate': 0.00012584615384615385, 'epoch': 0.06}
{'loss': 1.161, 'grad_norm': 1.8985570669174194, 'learning_rate': 0.00012615384615384615, 'epoch': 0.06}
{'loss': 0.7989, 'grad_norm': 2.1231436729431152, 'learning_rate': 0.00012646153846153848, 'epoch': 0.06}
{'loss': 0.8352, 'grad_norm': 2.0342867374420166, 'learning_rate': 0.00012676923076923078, 'epoch': 0.06}
{'loss': 1.3356, 'grad_norm': 2.102947950363159, 'learning_rate': 0.00012707692307692309, 'epoch': 0.06}
{'loss': 0.6807, 'grad_norm': 1.393454670906067, 'learning_rate': 0.0001273846153846154, 'epoch': 0.06}
{'loss': 1.0102, 'grad_norm': 3.0563735961914062, 'learning_rate': 0.0001276923076923077, 'epoch': 0.06}
{'loss': 1.3876, 'grad_norm': 2.923844337463379, 'learning_rate': 0.00012800000000000002, 'epoch': 0.06}
{'loss': 0.9486, 'grad_norm': 2.445382595062256, 'learning_rate': 0.0001283076923076923, 'epoch': 0.06}
{'loss': 0.8176, 'grad_norm': 3.335587501525879, 'learning_rate': 0.00012861538461538463, 'epoch': 0.06}
{'loss': 0.7923, 'grad_norm': 1.3495140075683594, 'learning_rate': 0.00012892307692307693, 'epoch': 0.06}
{'loss': 1.0301, 'grad_norm': 2.0518760681152344, 'learning_rate': 0.00012923076923076923, 'epoch': 0.06}
{'loss': 1.0437, 'grad_norm': 2.897170066833496, 'learning_rate': 0.00012953846153846153, 'epoch': 0.06}
{'loss': 0.8785, 'grad_norm': 2.006714344024658, 'learning_rate': 0.00012984615384615386, 'epoch': 0.06}
{'loss': 0.9822, 'grad_norm': 1.6411752700805664, 'learning_rate': 0.00013015384615384617, 'epoch': 0.07}
{'loss': 0.8825, 'grad_norm': 1.6830304861068726, 'learning_rate': 0.00013046153846153847, 'epoch': 0.07}
{'loss': 0.9273, 'grad_norm': 1.8288034200668335, 'learning_rate': 0.00013076923076923077, 'epoch': 0.07}
{'loss': 0.8574, 'grad_norm': 2.3181240558624268, 'learning_rate': 0.00013107692307692308, 'epoch': 0.07}
{'loss': 0.7986, 'grad_norm': 2.116513967514038, 'learning_rate': 0.0001313846153846154, 'epoch': 0.07}
{'loss': 0.9639, 'grad_norm': 3.446901321411133, 'learning_rate': 0.00013169230769230768, 'epoch': 0.07}
{'loss': 1.4299, 'grad_norm': 2.5439321994781494, 'learning_rate': 0.000132, 'epoch': 0.07}
{'loss': 1.1142, 'grad_norm': 1.970528483390808, 'learning_rate': 0.0001323076923076923, 'epoch': 0.07}
{'loss': 1.0515, 'grad_norm': 2.0790083408355713, 'learning_rate': 0.00013261538461538464, 'epoch': 0.07}
{'loss': 0.9717, 'grad_norm': 4.9322590827941895, 'learning_rate': 0.00013292307692307692, 'epoch': 0.07}
{'loss': 0.8043, 'grad_norm': 4.884662628173828, 'learning_rate': 0.00013323076923076925, 'epoch': 0.07}
{'loss': 1.0243, 'grad_norm': 2.628235101699829, 'learning_rate': 0.00013353846153846155, 'epoch': 0.07}
{'loss': 1.0457, 'grad_norm': 3.8267273902893066, 'learning_rate': 0.00013384615384615385, 'epoch': 0.07}
{'loss': 0.8695, 'grad_norm': 4.172523498535156, 'learning_rate': 0.00013415384615384616, 'epoch': 0.07}
{'loss': 1.045, 'grad_norm': 3.7264621257781982, 'learning_rate': 0.00013446153846153846, 'epoch': 0.07}
{'loss': 0.9594, 'grad_norm': 1.8336540460586548, 'learning_rate': 0.0001347692307692308, 'epoch': 0.07}
{'loss': 0.8972, 'grad_norm': 1.890696406364441, 'learning_rate': 0.0001350769230769231, 'epoch': 0.07}
{'loss': 1.22, 'grad_norm': 2.243028402328491, 'learning_rate': 0.0001353846153846154, 'epoch': 0.07}
{'loss': 1.1046, 'grad_norm': 1.7931783199310303, 'learning_rate': 0.0001356923076923077, 'epoch': 0.07}
{'loss': 1.418, 'grad_norm': 8.92876148223877, 'learning_rate': 0.00013600000000000003, 'epoch': 0.07}
{'loss': 0.9001, 'grad_norm': 2.160914182662964, 'learning_rate': 0.0001363076923076923, 'epoch': 0.07}
{'loss': 1.0043, 'grad_norm': 2.0122323036193848, 'learning_rate': 0.0001366153846153846, 'epoch': 0.07}
{'loss': 1.3361, 'grad_norm': 2.257516860961914, 'learning_rate': 0.00013692307692307693, 'epoch': 0.07}
{'loss': 0.8454, 'grad_norm': 1.8914775848388672, 'learning_rate': 0.00013723076923076924, 'epoch': 0.07}
{'loss': 0.7826, 'grad_norm': 1.8251286745071411, 'learning_rate': 0.00013753846153846154, 'epoch': 0.07}
{'loss': 0.7607, 'grad_norm': 2.0061209201812744, 'learning_rate': 0.00013784615384615384, 'epoch': 0.07}
{'loss': 1.1475, 'grad_norm': 1.7054413557052612, 'learning_rate': 0.00013815384615384617, 'epoch': 0.07}
{'loss': 0.9541, 'grad_norm': 1.6913154125213623, 'learning_rate': 0.00013846153846153847, 'epoch': 0.07}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1569, 'grad_norm': 1.557471513748169, 'learning_rate': 0.00013876923076923078, 'epoch': 0.07}
{'loss': 0.789, 'grad_norm': 3.4238088130950928, 'learning_rate': 0.00013907692307692308, 'epoch': 0.07}
{'loss': 0.9692, 'grad_norm': 2.2338778972625732, 'learning_rate': 0.0001393846153846154, 'epoch': 0.07}
{'loss': 0.8513, 'grad_norm': 1.8016303777694702, 'learning_rate': 0.0001396923076923077, 'epoch': 0.07}
{'loss': 0.9873, 'grad_norm': 1.8284507989883423, 'learning_rate': 0.00014, 'epoch': 0.07}
{'loss': 1.0804, 'grad_norm': 1.6690254211425781, 'learning_rate': 0.00014030769230769232, 'epoch': 0.07}
{'loss': 0.8072, 'grad_norm': 1.7679848670959473, 'learning_rate': 0.00014061538461538462, 'epoch': 0.07}
{'loss': 0.8582, 'grad_norm': 1.4391542673110962, 'learning_rate': 0.00014092307692307692, 'epoch': 0.07}
{'loss': 0.7169, 'grad_norm': 1.2194007635116577, 'learning_rate': 0.00014123076923076923, 'epoch': 0.07}
{'loss': 0.8661, 'grad_norm': 1.95350182056427, 'learning_rate': 0.00014153846153846156, 'epoch': 0.07}
{'loss': 0.7589, 'grad_norm': 2.082127332687378, 'learning_rate': 0.00014184615384615386, 'epoch': 0.07}
{'loss': 0.7199, 'grad_norm': 1.7338248491287231, 'learning_rate': 0.00014215384615384616, 'epoch': 0.07}
{'loss': 0.9675, 'grad_norm': 1.7860386371612549, 'learning_rate': 0.00014246153846153846, 'epoch': 0.07}
{'loss': 1.0424, 'grad_norm': 1.6329615116119385, 'learning_rate': 0.00014276923076923077, 'epoch': 0.07}
{'loss': 1.1262, 'grad_norm': 1.7247393131256104, 'learning_rate': 0.0001430769230769231, 'epoch': 0.07}
{'loss': 0.9173, 'grad_norm': 2.215653419494629, 'learning_rate': 0.00014338461538461537, 'epoch': 0.07}
{'loss': 1.1544, 'grad_norm': 2.415081024169922, 'learning_rate': 0.0001436923076923077, 'epoch': 0.07}
{'loss': 0.9271, 'grad_norm': 2.0138051509857178, 'learning_rate': 0.000144, 'epoch': 0.07}
{'loss': 1.1017, 'grad_norm': 2.0709736347198486, 'learning_rate': 0.00014430769230769233, 'epoch': 0.07}
{'loss': 0.6651, 'grad_norm': 2.263148784637451, 'learning_rate': 0.0001446153846153846, 'epoch': 0.07}
{'loss': 0.8975, 'grad_norm': 1.971418023109436, 'learning_rate': 0.00014492307692307694, 'epoch': 0.07}
{'loss': 0.9825, 'grad_norm': 1.8798320293426514, 'learning_rate': 0.00014523076923076924, 'epoch': 0.07}
{'loss': 0.9864, 'grad_norm': 1.8489753007888794, 'learning_rate': 0.00014553846153846154, 'epoch': 0.07}
{'loss': 0.8258, 'grad_norm': 1.5659807920455933, 'learning_rate': 0.00014584615384615385, 'epoch': 0.07}
{'loss': 0.8813, 'grad_norm': 1.2942578792572021, 'learning_rate': 0.00014615384615384615, 'epoch': 0.07}
{'loss': 0.8857, 'grad_norm': 2.197571277618408, 'learning_rate': 0.00014646153846153848, 'epoch': 0.07}
{'loss': 0.8863, 'grad_norm': 1.788681149482727, 'learning_rate': 0.00014676923076923078, 'epoch': 0.07}
{'loss': 0.9286, 'grad_norm': 3.0820095539093018, 'learning_rate': 0.00014707692307692308, 'epoch': 0.07}
{'loss': 1.2571, 'grad_norm': 2.3933308124542236, 'learning_rate': 0.0001473846153846154, 'epoch': 0.07}
{'loss': 0.9467, 'grad_norm': 2.9938924312591553, 'learning_rate': 0.00014769230769230772, 'epoch': 0.07}
{'loss': 0.9766, 'grad_norm': 1.4798877239227295, 'learning_rate': 0.000148, 'epoch': 0.07}
{'loss': 1.1903, 'grad_norm': 2.015042543411255, 'learning_rate': 0.00014830769230769232, 'epoch': 0.07}
{'loss': 0.9885, 'grad_norm': 2.671217203140259, 'learning_rate': 0.00014861538461538462, 'epoch': 0.07}
{'loss': 0.6816, 'grad_norm': 2.059702157974243, 'learning_rate': 0.00014892307692307693, 'epoch': 0.07}
{'loss': 0.9776, 'grad_norm': 2.5731661319732666, 'learning_rate': 0.00014923076923076923, 'epoch': 0.07}
{'loss': 1.0225, 'grad_norm': 1.5972713232040405, 'learning_rate': 0.00014953846153846153, 'epoch': 0.07}
{'loss': 1.0317, 'grad_norm': 2.0532937049865723, 'learning_rate': 0.00014984615384615386, 'epoch': 0.07}
{'loss': 1.0607, 'grad_norm': 1.8649307489395142, 'learning_rate': 0.00015015384615384617, 'epoch': 0.08}
{'loss': 0.7175, 'grad_norm': 1.762597680091858, 'learning_rate': 0.00015046153846153847, 'epoch': 0.08}
{'loss': 1.0495, 'grad_norm': 1.5804071426391602, 'learning_rate': 0.00015076923076923077, 'epoch': 0.08}
{'loss': 0.8196, 'grad_norm': 2.28822660446167, 'learning_rate': 0.0001510769230769231, 'epoch': 0.08}
{'loss': 1.1541, 'grad_norm': 1.4645545482635498, 'learning_rate': 0.0001513846153846154, 'epoch': 0.08}
{'loss': 0.9367, 'grad_norm': 2.16487717628479, 'learning_rate': 0.00015169230769230768, 'epoch': 0.08}
{'loss': 1.0235, 'grad_norm': 1.476569414138794, 'learning_rate': 0.000152, 'epoch': 0.08}
{'loss': 0.8692, 'grad_norm': 1.3386527299880981, 'learning_rate': 0.0001523076923076923, 'epoch': 0.08}
{'loss': 0.8629, 'grad_norm': 1.7980812788009644, 'learning_rate': 0.00015261538461538461, 'epoch': 0.08}
{'loss': 1.132, 'grad_norm': 3.020894765853882, 'learning_rate': 0.00015292307692307692, 'epoch': 0.08}
{'loss': 1.0659, 'grad_norm': 1.742433786392212, 'learning_rate': 0.00015323076923076925, 'epoch': 0.08}
{'loss': 1.1669, 'grad_norm': 2.0196621417999268, 'learning_rate': 0.00015353846153846155, 'epoch': 0.08}
{'loss': 0.9253, 'grad_norm': 1.9801918268203735, 'learning_rate': 0.00015384615384615385, 'epoch': 0.08}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.4846, 'grad_norm': 1.4976673126220703, 'learning_rate': 0.00015415384615384615, 'epoch': 0.08}
{'loss': 1.1134, 'grad_norm': 1.7776939868927002, 'learning_rate': 0.00015446153846153848, 'epoch': 0.08}
{'loss': 1.0903, 'grad_norm': 2.3164854049682617, 'learning_rate': 0.0001547692307692308, 'epoch': 0.08}
{'loss': 1.0248, 'grad_norm': 1.758689045906067, 'learning_rate': 0.00015507692307692306, 'epoch': 0.08}
{'loss': 0.8583, 'grad_norm': 2.4148077964782715, 'learning_rate': 0.0001553846153846154, 'epoch': 0.08}
{'loss': 1.5177, 'grad_norm': 1.3300418853759766, 'learning_rate': 0.0001556923076923077, 'epoch': 0.08}
{'loss': 1.0613, 'grad_norm': 1.43375825881958, 'learning_rate': 0.00015600000000000002, 'epoch': 0.08}
{'loss': 0.9975, 'grad_norm': 2.126352548599243, 'learning_rate': 0.0001563076923076923, 'epoch': 0.08}
{'loss': 0.8918, 'grad_norm': 1.699295163154602, 'learning_rate': 0.00015661538461538463, 'epoch': 0.08}
{'loss': 0.9413, 'grad_norm': 1.903391718864441, 'learning_rate': 0.00015692307692307693, 'epoch': 0.08}
{'loss': 1.1052, 'grad_norm': 1.613694667816162, 'learning_rate': 0.00015723076923076923, 'epoch': 0.08}
{'loss': 0.6951, 'grad_norm': 1.395499348640442, 'learning_rate': 0.00015753846153846154, 'epoch': 0.08}
{'loss': 0.6972, 'grad_norm': 2.060209274291992, 'learning_rate': 0.00015784615384615384, 'epoch': 0.08}
{'loss': 1.1177, 'grad_norm': 2.0233192443847656, 'learning_rate': 0.00015815384615384617, 'epoch': 0.08}
{'loss': 0.8769, 'grad_norm': 2.862499475479126, 'learning_rate': 0.00015846153846153847, 'epoch': 0.08}
{'loss': 1.023, 'grad_norm': 1.6106925010681152, 'learning_rate': 0.00015876923076923078, 'epoch': 0.08}
{'loss': 1.0432, 'grad_norm': 1.432740569114685, 'learning_rate': 0.00015907692307692308, 'epoch': 0.08}
{'loss': 0.8788, 'grad_norm': 1.2961853742599487, 'learning_rate': 0.0001593846153846154, 'epoch': 0.08}
{'loss': 1.0757, 'grad_norm': 1.5081079006195068, 'learning_rate': 0.00015969230769230768, 'epoch': 0.08}
{'loss': 0.8244, 'grad_norm': 1.5852198600769043, 'learning_rate': 0.00016, 'epoch': 0.08}
{'loss': 1.1886, 'grad_norm': 1.6873689889907837, 'learning_rate': 0.00016030769230769232, 'epoch': 0.08}
{'loss': 1.115, 'grad_norm': 15.78837776184082, 'learning_rate': 0.00016061538461538462, 'epoch': 0.08}
{'loss': 0.8746, 'grad_norm': 1.9450310468673706, 'learning_rate': 0.00016092307692307692, 'epoch': 0.08}
{'loss': 0.8792, 'grad_norm': 2.301302433013916, 'learning_rate': 0.00016123076923076922, 'epoch': 0.08}
{'loss': 1.0296, 'grad_norm': 1.7867145538330078, 'learning_rate': 0.00016153846153846155, 'epoch': 0.08}
{'loss': 0.9008, 'grad_norm': 3.5142505168914795, 'learning_rate': 0.00016184615384615386, 'epoch': 0.08}
{'loss': 1.0117, 'grad_norm': 1.7404015064239502, 'learning_rate': 0.00016215384615384616, 'epoch': 0.08}
{'loss': 0.6616, 'grad_norm': 2.0223639011383057, 'learning_rate': 0.00016246153846153846, 'epoch': 0.08}
{'loss': 0.9406, 'grad_norm': 1.6272590160369873, 'learning_rate': 0.0001627692307692308, 'epoch': 0.08}
{'loss': 1.0407, 'grad_norm': 1.6997947692871094, 'learning_rate': 0.0001630769230769231, 'epoch': 0.08}
{'loss': 0.9273, 'grad_norm': 1.8933380842208862, 'learning_rate': 0.0001633846153846154, 'epoch': 0.08}
{'loss': 0.9902, 'grad_norm': 5.056339263916016, 'learning_rate': 0.0001636923076923077, 'epoch': 0.08}
{'loss': 0.8155, 'grad_norm': 2.147843599319458, 'learning_rate': 0.000164, 'epoch': 0.08}
{'loss': 0.96, 'grad_norm': 3.3789541721343994, 'learning_rate': 0.0001643076923076923, 'epoch': 0.08}
{'loss': 1.2222, 'grad_norm': 5.734194278717041, 'learning_rate': 0.0001646153846153846, 'epoch': 0.08}
{'loss': 0.9969, 'grad_norm': 1.8531357049942017, 'learning_rate': 0.00016492307692307694, 'epoch': 0.08}
{'loss': 0.8827, 'grad_norm': 1.9995989799499512, 'learning_rate': 0.00016523076923076924, 'epoch': 0.08}
{'loss': 0.915, 'grad_norm': 3.6014187335968018, 'learning_rate': 0.00016553846153846154, 'epoch': 0.08}
{'loss': 0.8522, 'grad_norm': 1.6256252527236938, 'learning_rate': 0.00016584615384615384, 'epoch': 0.08}
{'loss': 1.1141, 'grad_norm': 2.009568214416504, 'learning_rate': 0.00016615384615384617, 'epoch': 0.08}
{'loss': 1.0006, 'grad_norm': 1.8920977115631104, 'learning_rate': 0.00016646153846153848, 'epoch': 0.08}
{'loss': 1.0653, 'grad_norm': 2.3283793926239014, 'learning_rate': 0.00016676923076923075, 'epoch': 0.08}
{'loss': 1.2331, 'grad_norm': 2.07073974609375, 'learning_rate': 0.00016707692307692308, 'epoch': 0.08}
{'loss': 0.7462, 'grad_norm': 2.5102620124816895, 'learning_rate': 0.00016738461538461539, 'epoch': 0.08}
{'loss': 0.978, 'grad_norm': 1.9257667064666748, 'learning_rate': 0.00016769230769230772, 'epoch': 0.08}
{'loss': 1.0316, 'grad_norm': 2.3179421424865723, 'learning_rate': 0.000168, 'epoch': 0.08}
{'loss': 1.0458, 'grad_norm': 1.9171805381774902, 'learning_rate': 0.00016830769230769232, 'epoch': 0.08}
{'loss': 0.7713, 'grad_norm': 1.8921587467193604, 'learning_rate': 0.00016861538461538462, 'epoch': 0.08}
{'loss': 0.8861, 'grad_norm': 1.3927576541900635, 'learning_rate': 0.00016892307692307695, 'epoch': 0.08}
{'loss': 0.84, 'grad_norm': 1.5007333755493164, 'learning_rate': 0.00016923076923076923, 'epoch': 0.08}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0992, 'grad_norm': 1.6923799514770508, 'learning_rate': 0.00016953846153846156, 'epoch': 0.08}
{'loss': 1.0615, 'grad_norm': 2.376660108566284, 'learning_rate': 0.00016984615384615386, 'epoch': 0.08}
{'loss': 1.0612, 'grad_norm': 2.2892332077026367, 'learning_rate': 0.00017015384615384616, 'epoch': 0.09}
{'loss': 0.9721, 'grad_norm': 2.5158541202545166, 'learning_rate': 0.00017046153846153847, 'epoch': 0.09}
{'loss': 0.8626, 'grad_norm': 1.9733728170394897, 'learning_rate': 0.00017076923076923077, 'epoch': 0.09}
{'loss': 0.8585, 'grad_norm': 1.8598986864089966, 'learning_rate': 0.0001710769230769231, 'epoch': 0.09}
{'loss': 1.0623, 'grad_norm': 1.9779186248779297, 'learning_rate': 0.0001713846153846154, 'epoch': 0.09}
{'loss': 1.0687, 'grad_norm': 1.6740974187850952, 'learning_rate': 0.0001716923076923077, 'epoch': 0.09}
{'loss': 0.8595, 'grad_norm': 1.6587705612182617, 'learning_rate': 0.000172, 'epoch': 0.09}
{'loss': 0.7365, 'grad_norm': 1.372334361076355, 'learning_rate': 0.00017230769230769234, 'epoch': 0.09}
{'loss': 1.1861, 'grad_norm': 1.715681791305542, 'learning_rate': 0.0001726153846153846, 'epoch': 0.09}
{'loss': 1.0141, 'grad_norm': 2.000380754470825, 'learning_rate': 0.00017292307692307691, 'epoch': 0.09}
{'loss': 1.0264, 'grad_norm': 1.8672983646392822, 'learning_rate': 0.00017323076923076924, 'epoch': 0.09}
{'loss': 0.9728, 'grad_norm': 1.4391565322875977, 'learning_rate': 0.00017353846153846155, 'epoch': 0.09}
{'loss': 0.8661, 'grad_norm': 1.7963145971298218, 'learning_rate': 0.00017384615384615385, 'epoch': 0.09}
{'loss': 1.1363, 'grad_norm': 2.1752095222473145, 'learning_rate': 0.00017415384615384615, 'epoch': 0.09}
{'loss': 0.9391, 'grad_norm': 3.171802282333374, 'learning_rate': 0.00017446153846153848, 'epoch': 0.09}
{'loss': 0.7659, 'grad_norm': 2.4356160163879395, 'learning_rate': 0.00017476923076923078, 'epoch': 0.09}
{'loss': 1.1495, 'grad_norm': 1.897735357284546, 'learning_rate': 0.0001750769230769231, 'epoch': 0.09}
{'loss': 0.9134, 'grad_norm': 1.4895482063293457, 'learning_rate': 0.0001753846153846154, 'epoch': 0.09}
{'loss': 1.0539, 'grad_norm': 1.5309360027313232, 'learning_rate': 0.00017569230769230772, 'epoch': 0.09}
{'loss': 0.9438, 'grad_norm': 1.5286284685134888, 'learning_rate': 0.00017600000000000002, 'epoch': 0.09}
{'loss': 0.9527, 'grad_norm': 2.324476718902588, 'learning_rate': 0.0001763076923076923, 'epoch': 0.09}
{'loss': 0.8978, 'grad_norm': 1.5688211917877197, 'learning_rate': 0.00017661538461538463, 'epoch': 0.09}
{'loss': 0.7676, 'grad_norm': 1.2795110940933228, 'learning_rate': 0.00017692307692307693, 'epoch': 0.09}
{'loss': 0.9152, 'grad_norm': 1.9689695835113525, 'learning_rate': 0.00017723076923076923, 'epoch': 0.09}
{'loss': 0.6835, 'grad_norm': 1.950444221496582, 'learning_rate': 0.00017753846153846154, 'epoch': 0.09}
{'loss': 0.9452, 'grad_norm': 1.9780195951461792, 'learning_rate': 0.00017784615384615387, 'epoch': 0.09}
{'loss': 0.9099, 'grad_norm': 1.3830782175064087, 'learning_rate': 0.00017815384615384617, 'epoch': 0.09}
{'loss': 0.7989, 'grad_norm': 1.3554250001907349, 'learning_rate': 0.00017846153846153847, 'epoch': 0.09}
{'loss': 1.1417, 'grad_norm': 1.5495141744613647, 'learning_rate': 0.00017876923076923077, 'epoch': 0.09}
{'loss': 0.9041, 'grad_norm': 1.3815866708755493, 'learning_rate': 0.00017907692307692308, 'epoch': 0.09}
{'loss': 1.0423, 'grad_norm': 1.3519126176834106, 'learning_rate': 0.0001793846153846154, 'epoch': 0.09}
{'loss': 1.0178, 'grad_norm': 1.8335199356079102, 'learning_rate': 0.00017969230769230768, 'epoch': 0.09}
{'loss': 0.8701, 'grad_norm': 2.136841058731079, 'learning_rate': 0.00018, 'epoch': 0.09}
{'loss': 0.7333, 'grad_norm': 1.4739476442337036, 'learning_rate': 0.00018030769230769231, 'epoch': 0.09}
{'loss': 0.9024, 'grad_norm': 1.7194491624832153, 'learning_rate': 0.00018061538461538464, 'epoch': 0.09}
{'loss': 1.3778, 'grad_norm': 2.1619153022766113, 'learning_rate': 0.00018092307692307692, 'epoch': 0.09}
{'loss': 0.6138, 'grad_norm': 1.447661280632019, 'learning_rate': 0.00018123076923076925, 'epoch': 0.09}
{'loss': 1.2826, 'grad_norm': 1.296197533607483, 'learning_rate': 0.00018153846153846155, 'epoch': 0.09}
{'loss': 1.0425, 'grad_norm': 2.387561082839966, 'learning_rate': 0.00018184615384615385, 'epoch': 0.09}
{'loss': 0.8226, 'grad_norm': 1.4981688261032104, 'learning_rate': 0.00018215384615384616, 'epoch': 0.09}
{'loss': 1.2337, 'grad_norm': 1.910456657409668, 'learning_rate': 0.00018246153846153846, 'epoch': 0.09}
{'loss': 1.0142, 'grad_norm': 1.7396466732025146, 'learning_rate': 0.0001827692307692308, 'epoch': 0.09}
{'loss': 0.9975, 'grad_norm': 1.8968596458435059, 'learning_rate': 0.0001830769230769231, 'epoch': 0.09}
{'loss': 1.1453, 'grad_norm': 1.3916617631912231, 'learning_rate': 0.0001833846153846154, 'epoch': 0.09}
{'loss': 1.3076, 'grad_norm': 1.319989800453186, 'learning_rate': 0.0001836923076923077, 'epoch': 0.09}
{'loss': 0.9269, 'grad_norm': 2.0320487022399902, 'learning_rate': 0.00018400000000000003, 'epoch': 0.09}
{'loss': 1.2095, 'grad_norm': 2.494645118713379, 'learning_rate': 0.0001843076923076923, 'epoch': 0.09}
{'loss': 0.9385, 'grad_norm': 2.0074245929718018, 'learning_rate': 0.00018461538461538463, 'epoch': 0.09}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1998, 'grad_norm': 1.5274914503097534, 'learning_rate': 0.00018492307692307694, 'epoch': 0.09}
{'loss': 0.9436, 'grad_norm': 1.6519644260406494, 'learning_rate': 0.00018523076923076924, 'epoch': 0.09}
{'loss': 1.0559, 'grad_norm': 1.8843005895614624, 'learning_rate': 0.00018553846153846154, 'epoch': 0.09}
{'loss': 1.2124, 'grad_norm': 2.1955952644348145, 'learning_rate': 0.00018584615384615384, 'epoch': 0.09}
{'loss': 0.8695, 'grad_norm': 1.270561933517456, 'learning_rate': 0.00018615384615384617, 'epoch': 0.09}
{'loss': 1.4617, 'grad_norm': 3.036747932434082, 'learning_rate': 0.00018646153846153848, 'epoch': 0.09}
{'loss': 1.1751, 'grad_norm': 1.249403476715088, 'learning_rate': 0.00018676923076923078, 'epoch': 0.09}
{'loss': 1.0975, 'grad_norm': 2.675311803817749, 'learning_rate': 0.00018707692307692308, 'epoch': 0.09}
{'loss': 1.0944, 'grad_norm': 1.4213613271713257, 'learning_rate': 0.0001873846153846154, 'epoch': 0.09}
{'loss': 1.2341, 'grad_norm': 1.3495151996612549, 'learning_rate': 0.0001876923076923077, 'epoch': 0.09}
{'loss': 0.959, 'grad_norm': 1.3150250911712646, 'learning_rate': 0.000188, 'epoch': 0.09}
{'loss': 1.2968, 'grad_norm': 1.3665649890899658, 'learning_rate': 0.00018830769230769232, 'epoch': 0.09}
{'loss': 1.1119, 'grad_norm': 2.573582410812378, 'learning_rate': 0.00018861538461538462, 'epoch': 0.09}
{'loss': 0.855, 'grad_norm': 1.3318302631378174, 'learning_rate': 0.00018892307692307692, 'epoch': 0.09}
{'loss': 1.2015, 'grad_norm': 2.003368854522705, 'learning_rate': 0.00018923076923076923, 'epoch': 0.09}
{'loss': 0.9909, 'grad_norm': 1.9266067743301392, 'learning_rate': 0.00018953846153846156, 'epoch': 0.09}
{'loss': 1.178, 'grad_norm': 1.6644936800003052, 'learning_rate': 0.00018984615384615386, 'epoch': 0.09}
{'loss': 0.7302, 'grad_norm': 1.4083530902862549, 'learning_rate': 0.00019015384615384616, 'epoch': 0.1}
{'loss': 0.9716, 'grad_norm': 1.3256585597991943, 'learning_rate': 0.00019046153846153846, 'epoch': 0.1}
{'loss': 0.8933, 'grad_norm': 1.7521425485610962, 'learning_rate': 0.0001907692307692308, 'epoch': 0.1}
{'loss': 1.14, 'grad_norm': 1.9883966445922852, 'learning_rate': 0.0001910769230769231, 'epoch': 0.1}
{'loss': 0.753, 'grad_norm': 1.766703724861145, 'learning_rate': 0.00019138461538461537, 'epoch': 0.1}
{'loss': 0.8866, 'grad_norm': 1.678038477897644, 'learning_rate': 0.0001916923076923077, 'epoch': 0.1}
{'loss': 0.8898, 'grad_norm': 1.3545581102371216, 'learning_rate': 0.000192, 'epoch': 0.1}
{'loss': 1.0797, 'grad_norm': 1.3032575845718384, 'learning_rate': 0.00019230769230769233, 'epoch': 0.1}
{'loss': 1.0883, 'grad_norm': 1.2186976671218872, 'learning_rate': 0.0001926153846153846, 'epoch': 0.1}
{'loss': 1.1984, 'grad_norm': 1.5400168895721436, 'learning_rate': 0.00019292307692307694, 'epoch': 0.1}
{'loss': 0.8756, 'grad_norm': 1.7177917957305908, 'learning_rate': 0.00019323076923076924, 'epoch': 0.1}
{'loss': 1.037, 'grad_norm': 1.6114822626113892, 'learning_rate': 0.00019353846153846155, 'epoch': 0.1}
{'loss': 0.6584, 'grad_norm': 1.460463285446167, 'learning_rate': 0.00019384615384615385, 'epoch': 0.1}
{'loss': 1.0933, 'grad_norm': 2.314605712890625, 'learning_rate': 0.00019415384615384615, 'epoch': 0.1}
{'loss': 0.9146, 'grad_norm': 1.9772852659225464, 'learning_rate': 0.00019446153846153848, 'epoch': 0.1}
{'loss': 1.1228, 'grad_norm': 1.2800157070159912, 'learning_rate': 0.00019476923076923078, 'epoch': 0.1}
{'loss': 1.1365, 'grad_norm': 1.4173706769943237, 'learning_rate': 0.00019507692307692309, 'epoch': 0.1}
{'loss': 0.9377, 'grad_norm': 1.7411198616027832, 'learning_rate': 0.0001953846153846154, 'epoch': 0.1}
{'loss': 0.9054, 'grad_norm': 1.6947311162948608, 'learning_rate': 0.00019569230769230772, 'epoch': 0.1}
{'loss': 0.8785, 'grad_norm': 1.29875648021698, 'learning_rate': 0.000196, 'epoch': 0.1}
{'loss': 1.0458, 'grad_norm': 1.691605806350708, 'learning_rate': 0.00019630769230769232, 'epoch': 0.1}
{'loss': 0.6705, 'grad_norm': 2.155177116394043, 'learning_rate': 0.00019661538461538463, 'epoch': 0.1}
{'loss': 0.8165, 'grad_norm': 1.2629660367965698, 'learning_rate': 0.00019692307692307696, 'epoch': 0.1}
{'loss': 1.044, 'grad_norm': 63.44906234741211, 'learning_rate': 0.00019723076923076923, 'epoch': 0.1}
{'loss': 0.9778, 'grad_norm': 1.6877853870391846, 'learning_rate': 0.00019753846153846153, 'epoch': 0.1}
{'loss': 1.2634, 'grad_norm': 1.8259716033935547, 'learning_rate': 0.00019784615384615386, 'epoch': 0.1}
{'loss': 1.1444, 'grad_norm': 5.4139275550842285, 'learning_rate': 0.00019815384615384617, 'epoch': 0.1}
{'loss': 1.1323, 'grad_norm': 2.098874092102051, 'learning_rate': 0.00019846153846153847, 'epoch': 0.1}
{'loss': 1.0252, 'grad_norm': 2.0820324420928955, 'learning_rate': 0.00019876923076923077, 'epoch': 0.1}
{'loss': 0.9995, 'grad_norm': 1.5973886251449585, 'learning_rate': 0.0001990769230769231, 'epoch': 0.1}
{'loss': 0.7592, 'grad_norm': 1.3163467645645142, 'learning_rate': 0.0001993846153846154, 'epoch': 0.1}
{'loss': 0.9378, 'grad_norm': 1.3505640029907227, 'learning_rate': 0.0001996923076923077, 'epoch': 0.1}
{'loss': 1.0754, 'grad_norm': 1.8983969688415527, 'learning_rate': 0.0002, 'epoch': 0.1}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0147, 'grad_norm': 1.8024122714996338, 'learning_rate': 0.00019999998558024084, 'epoch': 0.1}
{'loss': 1.1185, 'grad_norm': 1.2357978820800781, 'learning_rate': 0.00019999994232096748, 'epoch': 0.1}
{'loss': 0.9757, 'grad_norm': 1.1134074926376343, 'learning_rate': 0.00019999987022219245, 'epoch': 0.1}
{'loss': 0.9976, 'grad_norm': 2.341674566268921, 'learning_rate': 0.00019999976928393646, 'epoch': 0.1}
{'loss': 0.9461, 'grad_norm': 1.3039426803588867, 'learning_rate': 0.00019999963950622867, 'epoch': 0.1}
{'loss': 0.8178, 'grad_norm': 1.6850664615631104, 'learning_rate': 0.00019999948088910655, 'epoch': 0.1}
{'loss': 1.253, 'grad_norm': 1.6802898645401, 'learning_rate': 0.00019999929343261576, 'epoch': 0.1}
{'loss': 0.8416, 'grad_norm': 1.7316935062408447, 'learning_rate': 0.00019999907713681041, 'epoch': 0.1}
{'loss': 1.1375, 'grad_norm': 2.9309792518615723, 'learning_rate': 0.00019999883200175287, 'epoch': 0.1}
{'loss': 1.1786, 'grad_norm': 1.5190891027450562, 'learning_rate': 0.00019999855802751385, 'epoch': 0.1}
{'loss': 0.899, 'grad_norm': 1.4962854385375977, 'learning_rate': 0.00019999825521417234, 'epoch': 0.1}
{'loss': 1.0965, 'grad_norm': 1.2451560497283936, 'learning_rate': 0.00019999792356181566, 'epoch': 0.1}
{'loss': 0.9479, 'grad_norm': 1.5746699571609497, 'learning_rate': 0.00019999756307053948, 'epoch': 0.1}
{'loss': 0.8774, 'grad_norm': 1.5853184461593628, 'learning_rate': 0.00019999717374044777, 'epoch': 0.1}
{'loss': 1.1145, 'grad_norm': 1.469477891921997, 'learning_rate': 0.0001999967555716528, 'epoch': 0.1}
{'loss': 0.5812, 'grad_norm': 1.3781249523162842, 'learning_rate': 0.00019999630856427515, 'epoch': 0.1}
{'loss': 0.8888, 'grad_norm': 1.2461529970169067, 'learning_rate': 0.00019999583271844377, 'epoch': 0.1}
{'loss': 1.0565, 'grad_norm': 1.5150409936904907, 'learning_rate': 0.0001999953280342959, 'epoch': 0.1}
{'loss': 0.972, 'grad_norm': 1.902470350265503, 'learning_rate': 0.000199994794511977, 'epoch': 0.1}
{'loss': 1.1698, 'grad_norm': 1.35813570022583, 'learning_rate': 0.00019999423215164103, 'epoch': 0.1}
{'loss': 1.1112, 'grad_norm': 1.301895022392273, 'learning_rate': 0.00019999364095345015, 'epoch': 0.1}
{'loss': 0.8537, 'grad_norm': 1.4720784425735474, 'learning_rate': 0.00019999302091757484, 'epoch': 0.1}
{'loss': 0.9861, 'grad_norm': 1.2138981819152832, 'learning_rate': 0.00019999237204419392, 'epoch': 0.1}
{'loss': 0.905, 'grad_norm': 1.4151310920715332, 'learning_rate': 0.0001999916943334945, 'epoch': 0.1}
{'loss': 1.1549, 'grad_norm': 1.51947021484375, 'learning_rate': 0.00019999098778567212, 'epoch': 0.1}
{'loss': 1.1431, 'grad_norm': 1.4469521045684814, 'learning_rate': 0.00019999025240093044, 'epoch': 0.1}
{'loss': 0.9876, 'grad_norm': 2.019139051437378, 'learning_rate': 0.00019998948817948157, 'epoch': 0.1}
{'loss': 1.1187, 'grad_norm': 1.9558604955673218, 'learning_rate': 0.00019998869512154595, 'epoch': 0.1}
{'loss': 0.8231, 'grad_norm': 1.2672356367111206, 'learning_rate': 0.0001999878732273522, 'epoch': 0.1}
{'loss': 1.1469, 'grad_norm': 1.3705520629882812, 'learning_rate': 0.00019998702249713748, 'epoch': 0.1}
{'loss': 1.0378, 'grad_norm': 1.2434029579162598, 'learning_rate': 0.00019998614293114707, 'epoch': 0.1}
{'loss': 1.1442, 'grad_norm': 1.8624773025512695, 'learning_rate': 0.00019998523452963458, 'epoch': 0.1}
{'loss': 1.1008, 'grad_norm': 2.02156138420105, 'learning_rate': 0.00019998429729286204, 'epoch': 0.11}
{'loss': 1.2026, 'grad_norm': 1.5045888423919678, 'learning_rate': 0.0001999833312210998, 'epoch': 0.11}
{'loss': 0.9232, 'grad_norm': 1.7630772590637207, 'learning_rate': 0.0001999823363146264, 'epoch': 0.11}
{'loss': 1.0656, 'grad_norm': 1.824290156364441, 'learning_rate': 0.00019998131257372876, 'epoch': 0.11}
{'loss': 0.8244, 'grad_norm': 1.3368701934814453, 'learning_rate': 0.00019998025999870217, 'epoch': 0.11}
{'loss': 0.8501, 'grad_norm': 1.4614274501800537, 'learning_rate': 0.0001999791785898501, 'epoch': 0.11}
{'loss': 0.9592, 'grad_norm': 3.0155303478240967, 'learning_rate': 0.00019997806834748456, 'epoch': 0.11}
{'loss': 1.2378, 'grad_norm': 1.4414355754852295, 'learning_rate': 0.00019997692927192562, 'epoch': 0.11}
{'loss': 0.7173, 'grad_norm': 1.680038332939148, 'learning_rate': 0.00019997576136350184, 'epoch': 0.11}
{'loss': 1.01, 'grad_norm': 1.2295671701431274, 'learning_rate': 0.00019997456462255003, 'epoch': 0.11}
{'loss': 1.3776, 'grad_norm': 1.3012489080429077, 'learning_rate': 0.0001999733390494153, 'epoch': 0.11}
{'loss': 0.7893, 'grad_norm': 1.7283471822738647, 'learning_rate': 0.00019997208464445115, 'epoch': 0.11}
{'loss': 0.9665, 'grad_norm': 1.8666014671325684, 'learning_rate': 0.00019997080140801932, 'epoch': 0.11}
{'loss': 0.6881, 'grad_norm': 1.5078163146972656, 'learning_rate': 0.00019996948934048985, 'epoch': 0.11}
{'loss': 0.8753, 'grad_norm': 1.32041335105896, 'learning_rate': 0.00019996814844224119, 'epoch': 0.11}
{'loss': 0.9662, 'grad_norm': 1.508623719215393, 'learning_rate': 0.00019996677871366, 'epoch': 0.11}
{'loss': 0.9101, 'grad_norm': 1.306587815284729, 'learning_rate': 0.00019996538015514133, 'epoch': 0.11}
{'loss': 0.8673, 'grad_norm': 1.3218530416488647, 'learning_rate': 0.00019996395276708856, 'epoch': 0.11}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9437, 'grad_norm': 2.292161703109741, 'learning_rate': 0.00019996249654991326, 'epoch': 0.11}
{'loss': 0.8569, 'grad_norm': 1.7093970775604248, 'learning_rate': 0.00019996101150403543, 'epoch': 0.11}
{'loss': 1.1354, 'grad_norm': 1.6613014936447144, 'learning_rate': 0.0001999594976298834, 'epoch': 0.11}
{'loss': 1.1213, 'grad_norm': 1.4215868711471558, 'learning_rate': 0.0001999579549278937, 'epoch': 0.11}
{'loss': 0.908, 'grad_norm': 1.4470899105072021, 'learning_rate': 0.0001999563833985112, 'epoch': 0.11}
{'loss': 0.9671, 'grad_norm': 1.9105032682418823, 'learning_rate': 0.00019995478304218924, 'epoch': 0.11}
{'loss': 0.822, 'grad_norm': 1.3139138221740723, 'learning_rate': 0.0001999531538593893, 'epoch': 0.11}
{'loss': 0.9913, 'grad_norm': 1.5776094198226929, 'learning_rate': 0.00019995149585058116, 'epoch': 0.11}
{'loss': 0.9647, 'grad_norm': 1.5596725940704346, 'learning_rate': 0.0001999498090162431, 'epoch': 0.11}
{'loss': 0.8726, 'grad_norm': 1.38222336769104, 'learning_rate': 0.0001999480933568615, 'epoch': 0.11}
{'loss': 0.8377, 'grad_norm': 1.5280524492263794, 'learning_rate': 0.00019994634887293122, 'epoch': 0.11}
{'loss': 0.6744, 'grad_norm': 1.1548511981964111, 'learning_rate': 0.00019994457556495528, 'epoch': 0.11}
{'loss': 1.1634, 'grad_norm': 1.5504497289657593, 'learning_rate': 0.00019994277343344518, 'epoch': 0.11}
{'loss': 0.8962, 'grad_norm': 1.2610235214233398, 'learning_rate': 0.0001999409424789206, 'epoch': 0.11}
{'loss': 1.0323, 'grad_norm': 1.3409267663955688, 'learning_rate': 0.0001999390827019096, 'epoch': 0.11}
{'loss': 0.7869, 'grad_norm': 2.7514712810516357, 'learning_rate': 0.00019993719410294847, 'epoch': 0.11}
{'loss': 0.8165, 'grad_norm': 1.6106312274932861, 'learning_rate': 0.00019993527668258195, 'epoch': 0.11}
{'loss': 0.8659, 'grad_norm': 1.4179152250289917, 'learning_rate': 0.00019993333044136298, 'epoch': 0.11}
{'loss': 0.8329, 'grad_norm': 2.5526490211486816, 'learning_rate': 0.00019993135537985283, 'epoch': 0.11}
{'loss': 0.975, 'grad_norm': 1.1281406879425049, 'learning_rate': 0.00019992935149862115, 'epoch': 0.11}
{'loss': 0.9574, 'grad_norm': 1.317992925643921, 'learning_rate': 0.0001999273187982458, 'epoch': 0.11}
{'loss': 0.5646, 'grad_norm': 1.872959852218628, 'learning_rate': 0.00019992525727931303, 'epoch': 0.11}
{'loss': 0.7923, 'grad_norm': 1.4240026473999023, 'learning_rate': 0.00019992316694241735, 'epoch': 0.11}
{'loss': 1.0694, 'grad_norm': 1.2583953142166138, 'learning_rate': 0.0001999210477881616, 'epoch': 0.11}
{'loss': 0.6924, 'grad_norm': 1.3768393993377686, 'learning_rate': 0.00019991889981715698, 'epoch': 0.11}
{'loss': 1.1587, 'grad_norm': 1.5068329572677612, 'learning_rate': 0.0001999167230300229, 'epoch': 0.11}
{'loss': 0.988, 'grad_norm': 2.4372758865356445, 'learning_rate': 0.00019991451742738716, 'epoch': 0.11}
{'loss': 0.7887, 'grad_norm': 1.7858775854110718, 'learning_rate': 0.00019991228300988585, 'epoch': 0.11}
{'loss': 1.081, 'grad_norm': 1.4266645908355713, 'learning_rate': 0.00019991001977816335, 'epoch': 0.11}
{'loss': 0.8246, 'grad_norm': 1.3034025430679321, 'learning_rate': 0.0001999077277328724, 'epoch': 0.11}
{'loss': 1.1187, 'grad_norm': 1.6968529224395752, 'learning_rate': 0.0001999054068746739, 'epoch': 0.11}
{'loss': 1.1873, 'grad_norm': 1.6334474086761475, 'learning_rate': 0.00019990305720423733, 'epoch': 0.11}
{'loss': 0.9452, 'grad_norm': 1.11919367313385, 'learning_rate': 0.00019990067872224028, 'epoch': 0.11}
{'loss': 0.894, 'grad_norm': 1.7618962526321411, 'learning_rate': 0.00019989827142936862, 'epoch': 0.11}
{'loss': 0.8937, 'grad_norm': 1.2596116065979004, 'learning_rate': 0.00019989583532631667, 'epoch': 0.11}
{'loss': 0.9432, 'grad_norm': 2.6932966709136963, 'learning_rate': 0.00019989337041378697, 'epoch': 0.11}
{'loss': 1.2408, 'grad_norm': 1.7103025913238525, 'learning_rate': 0.00019989087669249037, 'epoch': 0.11}
{'loss': 1.2205, 'grad_norm': 1.398382544517517, 'learning_rate': 0.0001998883541631461, 'epoch': 0.11}
{'loss': 1.1902, 'grad_norm': 1.86851966381073, 'learning_rate': 0.0001998858028264816, 'epoch': 0.11}
{'loss': 1.1737, 'grad_norm': 1.5781821012496948, 'learning_rate': 0.00019988322268323268, 'epoch': 0.11}
{'loss': 1.1809, 'grad_norm': 1.2607630491256714, 'learning_rate': 0.0001998806137341434, 'epoch': 0.11}
{'loss': 1.4101, 'grad_norm': 1.9717563390731812, 'learning_rate': 0.00019987797597996622, 'epoch': 0.11}
{'loss': 0.8302, 'grad_norm': 1.5305848121643066, 'learning_rate': 0.00019987530942146186, 'epoch': 0.11}
{'loss': 1.0085, 'grad_norm': 1.2661550045013428, 'learning_rate': 0.00019987261405939933, 'epoch': 0.11}
{'loss': 1.0841, 'grad_norm': 1.3449962139129639, 'learning_rate': 0.00019986988989455592, 'epoch': 0.11}
{'loss': 0.9304, 'grad_norm': 1.6802899837493896, 'learning_rate': 0.00019986713692771732, 'epoch': 0.11}
{'loss': 0.9932, 'grad_norm': 2.1236863136291504, 'learning_rate': 0.00019986435515967742, 'epoch': 0.11}
{'loss': 1.1196, 'grad_norm': 1.785016417503357, 'learning_rate': 0.00019986154459123855, 'epoch': 0.12}
{'loss': 0.8649, 'grad_norm': 1.5949150323867798, 'learning_rate': 0.00019985870522321118, 'epoch': 0.12}
{'loss': 0.9614, 'grad_norm': 1.859848976135254, 'learning_rate': 0.00019985583705641418, 'epoch': 0.12}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0428, 'grad_norm': 1.2791820764541626, 'learning_rate': 0.0001998529400916748, 'epoch': 0.12}
{'loss': 1.2398, 'grad_norm': 1.0765498876571655, 'learning_rate': 0.0001998500143298284, 'epoch': 0.12}
{'loss': 0.9595, 'grad_norm': 1.3792600631713867, 'learning_rate': 0.0001998470597717188, 'epoch': 0.12}
{'loss': 0.8737, 'grad_norm': 2.5002105236053467, 'learning_rate': 0.00019984407641819812, 'epoch': 0.12}
{'loss': 0.9892, 'grad_norm': 1.5430395603179932, 'learning_rate': 0.00019984106427012668, 'epoch': 0.12}
{'loss': 0.9751, 'grad_norm': 1.2359977960586548, 'learning_rate': 0.0001998380233283732, 'epoch': 0.12}
{'loss': 0.9914, 'grad_norm': 1.5295381546020508, 'learning_rate': 0.0001998349535938147, 'epoch': 0.12}
{'loss': 1.0733, 'grad_norm': 1.6169332265853882, 'learning_rate': 0.0001998318550673364, 'epoch': 0.12}
{'loss': 1.0008, 'grad_norm': 1.5767382383346558, 'learning_rate': 0.000199828727749832, 'epoch': 0.12}
{'loss': 1.3003, 'grad_norm': 1.4554386138916016, 'learning_rate': 0.00019982557164220335, 'epoch': 0.12}
{'loss': 1.1971, 'grad_norm': 1.5679148435592651, 'learning_rate': 0.00019982238674536062, 'epoch': 0.12}
{'loss': 0.9313, 'grad_norm': 1.2052580118179321, 'learning_rate': 0.0001998191730602224, 'epoch': 0.12}
{'loss': 0.9626, 'grad_norm': 1.394109845161438, 'learning_rate': 0.00019981593058771544, 'epoch': 0.12}
{'loss': 0.8451, 'grad_norm': 1.435860514640808, 'learning_rate': 0.00019981265932877488, 'epoch': 0.12}
{'loss': 0.9824, 'grad_norm': 1.670164942741394, 'learning_rate': 0.0001998093592843441, 'epoch': 0.12}
{'loss': 1.1783, 'grad_norm': 1.1126203536987305, 'learning_rate': 0.00019980603045537487, 'epoch': 0.12}
{'loss': 0.9607, 'grad_norm': 1.3685970306396484, 'learning_rate': 0.00019980267284282717, 'epoch': 0.12}
{'loss': 1.1375, 'grad_norm': 1.6369515657424927, 'learning_rate': 0.00019979928644766934, 'epoch': 0.12}
{'loss': 1.1439, 'grad_norm': 1.2412813901901245, 'learning_rate': 0.000199795871270878, 'epoch': 0.12}
{'loss': 1.0963, 'grad_norm': 1.4086564779281616, 'learning_rate': 0.00019979242731343804, 'epoch': 0.12}
{'loss': 1.0062, 'grad_norm': 1.3278038501739502, 'learning_rate': 0.0001997889545763427, 'epoch': 0.12}
{'loss': 1.0797, 'grad_norm': 1.7258987426757812, 'learning_rate': 0.0001997854530605935, 'epoch': 0.12}
{'loss': 1.0735, 'grad_norm': 1.7493324279785156, 'learning_rate': 0.0001997819227672003, 'epoch': 0.12}
{'loss': 0.6898, 'grad_norm': 1.8813985586166382, 'learning_rate': 0.00019977836369718116, 'epoch': 0.12}
{'loss': 0.8408, 'grad_norm': 1.491434931755066, 'learning_rate': 0.00019977477585156252, 'epoch': 0.12}
{'loss': 0.8964, 'grad_norm': 1.7722007036209106, 'learning_rate': 0.00019977115923137912, 'epoch': 0.12}
{'loss': 1.221, 'grad_norm': 1.3553407192230225, 'learning_rate': 0.0001997675138376739, 'epoch': 0.12}
{'loss': 1.0006, 'grad_norm': 1.4838252067565918, 'learning_rate': 0.0001997638396714983, 'epoch': 0.12}
{'loss': 0.8987, 'grad_norm': 1.7790601253509521, 'learning_rate': 0.00019976013673391182, 'epoch': 0.12}
{'loss': 0.7431, 'grad_norm': 1.744990587234497, 'learning_rate': 0.00019975640502598244, 'epoch': 0.12}
{'loss': 1.3147, 'grad_norm': 1.2324422597885132, 'learning_rate': 0.00019975264454878634, 'epoch': 0.12}
{'loss': 0.8849, 'grad_norm': 1.3565186262130737, 'learning_rate': 0.00019974885530340796, 'epoch': 0.12}
{'loss': 1.0803, 'grad_norm': 1.1893442869186401, 'learning_rate': 0.00019974503729094023, 'epoch': 0.12}
{'loss': 1.0304, 'grad_norm': 1.675484538078308, 'learning_rate': 0.00019974119051248415, 'epoch': 0.12}
{'loss': 0.9306, 'grad_norm': 2.276994466781616, 'learning_rate': 0.00019973731496914914, 'epoch': 0.12}
{'loss': 1.1295, 'grad_norm': 1.04015052318573, 'learning_rate': 0.00019973341066205287, 'epoch': 0.12}
{'loss': 0.9645, 'grad_norm': 1.8047701120376587, 'learning_rate': 0.00019972947759232135, 'epoch': 0.12}
{'loss': 0.9746, 'grad_norm': 1.4200326204299927, 'learning_rate': 0.00019972551576108886, 'epoch': 0.12}
{'loss': 1.0323, 'grad_norm': 1.436449408531189, 'learning_rate': 0.00019972152516949796, 'epoch': 0.12}
{'loss': 1.073, 'grad_norm': 1.5532655715942383, 'learning_rate': 0.00019971750581869953, 'epoch': 0.12}
{'loss': 0.9858, 'grad_norm': 1.2012536525726318, 'learning_rate': 0.00019971345770985268, 'epoch': 0.12}
{'loss': 1.0683, 'grad_norm': 1.1282633543014526, 'learning_rate': 0.00019970938084412496, 'epoch': 0.12}
{'loss': 0.6936, 'grad_norm': 1.5175408124923706, 'learning_rate': 0.00019970527522269205, 'epoch': 0.12}
{'loss': 1.0071, 'grad_norm': 1.3709040880203247, 'learning_rate': 0.00019970114084673796, 'epoch': 0.12}
{'loss': 1.14, 'grad_norm': 1.2599736452102661, 'learning_rate': 0.00019969697771745511, 'epoch': 0.12}
{'loss': 1.0601, 'grad_norm': 1.2850791215896606, 'learning_rate': 0.0001996927858360441, 'epoch': 0.12}
{'loss': 0.9398, 'grad_norm': 1.4705368280410767, 'learning_rate': 0.0001996885652037138, 'epoch': 0.12}
{'loss': 0.862, 'grad_norm': 1.5451507568359375, 'learning_rate': 0.0001996843158216815, 'epoch': 0.12}
{'loss': 1.0961, 'grad_norm': 1.2035377025604248, 'learning_rate': 0.00019968003769117262, 'epoch': 0.12}
{'loss': 1.0093, 'grad_norm': 1.8706250190734863, 'learning_rate': 0.00019967573081342103, 'epoch': 0.12}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0211, 'grad_norm': 1.308117389678955, 'learning_rate': 0.0001996713951896687, 'epoch': 0.12}
{'loss': 0.8479, 'grad_norm': 1.5944052934646606, 'learning_rate': 0.00019966703082116615, 'epoch': 0.12}
{'loss': 1.0332, 'grad_norm': 1.4683235883712769, 'learning_rate': 0.00019966263770917193, 'epoch': 0.12}
{'loss': 1.1944, 'grad_norm': 1.291987657546997, 'learning_rate': 0.00019965821585495302, 'epoch': 0.12}
{'loss': 0.9877, 'grad_norm': 1.519616961479187, 'learning_rate': 0.0001996537652597847, 'epoch': 0.12}
{'loss': 0.8697, 'grad_norm': 1.8087204694747925, 'learning_rate': 0.00019964928592495045, 'epoch': 0.12}
{'loss': 1.0681, 'grad_norm': 1.2131649255752563, 'learning_rate': 0.00019964477785174213, 'epoch': 0.12}
{'loss': 1.0844, 'grad_norm': 2.3226981163024902, 'learning_rate': 0.0001996402410414598, 'epoch': 0.12}
{'loss': 0.6712, 'grad_norm': 1.5909810066223145, 'learning_rate': 0.0001996356754954119, 'epoch': 0.12}
{'loss': 1.0028, 'grad_norm': 1.243395447731018, 'learning_rate': 0.00019963108121491508, 'epoch': 0.12}
{'loss': 1.141, 'grad_norm': 1.5905272960662842, 'learning_rate': 0.00019962645820129432, 'epoch': 0.12}
{'loss': 1.3718, 'grad_norm': 1.4754176139831543, 'learning_rate': 0.0001996218064558829, 'epoch': 0.12}
{'loss': 1.0778, 'grad_norm': 0.9680562019348145, 'learning_rate': 0.0001996171259800223, 'epoch': 0.13}
{'loss': 0.9834, 'grad_norm': 1.347025752067566, 'learning_rate': 0.0001996124167750624, 'epoch': 0.13}
{'loss': 0.9514, 'grad_norm': 1.8631850481033325, 'learning_rate': 0.0001996076788423613, 'epoch': 0.13}
{'loss': 0.8583, 'grad_norm': 1.6183234453201294, 'learning_rate': 0.00019960291218328537, 'epoch': 0.13}
{'loss': 1.2175, 'grad_norm': 2.155707836151123, 'learning_rate': 0.00019959811679920938, 'epoch': 0.13}
{'loss': 0.9981, 'grad_norm': 1.874197244644165, 'learning_rate': 0.00019959329269151615, 'epoch': 0.13}
{'loss': 0.9851, 'grad_norm': 1.381471037864685, 'learning_rate': 0.00019958843986159704, 'epoch': 0.13}
{'loss': 0.8772, 'grad_norm': 1.6101073026657104, 'learning_rate': 0.00019958355831085155, 'epoch': 0.13}
{'loss': 0.6378, 'grad_norm': 1.2926005125045776, 'learning_rate': 0.0001995786480406875, 'epoch': 0.13}
{'loss': 0.9927, 'grad_norm': 1.3698816299438477, 'learning_rate': 0.000199573709052521, 'epoch': 0.13}
{'loss': 0.9608, 'grad_norm': 1.241773247718811, 'learning_rate': 0.0001995687413477764, 'epoch': 0.13}
{'loss': 1.0492, 'grad_norm': 1.3298817873001099, 'learning_rate': 0.0001995637449278864, 'epoch': 0.13}
{'loss': 0.8456, 'grad_norm': 1.460778832435608, 'learning_rate': 0.0001995587197942919, 'epoch': 0.13}
{'loss': 1.0057, 'grad_norm': 1.267938256263733, 'learning_rate': 0.00019955366594844213, 'epoch': 0.13}
{'loss': 0.9962, 'grad_norm': 1.2867759466171265, 'learning_rate': 0.00019954858339179464, 'epoch': 0.13}
{'loss': 0.9634, 'grad_norm': 1.5641982555389404, 'learning_rate': 0.00019954347212581514, 'epoch': 0.13}
{'loss': 1.0667, 'grad_norm': 2.0248398780822754, 'learning_rate': 0.0001995383321519778, 'epoch': 0.13}
{'loss': 1.0882, 'grad_norm': 1.6561671495437622, 'learning_rate': 0.00019953316347176488, 'epoch': 0.13}
{'loss': 1.2257, 'grad_norm': 1.2281913757324219, 'learning_rate': 0.000199527966086667, 'epoch': 0.13}
{'loss': 1.048, 'grad_norm': 1.398781657218933, 'learning_rate': 0.0001995227399981831, 'epoch': 0.13}
{'loss': 0.9305, 'grad_norm': 1.5168379545211792, 'learning_rate': 0.0001995174852078204, 'epoch': 0.13}
{'loss': 0.7387, 'grad_norm': 1.7407909631729126, 'learning_rate': 0.00019951220171709425, 'epoch': 0.13}
{'loss': 1.0735, 'grad_norm': 1.4266233444213867, 'learning_rate': 0.00019950688952752844, 'epoch': 0.13}
{'loss': 0.9079, 'grad_norm': 1.3865211009979248, 'learning_rate': 0.00019950154864065497, 'epoch': 0.13}
{'loss': 0.8713, 'grad_norm': 1.4956470727920532, 'learning_rate': 0.00019949617905801415, 'epoch': 0.13}
{'loss': 0.9967, 'grad_norm': 1.6718413829803467, 'learning_rate': 0.0001994907807811545, 'epoch': 0.13}
{'loss': 1.0923, 'grad_norm': 1.2091566324234009, 'learning_rate': 0.00019948535381163288, 'epoch': 0.13}
{'loss': 0.9693, 'grad_norm': 1.4571723937988281, 'learning_rate': 0.00019947989815101444, 'epoch': 0.13}
{'loss': 1.0309, 'grad_norm': 1.4129884243011475, 'learning_rate': 0.0001994744138008725, 'epoch': 0.13}
{'loss': 0.776, 'grad_norm': 1.707009196281433, 'learning_rate': 0.00019946890076278875, 'epoch': 0.13}
{'loss': 0.9169, 'grad_norm': 1.7338647842407227, 'learning_rate': 0.00019946335903835315, 'epoch': 0.13}
{'loss': 0.8038, 'grad_norm': 1.3284353017807007, 'learning_rate': 0.00019945778862916384, 'epoch': 0.13}
{'loss': 0.879, 'grad_norm': 1.7663674354553223, 'learning_rate': 0.00019945218953682734, 'epoch': 0.13}
{'loss': 0.8883, 'grad_norm': 1.6978565454483032, 'learning_rate': 0.0001994465617629584, 'epoch': 0.13}
{'loss': 1.0151, 'grad_norm': 1.3294854164123535, 'learning_rate': 0.00019944090530918005, 'epoch': 0.13}
{'loss': 0.8111, 'grad_norm': 1.5660514831542969, 'learning_rate': 0.00019943522017712358, 'epoch': 0.13}
{'loss': 0.8507, 'grad_norm': 1.714308500289917, 'learning_rate': 0.00019942950636842857, 'epoch': 0.13}
{'loss': 0.7958, 'grad_norm': 1.7570416927337646, 'learning_rate': 0.0001994237638847428, 'epoch': 0.13}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9228, 'grad_norm': 1.4737155437469482, 'learning_rate': 0.00019941799272772245, 'epoch': 0.13}
{'loss': 1.0486, 'grad_norm': 1.3893227577209473, 'learning_rate': 0.00019941219289903178, 'epoch': 0.13}
{'loss': 0.7706, 'grad_norm': 1.7676200866699219, 'learning_rate': 0.00019940636440034357, 'epoch': 0.13}
{'loss': 0.8573, 'grad_norm': 2.096855878829956, 'learning_rate': 0.00019940050723333866, 'epoch': 0.13}
{'loss': 1.2903, 'grad_norm': 1.295785665512085, 'learning_rate': 0.00019939462139970622, 'epoch': 0.13}
{'loss': 1.1699, 'grad_norm': 1.5366415977478027, 'learning_rate': 0.00019938870690114374, 'epoch': 0.13}
{'loss': 1.1017, 'grad_norm': 1.1981024742126465, 'learning_rate': 0.00019938276373935687, 'epoch': 0.13}
{'loss': 1.0888, 'grad_norm': 1.2691041231155396, 'learning_rate': 0.00019937679191605963, 'epoch': 0.13}
{'loss': 0.8696, 'grad_norm': 2.2968802452087402, 'learning_rate': 0.0001993707914329743, 'epoch': 0.13}
{'loss': 1.0734, 'grad_norm': 2.015411138534546, 'learning_rate': 0.00019936476229183133, 'epoch': 0.13}
{'loss': 0.8945, 'grad_norm': 1.287627100944519, 'learning_rate': 0.00019935870449436948, 'epoch': 0.13}
{'loss': 0.9389, 'grad_norm': 0.993667483329773, 'learning_rate': 0.00019935261804233587, 'epoch': 0.13}
{'loss': 0.9495, 'grad_norm': 1.588206171989441, 'learning_rate': 0.00019934650293748572, 'epoch': 0.13}
{'loss': 0.7907, 'grad_norm': 1.2805263996124268, 'learning_rate': 0.00019934035918158264, 'epoch': 0.13}
{'loss': 0.828, 'grad_norm': 1.1320927143096924, 'learning_rate': 0.00019933418677639848, 'epoch': 0.13}
{'loss': 1.0102, 'grad_norm': 1.7387346029281616, 'learning_rate': 0.0001993279857237133, 'epoch': 0.13}
{'loss': 1.4528, 'grad_norm': 2.388277053833008, 'learning_rate': 0.00019932175602531546, 'epoch': 0.13}
{'loss': 1.0576, 'grad_norm': 1.7040272951126099, 'learning_rate': 0.00019931549768300157, 'epoch': 0.13}
{'loss': 1.0813, 'grad_norm': 1.1014389991760254, 'learning_rate': 0.0001993092106985765, 'epoch': 0.13}
{'loss': 1.1044, 'grad_norm': 1.1808056831359863, 'learning_rate': 0.00019930289507385344, 'epoch': 0.13}
{'loss': 1.1702, 'grad_norm': 1.3447047472000122, 'learning_rate': 0.0001992965508106537, 'epoch': 0.13}
{'loss': 0.9391, 'grad_norm': 1.2747060060501099, 'learning_rate': 0.000199290177910807, 'epoch': 0.13}
{'loss': 1.1403, 'grad_norm': 1.925816535949707, 'learning_rate': 0.00019928377637615122, 'epoch': 0.13}
{'loss': 0.8641, 'grad_norm': 2.497751235961914, 'learning_rate': 0.00019927734620853253, 'epoch': 0.13}
{'loss': 1.1425, 'grad_norm': 1.3229891061782837, 'learning_rate': 0.0001992708874098054, 'epoch': 0.13}
{'loss': 0.8146, 'grad_norm': 1.5148379802703857, 'learning_rate': 0.00019926439998183248, 'epoch': 0.13}
{'loss': 0.6785, 'grad_norm': 1.4479880332946777, 'learning_rate': 0.00019925788392648473, 'epoch': 0.13}
{'loss': 0.8021, 'grad_norm': 1.2473665475845337, 'learning_rate': 0.00019925133924564135, 'epoch': 0.14}
{'loss': 1.1681, 'grad_norm': 2.109222650527954, 'learning_rate': 0.00019924476594118974, 'epoch': 0.14}
{'loss': 1.2442, 'grad_norm': 1.4118616580963135, 'learning_rate': 0.00019923816401502567, 'epoch': 0.14}
{'loss': 1.0051, 'grad_norm': 1.4219540357589722, 'learning_rate': 0.00019923153346905313, 'epoch': 0.14}
{'loss': 1.0048, 'grad_norm': 1.8230559825897217, 'learning_rate': 0.00019922487430518426, 'epoch': 0.14}
{'loss': 0.889, 'grad_norm': 1.098496913909912, 'learning_rate': 0.00019921818652533956, 'epoch': 0.14}
{'loss': 1.1464, 'grad_norm': 1.3887131214141846, 'learning_rate': 0.0001992114701314478, 'epoch': 0.14}
{'loss': 0.7966, 'grad_norm': 1.1947165727615356, 'learning_rate': 0.00019920472512544588, 'epoch': 0.14}
{'loss': 0.9818, 'grad_norm': 1.3284540176391602, 'learning_rate': 0.00019919795150927908, 'epoch': 0.14}
{'loss': 0.8945, 'grad_norm': 1.3873586654663086, 'learning_rate': 0.00019919114928490086, 'epoch': 0.14}
{'loss': 0.8214, 'grad_norm': 1.818739414215088, 'learning_rate': 0.00019918431845427294, 'epoch': 0.14}
{'loss': 0.9744, 'grad_norm': 1.7184735536575317, 'learning_rate': 0.0001991774590193653, 'epoch': 0.14}
{'loss': 0.7774, 'grad_norm': 1.2525280714035034, 'learning_rate': 0.0001991705709821562, 'epoch': 0.14}
{'loss': 1.0452, 'grad_norm': 1.6680545806884766, 'learning_rate': 0.0001991636543446321, 'epoch': 0.14}
{'loss': 0.985, 'grad_norm': 1.4851421117782593, 'learning_rate': 0.0001991567091087877, 'epoch': 0.14}
{'loss': 0.9162, 'grad_norm': 1.2786833047866821, 'learning_rate': 0.000199149735276626, 'epoch': 0.14}
{'loss': 0.9598, 'grad_norm': 1.4450936317443848, 'learning_rate': 0.00019914273285015822, 'epoch': 0.14}
{'loss': 0.8708, 'grad_norm': 1.4816620349884033, 'learning_rate': 0.00019913570183140378, 'epoch': 0.14}
{'loss': 0.8939, 'grad_norm': 1.9755202531814575, 'learning_rate': 0.00019912864222239044, 'epoch': 0.14}
{'loss': 0.8343, 'grad_norm': 1.8252733945846558, 'learning_rate': 0.00019912155402515417, 'epoch': 0.14}
{'loss': 1.1767, 'grad_norm': 1.1085959672927856, 'learning_rate': 0.00019911443724173914, 'epoch': 0.14}
{'loss': 0.9298, 'grad_norm': 1.4110050201416016, 'learning_rate': 0.00019910729187419781, 'epoch': 0.14}
{'loss': 0.7772, 'grad_norm': 2.1969268321990967, 'learning_rate': 0.00019910011792459087, 'epoch': 0.14}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8089, 'grad_norm': 1.2896857261657715, 'learning_rate': 0.0001990929153949872, 'epoch': 0.14}
{'loss': 1.4061, 'grad_norm': 4.639416694641113, 'learning_rate': 0.0001990856842874641, 'epoch': 0.14}
{'loss': 1.1186, 'grad_norm': 1.3943331241607666, 'learning_rate': 0.0001990784246041068, 'epoch': 0.14}
{'loss': 0.9293, 'grad_norm': 1.4778281450271606, 'learning_rate': 0.00019907113634700917, 'epoch': 0.14}
{'loss': 1.1227, 'grad_norm': 1.0114113092422485, 'learning_rate': 0.00019906381951827293, 'epoch': 0.14}
{'loss': 0.9262, 'grad_norm': 1.1126930713653564, 'learning_rate': 0.0001990564741200083, 'epoch': 0.14}
{'loss': 0.8659, 'grad_norm': 1.334959864616394, 'learning_rate': 0.00019904910015433372, 'epoch': 0.14}
{'loss': 1.0674, 'grad_norm': 1.2666338682174683, 'learning_rate': 0.0001990416976233757, 'epoch': 0.14}
{'loss': 1.239, 'grad_norm': 1.3562871217727661, 'learning_rate': 0.0001990342665292691, 'epoch': 0.14}
{'loss': 0.965, 'grad_norm': 1.7688497304916382, 'learning_rate': 0.00019902680687415705, 'epoch': 0.14}
{'loss': 1.1057, 'grad_norm': 1.2042560577392578, 'learning_rate': 0.00019901931866019089, 'epoch': 0.14}
{'loss': 0.9253, 'grad_norm': 1.3506648540496826, 'learning_rate': 0.00019901180188953012, 'epoch': 0.14}
{'loss': 1.0273, 'grad_norm': 1.3560882806777954, 'learning_rate': 0.00019900425656434263, 'epoch': 0.14}
{'loss': 1.0478, 'grad_norm': 1.7810368537902832, 'learning_rate': 0.0001989966826868044, 'epoch': 0.14}
{'loss': 0.8899, 'grad_norm': 1.2935127019882202, 'learning_rate': 0.00019898908025909972, 'epoch': 0.14}
{'loss': 1.1337, 'grad_norm': 1.8259016275405884, 'learning_rate': 0.00019898144928342105, 'epoch': 0.14}
{'loss': 0.9885, 'grad_norm': 1.1028112173080444, 'learning_rate': 0.0001989737897619692, 'epoch': 0.14}
{'loss': 0.8165, 'grad_norm': 1.8881142139434814, 'learning_rate': 0.00019896610169695307, 'epoch': 0.14}
{'loss': 0.9892, 'grad_norm': 1.1325228214263916, 'learning_rate': 0.0001989583850905899, 'epoch': 0.14}
{'loss': 1.1604, 'grad_norm': 1.3586946725845337, 'learning_rate': 0.0001989506399451051, 'epoch': 0.14}
{'loss': 1.0031, 'grad_norm': 1.649976372718811, 'learning_rate': 0.00019894286626273238, 'epoch': 0.14}
{'loss': 1.0137, 'grad_norm': 1.2595651149749756, 'learning_rate': 0.00019893506404571358, 'epoch': 0.14}
{'loss': 0.7988, 'grad_norm': 1.2954308986663818, 'learning_rate': 0.00019892723329629887, 'epoch': 0.14}
{'loss': 0.6403, 'grad_norm': 1.9065077304840088, 'learning_rate': 0.00019891937401674649, 'epoch': 0.14}
{'loss': 0.6698, 'grad_norm': 1.4146994352340698, 'learning_rate': 0.00019891148620932318, 'epoch': 0.14}
{'loss': 1.2489, 'grad_norm': 1.414668321609497, 'learning_rate': 0.00019890356987630362, 'epoch': 0.14}
{'loss': 0.8581, 'grad_norm': 1.1678636074066162, 'learning_rate': 0.0001988956250199709, 'epoch': 0.14}
{'loss': 0.911, 'grad_norm': 1.2254672050476074, 'learning_rate': 0.00019888765164261627, 'epoch': 0.14}
{'loss': 0.8979, 'grad_norm': 1.3837876319885254, 'learning_rate': 0.00019887964974653918, 'epoch': 0.14}
{'loss': 1.4, 'grad_norm': 1.5631221532821655, 'learning_rate': 0.0001988716193340474, 'epoch': 0.14}
{'loss': 0.8009, 'grad_norm': 2.102074146270752, 'learning_rate': 0.00019886356040745682, 'epoch': 0.14}
{'loss': 1.0227, 'grad_norm': 1.5634201765060425, 'learning_rate': 0.0001988554729690916, 'epoch': 0.14}
{'loss': 0.9455, 'grad_norm': 1.6999800205230713, 'learning_rate': 0.00019884735702128413, 'epoch': 0.14}
{'loss': 0.888, 'grad_norm': 1.1912267208099365, 'learning_rate': 0.000198839212566375, 'epoch': 0.14}
{'loss': 1.2348, 'grad_norm': 1.530907154083252, 'learning_rate': 0.00019883103960671305, 'epoch': 0.14}
{'loss': 0.6168, 'grad_norm': 1.2826790809631348, 'learning_rate': 0.0001988228381446553, 'epoch': 0.14}
{'loss': 0.6834, 'grad_norm': 1.4775508642196655, 'learning_rate': 0.000198814608182567, 'epoch': 0.14}
{'loss': 0.8816, 'grad_norm': 1.8155404329299927, 'learning_rate': 0.00019880634972282166, 'epoch': 0.14}
{'loss': 1.1651, 'grad_norm': 1.3552464246749878, 'learning_rate': 0.00019879806276780097, 'epoch': 0.14}
{'loss': 0.8805, 'grad_norm': 1.7355566024780273, 'learning_rate': 0.00019878974731989485, 'epoch': 0.14}
{'loss': 1.0465, 'grad_norm': 1.3489928245544434, 'learning_rate': 0.00019878140338150144, 'epoch': 0.14}
{'loss': 0.9199, 'grad_norm': 1.3946362733840942, 'learning_rate': 0.00019877303095502708, 'epoch': 0.14}
{'loss': 1.4225, 'grad_norm': 1.3550955057144165, 'learning_rate': 0.00019876463004288633, 'epoch': 0.15}
{'loss': 1.3306, 'grad_norm': 1.3734347820281982, 'learning_rate': 0.00019875620064750202, 'epoch': 0.15}
{'loss': 1.2583, 'grad_norm': 2.838240146636963, 'learning_rate': 0.0001987477427713051, 'epoch': 0.15}
{'loss': 0.7691, 'grad_norm': 1.5064064264297485, 'learning_rate': 0.0001987392564167348, 'epoch': 0.15}
{'loss': 0.9435, 'grad_norm': 1.3559225797653198, 'learning_rate': 0.0001987307415862385, 'epoch': 0.15}
{'loss': 1.1694, 'grad_norm': 1.5890426635742188, 'learning_rate': 0.00019872219828227187, 'epoch': 0.15}
{'loss': 1.285, 'grad_norm': 7.702914714813232, 'learning_rate': 0.0001987136265072988, 'epoch': 0.15}
{'loss': 0.8118, 'grad_norm': 1.4987988471984863, 'learning_rate': 0.00019870502626379127, 'epoch': 0.15}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2035, 'grad_norm': 1.567775011062622, 'learning_rate': 0.00019869639755422963, 'epoch': 0.15}
{'loss': 0.8902, 'grad_norm': 4.492615222930908, 'learning_rate': 0.0001986877403811023, 'epoch': 0.15}
{'loss': 0.9536, 'grad_norm': 2.572326183319092, 'learning_rate': 0.000198679054746906, 'epoch': 0.15}
{'loss': 1.0871, 'grad_norm': 1.339888572692871, 'learning_rate': 0.0001986703406541456, 'epoch': 0.15}
{'loss': 0.9622, 'grad_norm': 1.7159056663513184, 'learning_rate': 0.00019866159810533418, 'epoch': 0.15}
{'loss': 0.8737, 'grad_norm': 1.8719311952590942, 'learning_rate': 0.0001986528271029931, 'epoch': 0.15}
{'loss': 0.9016, 'grad_norm': 1.3630889654159546, 'learning_rate': 0.00019864402764965187, 'epoch': 0.15}
{'loss': 1.227, 'grad_norm': 1.2767717838287354, 'learning_rate': 0.00019863519974784816, 'epoch': 0.15}
{'loss': 1.2787, 'grad_norm': 1.1152105331420898, 'learning_rate': 0.00019862634340012795, 'epoch': 0.15}
{'loss': 0.9942, 'grad_norm': 1.4602551460266113, 'learning_rate': 0.00019861745860904538, 'epoch': 0.15}
{'loss': 0.8424, 'grad_norm': 1.7343268394470215, 'learning_rate': 0.0001986085453771627, 'epoch': 0.15}
{'loss': 1.0436, 'grad_norm': 1.1464060544967651, 'learning_rate': 0.0001985996037070505, 'epoch': 0.15}
{'loss': 0.9819, 'grad_norm': 1.4076979160308838, 'learning_rate': 0.00019859063360128752, 'epoch': 0.15}
{'loss': 1.081, 'grad_norm': 1.2102720737457275, 'learning_rate': 0.00019858163506246066, 'epoch': 0.15}
{'loss': 0.9613, 'grad_norm': 1.6433674097061157, 'learning_rate': 0.0001985726080931651, 'epoch': 0.15}
{'loss': 1.0624, 'grad_norm': 1.639805555343628, 'learning_rate': 0.00019856355269600413, 'epoch': 0.15}
{'loss': 0.9775, 'grad_norm': 1.4277887344360352, 'learning_rate': 0.00019855446887358932, 'epoch': 0.15}
{'loss': 0.7335, 'grad_norm': 1.3444132804870605, 'learning_rate': 0.00019854535662854037, 'epoch': 0.15}
{'loss': 0.8668, 'grad_norm': 1.2852239608764648, 'learning_rate': 0.00019853621596348524, 'epoch': 0.15}
{'loss': 1.0133, 'grad_norm': 1.1119118928909302, 'learning_rate': 0.00019852704688106003, 'epoch': 0.15}
{'loss': 0.9731, 'grad_norm': 1.2907735109329224, 'learning_rate': 0.00019851784938390904, 'epoch': 0.15}
{'loss': 0.8916, 'grad_norm': 1.4782005548477173, 'learning_rate': 0.00019850862347468488, 'epoch': 0.15}
{'loss': 1.215, 'grad_norm': 1.3708534240722656, 'learning_rate': 0.0001984993691560481, 'epoch': 0.15}
{'loss': 0.7463, 'grad_norm': 1.516111135482788, 'learning_rate': 0.00019849008643066772, 'epoch': 0.15}
{'loss': 0.7287, 'grad_norm': 1.0661711692810059, 'learning_rate': 0.00019848077530122083, 'epoch': 0.15}
{'loss': 1.0649, 'grad_norm': 1.1428567171096802, 'learning_rate': 0.00019847143577039265, 'epoch': 0.15}
{'loss': 0.8704, 'grad_norm': 1.3105908632278442, 'learning_rate': 0.0001984620678408767, 'epoch': 0.15}
{'loss': 1.2555, 'grad_norm': 1.2852455377578735, 'learning_rate': 0.00019845267151537464, 'epoch': 0.15}
{'loss': 1.3706, 'grad_norm': 1.1056756973266602, 'learning_rate': 0.0001984432467965963, 'epoch': 0.15}
{'loss': 0.9435, 'grad_norm': 1.4149466753005981, 'learning_rate': 0.00019843379368725977, 'epoch': 0.15}
{'loss': 0.8121, 'grad_norm': 1.1938356161117554, 'learning_rate': 0.00019842431219009127, 'epoch': 0.15}
{'loss': 1.0493, 'grad_norm': 1.3643662929534912, 'learning_rate': 0.00019841480230782519, 'epoch': 0.15}
{'loss': 0.9211, 'grad_norm': 1.1573894023895264, 'learning_rate': 0.00019840526404320415, 'epoch': 0.15}
{'loss': 0.9264, 'grad_norm': 2.1364381313323975, 'learning_rate': 0.00019839569739897895, 'epoch': 0.15}
{'loss': 1.1846, 'grad_norm': 1.7248969078063965, 'learning_rate': 0.0001983861023779085, 'epoch': 0.15}
{'loss': 0.9903, 'grad_norm': 1.237716794013977, 'learning_rate': 0.00019837647898276006, 'epoch': 0.15}
{'loss': 1.108, 'grad_norm': 1.4832035303115845, 'learning_rate': 0.00019836682721630894, 'epoch': 0.15}
{'loss': 0.9466, 'grad_norm': 1.8658323287963867, 'learning_rate': 0.00019835714708133862, 'epoch': 0.15}
{'loss': 1.1899, 'grad_norm': 1.4732966423034668, 'learning_rate': 0.0001983474385806408, 'epoch': 0.15}
{'loss': 1.1489, 'grad_norm': 1.2040647268295288, 'learning_rate': 0.00019833770171701542, 'epoch': 0.15}
{'loss': 1.0738, 'grad_norm': 1.1631888151168823, 'learning_rate': 0.0001983279364932705, 'epoch': 0.15}
{'loss': 0.8463, 'grad_norm': 1.1290457248687744, 'learning_rate': 0.00019831814291222232, 'epoch': 0.15}
{'loss': 0.9524, 'grad_norm': 1.6311208009719849, 'learning_rate': 0.0001983083209766953, 'epoch': 0.15}
{'loss': 1.1954, 'grad_norm': 1.2811305522918701, 'learning_rate': 0.00019829847068952202, 'epoch': 0.15}
{'loss': 0.9867, 'grad_norm': 1.056628704071045, 'learning_rate': 0.00019828859205354323, 'epoch': 0.15}
{'loss': 1.1481, 'grad_norm': 1.3001174926757812, 'learning_rate': 0.00019827868507160792, 'epoch': 0.15}
{'loss': 0.7206, 'grad_norm': 1.2728781700134277, 'learning_rate': 0.0001982687497465732, 'epoch': 0.15}
{'loss': 0.9566, 'grad_norm': 1.0647642612457275, 'learning_rate': 0.00019825878608130436, 'epoch': 0.15}
{'loss': 0.8131, 'grad_norm': 1.0951941013336182, 'learning_rate': 0.00019824879407867493, 'epoch': 0.15}
{'loss': 0.9076, 'grad_norm': 1.5021990537643433, 'learning_rate': 0.00019823877374156647, 'epoch': 0.15}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9172, 'grad_norm': 1.1749720573425293, 'learning_rate': 0.0001982287250728689, 'epoch': 0.15}
{'loss': 1.1194, 'grad_norm': 1.1705900430679321, 'learning_rate': 0.00019821864807548009, 'epoch': 0.15}
{'loss': 0.7891, 'grad_norm': 1.5118441581726074, 'learning_rate': 0.00019820854275230627, 'epoch': 0.15}
{'loss': 1.1119, 'grad_norm': 1.3761613368988037, 'learning_rate': 0.00019819840910626174, 'epoch': 0.15}
{'loss': 0.8419, 'grad_norm': 1.5691534280776978, 'learning_rate': 0.00019818824714026905, 'epoch': 0.15}
{'loss': 1.0162, 'grad_norm': 1.1275055408477783, 'learning_rate': 0.00019817805685725875, 'epoch': 0.15}
{'loss': 1.075, 'grad_norm': 1.6210932731628418, 'learning_rate': 0.0001981678382601698, 'epoch': 0.15}
{'loss': 1.1398, 'grad_norm': 1.547378420829773, 'learning_rate': 0.00019815759135194912, 'epoch': 0.16}
{'loss': 0.7581, 'grad_norm': 2.010002851486206, 'learning_rate': 0.00019814731613555184, 'epoch': 0.16}
{'loss': 0.898, 'grad_norm': 1.4448425769805908, 'learning_rate': 0.00019813701261394136, 'epoch': 0.16}
{'loss': 0.8802, 'grad_norm': 1.1810683012008667, 'learning_rate': 0.00019812668079008912, 'epoch': 0.16}
{'loss': 1.2309, 'grad_norm': 1.5267689228057861, 'learning_rate': 0.00019811632066697477, 'epoch': 0.16}
{'loss': 1.306, 'grad_norm': 1.1648646593093872, 'learning_rate': 0.00019810593224758614, 'epoch': 0.16}
{'loss': 1.1231, 'grad_norm': 1.734218955039978, 'learning_rate': 0.00019809551553491916, 'epoch': 0.16}
{'loss': 1.1974, 'grad_norm': 1.320974349975586, 'learning_rate': 0.00019808507053197802, 'epoch': 0.16}
{'loss': 1.0447, 'grad_norm': 1.3060295581817627, 'learning_rate': 0.00019807459724177496, 'epoch': 0.16}
{'loss': 0.8072, 'grad_norm': 1.1377880573272705, 'learning_rate': 0.00019806409566733044, 'epoch': 0.16}
{'loss': 0.8778, 'grad_norm': 1.3604928255081177, 'learning_rate': 0.00019805356581167308, 'epoch': 0.16}
{'loss': 0.8619, 'grad_norm': 1.5953395366668701, 'learning_rate': 0.00019804300767783958, 'epoch': 0.16}
{'loss': 1.1477, 'grad_norm': 1.6530086994171143, 'learning_rate': 0.00019803242126887493, 'epoch': 0.16}
{'loss': 1.025, 'grad_norm': 1.2774089574813843, 'learning_rate': 0.0001980218065878322, 'epoch': 0.16}
{'loss': 1.5116, 'grad_norm': 1.2149733304977417, 'learning_rate': 0.00019801116363777252, 'epoch': 0.16}
{'loss': 1.164, 'grad_norm': 1.165150761604309, 'learning_rate': 0.00019800049242176538, 'epoch': 0.16}
{'loss': 1.1518, 'grad_norm': 1.4440685510635376, 'learning_rate': 0.00019798979294288822, 'epoch': 0.16}
{'loss': 1.2074, 'grad_norm': 1.319006085395813, 'learning_rate': 0.00019797906520422677, 'epoch': 0.16}
{'loss': 1.1655, 'grad_norm': 1.039169192314148, 'learning_rate': 0.00019796830920887484, 'epoch': 0.16}
{'loss': 1.0033, 'grad_norm': 1.6390724182128906, 'learning_rate': 0.0001979575249599344, 'epoch': 0.16}
{'loss': 0.8672, 'grad_norm': 1.5463004112243652, 'learning_rate': 0.0001979467124605156, 'epoch': 0.16}
{'loss': 0.926, 'grad_norm': 1.0052801370620728, 'learning_rate': 0.0001979358717137367, 'epoch': 0.16}
{'loss': 1.091, 'grad_norm': 1.4334604740142822, 'learning_rate': 0.0001979250027227241, 'epoch': 0.16}
{'loss': 0.9172, 'grad_norm': 1.3925421237945557, 'learning_rate': 0.00019791410549061238, 'epoch': 0.16}
{'loss': 0.9092, 'grad_norm': 1.3514214754104614, 'learning_rate': 0.00019790318002054424, 'epoch': 0.16}
{'loss': 1.1358, 'grad_norm': 1.5096650123596191, 'learning_rate': 0.00019789222631567057, 'epoch': 0.16}
{'loss': 0.9765, 'grad_norm': 1.1763564348220825, 'learning_rate': 0.0001978812443791503, 'epoch': 0.16}
{'loss': 0.9599, 'grad_norm': 1.1231951713562012, 'learning_rate': 0.00019787023421415065, 'epoch': 0.16}
{'loss': 1.0065, 'grad_norm': 1.9335983991622925, 'learning_rate': 0.00019785919582384685, 'epoch': 0.16}
{'loss': 0.9055, 'grad_norm': 1.3297404050827026, 'learning_rate': 0.0001978481292114223, 'epoch': 0.16}
{'loss': 1.3442, 'grad_norm': 1.0972905158996582, 'learning_rate': 0.00019783703438006859, 'epoch': 0.16}
{'loss': 1.2676, 'grad_norm': 1.4265598058700562, 'learning_rate': 0.0001978259113329854, 'epoch': 0.16}
{'loss': 0.9611, 'grad_norm': 1.262605905532837, 'learning_rate': 0.00019781476007338058, 'epoch': 0.16}
{'loss': 1.0628, 'grad_norm': 1.4185068607330322, 'learning_rate': 0.00019780358060447005, 'epoch': 0.16}
{'loss': 0.8222, 'grad_norm': 1.2604418992996216, 'learning_rate': 0.00019779237292947804, 'epoch': 0.16}
{'loss': 1.1804, 'grad_norm': 1.1320271492004395, 'learning_rate': 0.00019778113705163662, 'epoch': 0.16}
{'loss': 1.0038, 'grad_norm': 1.5270018577575684, 'learning_rate': 0.00019776987297418628, 'epoch': 0.16}
{'loss': 1.2597, 'grad_norm': 1.313886284828186, 'learning_rate': 0.0001977585807003755, 'epoch': 0.16}
{'loss': 1.1061, 'grad_norm': 1.4185608625411987, 'learning_rate': 0.0001977472602334609, 'epoch': 0.16}
{'loss': 1.17, 'grad_norm': 1.1234488487243652, 'learning_rate': 0.00019773591157670728, 'epoch': 0.16}
{'loss': 1.0906, 'grad_norm': 1.5711132287979126, 'learning_rate': 0.00019772453473338749, 'epoch': 0.16}
{'loss': 1.0236, 'grad_norm': 1.5898444652557373, 'learning_rate': 0.00019771312970678258, 'epoch': 0.16}
{'loss': 1.16, 'grad_norm': 1.023917317390442, 'learning_rate': 0.00019770169650018172, 'epoch': 0.16}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8848, 'grad_norm': 1.2185343503952026, 'learning_rate': 0.00019769023511688217, 'epoch': 0.16}
{'loss': 0.6934, 'grad_norm': 1.3973007202148438, 'learning_rate': 0.00019767874556018934, 'epoch': 0.16}
{'loss': 1.0363, 'grad_norm': 1.6896381378173828, 'learning_rate': 0.0001976672278334168, 'epoch': 0.16}
{'loss': 0.9968, 'grad_norm': 1.078964352607727, 'learning_rate': 0.00019765568193988618, 'epoch': 0.16}
{'loss': 0.7753, 'grad_norm': 1.3186445236206055, 'learning_rate': 0.00019764410788292722, 'epoch': 0.16}
{'loss': 0.9594, 'grad_norm': 1.0824387073516846, 'learning_rate': 0.00019763250566587787, 'epoch': 0.16}
{'loss': 1.0516, 'grad_norm': 1.2946947813034058, 'learning_rate': 0.00019762087529208414, 'epoch': 0.16}
{'loss': 0.6323, 'grad_norm': 1.4164940118789673, 'learning_rate': 0.00019760921676490018, 'epoch': 0.16}
{'loss': 0.9648, 'grad_norm': 1.278598666191101, 'learning_rate': 0.00019759753008768827, 'epoch': 0.16}
{'loss': 0.9691, 'grad_norm': 1.420554518699646, 'learning_rate': 0.00019758581526381876, 'epoch': 0.16}
{'loss': 1.1314, 'grad_norm': 1.2304878234863281, 'learning_rate': 0.00019757407229667015, 'epoch': 0.16}
{'loss': 0.8833, 'grad_norm': 1.034614086151123, 'learning_rate': 0.0001975623011896291, 'epoch': 0.16}
{'loss': 1.0676, 'grad_norm': 1.029670000076294, 'learning_rate': 0.00019755050194609025, 'epoch': 0.16}
{'loss': 0.9207, 'grad_norm': 1.3370122909545898, 'learning_rate': 0.0001975386745694565, 'epoch': 0.16}
{'loss': 1.1834, 'grad_norm': 1.351923942565918, 'learning_rate': 0.00019752681906313888, 'epoch': 0.16}
{'loss': 1.0346, 'grad_norm': 1.388399362564087, 'learning_rate': 0.00019751493543055632, 'epoch': 0.16}
{'loss': 0.9224, 'grad_norm': 1.417837142944336, 'learning_rate': 0.00019750302367513612, 'epoch': 0.16}
{'loss': 1.2065, 'grad_norm': 1.2128087282180786, 'learning_rate': 0.0001974910838003135, 'epoch': 0.16}
{'loss': 1.0042, 'grad_norm': 1.306225299835205, 'learning_rate': 0.00019747911580953188, 'epoch': 0.16}
{'loss': 0.8878, 'grad_norm': 1.263405680656433, 'learning_rate': 0.0001974671197062428, 'epoch': 0.16}
{'loss': 0.939, 'grad_norm': 1.1174516677856445, 'learning_rate': 0.00019745509549390583, 'epoch': 0.16}
{'loss': 1.1595, 'grad_norm': 1.100337028503418, 'learning_rate': 0.00019744304317598874, 'epoch': 0.16}
{'loss': 0.883, 'grad_norm': 1.1379250288009644, 'learning_rate': 0.00019743096275596735, 'epoch': 0.17}
{'loss': 1.1968, 'grad_norm': 1.2560920715332031, 'learning_rate': 0.00019741885423732558, 'epoch': 0.17}
{'loss': 1.3803, 'grad_norm': 1.271122694015503, 'learning_rate': 0.00019740671762355548, 'epoch': 0.17}
{'loss': 1.055, 'grad_norm': 1.1734118461608887, 'learning_rate': 0.00019739455291815716, 'epoch': 0.17}
{'loss': 0.9147, 'grad_norm': 1.2602043151855469, 'learning_rate': 0.00019738236012463892, 'epoch': 0.17}
{'loss': 1.0838, 'grad_norm': 1.1527208089828491, 'learning_rate': 0.00019737013924651706, 'epoch': 0.17}
{'loss': 0.8059, 'grad_norm': 1.107492446899414, 'learning_rate': 0.00019735789028731604, 'epoch': 0.17}
{'loss': 1.0813, 'grad_norm': 1.2033696174621582, 'learning_rate': 0.00019734561325056838, 'epoch': 0.17}
{'loss': 1.017, 'grad_norm': 1.2341210842132568, 'learning_rate': 0.00019733330813981477, 'epoch': 0.17}
{'loss': 1.0323, 'grad_norm': 1.1727359294891357, 'learning_rate': 0.00019732097495860386, 'epoch': 0.17}
{'loss': 1.2299, 'grad_norm': 1.2945332527160645, 'learning_rate': 0.00019730861371049257, 'epoch': 0.17}
{'loss': 1.1133, 'grad_norm': 1.3658045530319214, 'learning_rate': 0.00019729622439904575, 'epoch': 0.17}
{'loss': 1.0245, 'grad_norm': 1.0324493646621704, 'learning_rate': 0.00019728380702783643, 'epoch': 0.17}
{'loss': 1.2722, 'grad_norm': 1.2918106317520142, 'learning_rate': 0.0001972713616004458, 'epoch': 0.17}
{'loss': 0.8973, 'grad_norm': 1.1399486064910889, 'learning_rate': 0.00019725888812046298, 'epoch': 0.17}
{'loss': 1.0298, 'grad_norm': 2.222517251968384, 'learning_rate': 0.00019724638659148527, 'epoch': 0.17}
{'loss': 1.0276, 'grad_norm': 1.3848950862884521, 'learning_rate': 0.00019723385701711808, 'epoch': 0.17}
{'loss': 0.7968, 'grad_norm': 1.2003397941589355, 'learning_rate': 0.00019722129940097488, 'epoch': 0.17}
{'loss': 1.1816, 'grad_norm': 1.116446614265442, 'learning_rate': 0.00019720871374667714, 'epoch': 0.17}
{'loss': 0.8771, 'grad_norm': 1.1491764783859253, 'learning_rate': 0.00019719610005785465, 'epoch': 0.17}
{'loss': 1.3029, 'grad_norm': 1.4970394372940063, 'learning_rate': 0.00019718345833814502, 'epoch': 0.17}
{'loss': 0.8874, 'grad_norm': 1.419714331626892, 'learning_rate': 0.0001971707885911941, 'epoch': 0.17}
{'loss': 1.2155, 'grad_norm': 1.4066352844238281, 'learning_rate': 0.00019715809082065578, 'epoch': 0.17}
{'loss': 0.7579, 'grad_norm': 1.7126585245132446, 'learning_rate': 0.00019714536503019202, 'epoch': 0.17}
{'loss': 1.0376, 'grad_norm': 1.594882845878601, 'learning_rate': 0.00019713261122347294, 'epoch': 0.17}
{'loss': 1.0595, 'grad_norm': 1.067538857460022, 'learning_rate': 0.00019711982940417663, 'epoch': 0.17}
{'loss': 1.0027, 'grad_norm': 1.245415449142456, 'learning_rate': 0.00019710701957598926, 'epoch': 0.17}
{'loss': 0.9543, 'grad_norm': 1.1365246772766113, 'learning_rate': 0.0001970941817426052, 'epoch': 0.17}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8929, 'grad_norm': 1.3418617248535156, 'learning_rate': 0.0001970813159077268, 'epoch': 0.17}
{'loss': 1.1655, 'grad_norm': 1.2778892517089844, 'learning_rate': 0.0001970684220750645, 'epoch': 0.17}
{'loss': 0.7674, 'grad_norm': 1.455820083618164, 'learning_rate': 0.00019705550024833678, 'epoch': 0.17}
{'loss': 1.0174, 'grad_norm': 1.4484167098999023, 'learning_rate': 0.00019704255043127028, 'epoch': 0.17}
{'loss': 1.0303, 'grad_norm': 1.8039958477020264, 'learning_rate': 0.00019702957262759965, 'epoch': 0.17}
{'loss': 0.9393, 'grad_norm': 1.8655308485031128, 'learning_rate': 0.00019701656684106763, 'epoch': 0.17}
{'loss': 0.7604, 'grad_norm': 1.5908288955688477, 'learning_rate': 0.00019700353307542504, 'epoch': 0.17}
{'loss': 1.12, 'grad_norm': 1.08613121509552, 'learning_rate': 0.0001969904713344307, 'epoch': 0.17}
{'loss': 0.956, 'grad_norm': 1.052190899848938, 'learning_rate': 0.00019697738162185161, 'epoch': 0.17}
{'loss': 0.6237, 'grad_norm': 1.2650120258331299, 'learning_rate': 0.00019696426394146278, 'epoch': 0.17}
{'loss': 0.8709, 'grad_norm': 1.6472526788711548, 'learning_rate': 0.00019695111829704724, 'epoch': 0.17}
{'loss': 1.1137, 'grad_norm': 2.0535268783569336, 'learning_rate': 0.0001969379446923962, 'epoch': 0.17}
{'loss': 0.9306, 'grad_norm': 0.9708751440048218, 'learning_rate': 0.00019692474313130878, 'epoch': 0.17}
{'loss': 1.2496, 'grad_norm': 1.7927428483963013, 'learning_rate': 0.0001969115136175923, 'epoch': 0.17}
{'loss': 0.9958, 'grad_norm': 1.4198771715164185, 'learning_rate': 0.00019689825615506207, 'epoch': 0.17}
{'loss': 1.0066, 'grad_norm': 1.2786091566085815, 'learning_rate': 0.0001968849707475415, 'epoch': 0.17}
{'loss': 1.3159, 'grad_norm': 1.424664855003357, 'learning_rate': 0.00019687165739886198, 'epoch': 0.17}
{'loss': 0.9322, 'grad_norm': 1.7918319702148438, 'learning_rate': 0.0001968583161128631, 'epoch': 0.17}
{'loss': 1.0135, 'grad_norm': 1.4016813039779663, 'learning_rate': 0.0001968449468933924, 'epoch': 0.17}
{'loss': 1.3519, 'grad_norm': 1.7592183351516724, 'learning_rate': 0.00019683154974430543, 'epoch': 0.17}
{'loss': 0.8577, 'grad_norm': 1.2991517782211304, 'learning_rate': 0.00019681812466946594, 'epoch': 0.17}
{'loss': 1.2331, 'grad_norm': 1.5433158874511719, 'learning_rate': 0.00019680467167274563, 'epoch': 0.17}
{'loss': 1.0407, 'grad_norm': 1.753230094909668, 'learning_rate': 0.0001967911907580243, 'epoch': 0.17}
{'loss': 1.2293, 'grad_norm': 1.074892282485962, 'learning_rate': 0.00019677768192918971, 'epoch': 0.17}
{'loss': 1.1084, 'grad_norm': 1.7701088190078735, 'learning_rate': 0.00019676414519013781, 'epoch': 0.17}
{'loss': 1.0345, 'grad_norm': 1.614627718925476, 'learning_rate': 0.00019675058054477253, 'epoch': 0.17}
{'loss': 0.9583, 'grad_norm': 1.069370985031128, 'learning_rate': 0.0001967369879970058, 'epoch': 0.17}
{'loss': 0.9934, 'grad_norm': 1.1909689903259277, 'learning_rate': 0.0001967233675507577, 'epoch': 0.17}
{'loss': 1.1038, 'grad_norm': 1.384772777557373, 'learning_rate': 0.00019670971920995628, 'epoch': 0.17}
{'loss': 1.039, 'grad_norm': 1.8049485683441162, 'learning_rate': 0.00019669604297853764, 'epoch': 0.17}
{'loss': 0.9038, 'grad_norm': 1.2968670129776, 'learning_rate': 0.00019668233886044597, 'epoch': 0.17}
{'loss': 1.4518, 'grad_norm': 1.1066336631774902, 'learning_rate': 0.00019666860685963342, 'epoch': 0.17}
{'loss': 0.9564, 'grad_norm': 1.2889297008514404, 'learning_rate': 0.00019665484698006027, 'epoch': 0.17}
{'loss': 1.0559, 'grad_norm': 1.7665117979049683, 'learning_rate': 0.00019664105922569483, 'epoch': 0.17}
{'loss': 1.1517, 'grad_norm': 1.5699338912963867, 'learning_rate': 0.00019662724360051335, 'epoch': 0.17}
{'loss': 0.8666, 'grad_norm': 1.2430886030197144, 'learning_rate': 0.00019661340010850026, 'epoch': 0.17}
{'loss': 1.0919, 'grad_norm': 1.2452754974365234, 'learning_rate': 0.0001965995287536479, 'epoch': 0.17}
{'loss': 0.9941, 'grad_norm': 1.3130549192428589, 'learning_rate': 0.00019658562953995677, 'epoch': 0.18}
{'loss': 1.0674, 'grad_norm': 1.0038049221038818, 'learning_rate': 0.00019657170247143525, 'epoch': 0.18}
{'loss': 0.8327, 'grad_norm': 1.5118372440338135, 'learning_rate': 0.0001965577475520999, 'epoch': 0.18}
{'loss': 0.9756, 'grad_norm': 1.791480302810669, 'learning_rate': 0.00019654376478597525, 'epoch': 0.18}
{'loss': 1.0713, 'grad_norm': 1.9531360864639282, 'learning_rate': 0.00019652975417709385, 'epoch': 0.18}
{'loss': 1.2581, 'grad_norm': 1.175906777381897, 'learning_rate': 0.00019651571572949626, 'epoch': 0.18}
{'loss': 1.1834, 'grad_norm': 1.8480671644210815, 'learning_rate': 0.00019650164944723115, 'epoch': 0.18}
{'loss': 1.0235, 'grad_norm': 1.1706414222717285, 'learning_rate': 0.00019648755533435518, 'epoch': 0.18}
{'loss': 1.0017, 'grad_norm': 1.2711588144302368, 'learning_rate': 0.00019647343339493294, 'epoch': 0.18}
{'loss': 0.6434, 'grad_norm': 1.2890337705612183, 'learning_rate': 0.0001964592836330372, 'epoch': 0.18}
{'loss': 1.069, 'grad_norm': 1.0258376598358154, 'learning_rate': 0.00019644510605274868, 'epoch': 0.18}
{'loss': 1.04, 'grad_norm': 1.2548905611038208, 'learning_rate': 0.00019643090065815612, 'epoch': 0.18}
{'loss': 1.0178, 'grad_norm': 1.3560371398925781, 'learning_rate': 0.00019641666745335624, 'epoch': 0.18}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1095, 'grad_norm': 1.6961548328399658, 'learning_rate': 0.0001964024064424539, 'epoch': 0.18}
{'loss': 1.0705, 'grad_norm': 1.1450682878494263, 'learning_rate': 0.00019638811762956186, 'epoch': 0.18}
{'loss': 1.2372, 'grad_norm': 2.4560964107513428, 'learning_rate': 0.00019637380101880097, 'epoch': 0.18}
{'loss': 0.8267, 'grad_norm': 1.142861247062683, 'learning_rate': 0.00019635945661430006, 'epoch': 0.18}
{'loss': 0.9445, 'grad_norm': 1.3606516122817993, 'learning_rate': 0.00019634508442019596, 'epoch': 0.18}
{'loss': 1.2056, 'grad_norm': 1.7360320091247559, 'learning_rate': 0.0001963306844406336, 'epoch': 0.18}
{'loss': 0.9453, 'grad_norm': 1.0943111181259155, 'learning_rate': 0.00019631625667976583, 'epoch': 0.18}
{'loss': 0.7663, 'grad_norm': 1.1336296796798706, 'learning_rate': 0.00019630180114175354, 'epoch': 0.18}
{'loss': 0.874, 'grad_norm': 1.5158535242080688, 'learning_rate': 0.00019628731783076563, 'epoch': 0.18}
{'loss': 0.9173, 'grad_norm': 1.2733670473098755, 'learning_rate': 0.00019627280675097908, 'epoch': 0.18}
{'loss': 1.0851, 'grad_norm': 1.3631583452224731, 'learning_rate': 0.00019625826790657875, 'epoch': 0.18}
{'loss': 1.0442, 'grad_norm': 1.4147814512252808, 'learning_rate': 0.0001962437013017576, 'epoch': 0.18}
{'loss': 1.168, 'grad_norm': 0.9674593210220337, 'learning_rate': 0.00019622910694071656, 'epoch': 0.18}
{'loss': 1.0445, 'grad_norm': 1.2252662181854248, 'learning_rate': 0.00019621448482766454, 'epoch': 0.18}
{'loss': 1.0512, 'grad_norm': 1.4594018459320068, 'learning_rate': 0.00019619983496681854, 'epoch': 0.18}
{'loss': 1.1017, 'grad_norm': 1.305100440979004, 'learning_rate': 0.00019618515736240352, 'epoch': 0.18}
{'loss': 0.7415, 'grad_norm': 1.2891488075256348, 'learning_rate': 0.0001961704520186524, 'epoch': 0.18}
{'loss': 0.9343, 'grad_norm': 1.2292190790176392, 'learning_rate': 0.00019615571893980612, 'epoch': 0.18}
{'loss': 1.0515, 'grad_norm': 1.2297457456588745, 'learning_rate': 0.00019614095813011364, 'epoch': 0.18}
{'loss': 0.9229, 'grad_norm': 1.3096126317977905, 'learning_rate': 0.0001961261695938319, 'epoch': 0.18}
{'loss': 1.0643, 'grad_norm': 1.2904574871063232, 'learning_rate': 0.00019611135333522586, 'epoch': 0.18}
{'loss': 0.9995, 'grad_norm': 1.0916273593902588, 'learning_rate': 0.00019609650935856844, 'epoch': 0.18}
{'loss': 1.2243, 'grad_norm': 1.1380329132080078, 'learning_rate': 0.0001960816376681406, 'epoch': 0.18}
{'loss': 0.8872, 'grad_norm': 1.5921112298965454, 'learning_rate': 0.00019606673826823122, 'epoch': 0.18}
{'loss': 0.9834, 'grad_norm': 1.3656977415084839, 'learning_rate': 0.00019605181116313724, 'epoch': 0.18}
{'loss': 1.2145, 'grad_norm': 1.0573958158493042, 'learning_rate': 0.00019603685635716356, 'epoch': 0.18}
{'loss': 0.9214, 'grad_norm': 1.8287899494171143, 'learning_rate': 0.0001960218738546231, 'epoch': 0.18}
{'loss': 0.9377, 'grad_norm': 1.4061528444290161, 'learning_rate': 0.00019600686365983672, 'epoch': 0.18}
{'loss': 0.7929, 'grad_norm': 1.3814537525177002, 'learning_rate': 0.00019599182577713327, 'epoch': 0.18}
{'loss': 0.7721, 'grad_norm': 1.3444430828094482, 'learning_rate': 0.00019597676021084964, 'epoch': 0.18}
{'loss': 1.0586, 'grad_norm': 1.7529115676879883, 'learning_rate': 0.0001959616669653306, 'epoch': 0.18}
{'loss': 1.0969, 'grad_norm': 1.8356575965881348, 'learning_rate': 0.00019594654604492906, 'epoch': 0.18}
{'loss': 1.4661, 'grad_norm': 1.3485710620880127, 'learning_rate': 0.00019593139745400576, 'epoch': 0.18}
{'loss': 0.9149, 'grad_norm': 1.172050952911377, 'learning_rate': 0.0001959162211969295, 'epoch': 0.18}
{'loss': 1.0726, 'grad_norm': 1.1674119234085083, 'learning_rate': 0.00019590101727807703, 'epoch': 0.18}
{'loss': 1.0211, 'grad_norm': 1.1731455326080322, 'learning_rate': 0.0001958857857018331, 'epoch': 0.18}
{'loss': 1.0309, 'grad_norm': 1.3388919830322266, 'learning_rate': 0.00019587052647259044, 'epoch': 0.18}
{'loss': 1.2157, 'grad_norm': 1.2154583930969238, 'learning_rate': 0.0001958552395947497, 'epoch': 0.18}
{'loss': 0.8002, 'grad_norm': 1.1083029508590698, 'learning_rate': 0.00019583992507271955, 'epoch': 0.18}
{'loss': 0.8364, 'grad_norm': 1.3045355081558228, 'learning_rate': 0.00019582458291091663, 'epoch': 0.18}
{'loss': 1.0368, 'grad_norm': 1.0134714841842651, 'learning_rate': 0.00019580921311376556, 'epoch': 0.18}
{'loss': 0.8944, 'grad_norm': 1.2173644304275513, 'learning_rate': 0.00019579381568569888, 'epoch': 0.18}
{'loss': 1.0615, 'grad_norm': 1.1956384181976318, 'learning_rate': 0.00019577839063115716, 'epoch': 0.18}
{'loss': 1.1474, 'grad_norm': 1.0991337299346924, 'learning_rate': 0.00019576293795458893, 'epoch': 0.18}
{'loss': 1.0817, 'grad_norm': 1.1285052299499512, 'learning_rate': 0.00019574745766045063, 'epoch': 0.18}
{'loss': 1.1799, 'grad_norm': 4.603753566741943, 'learning_rate': 0.00019573194975320673, 'epoch': 0.18}
{'loss': 0.9969, 'grad_norm': 1.0353890657424927, 'learning_rate': 0.00019571641423732963, 'epoch': 0.18}
{'loss': 0.9681, 'grad_norm': 1.3585636615753174, 'learning_rate': 0.00019570085111729964, 'epoch': 0.18}
{'loss': 1.1238, 'grad_norm': 1.0890856981277466, 'learning_rate': 0.0001956852603976052, 'epoch': 0.18}
{'loss': 0.7759, 'grad_norm': 1.1147289276123047, 'learning_rate': 0.00019566964208274254, 'epoch': 0.18}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9402, 'grad_norm': 1.2580972909927368, 'learning_rate': 0.00019565399617721586, 'epoch': 0.18}
{'loss': 0.8608, 'grad_norm': 1.3835511207580566, 'learning_rate': 0.00019563832268553743, 'epoch': 0.18}
{'loss': 0.7601, 'grad_norm': 1.3940231800079346, 'learning_rate': 0.0001956226216122274, 'epoch': 0.19}
{'loss': 0.8996, 'grad_norm': 1.449601173400879, 'learning_rate': 0.00019560689296181386, 'epoch': 0.19}
{'loss': 1.0924, 'grad_norm': 1.2966645956039429, 'learning_rate': 0.0001955911367388329, 'epoch': 0.19}
{'loss': 0.9837, 'grad_norm': 1.1126258373260498, 'learning_rate': 0.00019557535294782854, 'epoch': 0.19}
{'loss': 0.8154, 'grad_norm': 1.933350920677185, 'learning_rate': 0.0001955595415933527, 'epoch': 0.19}
{'loss': 0.8445, 'grad_norm': 1.1251083612442017, 'learning_rate': 0.00019554370267996538, 'epoch': 0.19}
{'loss': 1.2332, 'grad_norm': 1.2405034303665161, 'learning_rate': 0.00019552783621223436, 'epoch': 0.19}
{'loss': 1.0844, 'grad_norm': 1.266902208328247, 'learning_rate': 0.00019551194219473553, 'epoch': 0.19}
{'loss': 1.1896, 'grad_norm': 1.6961332559585571, 'learning_rate': 0.0001954960206320526, 'epoch': 0.19}
{'loss': 0.9844, 'grad_norm': 0.9682297110557556, 'learning_rate': 0.0001954800715287773, 'epoch': 0.19}
{'loss': 1.3323, 'grad_norm': 1.1531776189804077, 'learning_rate': 0.00019546409488950924, 'epoch': 0.19}
{'loss': 0.8488, 'grad_norm': 1.5765751600265503, 'learning_rate': 0.00019544809071885604, 'epoch': 0.19}
{'loss': 0.8052, 'grad_norm': 1.244543194770813, 'learning_rate': 0.0001954320590214332, 'epoch': 0.19}
{'loss': 0.9451, 'grad_norm': 1.2549066543579102, 'learning_rate': 0.0001954159998018642, 'epoch': 0.19}
{'loss': 0.7973, 'grad_norm': 1.2140493392944336, 'learning_rate': 0.00019539991306478046, 'epoch': 0.19}
{'loss': 0.8191, 'grad_norm': 1.1517642736434937, 'learning_rate': 0.00019538379881482127, 'epoch': 0.19}
{'loss': 1.2902, 'grad_norm': 1.0931082963943481, 'learning_rate': 0.00019536765705663393, 'epoch': 0.19}
{'loss': 0.916, 'grad_norm': 1.6996756792068481, 'learning_rate': 0.00019535148779487363, 'epoch': 0.19}
{'loss': 0.9836, 'grad_norm': 1.2104346752166748, 'learning_rate': 0.00019533529103420353, 'epoch': 0.19}
{'loss': 0.9058, 'grad_norm': 1.4221222400665283, 'learning_rate': 0.0001953190667792947, 'epoch': 0.19}
{'loss': 0.9059, 'grad_norm': 2.142704963684082, 'learning_rate': 0.00019530281503482612, 'epoch': 0.19}
{'loss': 0.9909, 'grad_norm': 1.309000015258789, 'learning_rate': 0.00019528653580548472, 'epoch': 0.19}
{'loss': 0.8618, 'grad_norm': 1.3798962831497192, 'learning_rate': 0.00019527022909596536, 'epoch': 0.19}
{'loss': 0.9424, 'grad_norm': 1.027076244354248, 'learning_rate': 0.0001952538949109708, 'epoch': 0.19}
{'loss': 0.9299, 'grad_norm': 2.965630054473877, 'learning_rate': 0.00019523753325521176, 'epoch': 0.19}
{'loss': 1.4217, 'grad_norm': 1.4054969549179077, 'learning_rate': 0.00019522114413340682, 'epoch': 0.19}
{'loss': 0.8123, 'grad_norm': 1.6830503940582275, 'learning_rate': 0.00019520472755028256, 'epoch': 0.19}
{'loss': 0.8863, 'grad_norm': 1.3291850090026855, 'learning_rate': 0.00019518828351057345, 'epoch': 0.19}
{'loss': 1.1294, 'grad_norm': 1.1585655212402344, 'learning_rate': 0.00019517181201902183, 'epoch': 0.19}
{'loss': 1.1172, 'grad_norm': 1.4099583625793457, 'learning_rate': 0.00019515531308037808, 'epoch': 0.19}
{'loss': 0.9099, 'grad_norm': 1.2260009050369263, 'learning_rate': 0.00019513878669940034, 'epoch': 0.19}
{'loss': 0.8833, 'grad_norm': 1.4389622211456299, 'learning_rate': 0.00019512223288085476, 'epoch': 0.19}
{'loss': 1.2271, 'grad_norm': 1.495901346206665, 'learning_rate': 0.00019510565162951537, 'epoch': 0.19}
{'loss': 1.0918, 'grad_norm': 1.1491714715957642, 'learning_rate': 0.00019508904295016414, 'epoch': 0.19}
{'loss': 0.9, 'grad_norm': 1.3105340003967285, 'learning_rate': 0.00019507240684759093, 'epoch': 0.19}
{'loss': 1.0232, 'grad_norm': 1.0350592136383057, 'learning_rate': 0.0001950557433265935, 'epoch': 0.19}
{'loss': 1.1353, 'grad_norm': 1.4246037006378174, 'learning_rate': 0.00019503905239197757, 'epoch': 0.19}
{'loss': 1.4085, 'grad_norm': 1.0561387538909912, 'learning_rate': 0.0001950223340485567, 'epoch': 0.19}
{'loss': 1.1162, 'grad_norm': 1.273281455039978, 'learning_rate': 0.00019500558830115233, 'epoch': 0.19}
{'loss': 1.1728, 'grad_norm': 1.3225101232528687, 'learning_rate': 0.00019498881515459394, 'epoch': 0.19}
{'loss': 1.2799, 'grad_norm': 0.965293288230896, 'learning_rate': 0.00019497201461371878, 'epoch': 0.19}
{'loss': 0.9619, 'grad_norm': 1.313551664352417, 'learning_rate': 0.00019495518668337201, 'epoch': 0.19}
{'loss': 0.9969, 'grad_norm': 1.268284559249878, 'learning_rate': 0.00019493833136840683, 'epoch': 0.19}
{'loss': 1.1792, 'grad_norm': 1.6776132583618164, 'learning_rate': 0.0001949214486736841, 'epoch': 0.19}
{'loss': 0.9136, 'grad_norm': 1.4032231569290161, 'learning_rate': 0.00019490453860407278, 'epoch': 0.19}
{'loss': 1.1789, 'grad_norm': 1.2388861179351807, 'learning_rate': 0.00019488760116444966, 'epoch': 0.19}
{'loss': 0.9786, 'grad_norm': 1.504838228225708, 'learning_rate': 0.0001948706363596994, 'epoch': 0.19}
{'loss': 0.9868, 'grad_norm': 1.3621677160263062, 'learning_rate': 0.00019485364419471454, 'epoch': 0.19}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0398, 'grad_norm': 1.3348233699798584, 'learning_rate': 0.00019483662467439561, 'epoch': 0.19}
{'loss': 0.8756, 'grad_norm': 1.2919059991836548, 'learning_rate': 0.0001948195778036509, 'epoch': 0.19}
{'loss': 1.0555, 'grad_norm': 1.6042283773422241, 'learning_rate': 0.00019480250358739663, 'epoch': 0.19}
{'loss': 0.7613, 'grad_norm': 0.991939902305603, 'learning_rate': 0.00019478540203055698, 'epoch': 0.19}
{'loss': 1.0778, 'grad_norm': 1.1684091091156006, 'learning_rate': 0.00019476827313806393, 'epoch': 0.19}
{'loss': 0.9682, 'grad_norm': 1.1589012145996094, 'learning_rate': 0.00019475111691485736, 'epoch': 0.19}
{'loss': 0.7263, 'grad_norm': 1.3442261219024658, 'learning_rate': 0.00019473393336588506, 'epoch': 0.19}
{'loss': 0.9099, 'grad_norm': 1.028322696685791, 'learning_rate': 0.00019471672249610266, 'epoch': 0.19}
{'loss': 0.9439, 'grad_norm': 1.081284999847412, 'learning_rate': 0.0001946994843104737, 'epoch': 0.19}
{'loss': 1.179, 'grad_norm': 1.171154260635376, 'learning_rate': 0.0001946822188139696, 'epoch': 0.19}
{'loss': 1.0878, 'grad_norm': 1.4636203050613403, 'learning_rate': 0.00019466492601156966, 'epoch': 0.19}
{'loss': 1.027, 'grad_norm': 1.666316032409668, 'learning_rate': 0.00019464760590826098, 'epoch': 0.19}
{'loss': 0.6807, 'grad_norm': 1.2675049304962158, 'learning_rate': 0.00019463025850903866, 'epoch': 0.19}
{'loss': 1.118, 'grad_norm': 1.443284034729004, 'learning_rate': 0.00019461288381890558, 'epoch': 0.19}
{'loss': 1.2606, 'grad_norm': 1.7535477876663208, 'learning_rate': 0.00019459548184287253, 'epoch': 0.19}
{'loss': 1.0958, 'grad_norm': 2.328664541244507, 'learning_rate': 0.00019457805258595815, 'epoch': 0.19}
{'loss': 1.038, 'grad_norm': 1.091407060623169, 'learning_rate': 0.00019456059605318894, 'epoch': 0.19}
{'loss': 0.9114, 'grad_norm': 1.4685630798339844, 'learning_rate': 0.00019454311224959927, 'epoch': 0.2}
{'loss': 0.7559, 'grad_norm': 1.5101162195205688, 'learning_rate': 0.0001945256011802314, 'epoch': 0.2}
{'loss': 1.1254, 'grad_norm': 1.2574317455291748, 'learning_rate': 0.00019450806285013547, 'epoch': 0.2}
{'loss': 0.9463, 'grad_norm': 0.972636342048645, 'learning_rate': 0.0001944904972643694, 'epoch': 0.2}
{'loss': 0.6573, 'grad_norm': 1.40968918800354, 'learning_rate': 0.00019447290442799905, 'epoch': 0.2}
{'loss': 1.1122, 'grad_norm': 1.2416024208068848, 'learning_rate': 0.0001944552843460981, 'epoch': 0.2}
{'loss': 0.7922, 'grad_norm': 1.091727614402771, 'learning_rate': 0.00019443763702374812, 'epoch': 0.2}
{'loss': 0.7443, 'grad_norm': 1.2089807987213135, 'learning_rate': 0.00019441996246603846, 'epoch': 0.2}
{'loss': 1.2562, 'grad_norm': 4.684742450714111, 'learning_rate': 0.00019440226067806644, 'epoch': 0.2}
{'loss': 0.8961, 'grad_norm': 1.1217408180236816, 'learning_rate': 0.00019438453166493712, 'epoch': 0.2}
{'loss': 0.9613, 'grad_norm': 1.4794068336486816, 'learning_rate': 0.00019436677543176347, 'epoch': 0.2}
{'loss': 1.0103, 'grad_norm': 1.1866121292114258, 'learning_rate': 0.00019434899198366633, 'epoch': 0.2}
{'loss': 1.0636, 'grad_norm': 1.1735846996307373, 'learning_rate': 0.0001943311813257743, 'epoch': 0.2}
{'loss': 0.8944, 'grad_norm': 1.198169469833374, 'learning_rate': 0.00019431334346322397, 'epoch': 0.2}
{'loss': 1.0178, 'grad_norm': 1.177542805671692, 'learning_rate': 0.00019429547840115968, 'epoch': 0.2}
{'loss': 0.9792, 'grad_norm': 1.4042011499404907, 'learning_rate': 0.00019427758614473355, 'epoch': 0.2}
{'loss': 1.031, 'grad_norm': 1.5728037357330322, 'learning_rate': 0.0001942596666991057, 'epoch': 0.2}
{'loss': 1.1576, 'grad_norm': 1.0613776445388794, 'learning_rate': 0.00019424172006944398, 'epoch': 0.2}
{'loss': 0.8375, 'grad_norm': 1.8684710264205933, 'learning_rate': 0.0001942237462609241, 'epoch': 0.2}
{'loss': 1.0378, 'grad_norm': 1.834547758102417, 'learning_rate': 0.00019420574527872968, 'epoch': 0.2}
{'loss': 1.0591, 'grad_norm': 1.1380615234375, 'learning_rate': 0.00019418771712805203, 'epoch': 0.2}
{'loss': 0.7862, 'grad_norm': 1.2927894592285156, 'learning_rate': 0.00019416966181409046, 'epoch': 0.2}
{'loss': 1.0636, 'grad_norm': 1.3177911043167114, 'learning_rate': 0.000194151579342052, 'epoch': 0.2}
{'loss': 1.3144, 'grad_norm': 1.7096199989318848, 'learning_rate': 0.00019413346971715152, 'epoch': 0.2}
{'loss': 1.1929, 'grad_norm': 1.690450668334961, 'learning_rate': 0.0001941153329446118, 'epoch': 0.2}
{'loss': 1.1285, 'grad_norm': 1.100394606590271, 'learning_rate': 0.00019409716902966335, 'epoch': 0.2}
{'loss': 1.4442, 'grad_norm': 1.2704391479492188, 'learning_rate': 0.0001940789779775446, 'epoch': 0.2}
{'loss': 1.3427, 'grad_norm': 0.9346479773521423, 'learning_rate': 0.00019406075979350174, 'epoch': 0.2}
{'loss': 0.7844, 'grad_norm': 1.2370973825454712, 'learning_rate': 0.0001940425144827888, 'epoch': 0.2}
{'loss': 1.2034, 'grad_norm': 1.1591297388076782, 'learning_rate': 0.00019402424205066765, 'epoch': 0.2}
{'loss': 1.0229, 'grad_norm': 0.9824618697166443, 'learning_rate': 0.00019400594250240798, 'epoch': 0.2}
{'loss': 0.7602, 'grad_norm': 1.0880109071731567, 'learning_rate': 0.00019398761584328727, 'epoch': 0.2}
{'loss': 1.2468, 'grad_norm': 1.348852276802063, 'learning_rate': 0.00019396926207859084, 'epoch': 0.2}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0487, 'grad_norm': 1.1096763610839844, 'learning_rate': 0.00019395088121361187, 'epoch': 0.2}
{'loss': 0.892, 'grad_norm': 1.2211700677871704, 'learning_rate': 0.00019393247325365126, 'epoch': 0.2}
{'loss': 0.77, 'grad_norm': 1.437476396560669, 'learning_rate': 0.00019391403820401778, 'epoch': 0.2}
{'loss': 0.8568, 'grad_norm': 1.7429593801498413, 'learning_rate': 0.00019389557607002805, 'epoch': 0.2}
{'loss': 1.064, 'grad_norm': 1.362336277961731, 'learning_rate': 0.00019387708685700646, 'epoch': 0.2}
{'loss': 1.1079, 'grad_norm': 1.4355732202529907, 'learning_rate': 0.00019385857057028518, 'epoch': 0.2}
{'loss': 1.0907, 'grad_norm': 1.2017050981521606, 'learning_rate': 0.0001938400272152042, 'epoch': 0.2}
{'loss': 1.0911, 'grad_norm': 1.1600474119186401, 'learning_rate': 0.00019382145679711142, 'epoch': 0.2}
{'loss': 0.8802, 'grad_norm': 1.4384180307388306, 'learning_rate': 0.00019380285932136233, 'epoch': 0.2}
{'loss': 1.0839, 'grad_norm': 1.3834630250930786, 'learning_rate': 0.00019378423479332046, 'epoch': 0.2}
{'loss': 0.8425, 'grad_norm': 1.9467898607254028, 'learning_rate': 0.00019376558321835698, 'epoch': 0.2}
{'loss': 0.9415, 'grad_norm': 1.0817421674728394, 'learning_rate': 0.00019374690460185093, 'epoch': 0.2}
{'loss': 1.0946, 'grad_norm': 1.794360876083374, 'learning_rate': 0.00019372819894918915, 'epoch': 0.2}
{'loss': 0.9549, 'grad_norm': 1.321266770362854, 'learning_rate': 0.00019370946626576626, 'epoch': 0.2}
{'loss': 1.0838, 'grad_norm': 1.2894377708435059, 'learning_rate': 0.00019369070655698463, 'epoch': 0.2}
{'loss': 1.2011, 'grad_norm': 1.0499128103256226, 'learning_rate': 0.0001936719198282545, 'epoch': 0.2}
{'loss': 0.9514, 'grad_norm': 1.1604459285736084, 'learning_rate': 0.0001936531060849939, 'epoch': 0.2}
{'loss': 1.2247, 'grad_norm': 1.1641122102737427, 'learning_rate': 0.00019363426533262857, 'epoch': 0.2}
{'loss': 0.8261, 'grad_norm': 1.2550289630889893, 'learning_rate': 0.0001936153975765921, 'epoch': 0.2}
{'loss': 1.3484, 'grad_norm': 1.1766060590744019, 'learning_rate': 0.0001935965028223259, 'epoch': 0.2}
{'loss': 1.0275, 'grad_norm': 1.8788201808929443, 'learning_rate': 0.00019357758107527913, 'epoch': 0.2}
{'loss': 1.1337, 'grad_norm': 1.083971619606018, 'learning_rate': 0.00019355863234090864, 'epoch': 0.2}
{'loss': 0.9967, 'grad_norm': 1.4685065746307373, 'learning_rate': 0.00019353965662467925, 'epoch': 0.2}
{'loss': 0.8626, 'grad_norm': 1.2080961465835571, 'learning_rate': 0.00019352065393206345, 'epoch': 0.2}
{'loss': 1.1436, 'grad_norm': 1.4153047800064087, 'learning_rate': 0.0001935016242685415, 'epoch': 0.2}
{'loss': 1.2987, 'grad_norm': 1.1442861557006836, 'learning_rate': 0.00019348256763960145, 'epoch': 0.2}
{'loss': 1.0492, 'grad_norm': 1.3272101879119873, 'learning_rate': 0.00019346348405073916, 'epoch': 0.2}
{'loss': 1.2427, 'grad_norm': 1.2117873430252075, 'learning_rate': 0.00019344437350745828, 'epoch': 0.2}
{'loss': 0.8902, 'grad_norm': 1.1405586004257202, 'learning_rate': 0.00019342523601527015, 'epoch': 0.2}
{'loss': 1.0342, 'grad_norm': 1.3461107015609741, 'learning_rate': 0.00019340607157969394, 'epoch': 0.2}
{'loss': 0.7369, 'grad_norm': 1.2724915742874146, 'learning_rate': 0.00019338688020625658, 'epoch': 0.2}
{'loss': 1.0345, 'grad_norm': 1.1472749710083008, 'learning_rate': 0.00019336766190049278, 'epoch': 0.2}
{'loss': 1.254, 'grad_norm': 1.2062273025512695, 'learning_rate': 0.000193348416667945, 'epoch': 0.21}
{'loss': 1.1281, 'grad_norm': 1.584038257598877, 'learning_rate': 0.00019332914451416347, 'epoch': 0.21}
{'loss': 0.9937, 'grad_norm': 1.502213716506958, 'learning_rate': 0.0001933098454447062, 'epoch': 0.21}
{'loss': 1.3032, 'grad_norm': 1.2425836324691772, 'learning_rate': 0.00019329051946513897, 'epoch': 0.21}
{'loss': 1.2447, 'grad_norm': 1.5157511234283447, 'learning_rate': 0.00019327116658103525, 'epoch': 0.21}
{'loss': 0.9986, 'grad_norm': 1.3591212034225464, 'learning_rate': 0.0001932517867979763, 'epoch': 0.21}
{'loss': 1.2315, 'grad_norm': 1.1881788969039917, 'learning_rate': 0.00019323238012155123, 'epoch': 0.21}
{'loss': 1.2248, 'grad_norm': 1.5227978229522705, 'learning_rate': 0.0001932129465573568, 'epoch': 0.21}
{'loss': 1.0653, 'grad_norm': 1.519622802734375, 'learning_rate': 0.00019319348611099752, 'epoch': 0.21}
{'loss': 1.1037, 'grad_norm': 1.4613854885101318, 'learning_rate': 0.00019317399878808575, 'epoch': 0.21}
{'loss': 0.9895, 'grad_norm': 1.2217686176300049, 'learning_rate': 0.0001931544845942415, 'epoch': 0.21}
{'loss': 1.0732, 'grad_norm': 1.2013007402420044, 'learning_rate': 0.00019313494353509258, 'epoch': 0.21}
{'loss': 0.8753, 'grad_norm': 1.2730377912521362, 'learning_rate': 0.00019311537561627454, 'epoch': 0.21}
{'loss': 0.8608, 'grad_norm': 1.38143789768219, 'learning_rate': 0.0001930957808434307, 'epoch': 0.21}
{'loss': 1.2099, 'grad_norm': 1.1728838682174683, 'learning_rate': 0.00019307615922221203, 'epoch': 0.21}
{'loss': 0.9107, 'grad_norm': 1.839340329170227, 'learning_rate': 0.00019305651075827735, 'epoch': 0.21}
{'loss': 0.7568, 'grad_norm': 1.1040642261505127, 'learning_rate': 0.00019303683545729322, 'epoch': 0.21}
{'loss': 1.1759, 'grad_norm': 1.1499754190444946, 'learning_rate': 0.00019301713332493386, 'epoch': 0.21}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8545, 'grad_norm': 1.3481285572052002, 'learning_rate': 0.00019299740436688124, 'epoch': 0.21}
{'loss': 1.0327, 'grad_norm': 1.5731470584869385, 'learning_rate': 0.00019297764858882514, 'epoch': 0.21}
{'loss': 1.2624, 'grad_norm': 1.1644234657287598, 'learning_rate': 0.00019295786599646305, 'epoch': 0.21}
{'loss': 0.9846, 'grad_norm': 1.296628713607788, 'learning_rate': 0.00019293805659550013, 'epoch': 0.21}
{'loss': 0.9722, 'grad_norm': 1.505091905593872, 'learning_rate': 0.00019291822039164933, 'epoch': 0.21}
{'loss': 0.602, 'grad_norm': 1.2395509481430054, 'learning_rate': 0.00019289835739063133, 'epoch': 0.21}
{'loss': 1.2658, 'grad_norm': 1.147770643234253, 'learning_rate': 0.00019287846759817451, 'epoch': 0.21}
{'loss': 1.2213, 'grad_norm': 1.0287065505981445, 'learning_rate': 0.00019285855102001498, 'epoch': 0.21}
{'loss': 0.995, 'grad_norm': 1.3170571327209473, 'learning_rate': 0.0001928386076618966, 'epoch': 0.21}
{'loss': 1.0052, 'grad_norm': 1.5842009782791138, 'learning_rate': 0.00019281863752957095, 'epoch': 0.21}
{'loss': 1.3749, 'grad_norm': 1.7085801362991333, 'learning_rate': 0.00019279864062879732, 'epoch': 0.21}
{'loss': 0.9798, 'grad_norm': 1.279175043106079, 'learning_rate': 0.00019277861696534267, 'epoch': 0.21}
{'loss': 1.2043, 'grad_norm': 1.4590290784835815, 'learning_rate': 0.00019275856654498179, 'epoch': 0.21}
{'loss': 1.1075, 'grad_norm': 1.1094120740890503, 'learning_rate': 0.0001927384893734971, 'epoch': 0.21}
{'loss': 1.1303, 'grad_norm': 0.9679877161979675, 'learning_rate': 0.00019271838545667876, 'epoch': 0.21}
{'loss': 1.1589, 'grad_norm': 1.177577018737793, 'learning_rate': 0.00019269825480032463, 'epoch': 0.21}
{'loss': 0.7428, 'grad_norm': 1.3362675905227661, 'learning_rate': 0.0001926780974102403, 'epoch': 0.21}
{'loss': 0.9732, 'grad_norm': 1.3356009721755981, 'learning_rate': 0.00019265791329223909, 'epoch': 0.21}
{'loss': 1.077, 'grad_norm': 1.6040115356445312, 'learning_rate': 0.00019263770245214198, 'epoch': 0.21}
{'loss': 0.9471, 'grad_norm': 1.3226369619369507, 'learning_rate': 0.00019261746489577765, 'epoch': 0.21}
{'loss': 0.9048, 'grad_norm': 1.2142488956451416, 'learning_rate': 0.00019259720062898258, 'epoch': 0.21}
{'loss': 0.7405, 'grad_norm': 1.3340662717819214, 'learning_rate': 0.00019257690965760084, 'epoch': 0.21}
{'loss': 1.1694, 'grad_norm': 1.2572582960128784, 'learning_rate': 0.0001925565919874843, 'epoch': 0.21}
{'loss': 0.9643, 'grad_norm': 1.4680784940719604, 'learning_rate': 0.00019253624762449238, 'epoch': 0.21}
{'loss': 0.833, 'grad_norm': 1.1858593225479126, 'learning_rate': 0.00019251587657449236, 'epoch': 0.21}
{'loss': 0.7499, 'grad_norm': 1.3609658479690552, 'learning_rate': 0.00019249547884335916, 'epoch': 0.21}
{'loss': 1.0034, 'grad_norm': 1.0727568864822388, 'learning_rate': 0.00019247505443697538, 'epoch': 0.21}
{'loss': 1.0496, 'grad_norm': 1.1206618547439575, 'learning_rate': 0.00019245460336123134, 'epoch': 0.21}
{'loss': 1.2126, 'grad_norm': 1.3478749990463257, 'learning_rate': 0.00019243412562202497, 'epoch': 0.21}
{'loss': 1.2177, 'grad_norm': 1.2577437162399292, 'learning_rate': 0.000192413621225262, 'epoch': 0.21}
{'loss': 1.2578, 'grad_norm': 1.3813425302505493, 'learning_rate': 0.0001923930901768558, 'epoch': 0.21}
{'loss': 1.1569, 'grad_norm': 1.1106795072555542, 'learning_rate': 0.0001923725324827274, 'epoch': 0.21}
{'loss': 0.9799, 'grad_norm': 1.1737289428710938, 'learning_rate': 0.00019235194814880554, 'epoch': 0.21}
{'loss': 1.1136, 'grad_norm': 1.924875020980835, 'learning_rate': 0.00019233133718102668, 'epoch': 0.21}
{'loss': 0.959, 'grad_norm': 1.4280250072479248, 'learning_rate': 0.0001923106995853349, 'epoch': 0.21}
{'loss': 0.9534, 'grad_norm': 1.3606115579605103, 'learning_rate': 0.00019229003536768197, 'epoch': 0.21}
{'loss': 1.3517, 'grad_norm': 2.178197145462036, 'learning_rate': 0.00019226934453402738, 'epoch': 0.21}
{'loss': 0.7236, 'grad_norm': 1.7051342725753784, 'learning_rate': 0.00019224862709033824, 'epoch': 0.21}
{'loss': 0.8284, 'grad_norm': 0.9024561047554016, 'learning_rate': 0.00019222788304258938, 'epoch': 0.21}
{'loss': 1.1774, 'grad_norm': 1.3160072565078735, 'learning_rate': 0.00019220711239676327, 'epoch': 0.21}
{'loss': 0.9457, 'grad_norm': 1.195328712463379, 'learning_rate': 0.00019218631515885006, 'epoch': 0.21}
{'loss': 0.8514, 'grad_norm': 1.7286876440048218, 'learning_rate': 0.0001921654913348476, 'epoch': 0.21}
{'loss': 0.6466, 'grad_norm': 1.2873780727386475, 'learning_rate': 0.00019214464093076137, 'epoch': 0.21}
{'loss': 0.9691, 'grad_norm': 1.064349889755249, 'learning_rate': 0.00019212376395260448, 'epoch': 0.21}
{'loss': 0.7852, 'grad_norm': 1.5220956802368164, 'learning_rate': 0.00019210286040639783, 'epoch': 0.21}
{'loss': 1.1623, 'grad_norm': 1.3130515813827515, 'learning_rate': 0.00019208193029816983, 'epoch': 0.21}
{'loss': 0.8645, 'grad_norm': 1.585161566734314, 'learning_rate': 0.0001920609736339567, 'epoch': 0.21}
{'loss': 1.0457, 'grad_norm': 1.3061435222625732, 'learning_rate': 0.00019203999041980213, 'epoch': 0.22}
{'loss': 1.0729, 'grad_norm': 1.3107627630233765, 'learning_rate': 0.0001920189806617577, 'epoch': 0.22}
{'loss': 0.9556, 'grad_norm': 1.1187140941619873, 'learning_rate': 0.00019199794436588243, 'epoch': 0.22}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0887, 'grad_norm': 1.2256172895431519, 'learning_rate': 0.00019197688153824313, 'epoch': 0.22}
{'loss': 1.1712, 'grad_norm': 1.3331894874572754, 'learning_rate': 0.0001919557921849142, 'epoch': 0.22}
{'loss': 0.7995, 'grad_norm': 1.1502538919448853, 'learning_rate': 0.00019193467631197777, 'epoch': 0.22}
{'loss': 0.9282, 'grad_norm': 1.2173082828521729, 'learning_rate': 0.00019191353392552344, 'epoch': 0.22}
{'loss': 0.7304, 'grad_norm': 1.4690971374511719, 'learning_rate': 0.0001918923650316487, 'epoch': 0.22}
{'loss': 1.2432, 'grad_norm': 1.2714931964874268, 'learning_rate': 0.00019187116963645842, 'epoch': 0.22}
{'loss': 0.7437, 'grad_norm': 1.2510788440704346, 'learning_rate': 0.0001918499477460654, 'epoch': 0.22}
{'loss': 1.0554, 'grad_norm': 1.1848127841949463, 'learning_rate': 0.0001918286993665898, 'epoch': 0.22}
{'loss': 1.2568, 'grad_norm': 1.241112470626831, 'learning_rate': 0.00019180742450415964, 'epoch': 0.22}
{'loss': 0.9897, 'grad_norm': 1.320712685585022, 'learning_rate': 0.0001917861231649104, 'epoch': 0.22}
{'loss': 1.4806, 'grad_norm': 1.0968366861343384, 'learning_rate': 0.0001917647953549854, 'epoch': 0.22}
{'loss': 0.9076, 'grad_norm': 1.2811763286590576, 'learning_rate': 0.00019174344108053537, 'epoch': 0.22}
{'loss': 0.702, 'grad_norm': 1.1705878973007202, 'learning_rate': 0.0001917220603477188, 'epoch': 0.22}
{'loss': 0.951, 'grad_norm': 1.4543650150299072, 'learning_rate': 0.00019170065316270187, 'epoch': 0.22}
{'loss': 1.1206, 'grad_norm': 1.0348565578460693, 'learning_rate': 0.00019167921953165825, 'epoch': 0.22}
{'loss': 0.9732, 'grad_norm': 1.2251403331756592, 'learning_rate': 0.00019165775946076928, 'epoch': 0.22}
{'loss': 1.164, 'grad_norm': 3.285101890563965, 'learning_rate': 0.00019163627295622397, 'epoch': 0.22}
{'loss': 0.7508, 'grad_norm': 1.648116111755371, 'learning_rate': 0.0001916147600242189, 'epoch': 0.22}
{'loss': 0.9611, 'grad_norm': 1.4445852041244507, 'learning_rate': 0.00019159322067095833, 'epoch': 0.22}
{'loss': 0.9283, 'grad_norm': 1.1720428466796875, 'learning_rate': 0.00019157165490265408, 'epoch': 0.22}
{'loss': 1.1852, 'grad_norm': 1.632607340812683, 'learning_rate': 0.00019155006272552561, 'epoch': 0.22}
{'loss': 0.864, 'grad_norm': 1.3342612981796265, 'learning_rate': 0.00019152844414580003, 'epoch': 0.22}
{'loss': 0.8638, 'grad_norm': 1.4867695569992065, 'learning_rate': 0.000191506799169712, 'epoch': 0.22}
{'loss': 1.0524, 'grad_norm': 1.1392205953598022, 'learning_rate': 0.00019148512780350384, 'epoch': 0.22}
{'loss': 1.1619, 'grad_norm': 1.6060024499893188, 'learning_rate': 0.00019146343005342547, 'epoch': 0.22}
{'loss': 0.7792, 'grad_norm': 1.3606369495391846, 'learning_rate': 0.00019144170592573444, 'epoch': 0.22}
{'loss': 1.1001, 'grad_norm': 1.300075650215149, 'learning_rate': 0.0001914199554266958, 'epoch': 0.22}
{'loss': 0.6828, 'grad_norm': 1.1950749158859253, 'learning_rate': 0.0001913981785625824, 'epoch': 0.22}
{'loss': 0.8484, 'grad_norm': 1.4400883913040161, 'learning_rate': 0.0001913763753396745, 'epoch': 0.22}
{'loss': 1.1697, 'grad_norm': 1.1586694717407227, 'learning_rate': 0.0001913545457642601, 'epoch': 0.22}
{'loss': 1.2161, 'grad_norm': 1.388918161392212, 'learning_rate': 0.00019133268984263473, 'epoch': 0.22}
{'loss': 1.1437, 'grad_norm': 0.9816541075706482, 'learning_rate': 0.00019131080758110148, 'epoch': 0.22}
{'loss': 0.9086, 'grad_norm': 1.208245038986206, 'learning_rate': 0.00019128889898597116, 'epoch': 0.22}
{'loss': 1.1505, 'grad_norm': 1.0965216159820557, 'learning_rate': 0.00019126696406356205, 'epoch': 0.22}
{'loss': 1.1039, 'grad_norm': 1.5133614540100098, 'learning_rate': 0.00019124500282020014, 'epoch': 0.22}
{'loss': 1.1014, 'grad_norm': 1.246640920639038, 'learning_rate': 0.0001912230152622189, 'epoch': 0.22}
{'loss': 1.0626, 'grad_norm': 1.0605213642120361, 'learning_rate': 0.0001912010013959594, 'epoch': 0.22}
{'loss': 1.0738, 'grad_norm': 1.2566293478012085, 'learning_rate': 0.00019117896122777044, 'epoch': 0.22}
{'loss': 1.3245, 'grad_norm': 1.1694436073303223, 'learning_rate': 0.00019115689476400816, 'epoch': 0.22}
{'loss': 0.789, 'grad_norm': 1.1465446949005127, 'learning_rate': 0.00019113480201103658, 'epoch': 0.22}
{'loss': 1.0746, 'grad_norm': 1.3057310581207275, 'learning_rate': 0.000191112682975227, 'epoch': 0.22}
{'loss': 1.0139, 'grad_norm': 0.9914039969444275, 'learning_rate': 0.0001910905376629585, 'epoch': 0.22}
{'loss': 0.9594, 'grad_norm': 1.4373687505722046, 'learning_rate': 0.00019106836608061772, 'epoch': 0.22}
{'loss': 1.0672, 'grad_norm': 1.6871941089630127, 'learning_rate': 0.0001910461682345988, 'epoch': 0.22}
{'loss': 0.7544, 'grad_norm': 0.976228654384613, 'learning_rate': 0.00019102394413130346, 'epoch': 0.22}
{'loss': 0.971, 'grad_norm': 1.4846056699752808, 'learning_rate': 0.0001910016937771411, 'epoch': 0.22}
{'loss': 1.1306, 'grad_norm': 1.8549383878707886, 'learning_rate': 0.00019097941717852854, 'epoch': 0.22}
{'loss': 1.0264, 'grad_norm': 1.1688337326049805, 'learning_rate': 0.0001909571143418903, 'epoch': 0.22}
{'loss': 0.92, 'grad_norm': 1.4123543500900269, 'learning_rate': 0.00019093478527365838, 'epoch': 0.22}
{'loss': 1.0492, 'grad_norm': 1.3279474973678589, 'learning_rate': 0.0001909124299802724, 'epoch': 0.22}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1325, 'grad_norm': 1.398626446723938, 'learning_rate': 0.00019089004846817946, 'epoch': 0.22}
{'loss': 0.9779, 'grad_norm': 1.4993494749069214, 'learning_rate': 0.00019086764074383435, 'epoch': 0.22}
{'loss': 0.9756, 'grad_norm': 1.2591112852096558, 'learning_rate': 0.0001908452068136993, 'epoch': 0.22}
{'loss': 0.7891, 'grad_norm': 1.047810435295105, 'learning_rate': 0.00019082274668424422, 'epoch': 0.22}
{'loss': 0.8712, 'grad_norm': 1.4915547370910645, 'learning_rate': 0.00019080026036194642, 'epoch': 0.22}
{'loss': 1.2464, 'grad_norm': 1.2367950677871704, 'learning_rate': 0.00019077774785329087, 'epoch': 0.22}
{'loss': 0.9072, 'grad_norm': 1.1302844285964966, 'learning_rate': 0.0001907552091647701, 'epoch': 0.22}
{'loss': 1.2874, 'grad_norm': 1.2066422700881958, 'learning_rate': 0.00019073264430288415, 'epoch': 0.22}
{'loss': 0.7567, 'grad_norm': 1.4527769088745117, 'learning_rate': 0.00019071005327414055, 'epoch': 0.22}
{'loss': 0.7617, 'grad_norm': 2.1153624057769775, 'learning_rate': 0.00019068743608505455, 'epoch': 0.22}
{'loss': 1.1379, 'grad_norm': 1.8483258485794067, 'learning_rate': 0.00019066479274214876, 'epoch': 0.22}
{'loss': 0.9782, 'grad_norm': 0.9987156391143799, 'learning_rate': 0.00019064212325195347, 'epoch': 0.22}
{'loss': 1.0879, 'grad_norm': 1.2763152122497559, 'learning_rate': 0.0001906194276210064, 'epoch': 0.23}
{'loss': 0.8918, 'grad_norm': 1.1547931432724, 'learning_rate': 0.0001905967058558529, 'epoch': 0.23}
{'loss': 0.8041, 'grad_norm': 1.4945709705352783, 'learning_rate': 0.00019057395796304578, 'epoch': 0.23}
{'loss': 0.9656, 'grad_norm': 1.0939589738845825, 'learning_rate': 0.00019055118394914545, 'epoch': 0.23}
{'loss': 0.9054, 'grad_norm': 1.3971891403198242, 'learning_rate': 0.0001905283838207198, 'epoch': 0.23}
{'loss': 0.8302, 'grad_norm': 1.4134607315063477, 'learning_rate': 0.00019050555758434433, 'epoch': 0.23}
{'loss': 1.0944, 'grad_norm': 1.4580143690109253, 'learning_rate': 0.00019048270524660196, 'epoch': 0.23}
{'loss': 0.985, 'grad_norm': 1.6337968111038208, 'learning_rate': 0.00019045982681408324, 'epoch': 0.23}
{'loss': 0.979, 'grad_norm': 0.9957546591758728, 'learning_rate': 0.00019043692229338613, 'epoch': 0.23}
{'loss': 0.9747, 'grad_norm': 0.9228186011314392, 'learning_rate': 0.00019041399169111625, 'epoch': 0.23}
{'loss': 0.9338, 'grad_norm': 1.1748701333999634, 'learning_rate': 0.00019039103501388666, 'epoch': 0.23}
{'loss': 0.8942, 'grad_norm': 1.1502801179885864, 'learning_rate': 0.00019036805226831796, 'epoch': 0.23}
{'loss': 1.1171, 'grad_norm': 1.1758604049682617, 'learning_rate': 0.00019034504346103823, 'epoch': 0.23}
{'loss': 1.0612, 'grad_norm': 2.2584776878356934, 'learning_rate': 0.00019032200859868314, 'epoch': 0.23}
{'loss': 0.989, 'grad_norm': 1.2602026462554932, 'learning_rate': 0.0001902989476878958, 'epoch': 0.23}
{'loss': 1.245, 'grad_norm': 1.1484490633010864, 'learning_rate': 0.0001902758607353269, 'epoch': 0.23}
{'loss': 0.8467, 'grad_norm': 1.362117052078247, 'learning_rate': 0.00019025274774763458, 'epoch': 0.23}
{'loss': 0.905, 'grad_norm': 1.205307960510254, 'learning_rate': 0.0001902296087314845, 'epoch': 0.23}
{'loss': 1.0565, 'grad_norm': 1.569158911705017, 'learning_rate': 0.00019020644369354987, 'epoch': 0.23}
{'loss': 1.0531, 'grad_norm': 1.2753206491470337, 'learning_rate': 0.0001901832526405114, 'epoch': 0.23}
{'loss': 0.8531, 'grad_norm': 1.1963070631027222, 'learning_rate': 0.0001901600355790572, 'epoch': 0.23}
{'loss': 0.8434, 'grad_norm': 1.6330361366271973, 'learning_rate': 0.00019013679251588303, 'epoch': 0.23}
{'loss': 1.1744, 'grad_norm': 1.0777981281280518, 'learning_rate': 0.00019011352345769204, 'epoch': 0.23}
{'loss': 0.7288, 'grad_norm': 1.2320808172225952, 'learning_rate': 0.00019009022841119495, 'epoch': 0.23}
{'loss': 1.0838, 'grad_norm': 1.5521963834762573, 'learning_rate': 0.00019006690738310989, 'epoch': 0.23}
{'loss': 0.9452, 'grad_norm': 0.9970855116844177, 'learning_rate': 0.00019004356038016256, 'epoch': 0.23}
{'loss': 1.0734, 'grad_norm': 1.3330525159835815, 'learning_rate': 0.0001900201874090861, 'epoch': 0.23}
{'loss': 0.9224, 'grad_norm': 1.5098997354507446, 'learning_rate': 0.0001899967884766212, 'epoch': 0.23}
{'loss': 0.989, 'grad_norm': 1.3964787721633911, 'learning_rate': 0.00018997336358951602, 'epoch': 0.23}
{'loss': 0.9385, 'grad_norm': 1.432352900505066, 'learning_rate': 0.00018994991275452612, 'epoch': 0.23}
{'loss': 1.1685, 'grad_norm': 1.2696419954299927, 'learning_rate': 0.00018992643597841462, 'epoch': 0.23}
{'loss': 1.1831, 'grad_norm': 1.4951118230819702, 'learning_rate': 0.0001899029332679521, 'epoch': 0.23}
{'loss': 1.118, 'grad_norm': 1.072430968284607, 'learning_rate': 0.0001898794046299167, 'epoch': 0.23}
{'loss': 1.161, 'grad_norm': 1.3554677963256836, 'learning_rate': 0.0001898558500710939, 'epoch': 0.23}
{'loss': 1.1003, 'grad_norm': 1.396876573562622, 'learning_rate': 0.00018983226959827675, 'epoch': 0.23}
{'loss': 1.2016, 'grad_norm': 1.299904704093933, 'learning_rate': 0.0001898086632182657, 'epoch': 0.23}
{'loss': 0.9694, 'grad_norm': 1.0322129726409912, 'learning_rate': 0.00018978503093786882, 'epoch': 0.23}
{'loss': 1.0136, 'grad_norm': 1.1269394159317017, 'learning_rate': 0.0001897613727639014, 'epoch': 0.23}
{'loss': 1.1652, 'grad_norm': 1.2807021141052246, 'learning_rate': 0.00018973768870318646, 'epoch': 0.23}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0719, 'grad_norm': 1.6753262281417847, 'learning_rate': 0.0001897139787625543, 'epoch': 0.23}
{'loss': 0.7542, 'grad_norm': 1.235398530960083, 'learning_rate': 0.00018969024294884283, 'epoch': 0.23}
{'loss': 1.1107, 'grad_norm': 2.384875535964966, 'learning_rate': 0.00018966648126889725, 'epoch': 0.23}
{'loss': 1.3567, 'grad_norm': 1.0179246664047241, 'learning_rate': 0.00018964269372957038, 'epoch': 0.23}
{'loss': 1.0116, 'grad_norm': 1.4321285486221313, 'learning_rate': 0.0001896188803377224, 'epoch': 0.23}
{'loss': 1.1212, 'grad_norm': 1.36360502243042, 'learning_rate': 0.00018959504110022102, 'epoch': 0.23}
{'loss': 1.1036, 'grad_norm': 1.5293301343917847, 'learning_rate': 0.0001895711760239413, 'epoch': 0.23}
{'loss': 1.032, 'grad_norm': 1.1579666137695312, 'learning_rate': 0.00018954728511576586, 'epoch': 0.23}
{'loss': 0.7133, 'grad_norm': 1.6139310598373413, 'learning_rate': 0.0001895233683825847, 'epoch': 0.23}
{'loss': 0.9953, 'grad_norm': 1.4872510433197021, 'learning_rate': 0.00018949942583129528, 'epoch': 0.23}
{'loss': 0.9661, 'grad_norm': 1.1771892309188843, 'learning_rate': 0.00018947545746880254, 'epoch': 0.23}
{'loss': 0.9749, 'grad_norm': 1.1666202545166016, 'learning_rate': 0.00018945146330201883, 'epoch': 0.23}
{'loss': 1.0748, 'grad_norm': 1.2455711364746094, 'learning_rate': 0.00018942744333786397, 'epoch': 0.23}
{'loss': 1.1418, 'grad_norm': 1.118559718132019, 'learning_rate': 0.0001894033975832652, 'epoch': 0.23}
{'loss': 0.8701, 'grad_norm': 1.0922789573669434, 'learning_rate': 0.00018937932604515714, 'epoch': 0.23}
{'loss': 1.053, 'grad_norm': 1.2296990156173706, 'learning_rate': 0.000189355228730482, 'epoch': 0.23}
{'loss': 1.1494, 'grad_norm': 1.0417886972427368, 'learning_rate': 0.00018933110564618927, 'epoch': 0.23}
{'loss': 1.0038, 'grad_norm': 1.1677067279815674, 'learning_rate': 0.00018930695679923593, 'epoch': 0.23}
{'loss': 1.2054, 'grad_norm': 1.397959589958191, 'learning_rate': 0.00018928278219658643, 'epoch': 0.23}
{'loss': 0.9142, 'grad_norm': 1.2836281061172485, 'learning_rate': 0.00018925858184521256, 'epoch': 0.23}
{'loss': 1.1316, 'grad_norm': 1.148574709892273, 'learning_rate': 0.0001892343557520936, 'epoch': 0.23}
{'loss': 0.954, 'grad_norm': 1.7512726783752441, 'learning_rate': 0.00018921010392421628, 'epoch': 0.23}
{'loss': 0.9761, 'grad_norm': 1.1802502870559692, 'learning_rate': 0.00018918582636857467, 'epoch': 0.23}
{'loss': 1.1438, 'grad_norm': 1.1275087594985962, 'learning_rate': 0.0001891615230921703, 'epoch': 0.23}
{'loss': 0.8029, 'grad_norm': 1.1399868726730347, 'learning_rate': 0.0001891371941020121, 'epoch': 0.23}
{'loss': 0.8897, 'grad_norm': 1.1168354749679565, 'learning_rate': 0.0001891128394051165, 'epoch': 0.23}
{'loss': 0.8667, 'grad_norm': 1.0857625007629395, 'learning_rate': 0.00018908845900850722, 'epoch': 0.24}
{'loss': 1.2959, 'grad_norm': 1.0236104726791382, 'learning_rate': 0.00018906405291921547, 'epoch': 0.24}
{'loss': 1.061, 'grad_norm': 1.1714677810668945, 'learning_rate': 0.00018903962114427984, 'epoch': 0.24}
{'loss': 1.0461, 'grad_norm': 1.3131917715072632, 'learning_rate': 0.00018901516369074635, 'epoch': 0.24}
{'loss': 1.0094, 'grad_norm': 0.992397129535675, 'learning_rate': 0.0001889906805656684, 'epoch': 0.24}
{'loss': 1.1716, 'grad_norm': 1.0718202590942383, 'learning_rate': 0.0001889661717761068, 'epoch': 0.24}
{'loss': 0.9291, 'grad_norm': 1.0908281803131104, 'learning_rate': 0.00018894163732912977, 'epoch': 0.24}
{'loss': 0.9592, 'grad_norm': 1.0543533563613892, 'learning_rate': 0.00018891707723181294, 'epoch': 0.24}
{'loss': 0.8155, 'grad_norm': 1.325939655303955, 'learning_rate': 0.00018889249149123927, 'epoch': 0.24}
{'loss': 1.0505, 'grad_norm': 1.035880446434021, 'learning_rate': 0.00018886788011449927, 'epoch': 0.24}
{'loss': 0.9333, 'grad_norm': 1.6932047605514526, 'learning_rate': 0.00018884324310869068, 'epoch': 0.24}
{'loss': 0.9748, 'grad_norm': 1.355486512184143, 'learning_rate': 0.0001888185804809187, 'epoch': 0.24}
{'loss': 0.9447, 'grad_norm': 1.1468151807785034, 'learning_rate': 0.00018879389223829592, 'epoch': 0.24}
{'loss': 0.9183, 'grad_norm': 1.425920844078064, 'learning_rate': 0.00018876917838794226, 'epoch': 0.24}
{'loss': 1.151, 'grad_norm': 1.0530601739883423, 'learning_rate': 0.00018874443893698514, 'epoch': 0.24}
{'loss': 0.845, 'grad_norm': 1.4827646017074585, 'learning_rate': 0.00018871967389255928, 'epoch': 0.24}
{'loss': 1.1239, 'grad_norm': 1.092607021331787, 'learning_rate': 0.00018869488326180679, 'epoch': 0.24}
{'loss': 0.9541, 'grad_norm': 1.0340625047683716, 'learning_rate': 0.0001886700670518772, 'epoch': 0.24}
{'loss': 1.2341, 'grad_norm': 1.4271737337112427, 'learning_rate': 0.00018864522526992734, 'epoch': 0.24}
{'loss': 0.887, 'grad_norm': 1.4829038381576538, 'learning_rate': 0.00018862035792312147, 'epoch': 0.24}
{'loss': 0.9569, 'grad_norm': 1.1255801916122437, 'learning_rate': 0.00018859546501863125, 'epoch': 0.24}
{'loss': 0.8918, 'grad_norm': 1.1484709978103638, 'learning_rate': 0.00018857054656363565, 'epoch': 0.24}
{'loss': 0.9503, 'grad_norm': 1.1222176551818848, 'learning_rate': 0.000188545602565321, 'epoch': 0.24}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8973, 'grad_norm': 1.5293877124786377, 'learning_rate': 0.00018852063303088107, 'epoch': 0.24}
{'loss': 1.0392, 'grad_norm': 1.3018977642059326, 'learning_rate': 0.00018849563796751695, 'epoch': 0.24}
{'loss': 0.8421, 'grad_norm': 1.2267329692840576, 'learning_rate': 0.0001884706173824371, 'epoch': 0.24}
{'loss': 0.8849, 'grad_norm': 1.2232719659805298, 'learning_rate': 0.00018844557128285729, 'epoch': 0.24}
{'loss': 0.7045, 'grad_norm': 1.3086117506027222, 'learning_rate': 0.00018842049967600076, 'epoch': 0.24}
{'loss': 0.9937, 'grad_norm': 1.3771616220474243, 'learning_rate': 0.00018839540256909799, 'epoch': 0.24}
{'loss': 0.9773, 'grad_norm': 1.1930397748947144, 'learning_rate': 0.0001883702799693869, 'epoch': 0.24}
{'loss': 0.9227, 'grad_norm': 1.0987658500671387, 'learning_rate': 0.00018834513188411268, 'epoch': 0.24}
{'loss': 0.9306, 'grad_norm': 1.0519955158233643, 'learning_rate': 0.000188319958320528, 'epoch': 0.24}
{'loss': 0.919, 'grad_norm': 1.7945002317428589, 'learning_rate': 0.00018829475928589271, 'epoch': 0.24}
{'loss': 1.053, 'grad_norm': 1.7940726280212402, 'learning_rate': 0.00018826953478747412, 'epoch': 0.24}
{'loss': 1.0556, 'grad_norm': 1.5187863111495972, 'learning_rate': 0.00018824428483254685, 'epoch': 0.24}
{'loss': 1.0365, 'grad_norm': 1.7013715505599976, 'learning_rate': 0.0001882190094283929, 'epoch': 0.24}
{'loss': 0.6435, 'grad_norm': 1.249428391456604, 'learning_rate': 0.00018819370858230152, 'epoch': 0.24}
{'loss': 1.2206, 'grad_norm': 1.4340674877166748, 'learning_rate': 0.00018816838230156942, 'epoch': 0.24}
{'loss': 1.0709, 'grad_norm': 1.5253779888153076, 'learning_rate': 0.0001881430305935005, 'epoch': 0.24}
{'loss': 0.8705, 'grad_norm': 1.3376233577728271, 'learning_rate': 0.0001881176534654061, 'epoch': 0.24}
{'loss': 1.0623, 'grad_norm': 1.4726061820983887, 'learning_rate': 0.00018809225092460488, 'epoch': 0.24}
{'loss': 1.2073, 'grad_norm': 0.9875738620758057, 'learning_rate': 0.0001880668229784228, 'epoch': 0.24}
{'loss': 1.0597, 'grad_norm': 1.1833395957946777, 'learning_rate': 0.00018804136963419316, 'epoch': 0.24}
{'loss': 0.7201, 'grad_norm': 1.0756174325942993, 'learning_rate': 0.00018801589089925656, 'epoch': 0.24}
{'loss': 1.4037, 'grad_norm': 1.8256480693817139, 'learning_rate': 0.00018799038678096096, 'epoch': 0.24}
{'loss': 1.0307, 'grad_norm': 1.3777883052825928, 'learning_rate': 0.00018796485728666165, 'epoch': 0.24}
{'loss': 1.0165, 'grad_norm': 1.3407621383666992, 'learning_rate': 0.0001879393024237212, 'epoch': 0.24}
{'loss': 1.2078, 'grad_norm': 1.3412772417068481, 'learning_rate': 0.00018791372219950948, 'epoch': 0.24}
{'loss': 1.0202, 'grad_norm': 1.3208520412445068, 'learning_rate': 0.0001878881166214037, 'epoch': 0.24}
{'loss': 1.0855, 'grad_norm': 1.4212044477462769, 'learning_rate': 0.00018786248569678846, 'epoch': 0.24}
{'loss': 1.2805, 'grad_norm': 1.1982247829437256, 'learning_rate': 0.0001878368294330555, 'epoch': 0.24}
{'loss': 1.0742, 'grad_norm': 1.2192634344100952, 'learning_rate': 0.00018781114783760402, 'epoch': 0.24}
{'loss': 0.9647, 'grad_norm': 1.129542350769043, 'learning_rate': 0.00018778544091784048, 'epoch': 0.24}
{'loss': 1.1065, 'grad_norm': 1.0736818313598633, 'learning_rate': 0.00018775970868117856, 'epoch': 0.24}
{'loss': 0.7263, 'grad_norm': 1.1384526491165161, 'learning_rate': 0.00018773395113503937, 'epoch': 0.24}
{'loss': 1.0044, 'grad_norm': 1.4620718955993652, 'learning_rate': 0.0001877081682868513, 'epoch': 0.24}
{'loss': 0.8501, 'grad_norm': 1.3784654140472412, 'learning_rate': 0.00018768236014404991, 'epoch': 0.24}
{'loss': 1.0632, 'grad_norm': 1.2188527584075928, 'learning_rate': 0.0001876565267140782, 'epoch': 0.24}
{'loss': 1.2038, 'grad_norm': 1.7457460165023804, 'learning_rate': 0.00018763066800438636, 'epoch': 0.24}
{'loss': 1.288, 'grad_norm': 1.0048956871032715, 'learning_rate': 0.00018760478402243198, 'epoch': 0.24}
{'loss': 1.171, 'grad_norm': 1.360477089881897, 'learning_rate': 0.00018757887477567983, 'epoch': 0.24}
{'loss': 1.0917, 'grad_norm': 1.3154146671295166, 'learning_rate': 0.00018755294027160204, 'epoch': 0.24}
{'loss': 0.9355, 'grad_norm': 1.460890769958496, 'learning_rate': 0.000187526980517678, 'epoch': 0.24}
{'loss': 1.0681, 'grad_norm': 1.7003670930862427, 'learning_rate': 0.00018750099552139433, 'epoch': 0.24}
{'loss': 1.028, 'grad_norm': 1.493714690208435, 'learning_rate': 0.000187474985290245, 'epoch': 0.24}
{'loss': 0.891, 'grad_norm': 1.3851068019866943, 'learning_rate': 0.00018744894983173128, 'epoch': 0.25}
{'loss': 1.1523, 'grad_norm': 1.0739926099777222, 'learning_rate': 0.00018742288915336164, 'epoch': 0.25}
{'loss': 1.178, 'grad_norm': 1.386392593383789, 'learning_rate': 0.0001873968032626518, 'epoch': 0.25}
{'loss': 0.9032, 'grad_norm': 1.2315722703933716, 'learning_rate': 0.00018737069216712488, 'epoch': 0.25}
{'loss': 1.0566, 'grad_norm': 2.174866199493408, 'learning_rate': 0.00018734455587431117, 'epoch': 0.25}
{'loss': 1.1576, 'grad_norm': 1.335051417350769, 'learning_rate': 0.00018731839439174825, 'epoch': 0.25}
{'loss': 0.7682, 'grad_norm': 1.41359543800354, 'learning_rate': 0.00018729220772698097, 'epoch': 0.25}
{'loss': 1.3391, 'grad_norm': 1.0651689767837524, 'learning_rate': 0.00018726599588756145, 'epoch': 0.25}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.949, 'grad_norm': 1.3471543788909912, 'learning_rate': 0.000187239758881049, 'epoch': 0.25}
{'loss': 0.9687, 'grad_norm': 1.541463017463684, 'learning_rate': 0.0001872134967150103, 'epoch': 0.25}
{'loss': 0.9823, 'grad_norm': 1.787900447845459, 'learning_rate': 0.00018718720939701924, 'epoch': 0.25}
{'loss': 1.0785, 'grad_norm': 1.1297640800476074, 'learning_rate': 0.00018716089693465696, 'epoch': 0.25}
{'loss': 1.2546, 'grad_norm': 1.4159114360809326, 'learning_rate': 0.00018713455933551176, 'epoch': 0.25}
{'loss': 1.083, 'grad_norm': 1.1145271062850952, 'learning_rate': 0.00018710819660717936, 'epoch': 0.25}
{'loss': 1.0755, 'grad_norm': 2.0347957611083984, 'learning_rate': 0.00018708180875726265, 'epoch': 0.25}
{'loss': 0.8762, 'grad_norm': 1.243101954460144, 'learning_rate': 0.0001870553957933717, 'epoch': 0.25}
{'loss': 1.0351, 'grad_norm': 1.328613519668579, 'learning_rate': 0.00018702895772312395, 'epoch': 0.25}
{'loss': 1.2912, 'grad_norm': 1.147655725479126, 'learning_rate': 0.00018700249455414394, 'epoch': 0.25}
{'loss': 1.228, 'grad_norm': 1.5748628377914429, 'learning_rate': 0.00018697600629406357, 'epoch': 0.25}
{'loss': 0.9802, 'grad_norm': 1.6791826486587524, 'learning_rate': 0.0001869494929505219, 'epoch': 0.25}
{'loss': 0.7553, 'grad_norm': 1.0423580408096313, 'learning_rate': 0.0001869229545311653, 'epoch': 0.25}
{'loss': 1.005, 'grad_norm': 1.3440157175064087, 'learning_rate': 0.00018689639104364723, 'epoch': 0.25}
{'loss': 1.2075, 'grad_norm': 0.968497633934021, 'learning_rate': 0.00018686980249562859, 'epoch': 0.25}
{'loss': 1.2437, 'grad_norm': 1.2987264394760132, 'learning_rate': 0.0001868431888947773, 'epoch': 0.25}
{'loss': 0.9319, 'grad_norm': 1.3867073059082031, 'learning_rate': 0.00018681655024876864, 'epoch': 0.25}
{'loss': 0.8656, 'grad_norm': 1.1777873039245605, 'learning_rate': 0.00018678988656528503, 'epoch': 0.25}
{'loss': 0.8695, 'grad_norm': 1.0912388563156128, 'learning_rate': 0.00018676319785201616, 'epoch': 0.25}
{'loss': 0.9836, 'grad_norm': 1.149177074432373, 'learning_rate': 0.00018673648411665893, 'epoch': 0.25}
{'loss': 1.0026, 'grad_norm': 1.3110979795455933, 'learning_rate': 0.00018670974536691746, 'epoch': 0.25}
{'loss': 1.2764, 'grad_norm': 1.3145592212677002, 'learning_rate': 0.00018668298161050309, 'epoch': 0.25}
{'loss': 1.1355, 'grad_norm': 1.1886318922042847, 'learning_rate': 0.00018665619285513434, 'epoch': 0.25}
{'loss': 1.0436, 'grad_norm': 0.9565160274505615, 'learning_rate': 0.00018662937910853694, 'epoch': 0.25}
{'loss': 1.087, 'grad_norm': 1.2855151891708374, 'learning_rate': 0.00018660254037844388, 'epoch': 0.25}
{'loss': 0.9557, 'grad_norm': 1.0629916191101074, 'learning_rate': 0.00018657567667259528, 'epoch': 0.25}
{'loss': 0.9029, 'grad_norm': 1.1373207569122314, 'learning_rate': 0.00018654878799873856, 'epoch': 0.25}
{'loss': 0.9629, 'grad_norm': 1.236778974533081, 'learning_rate': 0.00018652187436462824, 'epoch': 0.25}
{'loss': 1.2894, 'grad_norm': 1.136857509613037, 'learning_rate': 0.00018649493577802608, 'epoch': 0.25}
{'loss': 1.2732, 'grad_norm': 1.0521433353424072, 'learning_rate': 0.0001864679722467011, 'epoch': 0.25}
{'loss': 1.1737, 'grad_norm': 1.115321159362793, 'learning_rate': 0.00018644098377842934, 'epoch': 0.25}
{'loss': 1.2053, 'grad_norm': 1.1158791780471802, 'learning_rate': 0.00018641397038099427, 'epoch': 0.25}
{'loss': 1.1477, 'grad_norm': 0.8680473566055298, 'learning_rate': 0.0001863869320621863, 'epoch': 0.25}
{'loss': 0.8481, 'grad_norm': 1.5044827461242676, 'learning_rate': 0.00018635986882980325, 'epoch': 0.25}
{'loss': 1.2646, 'grad_norm': 1.1147663593292236, 'learning_rate': 0.00018633278069165, 'epoch': 0.25}
{'loss': 0.7546, 'grad_norm': 1.0150173902511597, 'learning_rate': 0.00018630566765553863, 'epoch': 0.25}
{'loss': 1.0799, 'grad_norm': 1.141839861869812, 'learning_rate': 0.00018627852972928838, 'epoch': 0.25}
{'loss': 0.9524, 'grad_norm': 1.1771321296691895, 'learning_rate': 0.00018625136692072575, 'epoch': 0.25}
{'loss': 0.7511, 'grad_norm': 1.270707130432129, 'learning_rate': 0.00018622417923768434, 'epoch': 0.25}
{'loss': 1.2275, 'grad_norm': 1.4510703086853027, 'learning_rate': 0.00018619696668800492, 'epoch': 0.25}
{'loss': 1.114, 'grad_norm': 1.2422624826431274, 'learning_rate': 0.00018616972927953552, 'epoch': 0.25}
{'loss': 1.2141, 'grad_norm': 1.2813045978546143, 'learning_rate': 0.00018614246702013123, 'epoch': 0.25}
{'loss': 1.0819, 'grad_norm': 1.4414482116699219, 'learning_rate': 0.00018611517991765433, 'epoch': 0.25}
{'loss': 1.0594, 'grad_norm': 1.2492300271987915, 'learning_rate': 0.00018608786797997436, 'epoch': 0.25}
{'loss': 0.7322, 'grad_norm': 1.1673035621643066, 'learning_rate': 0.00018606053121496792, 'epoch': 0.25}
{'loss': 0.8643, 'grad_norm': 1.0500729084014893, 'learning_rate': 0.00018603316963051878, 'epoch': 0.25}
{'loss': 0.7996, 'grad_norm': 1.8500030040740967, 'learning_rate': 0.0001860057832345179, 'epoch': 0.25}
{'loss': 1.1146, 'grad_norm': 1.4125221967697144, 'learning_rate': 0.0001859783720348634, 'epoch': 0.25}
{'loss': 1.2047, 'grad_norm': 1.629969596862793, 'learning_rate': 0.00018595093603946053, 'epoch': 0.25}
{'loss': 0.9133, 'grad_norm': 1.2824140787124634, 'learning_rate': 0.0001859234752562217, 'epoch': 0.25}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1716, 'grad_norm': 0.9895682334899902, 'learning_rate': 0.00018589598969306645, 'epoch': 0.25}
{'loss': 0.7091, 'grad_norm': 1.0221014022827148, 'learning_rate': 0.0001858684793579215, 'epoch': 0.25}
{'loss': 1.0739, 'grad_norm': 1.0210270881652832, 'learning_rate': 0.00018584094425872072, 'epoch': 0.25}
{'loss': 0.9875, 'grad_norm': 1.1615304946899414, 'learning_rate': 0.00018581338440340508, 'epoch': 0.25}
{'loss': 0.9947, 'grad_norm': 1.2643301486968994, 'learning_rate': 0.00018578579979992266, 'epoch': 0.25}
{'loss': 0.7674, 'grad_norm': 1.2428838014602661, 'learning_rate': 0.0001857581904562288, 'epoch': 0.25}
{'loss': 0.9797, 'grad_norm': 1.244743824005127, 'learning_rate': 0.0001857305563802859, 'epoch': 0.25}
{'loss': 0.7969, 'grad_norm': 1.1877429485321045, 'learning_rate': 0.00018570289758006346, 'epoch': 0.26}
{'loss': 0.7948, 'grad_norm': 1.4610273838043213, 'learning_rate': 0.00018567521406353814, 'epoch': 0.26}
{'loss': 0.9889, 'grad_norm': 1.347825288772583, 'learning_rate': 0.00018564750583869373, 'epoch': 0.26}
{'loss': 0.544, 'grad_norm': 1.2320045232772827, 'learning_rate': 0.0001856197729135212, 'epoch': 0.26}
{'loss': 1.1535, 'grad_norm': 1.7321933507919312, 'learning_rate': 0.00018559201529601854, 'epoch': 0.26}
{'loss': 1.1193, 'grad_norm': 1.3718969821929932, 'learning_rate': 0.00018556423299419096, 'epoch': 0.26}
{'loss': 0.9646, 'grad_norm': 1.4290876388549805, 'learning_rate': 0.00018553642601605068, 'epoch': 0.26}
{'loss': 0.8469, 'grad_norm': 2.0364809036254883, 'learning_rate': 0.00018550859436961714, 'epoch': 0.26}
{'loss': 1.1206, 'grad_norm': 1.5946426391601562, 'learning_rate': 0.00018548073806291685, 'epoch': 0.26}
{'loss': 1.0339, 'grad_norm': 1.4273828268051147, 'learning_rate': 0.00018545285710398342, 'epoch': 0.26}
{'loss': 0.93, 'grad_norm': 1.6518462896347046, 'learning_rate': 0.0001854249515008576, 'epoch': 0.26}
{'loss': 0.5726, 'grad_norm': 1.177677035331726, 'learning_rate': 0.00018539702126158723, 'epoch': 0.26}
{'loss': 0.7676, 'grad_norm': 1.353403091430664, 'learning_rate': 0.00018536906639422725, 'epoch': 0.26}
{'loss': 1.2321, 'grad_norm': 1.237911581993103, 'learning_rate': 0.0001853410869068397, 'epoch': 0.26}
{'loss': 0.865, 'grad_norm': 1.107620120048523, 'learning_rate': 0.00018531308280749374, 'epoch': 0.26}
{'loss': 0.982, 'grad_norm': 0.9896141290664673, 'learning_rate': 0.00018528505410426562, 'epoch': 0.26}
{'loss': 1.1348, 'grad_norm': 1.1790608167648315, 'learning_rate': 0.0001852570008052387, 'epoch': 0.26}
{'loss': 1.0196, 'grad_norm': 1.2199898958206177, 'learning_rate': 0.00018522892291850335, 'epoch': 0.26}
{'loss': 1.0463, 'grad_norm': 1.0164519548416138, 'learning_rate': 0.0001852008204521572, 'epoch': 0.26}
{'loss': 1.1238, 'grad_norm': 1.5591801404953003, 'learning_rate': 0.00018517269341430476, 'epoch': 0.26}
{'loss': 1.0598, 'grad_norm': 1.2822723388671875, 'learning_rate': 0.0001851445418130578, 'epoch': 0.26}
{'loss': 0.8994, 'grad_norm': 1.3852813243865967, 'learning_rate': 0.00018511636565653511, 'epoch': 0.26}
{'loss': 0.4566, 'grad_norm': 1.1215535402297974, 'learning_rate': 0.0001850881649528625, 'epoch': 0.26}
{'loss': 0.9852, 'grad_norm': 1.3390324115753174, 'learning_rate': 0.00018505993971017302, 'epoch': 0.26}
{'loss': 0.8924, 'grad_norm': 1.1085063219070435, 'learning_rate': 0.00018503168993660657, 'epoch': 0.26}
{'loss': 0.9513, 'grad_norm': 1.642665982246399, 'learning_rate': 0.0001850034156403103, 'epoch': 0.26}
{'loss': 0.9808, 'grad_norm': 1.0457338094711304, 'learning_rate': 0.0001849751168294384, 'epoch': 0.26}
{'loss': 0.9546, 'grad_norm': 1.2874929904937744, 'learning_rate': 0.0001849467935121521, 'epoch': 0.26}
{'loss': 1.0717, 'grad_norm': 1.2209018468856812, 'learning_rate': 0.0001849184456966197, 'epoch': 0.26}
{'loss': 1.043, 'grad_norm': 12.12631607055664, 'learning_rate': 0.0001848900733910166, 'epoch': 0.26}
{'loss': 0.8634, 'grad_norm': 1.5808374881744385, 'learning_rate': 0.0001848616766035252, 'epoch': 0.26}
{'loss': 1.1449, 'grad_norm': 0.910948634147644, 'learning_rate': 0.00018483325534233506, 'epoch': 0.26}
{'loss': 0.7568, 'grad_norm': 1.3011770248413086, 'learning_rate': 0.0001848048096156426, 'epoch': 0.26}
{'loss': 0.9877, 'grad_norm': 1.6251271963119507, 'learning_rate': 0.00018477633943165156, 'epoch': 0.26}
{'loss': 0.763, 'grad_norm': 1.0726335048675537, 'learning_rate': 0.00018474784479857257, 'epoch': 0.26}
{'loss': 0.9519, 'grad_norm': 1.545973300933838, 'learning_rate': 0.00018471932572462332, 'epoch': 0.26}
{'loss': 0.8175, 'grad_norm': 1.5792733430862427, 'learning_rate': 0.0001846907822180286, 'epoch': 0.26}
{'loss': 1.084, 'grad_norm': 1.0932016372680664, 'learning_rate': 0.0001846622142870202, 'epoch': 0.26}
{'loss': 0.9783, 'grad_norm': 0.8916715979576111, 'learning_rate': 0.00018463362193983703, 'epoch': 0.26}
{'loss': 1.1678, 'grad_norm': 1.313065528869629, 'learning_rate': 0.00018460500518472487, 'epoch': 0.26}
{'loss': 1.4143, 'grad_norm': 1.1323645114898682, 'learning_rate': 0.00018457636402993677, 'epoch': 0.26}
{'loss': 1.174, 'grad_norm': 1.0751937627792358, 'learning_rate': 0.00018454769848373262, 'epoch': 0.26}
{'loss': 0.664, 'grad_norm': 1.396610140800476, 'learning_rate': 0.0001845190085543795, 'epoch': 0.26}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1218, 'grad_norm': 1.6068564653396606, 'learning_rate': 0.00018449029425015136, 'epoch': 0.26}
{'loss': 1.0562, 'grad_norm': 1.1747264862060547, 'learning_rate': 0.00018446155557932934, 'epoch': 0.26}
{'loss': 0.8654, 'grad_norm': 1.3955079317092896, 'learning_rate': 0.00018443279255020152, 'epoch': 0.26}
{'loss': 0.957, 'grad_norm': 1.2904084920883179, 'learning_rate': 0.00018440400517106298, 'epoch': 0.26}
{'loss': 1.0906, 'grad_norm': 1.1782965660095215, 'learning_rate': 0.0001843751934502159, 'epoch': 0.26}
{'loss': 0.9367, 'grad_norm': 1.4001165628433228, 'learning_rate': 0.00018434635739596943, 'epoch': 0.26}
{'loss': 0.9804, 'grad_norm': 1.6808043718338013, 'learning_rate': 0.0001843174970166398, 'epoch': 0.26}
{'loss': 1.0132, 'grad_norm': 1.5754191875457764, 'learning_rate': 0.0001842886123205501, 'epoch': 0.26}
{'loss': 0.9105, 'grad_norm': 1.1293658018112183, 'learning_rate': 0.00018425970331603056, 'epoch': 0.26}
{'loss': 1.0766, 'grad_norm': 1.5047067403793335, 'learning_rate': 0.0001842307700114185, 'epoch': 0.26}
{'loss': 0.7673, 'grad_norm': 1.2908388376235962, 'learning_rate': 0.00018420181241505804, 'epoch': 0.26}
{'loss': 0.9646, 'grad_norm': 1.2017922401428223, 'learning_rate': 0.00018417283053530044, 'epoch': 0.26}
{'loss': 0.9886, 'grad_norm': 1.0780248641967773, 'learning_rate': 0.00018414382438050394, 'epoch': 0.26}
{'loss': 0.9175, 'grad_norm': 1.117925763130188, 'learning_rate': 0.0001841147939590338, 'epoch': 0.26}
{'loss': 0.9752, 'grad_norm': 1.46505868434906, 'learning_rate': 0.00018408573927926222, 'epoch': 0.26}
{'loss': 0.9085, 'grad_norm': 1.1463381052017212, 'learning_rate': 0.00018405666034956844, 'epoch': 0.26}
{'loss': 1.2769, 'grad_norm': 1.4254651069641113, 'learning_rate': 0.00018402755717833867, 'epoch': 0.26}
{'loss': 0.9375, 'grad_norm': 1.5891972780227661, 'learning_rate': 0.00018399842977396614, 'epoch': 0.26}
{'loss': 0.7296, 'grad_norm': 1.4381104707717896, 'learning_rate': 0.00018396927814485106, 'epoch': 0.26}
{'loss': 0.9367, 'grad_norm': 1.163398027420044, 'learning_rate': 0.0001839401022994006, 'epoch': 0.26}
{'loss': 0.796, 'grad_norm': 1.0934699773788452, 'learning_rate': 0.00018391090224602894, 'epoch': 0.26}
{'loss': 1.0172, 'grad_norm': 1.3978602886199951, 'learning_rate': 0.00018388167799315727, 'epoch': 0.26}
{'loss': 1.052, 'grad_norm': 1.0867348909378052, 'learning_rate': 0.00018385242954921366, 'epoch': 0.27}
{'loss': 0.6863, 'grad_norm': 1.3678008317947388, 'learning_rate': 0.00018382315692263323, 'epoch': 0.27}
{'loss': 1.0303, 'grad_norm': 0.9418544769287109, 'learning_rate': 0.00018379386012185814, 'epoch': 0.27}
{'loss': 0.7141, 'grad_norm': 0.9345759153366089, 'learning_rate': 0.00018376453915533734, 'epoch': 0.27}
{'loss': 0.9805, 'grad_norm': 1.0603926181793213, 'learning_rate': 0.00018373519403152696, 'epoch': 0.27}
{'loss': 0.9641, 'grad_norm': 1.2840253114700317, 'learning_rate': 0.00018370582475888992, 'epoch': 0.27}
{'loss': 0.8304, 'grad_norm': 1.1088250875473022, 'learning_rate': 0.00018367643134589617, 'epoch': 0.27}
{'loss': 0.9202, 'grad_norm': 1.2919352054595947, 'learning_rate': 0.00018364701380102266, 'epoch': 0.27}
{'loss': 1.0098, 'grad_norm': 1.2929753065109253, 'learning_rate': 0.0001836175721327533, 'epoch': 0.27}
{'loss': 1.1313, 'grad_norm': 2.015474557876587, 'learning_rate': 0.00018358810634957887, 'epoch': 0.27}
{'loss': 0.9742, 'grad_norm': 1.3883235454559326, 'learning_rate': 0.00018355861645999718, 'epoch': 0.27}
{'loss': 0.8645, 'grad_norm': 0.9758948087692261, 'learning_rate': 0.000183529102472513, 'epoch': 0.27}
{'loss': 0.9878, 'grad_norm': 1.2462245225906372, 'learning_rate': 0.00018349956439563794, 'epoch': 0.27}
{'loss': 0.9989, 'grad_norm': 1.6389009952545166, 'learning_rate': 0.0001834700022378907, 'epoch': 0.27}
{'loss': 0.7917, 'grad_norm': 1.1261210441589355, 'learning_rate': 0.0001834404160077969, 'epoch': 0.27}
{'loss': 0.9648, 'grad_norm': 1.3207087516784668, 'learning_rate': 0.00018341080571388898, 'epoch': 0.27}
{'loss': 0.8848, 'grad_norm': 1.7859104871749878, 'learning_rate': 0.00018338117136470648, 'epoch': 0.27}
{'loss': 0.8798, 'grad_norm': 1.6036101579666138, 'learning_rate': 0.00018335151296879575, 'epoch': 0.27}
{'loss': 0.7109, 'grad_norm': 1.2047183513641357, 'learning_rate': 0.00018332183053471014, 'epoch': 0.27}
{'loss': 0.8583, 'grad_norm': 1.454813838005066, 'learning_rate': 0.00018329212407100994, 'epoch': 0.27}
{'loss': 0.9447, 'grad_norm': 1.4390361309051514, 'learning_rate': 0.00018326239358626239, 'epoch': 0.27}
{'loss': 1.0878, 'grad_norm': 1.1120293140411377, 'learning_rate': 0.0001832326390890415, 'epoch': 0.27}
{'loss': 0.8729, 'grad_norm': 1.0526294708251953, 'learning_rate': 0.00018320286058792843, 'epoch': 0.27}
{'loss': 1.0801, 'grad_norm': 1.2679390907287598, 'learning_rate': 0.00018317305809151114, 'epoch': 0.27}
{'loss': 1.0012, 'grad_norm': 1.4271764755249023, 'learning_rate': 0.00018314323160838447, 'epoch': 0.27}
{'loss': 0.9876, 'grad_norm': 1.214479684829712, 'learning_rate': 0.0001831133811471503, 'epoch': 0.27}
{'loss': 0.8132, 'grad_norm': 1.3848063945770264, 'learning_rate': 0.0001830835067164173, 'epoch': 0.27}
{'loss': 1.1522, 'grad_norm': 1.112283706665039, 'learning_rate': 0.00018305360832480117, 'epoch': 0.27}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0044, 'grad_norm': 1.3068805932998657, 'learning_rate': 0.00018302368598092442, 'epoch': 0.27}
{'loss': 0.8975, 'grad_norm': 1.0798840522766113, 'learning_rate': 0.00018299373969341655, 'epoch': 0.27}
{'loss': 0.9637, 'grad_norm': 1.0262212753295898, 'learning_rate': 0.00018296376947091387, 'epoch': 0.27}
{'loss': 0.6288, 'grad_norm': 0.9669103026390076, 'learning_rate': 0.00018293377532205968, 'epoch': 0.27}
{'loss': 1.0399, 'grad_norm': 1.3571218252182007, 'learning_rate': 0.00018290375725550417, 'epoch': 0.27}
{'loss': 1.4516, 'grad_norm': 0.9208981990814209, 'learning_rate': 0.00018287371527990438, 'epoch': 0.27}
{'loss': 1.0608, 'grad_norm': 1.5128247737884521, 'learning_rate': 0.00018284364940392424, 'epoch': 0.27}
{'loss': 1.0731, 'grad_norm': 1.0610185861587524, 'learning_rate': 0.00018281355963623467, 'epoch': 0.27}
{'loss': 1.0782, 'grad_norm': 1.2608253955841064, 'learning_rate': 0.0001827834459855134, 'epoch': 0.27}
{'loss': 1.1407, 'grad_norm': 1.1798655986785889, 'learning_rate': 0.000182753308460445, 'epoch': 0.27}
{'loss': 1.0078, 'grad_norm': 1.16793692111969, 'learning_rate': 0.00018272314706972104, 'epoch': 0.27}
{'loss': 1.0693, 'grad_norm': 1.3981703519821167, 'learning_rate': 0.0001826929618220399, 'epoch': 0.27}
{'loss': 1.1383, 'grad_norm': 1.0857418775558472, 'learning_rate': 0.0001826627527261069, 'epoch': 0.27}
{'loss': 1.0165, 'grad_norm': 1.132709264755249, 'learning_rate': 0.00018263251979063413, 'epoch': 0.27}
{'loss': 0.9104, 'grad_norm': 1.5383871793746948, 'learning_rate': 0.0001826022630243407, 'epoch': 0.27}
{'loss': 1.0523, 'grad_norm': 1.362956166267395, 'learning_rate': 0.0001825719824359524, 'epoch': 0.27}
{'loss': 0.6819, 'grad_norm': 1.088578462600708, 'learning_rate': 0.00018254167803420215, 'epoch': 0.27}
{'loss': 1.1557, 'grad_norm': 1.3337030410766602, 'learning_rate': 0.00018251134982782952, 'epoch': 0.27}
{'loss': 1.0391, 'grad_norm': 1.3182992935180664, 'learning_rate': 0.000182480997825581, 'epoch': 0.27}
{'loss': 0.9848, 'grad_norm': 1.306214690208435, 'learning_rate': 0.00018245062203621002, 'epoch': 0.27}
{'loss': 1.0114, 'grad_norm': 1.2369930744171143, 'learning_rate': 0.00018242022246847675, 'epoch': 0.27}
{'loss': 1.1633, 'grad_norm': 1.296050786972046, 'learning_rate': 0.0001823897991311483, 'epoch': 0.27}
{'loss': 1.3195, 'grad_norm': 2.2386064529418945, 'learning_rate': 0.00018235935203299867, 'epoch': 0.27}
{'loss': 1.1541, 'grad_norm': 1.1826171875, 'learning_rate': 0.00018232888118280854, 'epoch': 0.27}
{'loss': 0.9173, 'grad_norm': 1.2185218334197998, 'learning_rate': 0.00018229838658936564, 'epoch': 0.27}
{'loss': 1.1193, 'grad_norm': 0.9569599032402039, 'learning_rate': 0.00018226786826146447, 'epoch': 0.27}
{'loss': 0.724, 'grad_norm': 1.3832063674926758, 'learning_rate': 0.00018223732620790633, 'epoch': 0.27}
{'loss': 1.0682, 'grad_norm': 1.2696937322616577, 'learning_rate': 0.0001822067604374994, 'epoch': 0.27}
{'loss': 0.8591, 'grad_norm': 1.7378913164138794, 'learning_rate': 0.00018217617095905874, 'epoch': 0.27}
{'loss': 1.0163, 'grad_norm': 0.9897080659866333, 'learning_rate': 0.00018214555778140617, 'epoch': 0.27}
{'loss': 1.0486, 'grad_norm': 1.261736512184143, 'learning_rate': 0.00018211492091337042, 'epoch': 0.27}
{'loss': 0.893, 'grad_norm': 1.2149078845977783, 'learning_rate': 0.00018208426036378698, 'epoch': 0.27}
{'loss': 1.201, 'grad_norm': 1.124709963798523, 'learning_rate': 0.00018205357614149822, 'epoch': 0.27}
{'loss': 0.7168, 'grad_norm': 0.9402560591697693, 'learning_rate': 0.0001820228682553533, 'epoch': 0.27}
{'loss': 1.0509, 'grad_norm': 1.2426618337631226, 'learning_rate': 0.00018199213671420827, 'epoch': 0.27}
{'loss': 0.9448, 'grad_norm': 1.0815305709838867, 'learning_rate': 0.00018196138152692593, 'epoch': 0.27}
{'loss': 1.0428, 'grad_norm': 1.223594069480896, 'learning_rate': 0.00018193060270237595, 'epoch': 0.27}
{'loss': 1.0289, 'grad_norm': 1.2000972032546997, 'learning_rate': 0.00018189980024943477, 'epoch': 0.28}
{'loss': 1.2625, 'grad_norm': 1.0920754671096802, 'learning_rate': 0.00018186897417698562, 'epoch': 0.28}
{'loss': 0.9523, 'grad_norm': 1.220906138420105, 'learning_rate': 0.0001818381244939187, 'epoch': 0.28}
{'loss': 0.8683, 'grad_norm': 1.1592214107513428, 'learning_rate': 0.00018180725120913084, 'epoch': 0.28}
{'loss': 0.8457, 'grad_norm': 1.2207739353179932, 'learning_rate': 0.0001817763543315258, 'epoch': 0.28}
{'loss': 0.927, 'grad_norm': 1.0159077644348145, 'learning_rate': 0.000181745433870014, 'epoch': 0.28}
{'loss': 1.1383, 'grad_norm': 0.9960275292396545, 'learning_rate': 0.00018171448983351284, 'epoch': 0.28}
{'loss': 1.0195, 'grad_norm': 1.3620572090148926, 'learning_rate': 0.0001816835222309464, 'epoch': 0.28}
{'loss': 1.1066, 'grad_norm': 1.2757738828659058, 'learning_rate': 0.0001816525310712456, 'epoch': 0.28}
{'loss': 1.4555, 'grad_norm': 5.129806041717529, 'learning_rate': 0.0001816215163633481, 'epoch': 0.28}
{'loss': 0.8232, 'grad_norm': 1.2623924016952515, 'learning_rate': 0.00018159047811619843, 'epoch': 0.28}
{'loss': 1.099, 'grad_norm': 1.6407479047775269, 'learning_rate': 0.00018155941633874787, 'epoch': 0.28}
{'loss': 0.8434, 'grad_norm': 1.5389686822891235, 'learning_rate': 0.00018152833103995443, 'epoch': 0.28}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9345, 'grad_norm': 1.3674217462539673, 'learning_rate': 0.00018149722222878304, 'epoch': 0.28}
{'loss': 1.0542, 'grad_norm': 1.0538396835327148, 'learning_rate': 0.00018146608991420534, 'epoch': 0.28}
{'loss': 0.9662, 'grad_norm': 1.5265649557113647, 'learning_rate': 0.00018143493410519964, 'epoch': 0.28}
{'loss': 0.7972, 'grad_norm': 1.1271995306015015, 'learning_rate': 0.0001814037548107512, 'epoch': 0.28}
{'loss': 0.9852, 'grad_norm': 1.1935250759124756, 'learning_rate': 0.00018137255203985197, 'epoch': 0.28}
{'loss': 0.9678, 'grad_norm': 1.4531660079956055, 'learning_rate': 0.00018134132580150064, 'epoch': 0.28}
{'loss': 1.024, 'grad_norm': 1.0142898559570312, 'learning_rate': 0.00018131007610470276, 'epoch': 0.28}
{'loss': 0.7882, 'grad_norm': 1.3378198146820068, 'learning_rate': 0.00018127880295847057, 'epoch': 0.28}
{'loss': 0.8845, 'grad_norm': 1.046745777130127, 'learning_rate': 0.00018124750637182313, 'epoch': 0.28}
{'loss': 0.5625, 'grad_norm': 1.2848328351974487, 'learning_rate': 0.00018121618635378616, 'epoch': 0.28}
{'loss': 0.8071, 'grad_norm': 1.3412353992462158, 'learning_rate': 0.0001811848429133922, 'epoch': 0.28}
{'loss': 0.8655, 'grad_norm': 1.4322940111160278, 'learning_rate': 0.00018115347605968061, 'epoch': 0.28}
{'loss': 1.0592, 'grad_norm': 1.665594458580017, 'learning_rate': 0.0001811220858016974, 'epoch': 0.28}
{'loss': 1.0862, 'grad_norm': 1.2635873556137085, 'learning_rate': 0.00018109067214849538, 'epoch': 0.28}
{'loss': 1.0171, 'grad_norm': 1.2546782493591309, 'learning_rate': 0.0001810592351091341, 'epoch': 0.28}
{'loss': 0.8566, 'grad_norm': 1.0213202238082886, 'learning_rate': 0.00018102777469267985, 'epoch': 0.28}
{'loss': 0.9648, 'grad_norm': 1.1282317638397217, 'learning_rate': 0.00018099629090820562, 'epoch': 0.28}
{'loss': 1.0797, 'grad_norm': 1.3088055849075317, 'learning_rate': 0.00018096478376479125, 'epoch': 0.28}
{'loss': 1.2233, 'grad_norm': 1.5210251808166504, 'learning_rate': 0.00018093325327152323, 'epoch': 0.28}
{'loss': 0.8903, 'grad_norm': 1.2707011699676514, 'learning_rate': 0.00018090169943749476, 'epoch': 0.28}
{'loss': 1.0866, 'grad_norm': 1.1025396585464478, 'learning_rate': 0.00018087012227180585, 'epoch': 0.28}
{'loss': 1.1171, 'grad_norm': 1.2786701917648315, 'learning_rate': 0.0001808385217835632, 'epoch': 0.28}
{'loss': 1.0232, 'grad_norm': 1.508967638015747, 'learning_rate': 0.00018080689798188022, 'epoch': 0.28}
{'loss': 1.5185, 'grad_norm': 1.9152040481567383, 'learning_rate': 0.0001807752508758771, 'epoch': 0.28}
{'loss': 0.9451, 'grad_norm': 1.7386142015457153, 'learning_rate': 0.0001807435804746807, 'epoch': 0.28}
{'loss': 1.089, 'grad_norm': 1.4642375707626343, 'learning_rate': 0.00018071188678742456, 'epoch': 0.28}
{'loss': 0.873, 'grad_norm': 1.258944034576416, 'learning_rate': 0.00018068016982324903, 'epoch': 0.28}
{'loss': 1.0871, 'grad_norm': 1.90398371219635, 'learning_rate': 0.00018064842959130117, 'epoch': 0.28}
{'loss': 1.058, 'grad_norm': 1.0123181343078613, 'learning_rate': 0.00018061666610073464, 'epoch': 0.28}
{'loss': 1.2357, 'grad_norm': 1.108139991760254, 'learning_rate': 0.00018058487936070992, 'epoch': 0.28}
{'loss': 1.3184, 'grad_norm': 1.1635736227035522, 'learning_rate': 0.0001805530693803941, 'epoch': 0.28}
{'loss': 1.0364, 'grad_norm': 1.1712861061096191, 'learning_rate': 0.0001805212361689611, 'epoch': 0.28}
{'loss': 1.0171, 'grad_norm': 1.4772993326187134, 'learning_rate': 0.0001804893797355914, 'epoch': 0.28}
{'loss': 1.1052, 'grad_norm': 0.9889839887619019, 'learning_rate': 0.0001804575000894723, 'epoch': 0.28}
{'loss': 0.9701, 'grad_norm': 1.3297535181045532, 'learning_rate': 0.0001804255972397977, 'epoch': 0.28}
{'loss': 1.0349, 'grad_norm': 1.2177470922470093, 'learning_rate': 0.00018039367119576825, 'epoch': 0.28}
{'loss': 0.9214, 'grad_norm': 1.2227661609649658, 'learning_rate': 0.00018036172196659117, 'epoch': 0.28}
{'loss': 0.927, 'grad_norm': 1.1396909952163696, 'learning_rate': 0.00018032974956148063, 'epoch': 0.28}
{'loss': 0.8862, 'grad_norm': 1.2974578142166138, 'learning_rate': 0.0001802977539896572, 'epoch': 0.28}
{'loss': 1.1829, 'grad_norm': 1.0634739398956299, 'learning_rate': 0.0001802657352603483, 'epoch': 0.28}
{'loss': 1.0323, 'grad_norm': 1.235336184501648, 'learning_rate': 0.00018023369338278795, 'epoch': 0.28}
{'loss': 0.708, 'grad_norm': 1.1665955781936646, 'learning_rate': 0.00018020162836621687, 'epoch': 0.28}
{'loss': 0.9493, 'grad_norm': 1.610604166984558, 'learning_rate': 0.00018016954021988246, 'epoch': 0.28}
{'loss': 0.8701, 'grad_norm': 1.31892991065979, 'learning_rate': 0.00018013742895303883, 'epoch': 0.28}
{'loss': 1.1203, 'grad_norm': 1.1706693172454834, 'learning_rate': 0.00018010529457494664, 'epoch': 0.28}
{'loss': 0.7893, 'grad_norm': 1.162392020225525, 'learning_rate': 0.00018007313709487334, 'epoch': 0.28}
{'loss': 0.8716, 'grad_norm': 1.028761625289917, 'learning_rate': 0.00018004095652209302, 'epoch': 0.28}
{'loss': 0.9453, 'grad_norm': 1.3743565082550049, 'learning_rate': 0.00018000875286588633, 'epoch': 0.28}
{'loss': 0.7847, 'grad_norm': 1.1202203035354614, 'learning_rate': 0.0001799765261355407, 'epoch': 0.28}
{'loss': 0.9227, 'grad_norm': 1.4162442684173584, 'learning_rate': 0.00017994427634035015, 'epoch': 0.28}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.996, 'grad_norm': 1.1230549812316895, 'learning_rate': 0.00017991200348961532, 'epoch': 0.28}
{'loss': 1.0322, 'grad_norm': 1.3284180164337158, 'learning_rate': 0.00017987970759264363, 'epoch': 0.28}
{'loss': 1.0611, 'grad_norm': 1.4064078330993652, 'learning_rate': 0.00017984738865874902, 'epoch': 0.29}
{'loss': 1.1765, 'grad_norm': 1.3725966215133667, 'learning_rate': 0.00017981504669725208, 'epoch': 0.29}
{'loss': 1.0545, 'grad_norm': 1.8975096940994263, 'learning_rate': 0.00017978268171748015, 'epoch': 0.29}
{'loss': 0.9379, 'grad_norm': 1.4603760242462158, 'learning_rate': 0.00017975029372876706, 'epoch': 0.29}
{'loss': 1.1457, 'grad_norm': 1.7018486261367798, 'learning_rate': 0.0001797178827404534, 'epoch': 0.29}
{'loss': 1.1215, 'grad_norm': 1.1886749267578125, 'learning_rate': 0.00017968544876188633, 'epoch': 0.29}
{'loss': 1.1016, 'grad_norm': 1.3502169847488403, 'learning_rate': 0.00017965299180241963, 'epoch': 0.29}
{'loss': 0.7961, 'grad_norm': 1.15347158908844, 'learning_rate': 0.0001796205118714138, 'epoch': 0.29}
{'loss': 1.3421, 'grad_norm': 1.053322196006775, 'learning_rate': 0.0001795880089782358, 'epoch': 0.29}
{'loss': 1.2201, 'grad_norm': 1.1658568382263184, 'learning_rate': 0.00017955548313225935, 'epoch': 0.29}
{'loss': 0.9651, 'grad_norm': 1.873044490814209, 'learning_rate': 0.0001795229343428648, 'epoch': 0.29}
{'loss': 0.9846, 'grad_norm': 1.0862579345703125, 'learning_rate': 0.00017949036261943896, 'epoch': 0.29}
{'loss': 1.0049, 'grad_norm': 1.1063636541366577, 'learning_rate': 0.00017945776797137543, 'epoch': 0.29}
{'loss': 0.9219, 'grad_norm': 1.3567090034484863, 'learning_rate': 0.00017942515040807435, 'epoch': 0.29}
{'loss': 0.9112, 'grad_norm': 1.3181606531143188, 'learning_rate': 0.00017939250993894245, 'epoch': 0.29}
{'loss': 0.7437, 'grad_norm': 1.8292022943496704, 'learning_rate': 0.0001793598465733931, 'epoch': 0.29}
{'loss': 1.266, 'grad_norm': 1.0740299224853516, 'learning_rate': 0.00017932716032084618, 'epoch': 0.29}
{'loss': 0.8171, 'grad_norm': 1.3087574243545532, 'learning_rate': 0.00017929445119072837, 'epoch': 0.29}
{'loss': 0.9295, 'grad_norm': 1.187620759010315, 'learning_rate': 0.00017926171919247275, 'epoch': 0.29}
{'loss': 1.0524, 'grad_norm': 1.6667616367340088, 'learning_rate': 0.00017922896433551907, 'epoch': 0.29}
{'loss': 0.9634, 'grad_norm': 1.2707743644714355, 'learning_rate': 0.0001791961866293137, 'epoch': 0.29}
{'loss': 1.0165, 'grad_norm': 1.3108941316604614, 'learning_rate': 0.0001791633860833096, 'epoch': 0.29}
{'loss': 1.042, 'grad_norm': 1.3322639465332031, 'learning_rate': 0.0001791305627069662, 'epoch': 0.29}
{'loss': 0.8323, 'grad_norm': 1.255172848701477, 'learning_rate': 0.0001790977165097497, 'epoch': 0.29}
{'loss': 0.7656, 'grad_norm': 1.2386703491210938, 'learning_rate': 0.0001790648475011327, 'epoch': 0.29}
{'loss': 0.9891, 'grad_norm': 1.0460017919540405, 'learning_rate': 0.00017903195569059452, 'epoch': 0.29}
{'loss': 0.9746, 'grad_norm': 1.5764483213424683, 'learning_rate': 0.000178999041087621, 'epoch': 0.29}
{'loss': 1.1941, 'grad_norm': 1.0656732320785522, 'learning_rate': 0.0001789661037017045, 'epoch': 0.29}
{'loss': 1.1286, 'grad_norm': 1.146889328956604, 'learning_rate': 0.00017893314354234407, 'epoch': 0.29}
{'loss': 1.0155, 'grad_norm': 1.1394221782684326, 'learning_rate': 0.00017890016061904523, 'epoch': 0.29}
{'loss': 1.0974, 'grad_norm': 1.0628665685653687, 'learning_rate': 0.00017886715494132006, 'epoch': 0.29}
{'loss': 1.1493, 'grad_norm': 1.1085180044174194, 'learning_rate': 0.00017883412651868735, 'epoch': 0.29}
{'loss': 0.7584, 'grad_norm': 1.4335614442825317, 'learning_rate': 0.00017880107536067218, 'epoch': 0.29}
{'loss': 0.743, 'grad_norm': 1.413183569908142, 'learning_rate': 0.0001787680014768065, 'epoch': 0.29}
{'loss': 0.9803, 'grad_norm': 1.3705023527145386, 'learning_rate': 0.00017873490487662857, 'epoch': 0.29}
{'loss': 1.1934, 'grad_norm': 1.2162516117095947, 'learning_rate': 0.0001787017855696833, 'epoch': 0.29}
{'loss': 0.9762, 'grad_norm': 1.3454540967941284, 'learning_rate': 0.00017866864356552213, 'epoch': 0.29}
{'loss': 0.8886, 'grad_norm': 1.2926397323608398, 'learning_rate': 0.0001786354788737031, 'epoch': 0.29}
{'loss': 0.9096, 'grad_norm': 1.132836937904358, 'learning_rate': 0.00017860229150379068, 'epoch': 0.29}
{'loss': 0.8293, 'grad_norm': 1.3420369625091553, 'learning_rate': 0.00017856908146535603, 'epoch': 0.29}
{'loss': 1.0312, 'grad_norm': 1.14109468460083, 'learning_rate': 0.00017853584876797668, 'epoch': 0.29}
{'loss': 0.9052, 'grad_norm': 1.2461661100387573, 'learning_rate': 0.00017850259342123685, 'epoch': 0.29}
{'loss': 1.0285, 'grad_norm': 1.1631273031234741, 'learning_rate': 0.0001784693154347272, 'epoch': 0.29}
{'loss': 0.9292, 'grad_norm': 1.353293538093567, 'learning_rate': 0.00017843601481804493, 'epoch': 0.29}
{'loss': 0.9567, 'grad_norm': 1.0999865531921387, 'learning_rate': 0.00017840269158079378, 'epoch': 0.29}
{'loss': 0.8169, 'grad_norm': 1.2052370309829712, 'learning_rate': 0.000178369345732584, 'epoch': 0.29}
{'loss': 1.0369, 'grad_norm': 1.1940492391586304, 'learning_rate': 0.00017833597728303236, 'epoch': 0.29}
{'loss': 1.0361, 'grad_norm': 1.8400325775146484, 'learning_rate': 0.00017830258624176225, 'epoch': 0.29}
{'loss': 0.889, 'grad_norm': 1.2015011310577393, 'learning_rate': 0.00017826917261840337, 'epoch': 0.29}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0185, 'grad_norm': 1.407031774520874, 'learning_rate': 0.0001782357364225921, 'epoch': 0.29}
{'loss': 1.0087, 'grad_norm': 1.1524921655654907, 'learning_rate': 0.0001782022776639713, 'epoch': 0.29}
{'loss': 1.1999, 'grad_norm': 1.3084681034088135, 'learning_rate': 0.00017816879635219028, 'epoch': 0.29}
{'loss': 1.037, 'grad_norm': 1.2941054105758667, 'learning_rate': 0.0001781352924969049, 'epoch': 0.29}
{'loss': 1.1012, 'grad_norm': 1.0785068273544312, 'learning_rate': 0.0001781017661077775, 'epoch': 0.29}
{'loss': 1.2478, 'grad_norm': 1.0351426601409912, 'learning_rate': 0.00017806821719447695, 'epoch': 0.29}
{'loss': 1.069, 'grad_norm': 1.2631397247314453, 'learning_rate': 0.00017803464576667859, 'epoch': 0.29}
{'loss': 0.9697, 'grad_norm': 1.1152487993240356, 'learning_rate': 0.00017800105183406423, 'epoch': 0.29}
{'loss': 0.8336, 'grad_norm': 1.2397695779800415, 'learning_rate': 0.00017796743540632223, 'epoch': 0.29}
{'loss': 1.1859, 'grad_norm': 2.004610300064087, 'learning_rate': 0.00017793379649314744, 'epoch': 0.29}
{'loss': 1.3021, 'grad_norm': 1.0299872159957886, 'learning_rate': 0.00017790013510424107, 'epoch': 0.29}
{'loss': 0.9703, 'grad_norm': 1.1226176023483276, 'learning_rate': 0.00017786645124931094, 'epoch': 0.29}
{'loss': 0.8783, 'grad_norm': 1.2386640310287476, 'learning_rate': 0.00017783274493807135, 'epoch': 0.29}
{'loss': 0.8097, 'grad_norm': 1.4163427352905273, 'learning_rate': 0.00017779901618024301, 'epoch': 0.29}
{'loss': 0.8128, 'grad_norm': 1.3629076480865479, 'learning_rate': 0.00017776526498555312, 'epoch': 0.29}
{'loss': 1.1264, 'grad_norm': 1.3978952169418335, 'learning_rate': 0.00017773149136373538, 'epoch': 0.29}
{'loss': 0.7772, 'grad_norm': 1.243290662765503, 'learning_rate': 0.0001776976953245299, 'epoch': 0.3}
{'loss': 1.0992, 'grad_norm': 1.1971429586410522, 'learning_rate': 0.0001776638768776834, 'epoch': 0.3}
{'loss': 0.8005, 'grad_norm': 1.212317705154419, 'learning_rate': 0.0001776300360329488, 'epoch': 0.3}
{'loss': 0.9241, 'grad_norm': 1.4031972885131836, 'learning_rate': 0.00017759617280008576, 'epoch': 0.3}
{'loss': 1.1838, 'grad_norm': 1.2073099613189697, 'learning_rate': 0.00017756228718886026, 'epoch': 0.3}
{'loss': 0.8361, 'grad_norm': 1.1289106607437134, 'learning_rate': 0.00017752837920904466, 'epoch': 0.3}
{'loss': 0.7879, 'grad_norm': 1.6350456476211548, 'learning_rate': 0.00017749444887041799, 'epoch': 0.3}
{'loss': 1.0177, 'grad_norm': 1.2259057760238647, 'learning_rate': 0.00017746049618276545, 'epoch': 0.3}
{'loss': 0.893, 'grad_norm': 1.0872000455856323, 'learning_rate': 0.00017742652115587896, 'epoch': 0.3}
{'loss': 0.8971, 'grad_norm': 1.3401718139648438, 'learning_rate': 0.00017739252379955672, 'epoch': 0.3}
{'loss': 1.0853, 'grad_norm': 1.1655068397521973, 'learning_rate': 0.00017735850412360331, 'epoch': 0.3}
{'loss': 1.0914, 'grad_norm': 1.3996813297271729, 'learning_rate': 0.00017732446213782996, 'epoch': 0.3}
{'loss': 1.0326, 'grad_norm': 1.5106797218322754, 'learning_rate': 0.0001772903978520542, 'epoch': 0.3}
{'loss': 1.2217, 'grad_norm': 1.1231533288955688, 'learning_rate': 0.0001772563112760999, 'epoch': 0.3}
{'loss': 1.1626, 'grad_norm': 1.1079078912734985, 'learning_rate': 0.0001772222024197976, 'epoch': 0.3}
{'loss': 0.9139, 'grad_norm': 1.3432974815368652, 'learning_rate': 0.00017718807129298407, 'epoch': 0.3}
{'loss': 0.9603, 'grad_norm': 1.0937920808792114, 'learning_rate': 0.00017715391790550252, 'epoch': 0.3}
{'loss': 1.0741, 'grad_norm': 1.2224925756454468, 'learning_rate': 0.00017711974226720272, 'epoch': 0.3}
{'loss': 0.8611, 'grad_norm': 1.6208925247192383, 'learning_rate': 0.00017708554438794068, 'epoch': 0.3}
{'loss': 0.8699, 'grad_norm': 1.2333579063415527, 'learning_rate': 0.00017705132427757895, 'epoch': 0.3}
{'loss': 0.7811, 'grad_norm': 1.2729651927947998, 'learning_rate': 0.0001770170819459864, 'epoch': 0.3}
{'loss': 1.1182, 'grad_norm': 0.8921011686325073, 'learning_rate': 0.00017698281740303836, 'epoch': 0.3}
{'loss': 0.7888, 'grad_norm': 1.1461619138717651, 'learning_rate': 0.00017694853065861662, 'epoch': 0.3}
{'loss': 0.7883, 'grad_norm': 1.0016649961471558, 'learning_rate': 0.00017691422172260923, 'epoch': 0.3}
{'loss': 0.9021, 'grad_norm': 0.9420514106750488, 'learning_rate': 0.00017687989060491077, 'epoch': 0.3}
{'loss': 0.9554, 'grad_norm': 1.5176646709442139, 'learning_rate': 0.00017684553731542218, 'epoch': 0.3}
{'loss': 1.0292, 'grad_norm': 1.1713087558746338, 'learning_rate': 0.00017681116186405073, 'epoch': 0.3}
{'loss': 0.9953, 'grad_norm': 1.0156601667404175, 'learning_rate': 0.00017677676426071018, 'epoch': 0.3}
{'loss': 1.0444, 'grad_norm': 1.0153623819351196, 'learning_rate': 0.00017674234451532065, 'epoch': 0.3}
{'loss': 1.1387, 'grad_norm': 1.1985082626342773, 'learning_rate': 0.00017670790263780856, 'epoch': 0.3}
{'loss': 0.9081, 'grad_norm': 1.3071281909942627, 'learning_rate': 0.00017667343863810681, 'epoch': 0.3}
{'loss': 0.9222, 'grad_norm': 1.357940912246704, 'learning_rate': 0.0001766389525261547, 'epoch': 0.3}
{'loss': 0.892, 'grad_norm': 1.1391510963439941, 'learning_rate': 0.0001766044443118978, 'epoch': 0.3}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9446, 'grad_norm': 1.048380970954895, 'learning_rate': 0.00017656991400528816, 'epoch': 0.3}
{'loss': 0.802, 'grad_norm': 1.0927728414535522, 'learning_rate': 0.0001765353616162841, 'epoch': 0.3}
{'loss': 1.2856, 'grad_norm': 1.1597167253494263, 'learning_rate': 0.0001765007871548504, 'epoch': 0.3}
{'loss': 0.86, 'grad_norm': 1.1100727319717407, 'learning_rate': 0.00017646619063095816, 'epoch': 0.3}
{'loss': 1.0322, 'grad_norm': 1.1258063316345215, 'learning_rate': 0.00017643157205458483, 'epoch': 0.3}
{'loss': 1.0801, 'grad_norm': 1.0709999799728394, 'learning_rate': 0.0001763969314357143, 'epoch': 0.3}
{'loss': 0.9629, 'grad_norm': 1.2046382427215576, 'learning_rate': 0.00017636226878433667, 'epoch': 0.3}
{'loss': 0.9085, 'grad_norm': 1.3489482402801514, 'learning_rate': 0.00017632758411044856, 'epoch': 0.3}
{'loss': 1.0224, 'grad_norm': 0.8981142044067383, 'learning_rate': 0.00017629287742405283, 'epoch': 0.3}
{'loss': 0.7859, 'grad_norm': 0.9916799068450928, 'learning_rate': 0.0001762581487351587, 'epoch': 0.3}
{'loss': 0.902, 'grad_norm': 1.1153310537338257, 'learning_rate': 0.0001762233980537818, 'epoch': 0.3}
{'loss': 0.8291, 'grad_norm': 1.0673203468322754, 'learning_rate': 0.00017618862538994402, 'epoch': 0.3}
{'loss': 0.9782, 'grad_norm': 1.3292758464813232, 'learning_rate': 0.0001761538307536737, 'epoch': 0.3}
{'loss': 1.2218, 'grad_norm': 1.2649072408676147, 'learning_rate': 0.00017611901415500535, 'epoch': 0.3}
{'loss': 1.2042, 'grad_norm': 1.1386173963546753, 'learning_rate': 0.00017608417560397994, 'epoch': 0.3}
{'loss': 1.2114, 'grad_norm': 1.218966007232666, 'learning_rate': 0.00017604931511064478, 'epoch': 0.3}
{'loss': 0.8083, 'grad_norm': 1.2150779962539673, 'learning_rate': 0.00017601443268505342, 'epoch': 0.3}
{'loss': 1.2435, 'grad_norm': 1.3140164613723755, 'learning_rate': 0.00017597952833726583, 'epoch': 0.3}
{'loss': 0.8445, 'grad_norm': 1.3442720174789429, 'learning_rate': 0.00017594460207734822, 'epoch': 0.3}
{'loss': 1.0042, 'grad_norm': 1.1822930574417114, 'learning_rate': 0.00017590965391537316, 'epoch': 0.3}
{'loss': 1.0052, 'grad_norm': 1.2885613441467285, 'learning_rate': 0.00017587468386141955, 'epoch': 0.3}
{'loss': 1.0124, 'grad_norm': 1.432991862297058, 'learning_rate': 0.00017583969192557256, 'epoch': 0.3}
{'loss': 1.0158, 'grad_norm': 1.2492823600769043, 'learning_rate': 0.0001758046781179237, 'epoch': 0.3}
{'loss': 0.9516, 'grad_norm': 1.089747667312622, 'learning_rate': 0.00017576964244857081, 'epoch': 0.3}
{'loss': 1.0261, 'grad_norm': 1.2426648139953613, 'learning_rate': 0.00017573458492761801, 'epoch': 0.3}
{'loss': 0.8517, 'grad_norm': 1.3174296617507935, 'learning_rate': 0.00017569950556517566, 'epoch': 0.3}
{'loss': 0.8905, 'grad_norm': 1.1783803701400757, 'learning_rate': 0.00017566440437136052, 'epoch': 0.3}
{'loss': 1.0853, 'grad_norm': 1.3110551834106445, 'learning_rate': 0.00017562928135629564, 'epoch': 0.3}
{'loss': 1.0275, 'grad_norm': 1.179530382156372, 'learning_rate': 0.00017559413653011024, 'epoch': 0.3}
{'loss': 0.8948, 'grad_norm': 1.1594752073287964, 'learning_rate': 0.00017555896990294003, 'epoch': 0.3}
{'loss': 1.098, 'grad_norm': 1.7948435544967651, 'learning_rate': 0.00017552378148492679, 'epoch': 0.3}
{'loss': 0.7916, 'grad_norm': 1.1714876890182495, 'learning_rate': 0.00017548857128621875, 'epoch': 0.3}
{'loss': 0.8865, 'grad_norm': 1.217826008796692, 'learning_rate': 0.00017545333931697036, 'epoch': 0.31}
{'loss': 0.9252, 'grad_norm': 1.4381314516067505, 'learning_rate': 0.00017541808558734233, 'epoch': 0.31}
{'loss': 0.8708, 'grad_norm': 1.2838423252105713, 'learning_rate': 0.0001753828101075017, 'epoch': 0.31}
{'loss': 0.8704, 'grad_norm': 1.21820867061615, 'learning_rate': 0.0001753475128876217, 'epoch': 0.31}
{'loss': 1.0385, 'grad_norm': 1.1284579038619995, 'learning_rate': 0.0001753121939378819, 'epoch': 0.31}
{'loss': 1.3139, 'grad_norm': 1.2248835563659668, 'learning_rate': 0.00017527685326846815, 'epoch': 0.31}
{'loss': 1.1185, 'grad_norm': 1.0367084741592407, 'learning_rate': 0.00017524149088957245, 'epoch': 0.31}
{'loss': 0.7977, 'grad_norm': 1.0133782625198364, 'learning_rate': 0.00017520610681139322, 'epoch': 0.31}
{'loss': 1.1764, 'grad_norm': 1.0498164892196655, 'learning_rate': 0.00017517070104413498, 'epoch': 0.31}
{'loss': 0.8695, 'grad_norm': 1.038369059562683, 'learning_rate': 0.00017513527359800865, 'epoch': 0.31}
{'loss': 0.9242, 'grad_norm': 1.7894153594970703, 'learning_rate': 0.00017509982448323132, 'epoch': 0.31}
{'loss': 0.7423, 'grad_norm': 1.6659491062164307, 'learning_rate': 0.00017506435371002633, 'epoch': 0.31}
{'loss': 1.1532, 'grad_norm': 1.0433670282363892, 'learning_rate': 0.0001750288612886233, 'epoch': 0.31}
{'loss': 0.9394, 'grad_norm': 1.3426226377487183, 'learning_rate': 0.00017499334722925804, 'epoch': 0.31}
{'loss': 0.8313, 'grad_norm': 0.9061630368232727, 'learning_rate': 0.00017495781154217265, 'epoch': 0.31}
{'loss': 0.9323, 'grad_norm': 1.2817468643188477, 'learning_rate': 0.0001749222542376155, 'epoch': 0.31}
{'loss': 1.0966, 'grad_norm': 1.0699669122695923, 'learning_rate': 0.00017488667532584103, 'epoch': 0.31}
{'loss': 0.9296, 'grad_norm': 1.0985764265060425, 'learning_rate': 0.00017485107481711012, 'epoch': 0.31}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2803, 'grad_norm': 1.1281946897506714, 'learning_rate': 0.00017481545272168977, 'epoch': 0.31}
{'loss': 0.8295, 'grad_norm': 1.1316279172897339, 'learning_rate': 0.0001747798090498532, 'epoch': 0.31}
{'loss': 0.9236, 'grad_norm': 1.2882763147354126, 'learning_rate': 0.0001747441438118799, 'epoch': 0.31}
{'loss': 1.1833, 'grad_norm': 1.182102918624878, 'learning_rate': 0.00017470845701805552, 'epoch': 0.31}
{'loss': 1.08, 'grad_norm': 1.0813384056091309, 'learning_rate': 0.000174672748678672, 'epoch': 0.31}
{'loss': 0.8823, 'grad_norm': 1.1607879400253296, 'learning_rate': 0.00017463701880402737, 'epoch': 0.31}
{'loss': 0.8584, 'grad_norm': 0.9934327006340027, 'learning_rate': 0.00017460126740442608, 'epoch': 0.31}
{'loss': 0.799, 'grad_norm': 1.0598679780960083, 'learning_rate': 0.00017456549449017857, 'epoch': 0.31}
{'loss': 1.2019, 'grad_norm': 1.082898497581482, 'learning_rate': 0.0001745297000716016, 'epoch': 0.31}
{'loss': 0.6485, 'grad_norm': 1.2759113311767578, 'learning_rate': 0.00017449388415901812, 'epoch': 0.31}
{'loss': 0.7652, 'grad_norm': 1.5326848030090332, 'learning_rate': 0.00017445804676275724, 'epoch': 0.31}
{'loss': 1.0517, 'grad_norm': 1.2054216861724854, 'learning_rate': 0.00017442218789315433, 'epoch': 0.31}
{'loss': 0.8722, 'grad_norm': 1.3028894662857056, 'learning_rate': 0.00017438630756055087, 'epoch': 0.31}
{'loss': 1.5297, 'grad_norm': 1.2359355688095093, 'learning_rate': 0.00017435040577529461, 'epoch': 0.31}
{'loss': 0.817, 'grad_norm': 1.1480633020401, 'learning_rate': 0.00017431448254773944, 'epoch': 0.31}
{'loss': 0.9802, 'grad_norm': 1.2463250160217285, 'learning_rate': 0.00017427853788824543, 'epoch': 0.31}
{'loss': 1.303, 'grad_norm': 1.303552269935608, 'learning_rate': 0.00017424257180717888, 'epoch': 0.31}
{'loss': 0.919, 'grad_norm': 0.9882374405860901, 'learning_rate': 0.00017420658431491223, 'epoch': 0.31}
{'loss': 0.7279, 'grad_norm': 1.0068022012710571, 'learning_rate': 0.00017417057542182406, 'epoch': 0.31}
{'loss': 1.0897, 'grad_norm': 1.0947407484054565, 'learning_rate': 0.00017413454513829922, 'epoch': 0.31}
{'loss': 0.9502, 'grad_norm': 1.5725864171981812, 'learning_rate': 0.0001740984934747286, 'epoch': 0.31}
{'loss': 1.0036, 'grad_norm': 1.2558815479278564, 'learning_rate': 0.00017406242044150938, 'epoch': 0.31}
{'loss': 0.9099, 'grad_norm': 1.1048952341079712, 'learning_rate': 0.00017402632604904486, 'epoch': 0.31}
{'loss': 1.0769, 'grad_norm': 1.1517499685287476, 'learning_rate': 0.00017399021030774442, 'epoch': 0.31}
{'loss': 1.0611, 'grad_norm': 1.0506623983383179, 'learning_rate': 0.00017395407322802372, 'epoch': 0.31}
{'loss': 1.1505, 'grad_norm': 1.4255685806274414, 'learning_rate': 0.00017391791482030455, 'epoch': 0.31}
{'loss': 1.114, 'grad_norm': 1.068660855293274, 'learning_rate': 0.00017388173509501472, 'epoch': 0.31}
{'loss': 0.8677, 'grad_norm': 0.8971011638641357, 'learning_rate': 0.00017384553406258842, 'epoch': 0.31}
{'loss': 1.0413, 'grad_norm': 1.1369025707244873, 'learning_rate': 0.0001738093117334657, 'epoch': 0.31}
{'loss': 1.0102, 'grad_norm': 1.172370433807373, 'learning_rate': 0.00017377306811809304, 'epoch': 0.31}
{'loss': 1.1823, 'grad_norm': 0.9304298758506775, 'learning_rate': 0.00017373680322692282, 'epoch': 0.31}
{'loss': 0.6308, 'grad_norm': 1.352273941040039, 'learning_rate': 0.00017370051707041374, 'epoch': 0.31}
{'loss': 1.2478, 'grad_norm': 1.3174892663955688, 'learning_rate': 0.00017366420965903053, 'epoch': 0.31}
{'loss': 1.1229, 'grad_norm': 1.9129544496536255, 'learning_rate': 0.00017362788100324408, 'epoch': 0.31}
{'loss': 0.9863, 'grad_norm': 0.9641779661178589, 'learning_rate': 0.00017359153111353134, 'epoch': 0.31}
{'loss': 0.5842, 'grad_norm': 1.2992029190063477, 'learning_rate': 0.00017355516000037554, 'epoch': 0.31}
{'loss': 1.0662, 'grad_norm': 1.097893476486206, 'learning_rate': 0.00017351876767426585, 'epoch': 0.31}
{'loss': 0.8307, 'grad_norm': 1.1169004440307617, 'learning_rate': 0.00017348235414569766, 'epoch': 0.31}
{'loss': 1.1236, 'grad_norm': 1.1768150329589844, 'learning_rate': 0.0001734459194251725, 'epoch': 0.31}
{'loss': 0.9142, 'grad_norm': 1.2732270956039429, 'learning_rate': 0.00017340946352319793, 'epoch': 0.31}
{'loss': 1.0721, 'grad_norm': 1.101061224937439, 'learning_rate': 0.00017337298645028764, 'epoch': 0.31}
{'loss': 0.8863, 'grad_norm': 1.3011817932128906, 'learning_rate': 0.0001733364882169615, 'epoch': 0.31}
{'loss': 1.0413, 'grad_norm': 1.4539538621902466, 'learning_rate': 0.00017329996883374538, 'epoch': 0.31}
{'loss': 0.9216, 'grad_norm': 0.8931457996368408, 'learning_rate': 0.00017326342831117128, 'epoch': 0.31}
{'loss': 0.7593, 'grad_norm': 1.1906708478927612, 'learning_rate': 0.00017322686665977737, 'epoch': 0.31}
{'loss': 1.2361, 'grad_norm': 0.9843812584877014, 'learning_rate': 0.00017319028389010778, 'epoch': 0.31}
{'loss': 1.5132, 'grad_norm': 1.2212404012680054, 'learning_rate': 0.0001731536800127129, 'epoch': 0.31}
{'loss': 0.6596, 'grad_norm': 1.1964775323867798, 'learning_rate': 0.000173117055038149, 'epoch': 0.32}
{'loss': 1.1485, 'grad_norm': 1.249210000038147, 'learning_rate': 0.00017308040897697863, 'epoch': 0.32}
{'loss': 1.0204, 'grad_norm': 1.1916078329086304, 'learning_rate': 0.00017304374183977033, 'epoch': 0.32}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0408, 'grad_norm': 1.1446924209594727, 'learning_rate': 0.00017300705363709867, 'epoch': 0.32}
{'loss': 0.7743, 'grad_norm': 1.1909488439559937, 'learning_rate': 0.00017297034437954443, 'epoch': 0.32}
{'loss': 0.6578, 'grad_norm': 1.3511605262756348, 'learning_rate': 0.00017293361407769427, 'epoch': 0.32}
{'loss': 1.071, 'grad_norm': 1.4439678192138672, 'learning_rate': 0.00017289686274214118, 'epoch': 0.32}
{'loss': 1.0848, 'grad_norm': 1.494996428489685, 'learning_rate': 0.00017286009038348395, 'epoch': 0.32}
{'loss': 1.0877, 'grad_norm': 0.9424797296524048, 'learning_rate': 0.00017282329701232758, 'epoch': 0.32}
{'loss': 1.0848, 'grad_norm': 1.1595916748046875, 'learning_rate': 0.0001727864826392831, 'epoch': 0.32}
{'loss': 1.0075, 'grad_norm': 1.126143217086792, 'learning_rate': 0.00017274964727496768, 'epoch': 0.32}
{'loss': 1.1029, 'grad_norm': 0.9727766513824463, 'learning_rate': 0.00017271279093000435, 'epoch': 0.32}
{'loss': 0.9688, 'grad_norm': 1.5230573415756226, 'learning_rate': 0.00017267591361502232, 'epoch': 0.32}
{'loss': 0.7923, 'grad_norm': 1.238605260848999, 'learning_rate': 0.0001726390153406569, 'epoch': 0.32}
{'loss': 1.1103, 'grad_norm': 1.0201966762542725, 'learning_rate': 0.0001726020961175493, 'epoch': 0.32}
{'loss': 0.9953, 'grad_norm': 1.106278657913208, 'learning_rate': 0.0001725651559563469, 'epoch': 0.32}
{'loss': 1.0026, 'grad_norm': 1.2324159145355225, 'learning_rate': 0.000172528194867703, 'epoch': 0.32}
{'loss': 0.9827, 'grad_norm': 1.1970220804214478, 'learning_rate': 0.00017249121286227705, 'epoch': 0.32}
{'loss': 0.9704, 'grad_norm': 1.1848185062408447, 'learning_rate': 0.0001724542099507345, 'epoch': 0.32}
{'loss': 1.1905, 'grad_norm': 1.0648465156555176, 'learning_rate': 0.00017241718614374678, 'epoch': 0.32}
{'loss': 1.0158, 'grad_norm': 1.2624908685684204, 'learning_rate': 0.00017238014145199134, 'epoch': 0.32}
{'loss': 1.0705, 'grad_norm': 1.145512342453003, 'learning_rate': 0.00017234307588615176, 'epoch': 0.32}
{'loss': 0.9392, 'grad_norm': 1.3959921598434448, 'learning_rate': 0.00017230598945691753, 'epoch': 0.32}
{'loss': 0.8268, 'grad_norm': 1.1821446418762207, 'learning_rate': 0.00017226888217498424, 'epoch': 0.32}
{'loss': 0.9012, 'grad_norm': 1.1505745649337769, 'learning_rate': 0.0001722317540510534, 'epoch': 0.32}
{'loss': 0.923, 'grad_norm': 1.4594011306762695, 'learning_rate': 0.0001721946050958326, 'epoch': 0.32}
{'loss': 1.0044, 'grad_norm': 1.1691654920578003, 'learning_rate': 0.00017215743532003543, 'epoch': 0.32}
{'loss': 0.9662, 'grad_norm': 1.1804920434951782, 'learning_rate': 0.00017212024473438147, 'epoch': 0.32}
{'loss': 0.9581, 'grad_norm': 1.3485130071640015, 'learning_rate': 0.0001720830333495963, 'epoch': 0.32}
{'loss': 1.0049, 'grad_norm': 1.2097752094268799, 'learning_rate': 0.0001720458011764115, 'epoch': 0.32}
{'loss': 0.974, 'grad_norm': 1.2846218347549438, 'learning_rate': 0.00017200854822556466, 'epoch': 0.32}
{'loss': 0.8532, 'grad_norm': 1.326764702796936, 'learning_rate': 0.00017197127450779934, 'epoch': 0.32}
{'loss': 1.0625, 'grad_norm': 2.3344171047210693, 'learning_rate': 0.0001719339800338651, 'epoch': 0.32}
{'loss': 1.0477, 'grad_norm': 1.016729474067688, 'learning_rate': 0.00017189666481451753, 'epoch': 0.32}
{'loss': 1.0106, 'grad_norm': 1.382667064666748, 'learning_rate': 0.00017185932886051812, 'epoch': 0.32}
{'loss': 1.2794, 'grad_norm': 1.0422728061676025, 'learning_rate': 0.00017182197218263437, 'epoch': 0.32}
{'loss': 1.2746, 'grad_norm': 1.5882959365844727, 'learning_rate': 0.00017178459479163976, 'epoch': 0.32}
{'loss': 0.9894, 'grad_norm': 1.3273786306381226, 'learning_rate': 0.00017174719669831384, 'epoch': 0.32}
{'loss': 1.1044, 'grad_norm': 1.0888397693634033, 'learning_rate': 0.0001717097779134419, 'epoch': 0.32}
{'loss': 1.0138, 'grad_norm': 0.9713884592056274, 'learning_rate': 0.00017167233844781542, 'epoch': 0.32}
{'loss': 1.0343, 'grad_norm': 1.3892872333526611, 'learning_rate': 0.00017163487831223177, 'epoch': 0.32}
{'loss': 0.9743, 'grad_norm': 1.23880934715271, 'learning_rate': 0.0001715973975174942, 'epoch': 0.32}
{'loss': 0.9651, 'grad_norm': 1.0620088577270508, 'learning_rate': 0.00017155989607441213, 'epoch': 0.32}
{'loss': 1.0215, 'grad_norm': 1.1948949098587036, 'learning_rate': 0.00017152237399380065, 'epoch': 0.32}
{'loss': 1.0579, 'grad_norm': 1.0389548540115356, 'learning_rate': 0.00017148483128648102, 'epoch': 0.32}
{'loss': 1.0009, 'grad_norm': 1.0802559852600098, 'learning_rate': 0.00017144726796328034, 'epoch': 0.32}
{'loss': 0.8111, 'grad_norm': 1.032460331916809, 'learning_rate': 0.00017140968403503174, 'epoch': 0.32}
{'loss': 0.9833, 'grad_norm': 1.0849573612213135, 'learning_rate': 0.0001713720795125742, 'epoch': 0.32}
{'loss': 1.1659, 'grad_norm': 0.9581489562988281, 'learning_rate': 0.0001713344544067527, 'epoch': 0.32}
{'loss': 1.0183, 'grad_norm': 1.104820966720581, 'learning_rate': 0.00017129680872841814, 'epoch': 0.32}
{'loss': 0.8366, 'grad_norm': 0.94722580909729, 'learning_rate': 0.00017125914248842736, 'epoch': 0.32}
{'loss': 1.1687, 'grad_norm': 1.0920476913452148, 'learning_rate': 0.0001712214556976431, 'epoch': 0.32}
{'loss': 1.0436, 'grad_norm': 1.0229445695877075, 'learning_rate': 0.00017118374836693406, 'epoch': 0.32}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.918, 'grad_norm': 1.3283836841583252, 'learning_rate': 0.00017114602050717487, 'epoch': 0.32}
{'loss': 1.0751, 'grad_norm': 1.2029800415039062, 'learning_rate': 0.00017110827212924603, 'epoch': 0.32}
{'loss': 1.1228, 'grad_norm': 1.9926525354385376, 'learning_rate': 0.000171070503244034, 'epoch': 0.32}
{'loss': 1.19, 'grad_norm': 1.0062849521636963, 'learning_rate': 0.00017103271386243122, 'epoch': 0.32}
{'loss': 0.9033, 'grad_norm': 1.9968438148498535, 'learning_rate': 0.00017099490399533583, 'epoch': 0.32}
{'loss': 0.8244, 'grad_norm': 1.222327709197998, 'learning_rate': 0.0001709570736536521, 'epoch': 0.32}
{'loss': 1.1673, 'grad_norm': 1.1197073459625244, 'learning_rate': 0.0001709192228482901, 'epoch': 0.32}
{'loss': 0.7951, 'grad_norm': 1.167375922203064, 'learning_rate': 0.00017088135159016584, 'epoch': 0.32}
{'loss': 0.6494, 'grad_norm': 1.0907588005065918, 'learning_rate': 0.00017084345989020117, 'epoch': 0.32}
{'loss': 1.2015, 'grad_norm': 1.134116530418396, 'learning_rate': 0.00017080554775932388, 'epoch': 0.32}
{'loss': 0.7413, 'grad_norm': 1.2577918767929077, 'learning_rate': 0.00017076761520846767, 'epoch': 0.32}
{'loss': 1.1185, 'grad_norm': 1.0413395166397095, 'learning_rate': 0.0001707296622485721, 'epoch': 0.32}
{'loss': 1.0425, 'grad_norm': 0.9528926014900208, 'learning_rate': 0.0001706916888905826, 'epoch': 0.33}
{'loss': 0.9303, 'grad_norm': 1.2907207012176514, 'learning_rate': 0.00017065369514545053, 'epoch': 0.33}
{'loss': 0.8866, 'grad_norm': 1.159236192703247, 'learning_rate': 0.00017061568102413307, 'epoch': 0.33}
{'loss': 1.0196, 'grad_norm': 1.590805172920227, 'learning_rate': 0.00017057764653759336, 'epoch': 0.33}
{'loss': 1.1656, 'grad_norm': 1.4146473407745361, 'learning_rate': 0.00017053959169680032, 'epoch': 0.33}
{'loss': 1.1301, 'grad_norm': 1.18729829788208, 'learning_rate': 0.0001705015165127288, 'epoch': 0.33}
{'loss': 0.7621, 'grad_norm': 1.4630143642425537, 'learning_rate': 0.00017046342099635948, 'epoch': 0.33}
{'loss': 0.8867, 'grad_norm': 0.9446837306022644, 'learning_rate': 0.00017042530515867896, 'epoch': 0.33}
{'loss': 0.9047, 'grad_norm': 1.4458998441696167, 'learning_rate': 0.00017038716901067962, 'epoch': 0.33}
{'loss': 0.8439, 'grad_norm': 1.4272847175598145, 'learning_rate': 0.00017034901256335978, 'epoch': 0.33}
{'loss': 0.8177, 'grad_norm': 1.2072347402572632, 'learning_rate': 0.00017031083582772354, 'epoch': 0.33}
{'loss': 1.1442, 'grad_norm': 1.1864970922470093, 'learning_rate': 0.00017027263881478093, 'epoch': 0.33}
{'loss': 0.9411, 'grad_norm': 1.149963617324829, 'learning_rate': 0.00017023442153554777, 'epoch': 0.33}
{'loss': 0.8941, 'grad_norm': 1.5013272762298584, 'learning_rate': 0.00017019618400104572, 'epoch': 0.33}
{'loss': 0.9493, 'grad_norm': 1.7359005212783813, 'learning_rate': 0.00017015792622230227, 'epoch': 0.33}
{'loss': 0.9829, 'grad_norm': 1.0070204734802246, 'learning_rate': 0.00017011964821035086, 'epoch': 0.33}
{'loss': 1.2385, 'grad_norm': 1.0523240566253662, 'learning_rate': 0.00017008134997623065, 'epoch': 0.33}
{'loss': 1.0309, 'grad_norm': 1.0696805715560913, 'learning_rate': 0.00017004303153098665, 'epoch': 0.33}
{'loss': 0.9101, 'grad_norm': 1.4652296304702759, 'learning_rate': 0.00017000469288566973, 'epoch': 0.33}
{'loss': 0.9433, 'grad_norm': 1.5556765794754028, 'learning_rate': 0.00016996633405133655, 'epoch': 0.33}
{'loss': 0.8408, 'grad_norm': 1.2010672092437744, 'learning_rate': 0.00016992795503904964, 'epoch': 0.33}
{'loss': 0.9001, 'grad_norm': 1.291804552078247, 'learning_rate': 0.0001698895558598773, 'epoch': 0.33}
{'loss': 0.9601, 'grad_norm': 1.2148767709732056, 'learning_rate': 0.00016985113652489374, 'epoch': 0.33}
{'loss': 1.0495, 'grad_norm': 0.8904768228530884, 'learning_rate': 0.00016981269704517877, 'epoch': 0.33}
{'loss': 0.9898, 'grad_norm': 1.3379937410354614, 'learning_rate': 0.00016977423743181827, 'epoch': 0.33}
{'loss': 1.0355, 'grad_norm': 1.3845958709716797, 'learning_rate': 0.00016973575769590378, 'epoch': 0.33}
{'loss': 1.0744, 'grad_norm': 1.1911145448684692, 'learning_rate': 0.00016969725784853263, 'epoch': 0.33}
{'loss': 0.9122, 'grad_norm': 1.1373134851455688, 'learning_rate': 0.00016965873790080805, 'epoch': 0.33}
{'loss': 0.8643, 'grad_norm': 1.5696659088134766, 'learning_rate': 0.00016962019786383894, 'epoch': 0.33}
{'loss': 0.9326, 'grad_norm': 1.1437053680419922, 'learning_rate': 0.00016958163774874013, 'epoch': 0.33}
{'loss': 0.9857, 'grad_norm': 1.379312515258789, 'learning_rate': 0.00016954305756663212, 'epoch': 0.33}
{'loss': 0.9746, 'grad_norm': 1.8389898538589478, 'learning_rate': 0.00016950445732864127, 'epoch': 0.33}
{'loss': 0.9007, 'grad_norm': 1.0492671728134155, 'learning_rate': 0.00016946583704589973, 'epoch': 0.33}
{'loss': 0.8792, 'grad_norm': 1.2121719121932983, 'learning_rate': 0.00016942719672954538, 'epoch': 0.33}
{'loss': 1.1914, 'grad_norm': 1.310241460800171, 'learning_rate': 0.00016938853639072185, 'epoch': 0.33}
{'loss': 0.8427, 'grad_norm': 1.0511382818222046, 'learning_rate': 0.00016934985604057868, 'epoch': 0.33}
{'loss': 1.2088, 'grad_norm': 1.1993309259414673, 'learning_rate': 0.00016931115569027104, 'epoch': 0.33}
{'loss': 1.1802, 'grad_norm': 1.2674771547317505, 'learning_rate': 0.00016927243535095997, 'epoch': 0.33}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1025, 'grad_norm': 1.4074031114578247, 'learning_rate': 0.00016923369503381216, 'epoch': 0.33}
{'loss': 0.9219, 'grad_norm': 1.0136557817459106, 'learning_rate': 0.0001691949347500002, 'epoch': 0.33}
{'loss': 0.617, 'grad_norm': 1.2662147283554077, 'learning_rate': 0.00016915615451070233, 'epoch': 0.33}
{'loss': 0.9816, 'grad_norm': 1.2868036031723022, 'learning_rate': 0.0001691173543271026, 'epoch': 0.33}
{'loss': 0.9185, 'grad_norm': 1.2197293043136597, 'learning_rate': 0.0001690785342103908, 'epoch': 0.33}
{'loss': 1.1468, 'grad_norm': 1.0776833295822144, 'learning_rate': 0.00016903969417176245, 'epoch': 0.33}
{'loss': 0.9226, 'grad_norm': 0.9690207242965698, 'learning_rate': 0.00016900083422241883, 'epoch': 0.33}
{'loss': 0.9806, 'grad_norm': 1.2446188926696777, 'learning_rate': 0.000168961954373567, 'epoch': 0.33}
{'loss': 1.3683, 'grad_norm': 1.0504462718963623, 'learning_rate': 0.00016892305463641965, 'epoch': 0.33}
{'loss': 1.1753, 'grad_norm': 1.176134705543518, 'learning_rate': 0.00016888413502219533, 'epoch': 0.33}
{'loss': 1.0639, 'grad_norm': 1.0878325700759888, 'learning_rate': 0.0001688451955421183, 'epoch': 0.33}
{'loss': 1.0466, 'grad_norm': 1.0272020101547241, 'learning_rate': 0.00016880623620741842, 'epoch': 0.33}
{'loss': 0.9911, 'grad_norm': 1.158835530281067, 'learning_rate': 0.00016876725702933144, 'epoch': 0.33}
{'loss': 1.0796, 'grad_norm': 0.9427304267883301, 'learning_rate': 0.0001687282580190988, 'epoch': 0.33}
{'loss': 0.7418, 'grad_norm': 1.2981764078140259, 'learning_rate': 0.00016868923918796753, 'epoch': 0.33}
{'loss': 1.2035, 'grad_norm': 1.0303380489349365, 'learning_rate': 0.00016865020054719057, 'epoch': 0.33}
{'loss': 0.9446, 'grad_norm': 1.1282345056533813, 'learning_rate': 0.0001686111421080264, 'epoch': 0.33}
{'loss': 0.79, 'grad_norm': 1.4949672222137451, 'learning_rate': 0.0001685720638817393, 'epoch': 0.33}
{'loss': 0.9953, 'grad_norm': 1.3749349117279053, 'learning_rate': 0.00016853296587959933, 'epoch': 0.33}
{'loss': 0.673, 'grad_norm': 1.3447856903076172, 'learning_rate': 0.00016849384811288204, 'epoch': 0.33}
{'loss': 1.125, 'grad_norm': 1.3221330642700195, 'learning_rate': 0.00016845471059286887, 'epoch': 0.33}
{'loss': 0.9996, 'grad_norm': 2.496342897415161, 'learning_rate': 0.0001684155533308469, 'epoch': 0.33}
{'loss': 1.1126, 'grad_norm': 1.488400936126709, 'learning_rate': 0.00016837637633810887, 'epoch': 0.33}
{'loss': 1.2244, 'grad_norm': 1.272518515586853, 'learning_rate': 0.00016833717962595326, 'epoch': 0.33}
{'loss': 0.9841, 'grad_norm': 1.3540135622024536, 'learning_rate': 0.00016829796320568416, 'epoch': 0.33}
{'loss': 1.1259, 'grad_norm': 1.5053656101226807, 'learning_rate': 0.00016825872708861147, 'epoch': 0.33}
{'loss': 1.2123, 'grad_norm': 1.09644615650177, 'learning_rate': 0.00016821947128605065, 'epoch': 0.33}
{'loss': 1.2415, 'grad_norm': 0.994430661201477, 'learning_rate': 0.00016818019580932294, 'epoch': 0.34}
{'loss': 1.1036, 'grad_norm': 0.9891332387924194, 'learning_rate': 0.00016814090066975509, 'epoch': 0.34}
{'loss': 1.1268, 'grad_norm': 1.0335934162139893, 'learning_rate': 0.00016810158587867973, 'epoch': 0.34}
{'loss': 1.0804, 'grad_norm': 1.148430585861206, 'learning_rate': 0.000168062251447435, 'epoch': 0.34}
{'loss': 1.3882, 'grad_norm': 1.5096384286880493, 'learning_rate': 0.00016802289738736481, 'epoch': 0.34}
{'loss': 0.7583, 'grad_norm': 1.1716597080230713, 'learning_rate': 0.00016798352370981862, 'epoch': 0.34}
{'loss': 1.2045, 'grad_norm': 1.1338609457015991, 'learning_rate': 0.00016794413042615168, 'epoch': 0.34}
{'loss': 0.8263, 'grad_norm': 1.166234016418457, 'learning_rate': 0.00016790471754772474, 'epoch': 0.34}
{'loss': 1.0672, 'grad_norm': 1.316856026649475, 'learning_rate': 0.00016786528508590435, 'epoch': 0.34}
{'loss': 0.93, 'grad_norm': 1.288203239440918, 'learning_rate': 0.00016782583305206263, 'epoch': 0.34}
{'loss': 1.1035, 'grad_norm': 1.1983462572097778, 'learning_rate': 0.00016778636145757734, 'epoch': 0.34}
{'loss': 0.9808, 'grad_norm': 1.2132083177566528, 'learning_rate': 0.00016774687031383188, 'epoch': 0.34}
{'loss': 0.7657, 'grad_norm': 1.3975648880004883, 'learning_rate': 0.0001677073596322154, 'epoch': 0.34}
{'loss': 0.9673, 'grad_norm': 1.1879618167877197, 'learning_rate': 0.00016766782942412246, 'epoch': 0.34}
{'loss': 0.9879, 'grad_norm': 1.160669207572937, 'learning_rate': 0.00016762827970095344, 'epoch': 0.34}
{'loss': 0.8401, 'grad_norm': 1.2037715911865234, 'learning_rate': 0.00016758871047411434, 'epoch': 0.34}
{'loss': 0.7446, 'grad_norm': 1.089651107788086, 'learning_rate': 0.00016754912175501665, 'epoch': 0.34}
{'loss': 0.8304, 'grad_norm': 1.0356104373931885, 'learning_rate': 0.00016750951355507763, 'epoch': 0.34}
{'loss': 1.06, 'grad_norm': 1.0783519744873047, 'learning_rate': 0.00016746988588572004, 'epoch': 0.34}
{'loss': 1.1506, 'grad_norm': 1.419188141822815, 'learning_rate': 0.00016743023875837233, 'epoch': 0.34}
{'loss': 1.096, 'grad_norm': 1.0173218250274658, 'learning_rate': 0.0001673905721844686, 'epoch': 0.34}
{'loss': 0.8836, 'grad_norm': 1.7024099826812744, 'learning_rate': 0.00016735088617544837, 'epoch': 0.34}
{'loss': 0.8483, 'grad_norm': 1.5944091081619263, 'learning_rate': 0.00016731118074275704, 'epoch': 0.34}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0069, 'grad_norm': 1.4249851703643799, 'learning_rate': 0.00016727145589784533, 'epoch': 0.34}
{'loss': 1.315, 'grad_norm': 1.1024001836776733, 'learning_rate': 0.0001672317116521698, 'epoch': 0.34}
{'loss': 0.9753, 'grad_norm': 1.5503305196762085, 'learning_rate': 0.00016719194801719244, 'epoch': 0.34}
{'loss': 0.8885, 'grad_norm': 1.3086017370224, 'learning_rate': 0.00016715216500438093, 'epoch': 0.34}
{'loss': 1.1732, 'grad_norm': 1.1510722637176514, 'learning_rate': 0.00016711236262520847, 'epoch': 0.34}
{'loss': 0.9242, 'grad_norm': 1.0862873792648315, 'learning_rate': 0.0001670725408911539, 'epoch': 0.34}
{'loss': 1.1054, 'grad_norm': 1.1319572925567627, 'learning_rate': 0.00016703269981370157, 'epoch': 0.34}
{'loss': 1.2387, 'grad_norm': 1.0278061628341675, 'learning_rate': 0.0001669928394043415, 'epoch': 0.34}
{'loss': 0.9028, 'grad_norm': 1.6152704954147339, 'learning_rate': 0.0001669529596745692, 'epoch': 0.34}
{'loss': 0.9761, 'grad_norm': 1.2657955884933472, 'learning_rate': 0.00016691306063588583, 'epoch': 0.34}
{'loss': 0.9811, 'grad_norm': 1.177276849746704, 'learning_rate': 0.00016687314229979806, 'epoch': 0.34}
{'loss': 0.987, 'grad_norm': 1.3931840658187866, 'learning_rate': 0.00016683320467781817, 'epoch': 0.34}
{'loss': 1.1456, 'grad_norm': 1.292815089225769, 'learning_rate': 0.00016679324778146395, 'epoch': 0.34}
{'loss': 1.1321, 'grad_norm': 1.480582594871521, 'learning_rate': 0.0001667532716222588, 'epoch': 0.34}
{'loss': 0.8236, 'grad_norm': 1.3940725326538086, 'learning_rate': 0.0001667132762117316, 'epoch': 0.34}
{'loss': 0.9565, 'grad_norm': 1.3948523998260498, 'learning_rate': 0.00016667326156141692, 'epoch': 0.34}
{'loss': 1.2176, 'grad_norm': 1.0187350511550903, 'learning_rate': 0.0001666332276828547, 'epoch': 0.34}
{'loss': 1.1556, 'grad_norm': 1.154619812965393, 'learning_rate': 0.00016659317458759057, 'epoch': 0.34}
{'loss': 1.3201, 'grad_norm': 1.292108178138733, 'learning_rate': 0.00016655310228717564, 'epoch': 0.34}
{'loss': 1.0075, 'grad_norm': 0.9501010179519653, 'learning_rate': 0.0001665130107931666, 'epoch': 0.34}
{'loss': 1.0795, 'grad_norm': 1.0475690364837646, 'learning_rate': 0.00016647290011712558, 'epoch': 0.34}
{'loss': 0.9076, 'grad_norm': 0.9925301671028137, 'learning_rate': 0.00016643277027062037, 'epoch': 0.34}
{'loss': 1.2373, 'grad_norm': 1.2371740341186523, 'learning_rate': 0.00016639262126522418, 'epoch': 0.34}
{'loss': 1.1993, 'grad_norm': 1.0435866117477417, 'learning_rate': 0.0001663524531125158, 'epoch': 0.34}
{'loss': 0.7259, 'grad_norm': 1.1163865327835083, 'learning_rate': 0.00016631226582407952, 'epoch': 0.34}
{'loss': 0.9514, 'grad_norm': 1.26633882522583, 'learning_rate': 0.0001662720594115052, 'epoch': 0.34}
{'loss': 1.0704, 'grad_norm': 0.9758613705635071, 'learning_rate': 0.00016623183388638814, 'epoch': 0.34}
{'loss': 0.8902, 'grad_norm': 1.1992998123168945, 'learning_rate': 0.0001661915892603292, 'epoch': 0.34}
{'loss': 0.8865, 'grad_norm': 1.326667308807373, 'learning_rate': 0.00016615132554493475, 'epoch': 0.34}
{'loss': 0.8511, 'grad_norm': 0.949193000793457, 'learning_rate': 0.00016611104275181664, 'epoch': 0.34}
{'loss': 0.7483, 'grad_norm': 1.0597091913223267, 'learning_rate': 0.00016607074089259222, 'epoch': 0.34}
{'loss': 0.9824, 'grad_norm': 1.273714303970337, 'learning_rate': 0.00016603041997888436, 'epoch': 0.34}
{'loss': 0.7938, 'grad_norm': 1.208543300628662, 'learning_rate': 0.00016599008002232143, 'epoch': 0.34}
{'loss': 0.8175, 'grad_norm': 1.6693917512893677, 'learning_rate': 0.00016594972103453726, 'epoch': 0.34}
{'loss': 0.9501, 'grad_norm': 1.224098563194275, 'learning_rate': 0.0001659093430271712, 'epoch': 0.34}
{'loss': 1.1378, 'grad_norm': 1.2130951881408691, 'learning_rate': 0.00016586894601186805, 'epoch': 0.34}
{'loss': 1.0088, 'grad_norm': 1.2876278162002563, 'learning_rate': 0.00016582853000027815, 'epoch': 0.34}
{'loss': 0.9045, 'grad_norm': 1.2245773077011108, 'learning_rate': 0.0001657880950040573, 'epoch': 0.34}
{'loss': 0.8668, 'grad_norm': 1.0434523820877075, 'learning_rate': 0.00016574764103486668, 'epoch': 0.34}
{'loss': 0.9067, 'grad_norm': 1.1630724668502808, 'learning_rate': 0.0001657071681043731, 'epoch': 0.34}
{'loss': 1.0019, 'grad_norm': 1.131206750869751, 'learning_rate': 0.00016566667622424868, 'epoch': 0.34}
{'loss': 0.974, 'grad_norm': 1.0645889043807983, 'learning_rate': 0.00016562616540617117, 'epoch': 0.34}
{'loss': 0.8712, 'grad_norm': 0.9931906461715698, 'learning_rate': 0.00016558563566182363, 'epoch': 0.35}
{'loss': 1.0282, 'grad_norm': 1.2512584924697876, 'learning_rate': 0.00016554508700289468, 'epoch': 0.35}
{'loss': 0.9079, 'grad_norm': 1.1657589673995972, 'learning_rate': 0.00016550451944107833, 'epoch': 0.35}
{'loss': 1.2015, 'grad_norm': 1.1467267274856567, 'learning_rate': 0.00016546393298807406, 'epoch': 0.35}
{'loss': 1.0346, 'grad_norm': 1.3468738794326782, 'learning_rate': 0.00016542332765558684, 'epoch': 0.35}
{'loss': 0.9742, 'grad_norm': 1.4135676622390747, 'learning_rate': 0.00016538270345532708, 'epoch': 0.35}
{'loss': 0.8849, 'grad_norm': 1.3425780534744263, 'learning_rate': 0.00016534206039901057, 'epoch': 0.35}
{'loss': 0.6113, 'grad_norm': 0.9533889889717102, 'learning_rate': 0.0001653013984983585, 'epoch': 0.35}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9198, 'grad_norm': 1.2038193941116333, 'learning_rate': 0.00016526071776509768, 'epoch': 0.35}
{'loss': 0.9467, 'grad_norm': 1.1348663568496704, 'learning_rate': 0.0001652200182109602, 'epoch': 0.35}
{'loss': 1.0939, 'grad_norm': 0.9946991801261902, 'learning_rate': 0.0001651792998476836, 'epoch': 0.35}
{'loss': 0.7784, 'grad_norm': 1.0510609149932861, 'learning_rate': 0.00016513856268701087, 'epoch': 0.35}
{'loss': 0.7814, 'grad_norm': 1.3966503143310547, 'learning_rate': 0.0001650978067406904, 'epoch': 0.35}
{'loss': 1.2486, 'grad_norm': 1.1588224172592163, 'learning_rate': 0.00016505703202047598, 'epoch': 0.35}
{'loss': 1.1755, 'grad_norm': 0.9794701933860779, 'learning_rate': 0.00016501623853812692, 'epoch': 0.35}
{'loss': 1.1576, 'grad_norm': 1.6115105152130127, 'learning_rate': 0.00016497542630540784, 'epoch': 0.35}
{'loss': 0.8035, 'grad_norm': 1.1710034608840942, 'learning_rate': 0.00016493459533408874, 'epoch': 0.35}
{'loss': 0.9511, 'grad_norm': 1.5001858472824097, 'learning_rate': 0.0001648937456359451, 'epoch': 0.35}
{'loss': 0.8776, 'grad_norm': 1.3181082010269165, 'learning_rate': 0.0001648528772227578, 'epoch': 0.35}
{'loss': 0.8269, 'grad_norm': 1.3816992044448853, 'learning_rate': 0.0001648119901063131, 'epoch': 0.35}
{'loss': 0.7783, 'grad_norm': 1.3364392518997192, 'learning_rate': 0.00016477108429840262, 'epoch': 0.35}
{'loss': 0.8098, 'grad_norm': 1.4136137962341309, 'learning_rate': 0.00016473015981082338, 'epoch': 0.35}
{'loss': 0.9351, 'grad_norm': 1.3477920293807983, 'learning_rate': 0.00016468921665537786, 'epoch': 0.35}
{'loss': 1.0435, 'grad_norm': 1.1998590230941772, 'learning_rate': 0.0001646482548438738, 'epoch': 0.35}
{'loss': 1.2182, 'grad_norm': 1.2253731489181519, 'learning_rate': 0.00016460727438812446, 'epoch': 0.35}
{'loss': 0.9394, 'grad_norm': 1.1941792964935303, 'learning_rate': 0.0001645662752999484, 'epoch': 0.35}
{'loss': 0.775, 'grad_norm': 1.1087970733642578, 'learning_rate': 0.00016452525759116945, 'epoch': 0.35}
{'loss': 1.1419, 'grad_norm': 1.197573184967041, 'learning_rate': 0.00016448422127361706, 'epoch': 0.35}
{'loss': 0.8828, 'grad_norm': 1.0392342805862427, 'learning_rate': 0.00016444316635912584, 'epoch': 0.35}
{'loss': 1.0392, 'grad_norm': 1.0694986581802368, 'learning_rate': 0.00016440209285953586, 'epoch': 0.35}
{'loss': 0.7532, 'grad_norm': 0.9614894986152649, 'learning_rate': 0.00016436100078669248, 'epoch': 0.35}
{'loss': 1.1078, 'grad_norm': 1.3317688703536987, 'learning_rate': 0.00016431989015244647, 'epoch': 0.35}
{'loss': 0.9838, 'grad_norm': 1.3377504348754883, 'learning_rate': 0.00016427876096865394, 'epoch': 0.35}
{'loss': 1.0281, 'grad_norm': 1.3045212030410767, 'learning_rate': 0.00016423761324717634, 'epoch': 0.35}
{'loss': 1.0389, 'grad_norm': 0.9650449156761169, 'learning_rate': 0.00016419644699988052, 'epoch': 0.35}
{'loss': 1.0121, 'grad_norm': 1.0539172887802124, 'learning_rate': 0.0001641552622386386, 'epoch': 0.35}
{'loss': 0.9517, 'grad_norm': 1.0002660751342773, 'learning_rate': 0.00016411405897532802, 'epoch': 0.35}
{'loss': 0.9691, 'grad_norm': 1.2161871194839478, 'learning_rate': 0.00016407283722183165, 'epoch': 0.35}
{'loss': 1.1804, 'grad_norm': 2.086852788925171, 'learning_rate': 0.00016403159699003767, 'epoch': 0.35}
{'loss': 1.1345, 'grad_norm': 0.9220873713493347, 'learning_rate': 0.0001639903382918395, 'epoch': 0.35}
{'loss': 0.8407, 'grad_norm': 0.8770057559013367, 'learning_rate': 0.00016394906113913603, 'epoch': 0.35}
{'loss': 1.083, 'grad_norm': 1.072365403175354, 'learning_rate': 0.0001639077655438313, 'epoch': 0.35}
{'loss': 1.074, 'grad_norm': 1.0102237462997437, 'learning_rate': 0.0001638664515178348, 'epoch': 0.35}
{'loss': 1.2995, 'grad_norm': 2.2161591053009033, 'learning_rate': 0.00016382511907306132, 'epoch': 0.35}
{'loss': 1.056, 'grad_norm': 1.1897975206375122, 'learning_rate': 0.00016378376822143094, 'epoch': 0.35}
{'loss': 0.9695, 'grad_norm': 1.1064767837524414, 'learning_rate': 0.000163742398974869, 'epoch': 0.35}
{'loss': 1.2084, 'grad_norm': 0.9638751149177551, 'learning_rate': 0.0001637010113453062, 'epoch': 0.35}
{'loss': 1.1395, 'grad_norm': 1.0110090970993042, 'learning_rate': 0.00016365960534467857, 'epoch': 0.35}
{'loss': 1.2471, 'grad_norm': 0.9877312183380127, 'learning_rate': 0.0001636181809849274, 'epoch': 0.35}
{'loss': 0.9897, 'grad_norm': 0.9791702628135681, 'learning_rate': 0.00016357673827799922, 'epoch': 0.35}
{'loss': 1.1308, 'grad_norm': 1.0965982675552368, 'learning_rate': 0.00016353527723584596, 'epoch': 0.35}
{'loss': 1.2182, 'grad_norm': 1.320193886756897, 'learning_rate': 0.00016349379787042477, 'epoch': 0.35}
{'loss': 1.1129, 'grad_norm': 1.2940428256988525, 'learning_rate': 0.0001634523001936981, 'epoch': 0.35}
{'loss': 1.344, 'grad_norm': 0.9467986226081848, 'learning_rate': 0.00016341078421763368, 'epoch': 0.35}
{'loss': 0.9531, 'grad_norm': 1.2752158641815186, 'learning_rate': 0.00016336924995420454, 'epoch': 0.35}
{'loss': 0.9612, 'grad_norm': 1.7254822254180908, 'learning_rate': 0.00016332769741538892, 'epoch': 0.35}
{'loss': 0.7885, 'grad_norm': 1.4191142320632935, 'learning_rate': 0.00016328612661317034, 'epoch': 0.35}
{'loss': 0.9452, 'grad_norm': 1.271636724472046, 'learning_rate': 0.00016324453755953773, 'epoch': 0.35}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7846, 'grad_norm': 1.3321375846862793, 'learning_rate': 0.0001632029302664851, 'epoch': 0.35}
{'loss': 1.0466, 'grad_norm': 1.2902841567993164, 'learning_rate': 0.00016316130474601178, 'epoch': 0.35}
{'loss': 0.7639, 'grad_norm': 1.496462345123291, 'learning_rate': 0.00016311966101012243, 'epoch': 0.35}
{'loss': 1.0742, 'grad_norm': 0.983281672000885, 'learning_rate': 0.00016307799907082683, 'epoch': 0.35}
{'loss': 0.8792, 'grad_norm': 1.5556542873382568, 'learning_rate': 0.00016303631894014014, 'epoch': 0.35}
{'loss': 1.2004, 'grad_norm': 0.964768648147583, 'learning_rate': 0.00016299462063008272, 'epoch': 0.35}
{'loss': 1.0557, 'grad_norm': 1.1504430770874023, 'learning_rate': 0.0001629529041526801, 'epoch': 0.35}
{'loss': 0.7073, 'grad_norm': 1.174380898475647, 'learning_rate': 0.00016291116951996313, 'epoch': 0.36}
{'loss': 0.5655, 'grad_norm': 1.183958888053894, 'learning_rate': 0.00016286941674396787, 'epoch': 0.36}
{'loss': 1.0975, 'grad_norm': 1.1338554620742798, 'learning_rate': 0.00016282764583673567, 'epoch': 0.36}
{'loss': 0.7436, 'grad_norm': 1.0802631378173828, 'learning_rate': 0.00016278585681031303, 'epoch': 0.36}
{'loss': 1.1057, 'grad_norm': 1.2302796840667725, 'learning_rate': 0.00016274404967675168, 'epoch': 0.36}
{'loss': 1.2946, 'grad_norm': 1.1405695676803589, 'learning_rate': 0.00016270222444810862, 'epoch': 0.36}
{'loss': 1.1371, 'grad_norm': 2.241692543029785, 'learning_rate': 0.00016266038113644607, 'epoch': 0.36}
{'loss': 0.8871, 'grad_norm': 1.309064269065857, 'learning_rate': 0.00016261851975383137, 'epoch': 0.36}
{'loss': 0.9242, 'grad_norm': 1.0182981491088867, 'learning_rate': 0.00016257664031233722, 'epoch': 0.36}
{'loss': 0.8688, 'grad_norm': 1.1006144285202026, 'learning_rate': 0.00016253474282404139, 'epoch': 0.36}
{'loss': 1.0856, 'grad_norm': 1.0910108089447021, 'learning_rate': 0.00016249282730102692, 'epoch': 0.36}
{'loss': 0.9048, 'grad_norm': 0.9902209043502808, 'learning_rate': 0.00016245089375538206, 'epoch': 0.36}
{'loss': 1.1606, 'grad_norm': 1.0400025844573975, 'learning_rate': 0.0001624089421992003, 'epoch': 0.36}
{'loss': 0.9874, 'grad_norm': 1.1903791427612305, 'learning_rate': 0.00016236697264458014, 'epoch': 0.36}
{'loss': 0.9145, 'grad_norm': 1.1394203901290894, 'learning_rate': 0.00016232498510362548, 'epoch': 0.36}
{'loss': 0.8582, 'grad_norm': 0.9714674353599548, 'learning_rate': 0.00016228297958844533, 'epoch': 0.36}
{'loss': 1.012, 'grad_norm': 0.9816327095031738, 'learning_rate': 0.00016224095611115384, 'epoch': 0.36}
{'loss': 0.7817, 'grad_norm': 1.2798030376434326, 'learning_rate': 0.0001621989146838704, 'epoch': 0.36}
{'loss': 1.0028, 'grad_norm': 1.0759592056274414, 'learning_rate': 0.00016215685531871955, 'epoch': 0.36}
{'loss': 1.1222, 'grad_norm': 1.0488409996032715, 'learning_rate': 0.00016211477802783103, 'epoch': 0.36}
{'loss': 1.1174, 'grad_norm': 0.9315440654754639, 'learning_rate': 0.00016207268282333973, 'epoch': 0.36}
{'loss': 0.8521, 'grad_norm': 1.2398697137832642, 'learning_rate': 0.00016203056971738567, 'epoch': 0.36}
{'loss': 0.8088, 'grad_norm': 1.1024775505065918, 'learning_rate': 0.00016198843872211404, 'epoch': 0.36}
{'loss': 0.8231, 'grad_norm': 1.2990578413009644, 'learning_rate': 0.00016194628984967526, 'epoch': 0.36}
{'loss': 0.8201, 'grad_norm': 1.003480315208435, 'learning_rate': 0.00016190412311222488, 'epoch': 0.36}
{'loss': 0.7827, 'grad_norm': 1.1771013736724854, 'learning_rate': 0.00016186193852192355, 'epoch': 0.36}
{'loss': 0.9536, 'grad_norm': 1.5914769172668457, 'learning_rate': 0.00016181973609093715, 'epoch': 0.36}
{'loss': 0.8064, 'grad_norm': 1.2762283086776733, 'learning_rate': 0.00016177751583143659, 'epoch': 0.36}
{'loss': 1.057, 'grad_norm': 1.2194103002548218, 'learning_rate': 0.000161735277755598, 'epoch': 0.36}
{'loss': 0.8578, 'grad_norm': 0.9861248731613159, 'learning_rate': 0.0001616930218756027, 'epoch': 0.36}
{'loss': 0.8964, 'grad_norm': 1.7115209102630615, 'learning_rate': 0.00016165074820363704, 'epoch': 0.36}
{'loss': 1.1897, 'grad_norm': 1.152795672416687, 'learning_rate': 0.00016160845675189254, 'epoch': 0.36}
{'loss': 0.849, 'grad_norm': 1.1162848472595215, 'learning_rate': 0.0001615661475325658, 'epoch': 0.36}
{'loss': 1.0497, 'grad_norm': 1.0898401737213135, 'learning_rate': 0.00016152382055785873, 'epoch': 0.36}
{'loss': 1.1121, 'grad_norm': 0.9684547781944275, 'learning_rate': 0.00016148147583997812, 'epoch': 0.36}
{'loss': 0.9459, 'grad_norm': 1.0712848901748657, 'learning_rate': 0.000161439113391136, 'epoch': 0.36}
{'loss': 0.9634, 'grad_norm': 1.2003028392791748, 'learning_rate': 0.00016139673322354954, 'epoch': 0.36}
{'loss': 1.0127, 'grad_norm': 1.1370351314544678, 'learning_rate': 0.0001613543353494409, 'epoch': 0.36}
{'loss': 0.8412, 'grad_norm': 1.7878832817077637, 'learning_rate': 0.00016131191978103747, 'epoch': 0.36}
{'loss': 1.1396, 'grad_norm': 1.284414529800415, 'learning_rate': 0.0001612694865305717, 'epoch': 0.36}
{'loss': 0.8732, 'grad_norm': 1.1568330526351929, 'learning_rate': 0.00016122703561028114, 'epoch': 0.36}
{'loss': 0.8975, 'grad_norm': 1.157451868057251, 'learning_rate': 0.0001611845670324084, 'epoch': 0.36}
{'loss': 1.0125, 'grad_norm': 0.9257718920707703, 'learning_rate': 0.00016114208080920123, 'epoch': 0.36}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2922, 'grad_norm': 1.090985655784607, 'learning_rate': 0.00016109957695291244, 'epoch': 0.36}
{'loss': 0.9606, 'grad_norm': 0.9953566193580627, 'learning_rate': 0.00016105705547579997, 'epoch': 0.36}
{'loss': 0.9894, 'grad_norm': 1.0582834482192993, 'learning_rate': 0.0001610145163901268, 'epoch': 0.36}
{'loss': 0.8735, 'grad_norm': 1.022506833076477, 'learning_rate': 0.00016097195970816094, 'epoch': 0.36}
{'loss': 1.1516, 'grad_norm': 1.2445781230926514, 'learning_rate': 0.0001609293854421756, 'epoch': 0.36}
{'loss': 1.1911, 'grad_norm': 1.253806233406067, 'learning_rate': 0.00016088679360444895, 'epoch': 0.36}
{'loss': 0.8396, 'grad_norm': 1.2534003257751465, 'learning_rate': 0.00016084418420726432, 'epoch': 0.36}
{'loss': 1.2706, 'grad_norm': 1.90035080909729, 'learning_rate': 0.00016080155726291005, 'epoch': 0.36}
{'loss': 1.02, 'grad_norm': 1.1222169399261475, 'learning_rate': 0.0001607589127836795, 'epoch': 0.36}
{'loss': 0.9554, 'grad_norm': 1.481156587600708, 'learning_rate': 0.00016071625078187114, 'epoch': 0.36}
{'loss': 1.1875, 'grad_norm': 0.9517109394073486, 'learning_rate': 0.0001606735712697885, 'epoch': 0.36}
{'loss': 1.2962, 'grad_norm': 1.0081446170806885, 'learning_rate': 0.00016063087425974016, 'epoch': 0.36}
{'loss': 0.9509, 'grad_norm': 1.439063310623169, 'learning_rate': 0.00016058815976403973, 'epoch': 0.36}
{'loss': 1.0259, 'grad_norm': 1.200011968612671, 'learning_rate': 0.0001605454277950058, 'epoch': 0.36}
{'loss': 0.9611, 'grad_norm': 1.3689687252044678, 'learning_rate': 0.0001605026783649622, 'epoch': 0.36}
{'loss': 0.9233, 'grad_norm': 1.1113722324371338, 'learning_rate': 0.0001604599114862375, 'epoch': 0.36}
{'loss': 1.1268, 'grad_norm': 1.0465985536575317, 'learning_rate': 0.00016041712717116554, 'epoch': 0.36}
{'loss': 0.986, 'grad_norm': 1.072267770767212, 'learning_rate': 0.00016037432543208516, 'epoch': 0.36}
{'loss': 0.9796, 'grad_norm': 1.6088858842849731, 'learning_rate': 0.00016033150628134011, 'epoch': 0.36}
{'loss': 0.7612, 'grad_norm': 1.4661537408828735, 'learning_rate': 0.00016028866973127922, 'epoch': 0.36}
{'loss': 0.8018, 'grad_norm': 1.2312225103378296, 'learning_rate': 0.0001602458157942564, 'epoch': 0.36}
{'loss': 1.2311, 'grad_norm': 1.1248586177825928, 'learning_rate': 0.00016020294448263045, 'epoch': 0.36}
{'loss': 1.0631, 'grad_norm': 1.188460111618042, 'learning_rate': 0.0001601600558087653, 'epoch': 0.37}
{'loss': 0.987, 'grad_norm': 1.3949965238571167, 'learning_rate': 0.0001601171497850298, 'epoch': 0.37}
{'loss': 1.0991, 'grad_norm': 1.1182695627212524, 'learning_rate': 0.0001600742264237979, 'epoch': 0.37}
{'loss': 1.0584, 'grad_norm': 0.9704667329788208, 'learning_rate': 0.00016003128573744845, 'epoch': 0.37}
{'loss': 0.9617, 'grad_norm': 3.2554168701171875, 'learning_rate': 0.00015998832773836532, 'epoch': 0.37}
{'loss': 0.8825, 'grad_norm': 1.1455594301223755, 'learning_rate': 0.0001599453524389374, 'epoch': 0.37}
{'loss': 0.9078, 'grad_norm': 1.0634113550186157, 'learning_rate': 0.0001599023598515586, 'epoch': 0.37}
{'loss': 0.8476, 'grad_norm': 1.120444655418396, 'learning_rate': 0.00015985934998862772, 'epoch': 0.37}
{'loss': 1.0447, 'grad_norm': 1.272222876548767, 'learning_rate': 0.00015981632286254864, 'epoch': 0.37}
{'loss': 1.246, 'grad_norm': 1.4722663164138794, 'learning_rate': 0.00015977327848573015, 'epoch': 0.37}
{'loss': 0.8008, 'grad_norm': 1.2149901390075684, 'learning_rate': 0.00015973021687058604, 'epoch': 0.37}
{'loss': 0.7073, 'grad_norm': 1.1987512111663818, 'learning_rate': 0.0001596871380295351, 'epoch': 0.37}
{'loss': 0.8889, 'grad_norm': 1.2341383695602417, 'learning_rate': 0.00015964404197500102, 'epoch': 0.37}
{'loss': 1.0672, 'grad_norm': 1.3225641250610352, 'learning_rate': 0.00015960092871941255, 'epoch': 0.37}
{'loss': 0.936, 'grad_norm': 1.1565433740615845, 'learning_rate': 0.00015955779827520327, 'epoch': 0.37}
{'loss': 0.7549, 'grad_norm': 1.1260064840316772, 'learning_rate': 0.00015951465065481183, 'epoch': 0.37}
{'loss': 0.5616, 'grad_norm': 1.0419766902923584, 'learning_rate': 0.00015947148587068183, 'epoch': 0.37}
{'loss': 0.8703, 'grad_norm': 1.0717695951461792, 'learning_rate': 0.00015942830393526176, 'epoch': 0.37}
{'loss': 1.0092, 'grad_norm': 1.312180519104004, 'learning_rate': 0.00015938510486100506, 'epoch': 0.37}
{'loss': 1.0747, 'grad_norm': 1.858108639717102, 'learning_rate': 0.00015934188866037016, 'epoch': 0.37}
{'loss': 0.9452, 'grad_norm': 1.1908656358718872, 'learning_rate': 0.00015929865534582038, 'epoch': 0.37}
{'loss': 0.9872, 'grad_norm': 1.0774067640304565, 'learning_rate': 0.00015925540492982404, 'epoch': 0.37}
{'loss': 1.0697, 'grad_norm': 1.2131223678588867, 'learning_rate': 0.0001592121374248543, 'epoch': 0.37}
{'loss': 0.9011, 'grad_norm': 1.6654208898544312, 'learning_rate': 0.00015916885284338937, 'epoch': 0.37}
{'loss': 1.0025, 'grad_norm': 1.3662595748901367, 'learning_rate': 0.00015912555119791222, 'epoch': 0.37}
{'loss': 0.813, 'grad_norm': 1.1913132667541504, 'learning_rate': 0.00015908223250091092, 'epoch': 0.37}
{'loss': 0.9697, 'grad_norm': 1.0582209825515747, 'learning_rate': 0.00015903889676487833, 'epoch': 0.37}
{'loss': 1.0457, 'grad_norm': 0.9958205819129944, 'learning_rate': 0.00015899554400231232, 'epoch': 0.37}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9653, 'grad_norm': 1.212401032447815, 'learning_rate': 0.00015895217422571553, 'epoch': 0.37}
{'loss': 1.1403, 'grad_norm': 1.0816729068756104, 'learning_rate': 0.00015890878744759563, 'epoch': 0.37}
{'loss': 0.977, 'grad_norm': 1.0374979972839355, 'learning_rate': 0.00015886538368046522, 'epoch': 0.37}
{'loss': 1.3069, 'grad_norm': 1.0983985662460327, 'learning_rate': 0.00015882196293684166, 'epoch': 0.37}
{'loss': 0.9683, 'grad_norm': 0.9272429943084717, 'learning_rate': 0.00015877852522924732, 'epoch': 0.37}
{'loss': 0.8683, 'grad_norm': 0.9629650115966797, 'learning_rate': 0.00015873507057020942, 'epoch': 0.37}
{'loss': 0.8216, 'grad_norm': 1.3117848634719849, 'learning_rate': 0.00015869159897226006, 'epoch': 0.37}
{'loss': 0.954, 'grad_norm': 1.8754154443740845, 'learning_rate': 0.00015864811044793625, 'epoch': 0.37}
{'loss': 1.2528, 'grad_norm': 0.9785452485084534, 'learning_rate': 0.0001586046050097799, 'epoch': 0.37}
{'loss': 0.8666, 'grad_norm': 1.3737128973007202, 'learning_rate': 0.00015856108267033769, 'epoch': 0.37}
{'loss': 0.9148, 'grad_norm': 1.2645602226257324, 'learning_rate': 0.00015851754344216133, 'epoch': 0.37}
{'loss': 1.38, 'grad_norm': 1.0916844606399536, 'learning_rate': 0.00015847398733780728, 'epoch': 0.37}
{'loss': 0.5962, 'grad_norm': 1.1446714401245117, 'learning_rate': 0.00015843041436983696, 'epoch': 0.37}
{'loss': 0.7894, 'grad_norm': 1.1735998392105103, 'learning_rate': 0.00015838682455081657, 'epoch': 0.37}
{'loss': 0.9178, 'grad_norm': 1.2368906736373901, 'learning_rate': 0.00015834321789331716, 'epoch': 0.37}
{'loss': 1.1744, 'grad_norm': 0.9532517194747925, 'learning_rate': 0.00015829959440991477, 'epoch': 0.37}
{'loss': 0.9454, 'grad_norm': 1.294504165649414, 'learning_rate': 0.00015825595411319014, 'epoch': 0.37}
{'loss': 0.9082, 'grad_norm': 1.4747607707977295, 'learning_rate': 0.00015821229701572896, 'epoch': 0.37}
{'loss': 0.7661, 'grad_norm': 1.2885966300964355, 'learning_rate': 0.00015816862313012167, 'epoch': 0.37}
{'loss': 1.0393, 'grad_norm': 1.0767722129821777, 'learning_rate': 0.00015812493246896366, 'epoch': 0.37}
{'loss': 0.9062, 'grad_norm': 0.9856582880020142, 'learning_rate': 0.0001580812250448551, 'epoch': 0.37}
{'loss': 0.9878, 'grad_norm': 1.2368375062942505, 'learning_rate': 0.00015803750087040099, 'epoch': 0.37}
{'loss': 1.0233, 'grad_norm': 1.1402608156204224, 'learning_rate': 0.00015799375995821118, 'epoch': 0.37}
{'loss': 1.1538, 'grad_norm': 1.2032780647277832, 'learning_rate': 0.00015795000232090033, 'epoch': 0.37}
{'loss': 0.7575, 'grad_norm': 1.251457691192627, 'learning_rate': 0.0001579062279710879, 'epoch': 0.37}
{'loss': 1.0068, 'grad_norm': 1.7815874814987183, 'learning_rate': 0.00015786243692139827, 'epoch': 0.37}
{'loss': 1.1865, 'grad_norm': 1.0794646739959717, 'learning_rate': 0.0001578186291844605, 'epoch': 0.37}
{'loss': 0.9475, 'grad_norm': 1.2824493646621704, 'learning_rate': 0.0001577748047729086, 'epoch': 0.37}
{'loss': 0.9034, 'grad_norm': 1.2935577630996704, 'learning_rate': 0.00015773096369938125, 'epoch': 0.37}
{'loss': 1.1553, 'grad_norm': 1.0321072340011597, 'learning_rate': 0.00015768710597652204, 'epoch': 0.37}
{'loss': 1.0561, 'grad_norm': 1.3808417320251465, 'learning_rate': 0.00015764323161697935, 'epoch': 0.37}
{'loss': 1.0779, 'grad_norm': 1.2421869039535522, 'learning_rate': 0.00015759934063340627, 'epoch': 0.37}
{'loss': 1.105, 'grad_norm': 1.095123291015625, 'learning_rate': 0.0001575554330384608, 'epoch': 0.37}
{'loss': 0.9448, 'grad_norm': 1.2516047954559326, 'learning_rate': 0.0001575115088448057, 'epoch': 0.37}
{'loss': 0.8216, 'grad_norm': 1.812723994255066, 'learning_rate': 0.00015746756806510838, 'epoch': 0.37}
{'loss': 1.1637, 'grad_norm': 1.2381983995437622, 'learning_rate': 0.00015742361071204128, 'epoch': 0.37}
{'loss': 1.0161, 'grad_norm': 1.590900182723999, 'learning_rate': 0.00015737963679828142, 'epoch': 0.37}
{'loss': 0.6988, 'grad_norm': 1.7460519075393677, 'learning_rate': 0.0001573356463365107, 'epoch': 0.38}
{'loss': 1.0654, 'grad_norm': 1.3244205713272095, 'learning_rate': 0.00015729163933941574, 'epoch': 0.38}
{'loss': 1.0659, 'grad_norm': 1.138054370880127, 'learning_rate': 0.00015724761581968792, 'epoch': 0.38}
{'loss': 0.9641, 'grad_norm': 0.9452161192893982, 'learning_rate': 0.00015720357579002345, 'epoch': 0.38}
{'loss': 0.8889, 'grad_norm': 1.846325159072876, 'learning_rate': 0.00015715951926312325, 'epoch': 0.38}
{'loss': 1.0671, 'grad_norm': 1.3464958667755127, 'learning_rate': 0.000157115446251693, 'epoch': 0.38}
{'loss': 1.0826, 'grad_norm': 1.104302167892456, 'learning_rate': 0.0001570713567684432, 'epoch': 0.38}
{'loss': 1.0614, 'grad_norm': 1.1057164669036865, 'learning_rate': 0.00015702725082608894, 'epoch': 0.38}
{'loss': 1.3071, 'grad_norm': 1.3758738040924072, 'learning_rate': 0.00015698312843735024, 'epoch': 0.38}
{'loss': 1.0821, 'grad_norm': 1.3272075653076172, 'learning_rate': 0.00015693898961495177, 'epoch': 0.38}
{'loss': 1.3217, 'grad_norm': 0.9756810069084167, 'learning_rate': 0.00015689483437162293, 'epoch': 0.38}
{'loss': 1.2421, 'grad_norm': 1.3367899656295776, 'learning_rate': 0.00015685066272009792, 'epoch': 0.38}
{'loss': 1.0695, 'grad_norm': 1.8324127197265625, 'learning_rate': 0.00015680647467311557, 'epoch': 0.38}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0958, 'grad_norm': 1.4593348503112793, 'learning_rate': 0.0001567622702434196, 'epoch': 0.38}
{'loss': 0.9097, 'grad_norm': 1.361284613609314, 'learning_rate': 0.00015671804944375825, 'epoch': 0.38}
{'loss': 0.9951, 'grad_norm': 1.0803197622299194, 'learning_rate': 0.00015667381228688461, 'epoch': 0.38}
{'loss': 0.97, 'grad_norm': 1.0053240060806274, 'learning_rate': 0.00015662955878555652, 'epoch': 0.38}
{'loss': 0.8969, 'grad_norm': 1.1880663633346558, 'learning_rate': 0.00015658528895253642, 'epoch': 0.38}
{'loss': 0.9391, 'grad_norm': 1.3487643003463745, 'learning_rate': 0.00015654100280059155, 'epoch': 0.38}
{'loss': 1.0767, 'grad_norm': 1.189090609550476, 'learning_rate': 0.0001564967003424938, 'epoch': 0.38}
{'loss': 1.0161, 'grad_norm': 1.1798255443572998, 'learning_rate': 0.0001564523815910198, 'epoch': 0.38}
{'loss': 1.1779, 'grad_norm': 1.1521227359771729, 'learning_rate': 0.00015640804655895084, 'epoch': 0.38}
{'loss': 0.9766, 'grad_norm': 1.0474669933319092, 'learning_rate': 0.00015636369525907296, 'epoch': 0.38}
{'loss': 0.8789, 'grad_norm': 1.0714739561080933, 'learning_rate': 0.00015631932770417684, 'epoch': 0.38}
{'loss': 0.7697, 'grad_norm': 1.1397818326950073, 'learning_rate': 0.0001562749439070579, 'epoch': 0.38}
{'loss': 1.0836, 'grad_norm': 1.3117495775222778, 'learning_rate': 0.00015623054388051616, 'epoch': 0.38}
{'loss': 1.0422, 'grad_norm': 1.1583218574523926, 'learning_rate': 0.00015618612763735644, 'epoch': 0.38}
{'loss': 0.9172, 'grad_norm': 1.1571418046951294, 'learning_rate': 0.0001561416951903881, 'epoch': 0.38}
{'loss': 0.782, 'grad_norm': 1.2485920190811157, 'learning_rate': 0.0001560972465524253, 'epoch': 0.38}
{'loss': 1.1377, 'grad_norm': 1.5873069763183594, 'learning_rate': 0.0001560527817362868, 'epoch': 0.38}
{'loss': 1.0933, 'grad_norm': 1.2783284187316895, 'learning_rate': 0.00015600830075479603, 'epoch': 0.38}
{'loss': 1.0773, 'grad_norm': 1.8476274013519287, 'learning_rate': 0.00015596380362078107, 'epoch': 0.38}
{'loss': 0.9731, 'grad_norm': 1.0977394580841064, 'learning_rate': 0.0001559192903470747, 'epoch': 0.38}
{'loss': 1.0076, 'grad_norm': 1.410811424255371, 'learning_rate': 0.00015587476094651434, 'epoch': 0.38}
{'loss': 1.1468, 'grad_norm': 1.0406708717346191, 'learning_rate': 0.00015583021543194206, 'epoch': 0.38}
{'loss': 1.3048, 'grad_norm': 1.0419358015060425, 'learning_rate': 0.00015578565381620455, 'epoch': 0.38}
{'loss': 0.8451, 'grad_norm': 1.325322151184082, 'learning_rate': 0.00015574107611215319, 'epoch': 0.38}
{'loss': 0.8703, 'grad_norm': 0.8116989135742188, 'learning_rate': 0.00015569648233264394, 'epoch': 0.38}
{'loss': 0.7832, 'grad_norm': 1.1987696886062622, 'learning_rate': 0.00015565187249053748, 'epoch': 0.38}
{'loss': 1.3427, 'grad_norm': 1.3799948692321777, 'learning_rate': 0.00015560724659869902, 'epoch': 0.38}
{'loss': 0.6631, 'grad_norm': 1.1746453046798706, 'learning_rate': 0.0001555626046699985, 'epoch': 0.38}
{'loss': 0.7924, 'grad_norm': 1.3657089471817017, 'learning_rate': 0.0001555179467173104, 'epoch': 0.38}
{'loss': 0.8377, 'grad_norm': 1.2689087390899658, 'learning_rate': 0.0001554732727535139, 'epoch': 0.38}
{'loss': 0.9576, 'grad_norm': 1.168026328086853, 'learning_rate': 0.0001554285827914927, 'epoch': 0.38}
{'loss': 1.0476, 'grad_norm': 1.0891289710998535, 'learning_rate': 0.00015538387684413524, 'epoch': 0.38}
{'loss': 0.8176, 'grad_norm': 1.0838967561721802, 'learning_rate': 0.00015533915492433443, 'epoch': 0.38}
{'loss': 1.2214, 'grad_norm': 1.1195341348648071, 'learning_rate': 0.0001552944170449879, 'epoch': 0.38}
{'loss': 1.1682, 'grad_norm': 1.0685423612594604, 'learning_rate': 0.00015524966321899778, 'epoch': 0.38}
{'loss': 1.0478, 'grad_norm': 1.2318707704544067, 'learning_rate': 0.00015520489345927096, 'epoch': 0.38}
{'loss': 1.011, 'grad_norm': 1.025228500366211, 'learning_rate': 0.00015516010777871876, 'epoch': 0.38}
{'loss': 0.8057, 'grad_norm': 1.2562776803970337, 'learning_rate': 0.00015511530619025714, 'epoch': 0.38}
{'loss': 0.9381, 'grad_norm': 2.4864320755004883, 'learning_rate': 0.00015507048870680668, 'epoch': 0.38}
{'loss': 1.0103, 'grad_norm': 1.441139578819275, 'learning_rate': 0.00015502565534129253, 'epoch': 0.38}
{'loss': 0.9521, 'grad_norm': 1.0693912506103516, 'learning_rate': 0.00015498080610664444, 'epoch': 0.38}
{'loss': 0.6078, 'grad_norm': 1.2862406969070435, 'learning_rate': 0.00015493594101579668, 'epoch': 0.38}
{'loss': 0.792, 'grad_norm': 1.4296988248825073, 'learning_rate': 0.0001548910600816881, 'epoch': 0.38}
{'loss': 1.0704, 'grad_norm': 1.4026226997375488, 'learning_rate': 0.00015484616331726224, 'epoch': 0.38}
{'loss': 1.0525, 'grad_norm': 1.0695255994796753, 'learning_rate': 0.00015480125073546704, 'epoch': 0.38}
{'loss': 0.9941, 'grad_norm': 1.1537935733795166, 'learning_rate': 0.00015475632234925504, 'epoch': 0.38}
{'loss': 0.7377, 'grad_norm': 1.8409229516983032, 'learning_rate': 0.00015471137817158343, 'epoch': 0.38}
{'loss': 1.0028, 'grad_norm': 1.0571765899658203, 'learning_rate': 0.00015466641821541388, 'epoch': 0.38}
{'loss': 1.2486, 'grad_norm': 1.083929181098938, 'learning_rate': 0.00015462144249371265, 'epoch': 0.38}
{'loss': 1.2756, 'grad_norm': 2.126100540161133, 'learning_rate': 0.00015457645101945046, 'epoch': 0.38}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6828, 'grad_norm': 1.3435224294662476, 'learning_rate': 0.0001545314438056027, 'epoch': 0.38}
{'loss': 0.9597, 'grad_norm': 1.070417046546936, 'learning_rate': 0.00015448642086514917, 'epoch': 0.38}
{'loss': 0.7034, 'grad_norm': 1.1300925016403198, 'learning_rate': 0.00015444138221107432, 'epoch': 0.39}
{'loss': 1.2317, 'grad_norm': 1.6046828031539917, 'learning_rate': 0.00015439632785636706, 'epoch': 0.39}
{'loss': 0.8878, 'grad_norm': 1.1996643543243408, 'learning_rate': 0.00015435125781402086, 'epoch': 0.39}
{'loss': 1.2037, 'grad_norm': 1.0484974384307861, 'learning_rate': 0.0001543061720970337, 'epoch': 0.39}
{'loss': 0.8268, 'grad_norm': 1.2856987714767456, 'learning_rate': 0.00015426107071840806, 'epoch': 0.39}
{'loss': 0.9778, 'grad_norm': 1.3760464191436768, 'learning_rate': 0.00015421595369115098, 'epoch': 0.39}
{'loss': 0.8761, 'grad_norm': 0.8726420998573303, 'learning_rate': 0.000154170821028274, 'epoch': 0.39}
{'loss': 0.9736, 'grad_norm': 1.0059328079223633, 'learning_rate': 0.00015412567274279316, 'epoch': 0.39}
{'loss': 1.0214, 'grad_norm': 1.1776937246322632, 'learning_rate': 0.00015408050884772897, 'epoch': 0.39}
{'loss': 0.9478, 'grad_norm': 1.1281166076660156, 'learning_rate': 0.00015403532935610653, 'epoch': 0.39}
{'loss': 1.0106, 'grad_norm': 1.1373357772827148, 'learning_rate': 0.0001539901342809554, 'epoch': 0.39}
{'loss': 1.0363, 'grad_norm': 1.442082405090332, 'learning_rate': 0.00015394492363530958, 'epoch': 0.39}
{'loss': 0.9987, 'grad_norm': 1.2296180725097656, 'learning_rate': 0.0001538996974322076, 'epoch': 0.39}
{'loss': 1.1056, 'grad_norm': 1.4783127307891846, 'learning_rate': 0.0001538544556846925, 'epoch': 0.39}
{'loss': 0.8321, 'grad_norm': 1.3638386726379395, 'learning_rate': 0.00015380919840581175, 'epoch': 0.39}
{'loss': 0.717, 'grad_norm': 1.0981807708740234, 'learning_rate': 0.0001537639256086174, 'epoch': 0.39}
{'loss': 0.9862, 'grad_norm': 1.2493683099746704, 'learning_rate': 0.00015371863730616586, 'epoch': 0.39}
{'loss': 1.1283, 'grad_norm': 1.491303563117981, 'learning_rate': 0.00015367333351151802, 'epoch': 0.39}
{'loss': 0.7477, 'grad_norm': 1.1753425598144531, 'learning_rate': 0.00015362801423773932, 'epoch': 0.39}
{'loss': 0.8153, 'grad_norm': 1.1520395278930664, 'learning_rate': 0.00015358267949789966, 'epoch': 0.39}
{'loss': 0.9415, 'grad_norm': 1.2346282005310059, 'learning_rate': 0.0001535373293050733, 'epoch': 0.39}
{'loss': 0.679, 'grad_norm': 1.2372065782546997, 'learning_rate': 0.00015349196367233904, 'epoch': 0.39}
{'loss': 1.0227, 'grad_norm': 1.1997543573379517, 'learning_rate': 0.0001534465826127801, 'epoch': 0.39}
{'loss': 0.724, 'grad_norm': 1.4085584878921509, 'learning_rate': 0.00015340118613948417, 'epoch': 0.39}
{'loss': 1.0328, 'grad_norm': 1.1920268535614014, 'learning_rate': 0.0001533557742655434, 'epoch': 0.39}
{'loss': 1.1043, 'grad_norm': 1.7319415807724, 'learning_rate': 0.00015331034700405432, 'epoch': 0.39}
{'loss': 1.1957, 'grad_norm': 1.4264626502990723, 'learning_rate': 0.0001532649043681179, 'epoch': 0.39}
{'loss': 0.8947, 'grad_norm': 1.1346896886825562, 'learning_rate': 0.00015321944637083963, 'epoch': 0.39}
{'loss': 1.126, 'grad_norm': 1.0527619123458862, 'learning_rate': 0.00015317397302532936, 'epoch': 0.39}
{'loss': 1.1349, 'grad_norm': 1.2124158143997192, 'learning_rate': 0.00015312848434470138, 'epoch': 0.39}
{'loss': 1.1557, 'grad_norm': 1.1010358333587646, 'learning_rate': 0.00015308298034207441, 'epoch': 0.39}
{'loss': 0.9795, 'grad_norm': 1.0342645645141602, 'learning_rate': 0.00015303746103057162, 'epoch': 0.39}
{'loss': 0.9129, 'grad_norm': 1.24017333984375, 'learning_rate': 0.0001529919264233205, 'epoch': 0.39}
{'loss': 1.0495, 'grad_norm': 1.06394624710083, 'learning_rate': 0.00015294637653345307, 'epoch': 0.39}
{'loss': 1.119, 'grad_norm': 1.00811767578125, 'learning_rate': 0.00015290081137410563, 'epoch': 0.39}
{'loss': 0.9057, 'grad_norm': 1.4001810550689697, 'learning_rate': 0.00015285523095841903, 'epoch': 0.39}
{'loss': 0.755, 'grad_norm': 1.1728602647781372, 'learning_rate': 0.0001528096352995384, 'epoch': 0.39}
{'loss': 0.9175, 'grad_norm': 1.2288728952407837, 'learning_rate': 0.0001527640244106133, 'epoch': 0.39}
{'loss': 1.2277, 'grad_norm': 1.0798532962799072, 'learning_rate': 0.00015271839830479768, 'epoch': 0.39}
{'loss': 1.0597, 'grad_norm': 1.2126226425170898, 'learning_rate': 0.00015267275699524992, 'epoch': 0.39}
{'loss': 0.7201, 'grad_norm': 1.1412585973739624, 'learning_rate': 0.0001526271004951328, 'epoch': 0.39}
{'loss': 0.9155, 'grad_norm': 1.0785285234451294, 'learning_rate': 0.00015258142881761333, 'epoch': 0.39}
{'loss': 1.2786, 'grad_norm': 1.247642993927002, 'learning_rate': 0.00015253574197586308, 'epoch': 0.39}
{'loss': 0.8785, 'grad_norm': 0.9795162677764893, 'learning_rate': 0.00015249003998305785, 'epoch': 0.39}
{'loss': 0.9465, 'grad_norm': 1.0540810823440552, 'learning_rate': 0.00015244432285237795, 'epoch': 0.39}
{'loss': 1.2072, 'grad_norm': 1.1573777198791504, 'learning_rate': 0.00015239859059700794, 'epoch': 0.39}
{'loss': 0.6877, 'grad_norm': 1.0529323816299438, 'learning_rate': 0.00015235284323013675, 'epoch': 0.39}
{'loss': 1.0944, 'grad_norm': 1.2917479276657104, 'learning_rate': 0.00015230708076495775, 'epoch': 0.39}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8043, 'grad_norm': 1.3019239902496338, 'learning_rate': 0.00015226130321466865, 'epoch': 0.39}
{'loss': 1.2075, 'grad_norm': 1.178586483001709, 'learning_rate': 0.00015221551059247136, 'epoch': 0.39}
{'loss': 1.2114, 'grad_norm': 0.9850558042526245, 'learning_rate': 0.00015216970291157233, 'epoch': 0.39}
{'loss': 0.9911, 'grad_norm': 1.1087127923965454, 'learning_rate': 0.00015212388018518227, 'epoch': 0.39}
{'loss': 0.9865, 'grad_norm': 1.1233631372451782, 'learning_rate': 0.00015207804242651626, 'epoch': 0.39}
{'loss': 0.7718, 'grad_norm': 1.3456497192382812, 'learning_rate': 0.0001520321896487936, 'epoch': 0.39}
{'loss': 0.7066, 'grad_norm': 1.0055103302001953, 'learning_rate': 0.00015198632186523808, 'epoch': 0.39}
{'loss': 1.0116, 'grad_norm': 1.2327227592468262, 'learning_rate': 0.00015194043908907775, 'epoch': 0.39}
{'loss': 0.9712, 'grad_norm': 1.099401593208313, 'learning_rate': 0.00015189454133354493, 'epoch': 0.39}
{'loss': 1.0514, 'grad_norm': 1.3640613555908203, 'learning_rate': 0.00015184862861187637, 'epoch': 0.39}
{'loss': 1.0402, 'grad_norm': 1.0726184844970703, 'learning_rate': 0.00015180270093731303, 'epoch': 0.39}
{'loss': 0.7461, 'grad_norm': 1.250011920928955, 'learning_rate': 0.00015175675832310027, 'epoch': 0.39}
{'loss': 0.88, 'grad_norm': 1.5540070533752441, 'learning_rate': 0.0001517108007824877, 'epoch': 0.39}
{'loss': 0.8198, 'grad_norm': 1.3783506155014038, 'learning_rate': 0.00015166482832872923, 'epoch': 0.39}
{'loss': 1.1787, 'grad_norm': 1.162194848060608, 'learning_rate': 0.00015161884097508318, 'epoch': 0.39}
{'loss': 0.8909, 'grad_norm': 1.011314034461975, 'learning_rate': 0.00015157283873481195, 'epoch': 0.39}
{'loss': 1.0878, 'grad_norm': 1.2738796472549438, 'learning_rate': 0.0001515268216211825, 'epoch': 0.39}
{'loss': 1.2247, 'grad_norm': 1.1531338691711426, 'learning_rate': 0.0001514807896474658, 'epoch': 0.4}
{'loss': 0.832, 'grad_norm': 1.531446099281311, 'learning_rate': 0.0001514347428269374, 'epoch': 0.4}
{'loss': 0.7766, 'grad_norm': 1.108512043952942, 'learning_rate': 0.0001513886811728769, 'epoch': 0.4}
{'loss': 1.0107, 'grad_norm': 1.2150914669036865, 'learning_rate': 0.00015134260469856824, 'epoch': 0.4}
{'loss': 0.9599, 'grad_norm': 0.9679829478263855, 'learning_rate': 0.00015129651341729968, 'epoch': 0.4}
{'loss': 0.8243, 'grad_norm': 1.118577003479004, 'learning_rate': 0.00015125040734236374, 'epoch': 0.4}
{'loss': 0.9509, 'grad_norm': 0.9639583826065063, 'learning_rate': 0.00015120428648705717, 'epoch': 0.4}
{'loss': 0.8565, 'grad_norm': 1.0880695581436157, 'learning_rate': 0.00015115815086468102, 'epoch': 0.4}
{'loss': 0.7507, 'grad_norm': 0.9534202814102173, 'learning_rate': 0.00015111200048854056, 'epoch': 0.4}
{'loss': 1.0549, 'grad_norm': 1.1417597532272339, 'learning_rate': 0.00015106583537194532, 'epoch': 0.4}
{'loss': 1.2436, 'grad_norm': 1.1745713949203491, 'learning_rate': 0.00015101965552820915, 'epoch': 0.4}
{'loss': 0.7643, 'grad_norm': 1.3194726705551147, 'learning_rate': 0.00015097346097065007, 'epoch': 0.4}
{'loss': 1.3596, 'grad_norm': 1.3250641822814941, 'learning_rate': 0.00015092725171259036, 'epoch': 0.4}
{'loss': 1.032, 'grad_norm': 1.0413975715637207, 'learning_rate': 0.00015088102776735656, 'epoch': 0.4}
{'loss': 1.1989, 'grad_norm': 1.3596010208129883, 'learning_rate': 0.0001508347891482794, 'epoch': 0.4}
{'loss': 0.7203, 'grad_norm': 1.0409493446350098, 'learning_rate': 0.00015078853586869393, 'epoch': 0.4}
{'loss': 1.1235, 'grad_norm': 1.2330940961837769, 'learning_rate': 0.00015074226794193932, 'epoch': 0.4}
{'loss': 1.2684, 'grad_norm': 0.9475038647651672, 'learning_rate': 0.00015069598538135906, 'epoch': 0.4}
{'loss': 0.9528, 'grad_norm': 1.3651189804077148, 'learning_rate': 0.0001506496882003008, 'epoch': 0.4}
{'loss': 0.9719, 'grad_norm': 1.2675553560256958, 'learning_rate': 0.00015060337641211637, 'epoch': 0.4}
{'loss': 1.2694, 'grad_norm': 1.2383610010147095, 'learning_rate': 0.00015055705003016198, 'epoch': 0.4}
{'loss': 0.9698, 'grad_norm': 1.207236647605896, 'learning_rate': 0.00015051070906779788, 'epoch': 0.4}
{'loss': 1.0637, 'grad_norm': 1.416835904121399, 'learning_rate': 0.00015046435353838853, 'epoch': 0.4}
{'loss': 0.9827, 'grad_norm': 1.1878477334976196, 'learning_rate': 0.0001504179834553027, 'epoch': 0.4}
{'loss': 0.9465, 'grad_norm': 1.3540928363800049, 'learning_rate': 0.00015037159883191328, 'epoch': 0.4}
{'loss': 0.8059, 'grad_norm': 1.5886468887329102, 'learning_rate': 0.00015032519968159738, 'epoch': 0.4}
{'loss': 1.0077, 'grad_norm': 1.3078815937042236, 'learning_rate': 0.00015027878601773633, 'epoch': 0.4}
{'loss': 0.9262, 'grad_norm': 1.2263695001602173, 'learning_rate': 0.00015023235785371553, 'epoch': 0.4}
{'loss': 1.0201, 'grad_norm': 1.138297438621521, 'learning_rate': 0.00015018591520292468, 'epoch': 0.4}
{'loss': 0.988, 'grad_norm': 1.203003168106079, 'learning_rate': 0.00015013945807875758, 'epoch': 0.4}
{'loss': 0.9283, 'grad_norm': 1.1512916088104248, 'learning_rate': 0.0001500929864946123, 'epoch': 0.4}
{'loss': 0.8986, 'grad_norm': 1.3689916133880615, 'learning_rate': 0.00015004650046389098, 'epoch': 0.4}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9461, 'grad_norm': 1.2712544202804565, 'learning_rate': 0.00015000000000000001, 'epoch': 0.4}
{'loss': 0.8874, 'grad_norm': 1.3326736688613892, 'learning_rate': 0.00014995348511634985, 'epoch': 0.4}
{'loss': 1.1392, 'grad_norm': 0.9647597670555115, 'learning_rate': 0.00014990695582635518, 'epoch': 0.4}
{'loss': 1.1313, 'grad_norm': 1.5914252996444702, 'learning_rate': 0.00014986041214343486, 'epoch': 0.4}
{'loss': 0.5982, 'grad_norm': 0.8989751935005188, 'learning_rate': 0.0001498138540810118, 'epoch': 0.4}
{'loss': 0.7815, 'grad_norm': 1.172212839126587, 'learning_rate': 0.00014976728165251317, 'epoch': 0.4}
{'loss': 1.0321, 'grad_norm': 1.2672346830368042, 'learning_rate': 0.00014972069487137024, 'epoch': 0.4}
{'loss': 0.9899, 'grad_norm': 1.1478341817855835, 'learning_rate': 0.00014967409375101837, 'epoch': 0.4}
{'loss': 0.8683, 'grad_norm': 1.12222421169281, 'learning_rate': 0.00014962747830489712, 'epoch': 0.4}
{'loss': 0.7016, 'grad_norm': 1.1080739498138428, 'learning_rate': 0.0001495808485464502, 'epoch': 0.4}
{'loss': 1.104, 'grad_norm': 1.9224034547805786, 'learning_rate': 0.00014953420448912533, 'epoch': 0.4}
{'loss': 0.8318, 'grad_norm': 1.0316563844680786, 'learning_rate': 0.00014948754614637448, 'epoch': 0.4}
{'loss': 0.7928, 'grad_norm': 1.2191908359527588, 'learning_rate': 0.0001494408735316537, 'epoch': 0.4}
{'loss': 1.1197, 'grad_norm': 0.8894824385643005, 'learning_rate': 0.0001493941866584231, 'epoch': 0.4}
{'loss': 0.9785, 'grad_norm': 1.3734443187713623, 'learning_rate': 0.00014934748554014702, 'epoch': 0.4}
{'loss': 0.7859, 'grad_norm': 1.3803049325942993, 'learning_rate': 0.00014930077019029375, 'epoch': 0.4}
{'loss': 0.838, 'grad_norm': 1.427182912826538, 'learning_rate': 0.00014925404062233585, 'epoch': 0.4}
{'loss': 1.0588, 'grad_norm': 1.3634350299835205, 'learning_rate': 0.00014920729684974986, 'epoch': 0.4}
{'loss': 0.9418, 'grad_norm': 1.0822175741195679, 'learning_rate': 0.0001491605388860165, 'epoch': 0.4}
{'loss': 0.8268, 'grad_norm': 1.167699933052063, 'learning_rate': 0.00014911376674462047, 'epoch': 0.4}
{'loss': 0.8801, 'grad_norm': 1.0415539741516113, 'learning_rate': 0.00014906698043905066, 'epoch': 0.4}
{'loss': 1.1169, 'grad_norm': 2.0339725017547607, 'learning_rate': 0.0001490201799828001, 'epoch': 0.4}
{'loss': 1.2567, 'grad_norm': 1.0161738395690918, 'learning_rate': 0.0001489733653893657, 'epoch': 0.4}
{'loss': 0.9165, 'grad_norm': 0.9668237566947937, 'learning_rate': 0.0001489265366722486, 'epoch': 0.4}
{'loss': 1.4645, 'grad_norm': 0.96357262134552, 'learning_rate': 0.00014887969384495402, 'epoch': 0.4}
{'loss': 0.5772, 'grad_norm': 1.0957870483398438, 'learning_rate': 0.00014883283692099112, 'epoch': 0.4}
{'loss': 0.848, 'grad_norm': 1.1096395254135132, 'learning_rate': 0.0001487859659138733, 'epoch': 0.4}
{'loss': 0.994, 'grad_norm': 0.9370837807655334, 'learning_rate': 0.00014873908083711784, 'epoch': 0.4}
{'loss': 1.074, 'grad_norm': 1.0630558729171753, 'learning_rate': 0.00014869218170424626, 'epoch': 0.4}
{'loss': 0.927, 'grad_norm': 1.2013838291168213, 'learning_rate': 0.00014864526852878402, 'epoch': 0.4}
{'loss': 0.723, 'grad_norm': 1.3393046855926514, 'learning_rate': 0.0001485983413242606, 'epoch': 0.4}
{'loss': 1.2834, 'grad_norm': 1.1128654479980469, 'learning_rate': 0.00014855140010420966, 'epoch': 0.4}
{'loss': 1.1676, 'grad_norm': 1.5709500312805176, 'learning_rate': 0.00014850444488216877, 'epoch': 0.4}
{'loss': 0.9452, 'grad_norm': 1.2366148233413696, 'learning_rate': 0.00014845747567167962, 'epoch': 0.41}
{'loss': 0.9578, 'grad_norm': 1.3846359252929688, 'learning_rate': 0.00014841049248628783, 'epoch': 0.41}
{'loss': 0.8786, 'grad_norm': 1.114261507987976, 'learning_rate': 0.00014836349533954324, 'epoch': 0.41}
{'loss': 0.9028, 'grad_norm': 1.382102131843567, 'learning_rate': 0.00014831648424499952, 'epoch': 0.41}
{'loss': 0.7306, 'grad_norm': 1.137660264968872, 'learning_rate': 0.00014826945921621448, 'epoch': 0.41}
{'loss': 1.0562, 'grad_norm': 1.160616397857666, 'learning_rate': 0.00014822242026674989, 'epoch': 0.41}
{'loss': 1.1557, 'grad_norm': 1.0904532670974731, 'learning_rate': 0.00014817536741017152, 'epoch': 0.41}
{'loss': 1.138, 'grad_norm': 1.4024193286895752, 'learning_rate': 0.00014812830066004927, 'epoch': 0.41}
{'loss': 0.6012, 'grad_norm': 1.3807181119918823, 'learning_rate': 0.00014808122002995692, 'epoch': 0.41}
{'loss': 0.7842, 'grad_norm': 1.0506222248077393, 'learning_rate': 0.0001480341255334723, 'epoch': 0.41}
{'loss': 0.9485, 'grad_norm': 1.3181887865066528, 'learning_rate': 0.00014798701718417726, 'epoch': 0.41}
{'loss': 1.2493, 'grad_norm': 1.0233122110366821, 'learning_rate': 0.00014793989499565756, 'epoch': 0.41}
{'loss': 0.943, 'grad_norm': 1.3249404430389404, 'learning_rate': 0.00014789275898150308, 'epoch': 0.41}
{'loss': 0.9687, 'grad_norm': 1.062882900238037, 'learning_rate': 0.0001478456091553076, 'epoch': 0.41}
{'loss': 0.7128, 'grad_norm': 1.4510380029678345, 'learning_rate': 0.00014779844553066887, 'epoch': 0.41}
{'loss': 1.0214, 'grad_norm': 1.4276190996170044, 'learning_rate': 0.00014775126812118864, 'epoch': 0.41}
{'loss': 1.1504, 'grad_norm': 1.0731855630874634, 'learning_rate': 0.00014770407694047273, 'epoch': 0.41}
{'loss': 0.8551, 'grad_norm': 1.0497353076934814, 'learning_rate': 0.0001476568720021308, 'epoch': 0.41}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0068, 'grad_norm': 1.1782591342926025, 'learning_rate': 0.00014760965331977652, 'epoch': 0.41}
{'loss': 1.1233, 'grad_norm': 1.2668371200561523, 'learning_rate': 0.00014756242090702756, 'epoch': 0.41}
{'loss': 1.061, 'grad_norm': 1.5692436695098877, 'learning_rate': 0.00014751517477750543, 'epoch': 0.41}
{'loss': 0.9079, 'grad_norm': 1.2130109071731567, 'learning_rate': 0.00014746791494483583, 'epoch': 0.41}
{'loss': 0.8444, 'grad_norm': 1.4301248788833618, 'learning_rate': 0.00014742064142264817, 'epoch': 0.41}
{'loss': 1.1607, 'grad_norm': 1.30422043800354, 'learning_rate': 0.00014737335422457593, 'epoch': 0.41}
{'loss': 1.2269, 'grad_norm': 1.7024853229522705, 'learning_rate': 0.00014732605336425651, 'epoch': 0.41}
{'loss': 0.9288, 'grad_norm': 1.4351530075073242, 'learning_rate': 0.00014727873885533123, 'epoch': 0.41}
{'loss': 0.8382, 'grad_norm': 1.281205415725708, 'learning_rate': 0.0001472314107114454, 'epoch': 0.41}
{'loss': 0.7664, 'grad_norm': 1.2866599559783936, 'learning_rate': 0.0001471840689462482, 'epoch': 0.41}
{'loss': 1.2275, 'grad_norm': 0.9323011636734009, 'learning_rate': 0.00014713671357339283, 'epoch': 0.41}
{'loss': 1.2134, 'grad_norm': 1.3600314855575562, 'learning_rate': 0.0001470893446065363, 'epoch': 0.41}
{'loss': 0.8587, 'grad_norm': 1.2347170114517212, 'learning_rate': 0.00014704196205933955, 'epoch': 0.41}
{'loss': 1.2018, 'grad_norm': 1.4666167497634888, 'learning_rate': 0.00014699456594546756, 'epoch': 0.41}
{'loss': 1.0893, 'grad_norm': 0.947353184223175, 'learning_rate': 0.00014694715627858908, 'epoch': 0.41}
{'loss': 1.0089, 'grad_norm': 1.1218878030776978, 'learning_rate': 0.00014689973307237687, 'epoch': 0.41}
{'loss': 0.9948, 'grad_norm': 1.0011500120162964, 'learning_rate': 0.00014685229634050755, 'epoch': 0.41}
{'loss': 1.0585, 'grad_norm': 1.1295453310012817, 'learning_rate': 0.00014680484609666162, 'epoch': 0.41}
{'loss': 1.0491, 'grad_norm': 1.298147201538086, 'learning_rate': 0.00014675738235452352, 'epoch': 0.41}
{'loss': 1.5086, 'grad_norm': 2.6175055503845215, 'learning_rate': 0.00014670990512778156, 'epoch': 0.41}
{'loss': 0.8169, 'grad_norm': 1.284923791885376, 'learning_rate': 0.00014666241443012792, 'epoch': 0.41}
{'loss': 0.9146, 'grad_norm': 1.2918633222579956, 'learning_rate': 0.00014661491027525872, 'epoch': 0.41}
{'loss': 0.9527, 'grad_norm': 1.2479726076126099, 'learning_rate': 0.00014656739267687396, 'epoch': 0.41}
{'loss': 1.0531, 'grad_norm': 0.9610816836357117, 'learning_rate': 0.00014651986164867744, 'epoch': 0.41}
{'loss': 1.0363, 'grad_norm': 1.2963275909423828, 'learning_rate': 0.00014647231720437686, 'epoch': 0.41}
{'loss': 1.2506, 'grad_norm': 1.1279782056808472, 'learning_rate': 0.00014642475935768386, 'epoch': 0.41}
{'loss': 1.0687, 'grad_norm': 1.7165412902832031, 'learning_rate': 0.00014637718812231384, 'epoch': 0.41}
{'loss': 0.6689, 'grad_norm': 2.792490005493164, 'learning_rate': 0.00014632960351198618, 'epoch': 0.41}
{'loss': 0.8952, 'grad_norm': 1.126249074935913, 'learning_rate': 0.00014628200554042402, 'epoch': 0.41}
{'loss': 1.0834, 'grad_norm': 0.9927439093589783, 'learning_rate': 0.00014623439422135434, 'epoch': 0.41}
{'loss': 1.1913, 'grad_norm': 1.0210522413253784, 'learning_rate': 0.0001461867695685081, 'epoch': 0.41}
{'loss': 0.926, 'grad_norm': 1.321481466293335, 'learning_rate': 0.00014613913159561997, 'epoch': 0.41}
{'loss': 1.2765, 'grad_norm': 1.381413221359253, 'learning_rate': 0.00014609148031642852, 'epoch': 0.41}
{'loss': 0.8911, 'grad_norm': 1.2696231603622437, 'learning_rate': 0.00014604381574467615, 'epoch': 0.41}
{'loss': 1.2655, 'grad_norm': 1.4661043882369995, 'learning_rate': 0.0001459961378941091, 'epoch': 0.41}
{'loss': 0.7503, 'grad_norm': 1.2441264390945435, 'learning_rate': 0.00014594844677847743, 'epoch': 0.41}
{'loss': 0.8708, 'grad_norm': 1.2551140785217285, 'learning_rate': 0.000145900742411535, 'epoch': 0.41}
{'loss': 0.9033, 'grad_norm': 1.1359877586364746, 'learning_rate': 0.00014585302480703958, 'epoch': 0.41}
{'loss': 1.0551, 'grad_norm': 1.2699987888336182, 'learning_rate': 0.00014580529397875265, 'epoch': 0.41}
{'loss': 0.9128, 'grad_norm': 1.1712172031402588, 'learning_rate': 0.00014575754994043956, 'epoch': 0.41}
{'loss': 0.7385, 'grad_norm': 1.3026454448699951, 'learning_rate': 0.00014570979270586945, 'epoch': 0.41}
{'loss': 1.1118, 'grad_norm': 1.5705924034118652, 'learning_rate': 0.00014566202228881526, 'epoch': 0.41}
{'loss': 0.935, 'grad_norm': 0.9105440974235535, 'learning_rate': 0.00014561423870305382, 'epoch': 0.41}
{'loss': 1.1491, 'grad_norm': 1.5381746292114258, 'learning_rate': 0.00014556644196236565, 'epoch': 0.41}
{'loss': 0.9082, 'grad_norm': 1.1998778581619263, 'learning_rate': 0.00014551863208053507, 'epoch': 0.41}
{'loss': 1.1008, 'grad_norm': 1.1129662990570068, 'learning_rate': 0.00014547080907135026, 'epoch': 0.41}
{'loss': 0.8981, 'grad_norm': 1.0756622552871704, 'learning_rate': 0.00014542297294860312, 'epoch': 0.41}
{'loss': 0.8847, 'grad_norm': 1.0532909631729126, 'learning_rate': 0.00014537512372608937, 'epoch': 0.42}
{'loss': 1.1663, 'grad_norm': 1.3193986415863037, 'learning_rate': 0.00014532726141760848, 'epoch': 0.42}
{'loss': 1.0068, 'grad_norm': 1.1975983381271362, 'learning_rate': 0.00014527938603696376, 'epoch': 0.42}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2012, 'grad_norm': 1.24229097366333, 'learning_rate': 0.00014523149759796216, 'epoch': 0.42}
{'loss': 1.1971, 'grad_norm': 1.2552624940872192, 'learning_rate': 0.0001451835961144145, 'epoch': 0.42}
{'loss': 0.9328, 'grad_norm': 1.3121730089187622, 'learning_rate': 0.00014513568160013537, 'epoch': 0.42}
{'loss': 1.0573, 'grad_norm': 1.5616505146026611, 'learning_rate': 0.00014508775406894307, 'epoch': 0.42}
{'loss': 0.9925, 'grad_norm': 1.077684998512268, 'learning_rate': 0.0001450398135346597, 'epoch': 0.42}
{'loss': 1.0803, 'grad_norm': 1.0059853792190552, 'learning_rate': 0.000144991860011111, 'epoch': 0.42}
{'loss': 1.2377, 'grad_norm': 1.7621394395828247, 'learning_rate': 0.00014494389351212662, 'epoch': 0.42}
{'loss': 1.1628, 'grad_norm': 1.205089807510376, 'learning_rate': 0.00014489591405153978, 'epoch': 0.42}
{'loss': 0.8864, 'grad_norm': 1.0349913835525513, 'learning_rate': 0.0001448479216431876, 'epoch': 0.42}
{'loss': 0.7437, 'grad_norm': 1.865649938583374, 'learning_rate': 0.00014479991630091083, 'epoch': 0.42}
{'loss': 1.0295, 'grad_norm': 1.0503442287445068, 'learning_rate': 0.00014475189803855399, 'epoch': 0.42}
{'loss': 0.9819, 'grad_norm': 1.067956566810608, 'learning_rate': 0.00014470386686996528, 'epoch': 0.42}
{'loss': 0.9079, 'grad_norm': 1.1584409475326538, 'learning_rate': 0.0001446558228089967, 'epoch': 0.42}
{'loss': 1.0258, 'grad_norm': 0.9816069006919861, 'learning_rate': 0.00014460776586950393, 'epoch': 0.42}
{'loss': 0.7936, 'grad_norm': 1.0607051849365234, 'learning_rate': 0.0001445596960653463, 'epoch': 0.42}
{'loss': 0.9718, 'grad_norm': 1.4627642631530762, 'learning_rate': 0.000144511613410387, 'epoch': 0.42}
{'loss': 0.6713, 'grad_norm': 1.1436874866485596, 'learning_rate': 0.00014446351791849276, 'epoch': 0.42}
{'loss': 0.8299, 'grad_norm': 0.939011812210083, 'learning_rate': 0.00014441540960353412, 'epoch': 0.42}
{'loss': 1.0119, 'grad_norm': 2.3252978324890137, 'learning_rate': 0.0001443672884793853, 'epoch': 0.42}
{'loss': 0.7176, 'grad_norm': 1.1864080429077148, 'learning_rate': 0.00014431915455992414, 'epoch': 0.42}
{'loss': 1.0427, 'grad_norm': 1.3463923931121826, 'learning_rate': 0.00014427100785903231, 'epoch': 0.42}
{'loss': 1.0013, 'grad_norm': 1.3599536418914795, 'learning_rate': 0.00014422284839059503, 'epoch': 0.42}
{'loss': 0.7798, 'grad_norm': 1.3704524040222168, 'learning_rate': 0.00014417467616850127, 'epoch': 0.42}
{'loss': 0.9321, 'grad_norm': 1.228219985961914, 'learning_rate': 0.0001441264912066437, 'epoch': 0.42}
{'loss': 0.7912, 'grad_norm': 1.0251336097717285, 'learning_rate': 0.00014407829351891857, 'epoch': 0.42}
{'loss': 1.0288, 'grad_norm': 1.0615242719650269, 'learning_rate': 0.00014403008311922593, 'epoch': 0.42}
{'loss': 0.9794, 'grad_norm': 1.153692364692688, 'learning_rate': 0.00014398186002146938, 'epoch': 0.42}
{'loss': 1.0708, 'grad_norm': 1.801125407218933, 'learning_rate': 0.00014393362423955627, 'epoch': 0.42}
{'loss': 0.6295, 'grad_norm': 1.100380539894104, 'learning_rate': 0.0001438853757873975, 'epoch': 0.42}
{'loss': 1.1597, 'grad_norm': 1.1211498975753784, 'learning_rate': 0.00014383711467890774, 'epoch': 0.42}
{'loss': 1.0911, 'grad_norm': 1.1394445896148682, 'learning_rate': 0.00014378884092800526, 'epoch': 0.42}
{'loss': 0.9293, 'grad_norm': 1.1019010543823242, 'learning_rate': 0.000143740554548612, 'epoch': 0.42}
{'loss': 1.0445, 'grad_norm': 1.4307899475097656, 'learning_rate': 0.00014369225555465345, 'epoch': 0.42}
{'loss': 1.1052, 'grad_norm': 0.9800581932067871, 'learning_rate': 0.00014364394396005885, 'epoch': 0.42}
{'loss': 1.3927, 'grad_norm': 1.0792258977890015, 'learning_rate': 0.00014359561977876102, 'epoch': 0.42}
{'loss': 1.0364, 'grad_norm': 0.9936581254005432, 'learning_rate': 0.00014354728302469645, 'epoch': 0.42}
{'loss': 1.0569, 'grad_norm': 1.0238683223724365, 'learning_rate': 0.0001434989337118052, 'epoch': 0.42}
{'loss': 0.8097, 'grad_norm': 1.1551513671875, 'learning_rate': 0.000143450571854031, 'epoch': 0.42}
{'loss': 0.8737, 'grad_norm': 1.2166754007339478, 'learning_rate': 0.0001434021974653211, 'epoch': 0.42}
{'loss': 0.7694, 'grad_norm': 1.1753575801849365, 'learning_rate': 0.00014335381055962656, 'epoch': 0.42}
{'loss': 0.926, 'grad_norm': 1.16217041015625, 'learning_rate': 0.00014330541115090188, 'epoch': 0.42}
{'loss': 1.1258, 'grad_norm': 1.3087632656097412, 'learning_rate': 0.00014325699925310517, 'epoch': 0.42}
{'loss': 0.8548, 'grad_norm': 0.9895442128181458, 'learning_rate': 0.00014320857488019824, 'epoch': 0.42}
{'loss': 0.8574, 'grad_norm': 1.2190965414047241, 'learning_rate': 0.00014316013804614643, 'epoch': 0.42}
{'loss': 0.8113, 'grad_norm': 1.4775035381317139, 'learning_rate': 0.00014311168876491868, 'epoch': 0.42}
{'loss': 0.9576, 'grad_norm': 1.5761020183563232, 'learning_rate': 0.0001430632270504876, 'epoch': 0.42}
{'loss': 0.8407, 'grad_norm': 0.9459392428398132, 'learning_rate': 0.0001430147529168292, 'epoch': 0.42}
{'loss': 0.9126, 'grad_norm': 1.4221256971359253, 'learning_rate': 0.00014296626637792326, 'epoch': 0.42}
{'loss': 0.9272, 'grad_norm': 1.152367353439331, 'learning_rate': 0.00014291776744775305, 'epoch': 0.42}
{'loss': 1.1063, 'grad_norm': 1.1932532787322998, 'learning_rate': 0.00014286925614030542, 'epoch': 0.42}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8416, 'grad_norm': 0.8556832075119019, 'learning_rate': 0.00014282073246957083, 'epoch': 0.42}
{'loss': 0.8957, 'grad_norm': 1.059822678565979, 'learning_rate': 0.00014277219644954323, 'epoch': 0.42}
{'loss': 0.9238, 'grad_norm': 1.0762104988098145, 'learning_rate': 0.00014272364809422017, 'epoch': 0.42}
{'loss': 0.9146, 'grad_norm': 1.0612680912017822, 'learning_rate': 0.0001426750874176028, 'epoch': 0.42}
{'loss': 1.0544, 'grad_norm': 1.203360915184021, 'learning_rate': 0.00014262651443369576, 'epoch': 0.42}
{'loss': 1.1925, 'grad_norm': 1.0467731952667236, 'learning_rate': 0.00014257792915650728, 'epoch': 0.42}
{'loss': 0.842, 'grad_norm': 1.258419156074524, 'learning_rate': 0.0001425293316000491, 'epoch': 0.42}
{'loss': 0.8742, 'grad_norm': 1.118122935295105, 'learning_rate': 0.00014248072177833654, 'epoch': 0.42}
{'loss': 0.6294, 'grad_norm': 1.3102160692214966, 'learning_rate': 0.00014243209970538846, 'epoch': 0.42}
{'loss': 0.7576, 'grad_norm': 1.1885931491851807, 'learning_rate': 0.00014238346539522715, 'epoch': 0.42}
{'loss': 1.0052, 'grad_norm': 1.2112618684768677, 'learning_rate': 0.0001423348188618786, 'epoch': 0.42}
{'loss': 0.9852, 'grad_norm': 1.0460952520370483, 'learning_rate': 0.0001422861601193722, 'epoch': 0.42}
{'loss': 0.8451, 'grad_norm': 1.1332244873046875, 'learning_rate': 0.00014223748918174088, 'epoch': 0.43}
{'loss': 0.9491, 'grad_norm': 1.4619826078414917, 'learning_rate': 0.00014218880606302114, 'epoch': 0.43}
{'loss': 0.8867, 'grad_norm': 1.0773963928222656, 'learning_rate': 0.00014214011077725292, 'epoch': 0.43}
{'loss': 0.7915, 'grad_norm': 1.6347240209579468, 'learning_rate': 0.00014209140333847973, 'epoch': 0.43}
{'loss': 1.081, 'grad_norm': 0.9728997349739075, 'learning_rate': 0.00014204268376074856, 'epoch': 0.43}
{'loss': 1.1726, 'grad_norm': 1.0906485319137573, 'learning_rate': 0.00014199395205810987, 'epoch': 0.43}
{'loss': 1.2698, 'grad_norm': 1.1137784719467163, 'learning_rate': 0.00014194520824461771, 'epoch': 0.43}
{'loss': 1.0713, 'grad_norm': 0.8976686000823975, 'learning_rate': 0.00014189645233432952, 'epoch': 0.43}
{'loss': 1.1293, 'grad_norm': 1.1912094354629517, 'learning_rate': 0.0001418476843413063, 'epoch': 0.43}
{'loss': 0.9721, 'grad_norm': 1.197907567024231, 'learning_rate': 0.00014179890427961243, 'epoch': 0.43}
{'loss': 0.5553, 'grad_norm': 1.16289484500885, 'learning_rate': 0.0001417501121633159, 'epoch': 0.43}
{'loss': 1.0815, 'grad_norm': 1.069916844367981, 'learning_rate': 0.00014170130800648814, 'epoch': 0.43}
{'loss': 1.0815, 'grad_norm': 1.072052240371704, 'learning_rate': 0.00014165249182320402, 'epoch': 0.43}
{'loss': 1.0004, 'grad_norm': 0.9340102076530457, 'learning_rate': 0.00014160366362754183, 'epoch': 0.43}
{'loss': 1.0423, 'grad_norm': 1.4093962907791138, 'learning_rate': 0.00014155482343358345, 'epoch': 0.43}
{'loss': 0.5686, 'grad_norm': 1.4341371059417725, 'learning_rate': 0.00014150597125541417, 'epoch': 0.43}
{'loss': 0.8658, 'grad_norm': 1.5162672996520996, 'learning_rate': 0.0001414571071071227, 'epoch': 0.43}
{'loss': 0.6717, 'grad_norm': 1.1617205142974854, 'learning_rate': 0.0001414082310028012, 'epoch': 0.43}
{'loss': 0.9846, 'grad_norm': 1.3234800100326538, 'learning_rate': 0.00014135934295654535, 'epoch': 0.43}
{'loss': 1.214, 'grad_norm': 1.4530447721481323, 'learning_rate': 0.0001413104429824542, 'epoch': 0.43}
{'loss': 0.9191, 'grad_norm': 0.9582167863845825, 'learning_rate': 0.00014126153109463024, 'epoch': 0.43}
{'loss': 0.787, 'grad_norm': 1.0567055940628052, 'learning_rate': 0.0001412126073071795, 'epoch': 0.43}
{'loss': 0.8998, 'grad_norm': 1.1724708080291748, 'learning_rate': 0.0001411636716342113, 'epoch': 0.43}
{'loss': 0.9487, 'grad_norm': 0.8925073146820068, 'learning_rate': 0.00014111472408983843, 'epoch': 0.43}
{'loss': 1.127, 'grad_norm': 0.9628040790557861, 'learning_rate': 0.0001410657646881772, 'epoch': 0.43}
{'loss': 1.1159, 'grad_norm': 1.0535948276519775, 'learning_rate': 0.0001410167934433472, 'epoch': 0.43}
{'loss': 1.0026, 'grad_norm': 1.0995153188705444, 'learning_rate': 0.00014096781036947157, 'epoch': 0.43}
{'loss': 0.7465, 'grad_norm': 1.0991462469100952, 'learning_rate': 0.00014091881548067677, 'epoch': 0.43}
{'loss': 1.0276, 'grad_norm': 1.0550189018249512, 'learning_rate': 0.00014086980879109265, 'epoch': 0.43}
{'loss': 0.7344, 'grad_norm': 1.234094500541687, 'learning_rate': 0.0001408207903148525, 'epoch': 0.43}
{'loss': 1.1264, 'grad_norm': 1.096191644668579, 'learning_rate': 0.00014077176006609308, 'epoch': 0.43}
{'loss': 0.8369, 'grad_norm': 1.1230937242507935, 'learning_rate': 0.00014072271805895442, 'epoch': 0.43}
{'loss': 0.7906, 'grad_norm': 1.0540672540664673, 'learning_rate': 0.00014067366430758004, 'epoch': 0.43}
{'loss': 0.9144, 'grad_norm': 1.1029185056686401, 'learning_rate': 0.00014062459882611676, 'epoch': 0.43}
{'loss': 1.1049, 'grad_norm': 1.0953137874603271, 'learning_rate': 0.00014057552162871485, 'epoch': 0.43}
{'loss': 0.7657, 'grad_norm': 1.5278956890106201, 'learning_rate': 0.00014052643272952794, 'epoch': 0.43}
{'loss': 0.806, 'grad_norm': 1.4083796739578247, 'learning_rate': 0.00014047733214271304, 'epoch': 0.43}
{'loss': 1.0929, 'grad_norm': 0.9147701859474182, 'learning_rate': 0.0001404282198824305, 'epoch': 0.43}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9818, 'grad_norm': 1.0514371395111084, 'learning_rate': 0.00014037909596284408, 'epoch': 0.43}
{'loss': 1.1252, 'grad_norm': 1.2933845520019531, 'learning_rate': 0.00014032996039812088, 'epoch': 0.43}
{'loss': 0.9873, 'grad_norm': 1.564124345779419, 'learning_rate': 0.00014028081320243137, 'epoch': 0.43}
{'loss': 0.8569, 'grad_norm': 1.0741173028945923, 'learning_rate': 0.0001402316543899493, 'epoch': 0.43}
{'loss': 0.9642, 'grad_norm': 1.253271460533142, 'learning_rate': 0.00014018248397485193, 'epoch': 0.43}
{'loss': 0.865, 'grad_norm': 0.8899235129356384, 'learning_rate': 0.00014013330197131972, 'epoch': 0.43}
{'loss': 1.2126, 'grad_norm': 1.3546289205551147, 'learning_rate': 0.0001400841083935365, 'epoch': 0.43}
{'loss': 0.8168, 'grad_norm': 1.1310312747955322, 'learning_rate': 0.00014003490325568954, 'epoch': 0.43}
{'loss': 0.9606, 'grad_norm': 1.0007208585739136, 'learning_rate': 0.00013998568657196925, 'epoch': 0.43}
{'loss': 0.7051, 'grad_norm': 1.4222677946090698, 'learning_rate': 0.00013993645835656953, 'epoch': 0.43}
{'loss': 0.872, 'grad_norm': 1.1163004636764526, 'learning_rate': 0.00013988721862368765, 'epoch': 0.43}
{'loss': 1.1354, 'grad_norm': 1.2087185382843018, 'learning_rate': 0.000139837967387524, 'epoch': 0.43}
{'loss': 1.0997, 'grad_norm': 1.0696746110916138, 'learning_rate': 0.00013978870466228247, 'epoch': 0.43}
{'loss': 0.9692, 'grad_norm': 2.637852191925049, 'learning_rate': 0.00013973943046217014, 'epoch': 0.43}
{'loss': 0.8535, 'grad_norm': 1.2918717861175537, 'learning_rate': 0.00013969014480139747, 'epoch': 0.43}
{'loss': 0.9309, 'grad_norm': 1.6423308849334717, 'learning_rate': 0.0001396408476941782, 'epoch': 0.43}
{'loss': 1.2282, 'grad_norm': 1.121027946472168, 'learning_rate': 0.00013959153915472943, 'epoch': 0.43}
{'loss': 0.9502, 'grad_norm': 1.2473710775375366, 'learning_rate': 0.00013954221919727146, 'epoch': 0.43}
{'loss': 1.3808, 'grad_norm': 0.9747276306152344, 'learning_rate': 0.0001394928878360279, 'epoch': 0.43}
{'loss': 0.8855, 'grad_norm': 1.2697871923446655, 'learning_rate': 0.00013944354508522574, 'epoch': 0.43}
{'loss': 1.0179, 'grad_norm': 1.3747752904891968, 'learning_rate': 0.00013939419095909512, 'epoch': 0.43}
{'loss': 0.6771, 'grad_norm': 1.2880942821502686, 'learning_rate': 0.0001393448254718696, 'epoch': 0.43}
{'loss': 0.9269, 'grad_norm': 1.1762396097183228, 'learning_rate': 0.0001392954486377859, 'epoch': 0.43}
{'loss': 0.6597, 'grad_norm': 0.9869173765182495, 'learning_rate': 0.0001392460604710841, 'epoch': 0.43}
{'loss': 0.9404, 'grad_norm': 1.055739164352417, 'learning_rate': 0.00013919666098600753, 'epoch': 0.43}
{'loss': 1.2715, 'grad_norm': 1.1259726285934448, 'learning_rate': 0.00013914725019680268, 'epoch': 0.43}
{'loss': 0.9859, 'grad_norm': 1.1844255924224854, 'learning_rate': 0.00013909782811771944, 'epoch': 0.43}
{'loss': 0.6617, 'grad_norm': 1.0872702598571777, 'learning_rate': 0.0001390483947630109, 'epoch': 0.44}
{'loss': 1.5165, 'grad_norm': 1.3414627313613892, 'learning_rate': 0.0001389989501469334, 'epoch': 0.44}
{'loss': 0.8775, 'grad_norm': 1.0123612880706787, 'learning_rate': 0.00013894949428374647, 'epoch': 0.44}
{'loss': 0.903, 'grad_norm': 1.5133817195892334, 'learning_rate': 0.00013890002718771303, 'epoch': 0.44}
{'loss': 0.921, 'grad_norm': 1.078460693359375, 'learning_rate': 0.0001388505488730991, 'epoch': 0.44}
{'loss': 0.8942, 'grad_norm': 1.2798339128494263, 'learning_rate': 0.00013880105935417403, 'epoch': 0.44}
{'loss': 1.1107, 'grad_norm': 1.770263910293579, 'learning_rate': 0.0001387515586452103, 'epoch': 0.44}
{'loss': 0.8578, 'grad_norm': 1.3046834468841553, 'learning_rate': 0.00013870204676048374, 'epoch': 0.44}
{'loss': 0.8632, 'grad_norm': 0.9701244235038757, 'learning_rate': 0.0001386525237142733, 'epoch': 0.44}
{'loss': 1.1554, 'grad_norm': 1.187163233757019, 'learning_rate': 0.00013860298952086118, 'epoch': 0.44}
{'loss': 1.0234, 'grad_norm': 1.1505600214004517, 'learning_rate': 0.0001385534441945328, 'epoch': 0.44}
{'loss': 1.0522, 'grad_norm': 1.3131499290466309, 'learning_rate': 0.00013850388774957684, 'epoch': 0.44}
{'loss': 1.2786, 'grad_norm': 1.1133506298065186, 'learning_rate': 0.0001384543202002851, 'epoch': 0.44}
{'loss': 1.0105, 'grad_norm': 1.2257049083709717, 'learning_rate': 0.00013840474156095263, 'epoch': 0.44}
{'loss': 1.1815, 'grad_norm': 1.3599622249603271, 'learning_rate': 0.00013835515184587767, 'epoch': 0.44}
{'loss': 1.1667, 'grad_norm': 1.0303107500076294, 'learning_rate': 0.00013830555106936164, 'epoch': 0.44}
{'loss': 1.1609, 'grad_norm': 1.3114904165267944, 'learning_rate': 0.00013825593924570923, 'epoch': 0.44}
{'loss': 0.7144, 'grad_norm': 1.2754837274551392, 'learning_rate': 0.0001382063163892282, 'epoch': 0.44}
{'loss': 1.1343, 'grad_norm': 1.0508273839950562, 'learning_rate': 0.00013815668251422952, 'epoch': 0.44}
{'loss': 0.9589, 'grad_norm': 1.092347264289856, 'learning_rate': 0.00013810703763502744, 'epoch': 0.44}
{'loss': 0.8318, 'grad_norm': 1.32975435256958, 'learning_rate': 0.0001380573817659392, 'epoch': 0.44}
{'loss': 1.218, 'grad_norm': 1.166948914527893, 'learning_rate': 0.00013800771492128536, 'epoch': 0.44}
{'loss': 1.0243, 'grad_norm': 1.2313013076782227, 'learning_rate': 0.00013795803711538966, 'epoch': 0.44}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.852, 'grad_norm': 1.3896946907043457, 'learning_rate': 0.00013790834836257884, 'epoch': 0.44}
{'loss': 1.1235, 'grad_norm': 0.954036295413971, 'learning_rate': 0.00013785864867718294, 'epoch': 0.44}
{'loss': 1.1508, 'grad_norm': 2.9762916564941406, 'learning_rate': 0.00013780893807353512, 'epoch': 0.44}
{'loss': 1.3585, 'grad_norm': 1.5325682163238525, 'learning_rate': 0.0001377592165659717, 'epoch': 0.44}
{'loss': 0.7815, 'grad_norm': 1.0679361820220947, 'learning_rate': 0.00013770948416883205, 'epoch': 0.44}
{'loss': 1.1955, 'grad_norm': 1.1123214960098267, 'learning_rate': 0.00013765974089645884, 'epoch': 0.44}
{'loss': 1.286, 'grad_norm': 1.3035807609558105, 'learning_rate': 0.0001376099867631977, 'epoch': 0.44}
{'loss': 1.0091, 'grad_norm': 1.2035871744155884, 'learning_rate': 0.00013756022178339756, 'epoch': 0.44}
{'loss': 0.769, 'grad_norm': 1.309489130973816, 'learning_rate': 0.00013751044597141036, 'epoch': 0.44}
{'loss': 0.8463, 'grad_norm': 1.2701700925827026, 'learning_rate': 0.00013746065934159123, 'epoch': 0.44}
{'loss': 0.8075, 'grad_norm': 1.284650206565857, 'learning_rate': 0.00013741086190829837, 'epoch': 0.44}
{'loss': 1.0627, 'grad_norm': 1.3011304140090942, 'learning_rate': 0.0001373610536858931, 'epoch': 0.44}
{'loss': 0.9311, 'grad_norm': 0.9883024096488953, 'learning_rate': 0.00013731123468873992, 'epoch': 0.44}
{'loss': 1.1523, 'grad_norm': 1.4875619411468506, 'learning_rate': 0.0001372614049312064, 'epoch': 0.44}
{'loss': 0.906, 'grad_norm': 1.773911476135254, 'learning_rate': 0.00013721156442766312, 'epoch': 0.44}
{'loss': 1.1081, 'grad_norm': 1.6101360321044922, 'learning_rate': 0.00013716171319248392, 'epoch': 0.44}
{'loss': 1.0815, 'grad_norm': 1.3662495613098145, 'learning_rate': 0.0001371118512400456, 'epoch': 0.44}
{'loss': 0.9998, 'grad_norm': 0.9801734685897827, 'learning_rate': 0.0001370619785847282, 'epoch': 0.44}
{'loss': 0.9412, 'grad_norm': 1.1395788192749023, 'learning_rate': 0.00013701209524091465, 'epoch': 0.44}
{'loss': 0.7313, 'grad_norm': 1.1568021774291992, 'learning_rate': 0.00013696220122299112, 'epoch': 0.44}
{'loss': 0.6785, 'grad_norm': 1.0836985111236572, 'learning_rate': 0.00013691229654534676, 'epoch': 0.44}
{'loss': 0.8902, 'grad_norm': 1.1037160158157349, 'learning_rate': 0.0001368623812223739, 'epoch': 0.44}
{'loss': 0.9705, 'grad_norm': 1.4599835872650146, 'learning_rate': 0.00013681245526846783, 'epoch': 0.44}
{'loss': 1.1462, 'grad_norm': 1.10838782787323, 'learning_rate': 0.00013676251869802693, 'epoch': 0.44}
{'loss': 0.852, 'grad_norm': 1.0013062953948975, 'learning_rate': 0.00013671257152545277, 'epoch': 0.44}
{'loss': 1.3012, 'grad_norm': 1.5997529029846191, 'learning_rate': 0.00013666261376514977, 'epoch': 0.44}
{'loss': 0.9807, 'grad_norm': 1.254546880722046, 'learning_rate': 0.0001366126454315256, 'epoch': 0.44}
{'loss': 1.1637, 'grad_norm': 1.208256483078003, 'learning_rate': 0.0001365626665389908, 'epoch': 0.44}
{'loss': 1.0019, 'grad_norm': 0.8833613395690918, 'learning_rate': 0.00013651267710195907, 'epoch': 0.44}
{'loss': 0.823, 'grad_norm': 1.2470163106918335, 'learning_rate': 0.00013646267713484717, 'epoch': 0.44}
{'loss': 1.0788, 'grad_norm': 1.9775530099868774, 'learning_rate': 0.00013641266665207478, 'epoch': 0.44}
{'loss': 0.7987, 'grad_norm': 1.5996301174163818, 'learning_rate': 0.0001363626456680647, 'epoch': 0.44}
{'loss': 1.1955, 'grad_norm': 1.0839295387268066, 'learning_rate': 0.0001363126141972428, 'epoch': 0.44}
{'loss': 0.8073, 'grad_norm': 1.1084284782409668, 'learning_rate': 0.00013626257225403783, 'epoch': 0.44}
{'loss': 1.1535, 'grad_norm': 1.212683081626892, 'learning_rate': 0.0001362125198528817, 'epoch': 0.44}
{'loss': 1.0934, 'grad_norm': 1.2123454809188843, 'learning_rate': 0.00013616245700820922, 'epoch': 0.44}
{'loss': 1.2211, 'grad_norm': 1.0279712677001953, 'learning_rate': 0.00013611238373445838, 'epoch': 0.44}
{'loss': 0.8219, 'grad_norm': 1.1002246141433716, 'learning_rate': 0.00013606230004607, 'epoch': 0.44}
{'loss': 1.4936, 'grad_norm': 4.073747634887695, 'learning_rate': 0.000136012205957488, 'epoch': 0.44}
{'loss': 1.4941, 'grad_norm': 1.1598258018493652, 'learning_rate': 0.00013596210148315917, 'epoch': 0.44}
{'loss': 0.9804, 'grad_norm': 0.9171764254570007, 'learning_rate': 0.00013591198663753356, 'epoch': 0.44}
{'loss': 0.9637, 'grad_norm': 0.9060701727867126, 'learning_rate': 0.000135861861435064, 'epoch': 0.44}
{'loss': 1.2646, 'grad_norm': 1.256659984588623, 'learning_rate': 0.0001358117258902063, 'epoch': 0.45}
{'loss': 1.0295, 'grad_norm': 1.4685415029525757, 'learning_rate': 0.00013576158001741932, 'epoch': 0.45}
{'loss': 0.8947, 'grad_norm': 1.425645112991333, 'learning_rate': 0.00013571142383116496, 'epoch': 0.45}
{'loss': 0.9036, 'grad_norm': 1.4005283117294312, 'learning_rate': 0.00013566125734590793, 'epoch': 0.45}
{'loss': 0.9021, 'grad_norm': 1.2055597305297852, 'learning_rate': 0.00013561108057611606, 'epoch': 0.45}
{'loss': 1.106, 'grad_norm': 1.1731268167495728, 'learning_rate': 0.0001355608935362601, 'epoch': 0.45}
{'loss': 0.9485, 'grad_norm': 1.06853449344635, 'learning_rate': 0.0001355106962408137, 'epoch': 0.45}
{'loss': 0.7525, 'grad_norm': 1.3742575645446777, 'learning_rate': 0.00013546048870425356, 'epoch': 0.45}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0917, 'grad_norm': 1.8084834814071655, 'learning_rate': 0.00013541027094105928, 'epoch': 0.45}
{'loss': 1.2113, 'grad_norm': 1.0752309560775757, 'learning_rate': 0.00013536004296571343, 'epoch': 0.45}
{'loss': 0.7877, 'grad_norm': 1.7871465682983398, 'learning_rate': 0.0001353098047927015, 'epoch': 0.45}
{'loss': 1.0303, 'grad_norm': 1.2005149126052856, 'learning_rate': 0.00013525955643651193, 'epoch': 0.45}
{'loss': 0.9683, 'grad_norm': 1.2777106761932373, 'learning_rate': 0.00013520929791163607, 'epoch': 0.45}
{'loss': 1.0939, 'grad_norm': 1.0051789283752441, 'learning_rate': 0.0001351590292325683, 'epoch': 0.45}
{'loss': 0.9438, 'grad_norm': 1.4537103176116943, 'learning_rate': 0.0001351087504138059, 'epoch': 0.45}
{'loss': 0.7433, 'grad_norm': 0.9447715282440186, 'learning_rate': 0.00013505846146984895, 'epoch': 0.45}
{'loss': 1.1067, 'grad_norm': 1.1343176364898682, 'learning_rate': 0.00013500816241520058, 'epoch': 0.45}
{'loss': 0.9597, 'grad_norm': 1.0039372444152832, 'learning_rate': 0.0001349578532643668, 'epoch': 0.45}
{'loss': 1.1016, 'grad_norm': 1.080544114112854, 'learning_rate': 0.0001349075340318565, 'epoch': 0.45}
{'loss': 0.8307, 'grad_norm': 1.1833150386810303, 'learning_rate': 0.00013485720473218154, 'epoch': 0.45}
{'loss': 1.0325, 'grad_norm': 1.2024492025375366, 'learning_rate': 0.0001348068653798566, 'epoch': 0.45}
{'loss': 1.0218, 'grad_norm': 0.9216597080230713, 'learning_rate': 0.00013475651598939936, 'epoch': 0.45}
{'loss': 1.309, 'grad_norm': 0.9603034853935242, 'learning_rate': 0.0001347061565753303, 'epoch': 0.45}
{'loss': 0.947, 'grad_norm': 0.956059992313385, 'learning_rate': 0.0001346557871521729, 'epoch': 0.45}
{'loss': 0.764, 'grad_norm': 1.205014705657959, 'learning_rate': 0.0001346054077344533, 'epoch': 0.45}
{'loss': 0.9064, 'grad_norm': 1.1678422689437866, 'learning_rate': 0.00013455501833670088, 'epoch': 0.45}
{'loss': 1.0534, 'grad_norm': 1.1334116458892822, 'learning_rate': 0.00013450461897344758, 'epoch': 0.45}
{'loss': 1.16, 'grad_norm': 1.181060552597046, 'learning_rate': 0.00013445420965922837, 'epoch': 0.45}
{'loss': 1.0793, 'grad_norm': 1.0428917407989502, 'learning_rate': 0.00013440379040858104, 'epoch': 0.45}
{'loss': 0.9816, 'grad_norm': 1.2269304990768433, 'learning_rate': 0.00013435336123604626, 'epoch': 0.45}
{'loss': 1.0472, 'grad_norm': 1.6973479986190796, 'learning_rate': 0.0001343029221561676, 'epoch': 0.45}
{'loss': 1.1028, 'grad_norm': 1.1418845653533936, 'learning_rate': 0.00013425247318349137, 'epoch': 0.45}
{'loss': 0.9148, 'grad_norm': 1.1084561347961426, 'learning_rate': 0.00013420201433256689, 'epoch': 0.45}
{'loss': 1.0319, 'grad_norm': 1.3999861478805542, 'learning_rate': 0.0001341515456179462, 'epoch': 0.45}
{'loss': 1.0366, 'grad_norm': 1.4313137531280518, 'learning_rate': 0.00013410106705418425, 'epoch': 0.45}
{'loss': 1.0908, 'grad_norm': 1.0965336561203003, 'learning_rate': 0.00013405057865583877, 'epoch': 0.45}
{'loss': 0.8298, 'grad_norm': 0.9725233912467957, 'learning_rate': 0.00013400008043747046, 'epoch': 0.45}
{'loss': 1.0301, 'grad_norm': 1.1205073595046997, 'learning_rate': 0.00013394957241364273, 'epoch': 0.45}
{'loss': 0.8328, 'grad_norm': 1.0557690858840942, 'learning_rate': 0.00013389905459892183, 'epoch': 0.45}
{'loss': 1.0775, 'grad_norm': 1.1560819149017334, 'learning_rate': 0.00013384852700787681, 'epoch': 0.45}
{'loss': 1.1997, 'grad_norm': 1.1057894229888916, 'learning_rate': 0.00013379798965507968, 'epoch': 0.45}
{'loss': 0.9646, 'grad_norm': 1.3395113945007324, 'learning_rate': 0.00013374744255510512, 'epoch': 0.45}
{'loss': 0.9506, 'grad_norm': 1.2637939453125, 'learning_rate': 0.0001336968857225307, 'epoch': 0.45}
{'loss': 1.0016, 'grad_norm': 1.251754641532898, 'learning_rate': 0.0001336463191719367, 'epoch': 0.45}
{'loss': 1.0346, 'grad_norm': 1.2791434526443481, 'learning_rate': 0.00013359574291790634, 'epoch': 0.45}
{'loss': 0.9008, 'grad_norm': 0.9786027669906616, 'learning_rate': 0.00013354515697502553, 'epoch': 0.45}
{'loss': 1.0373, 'grad_norm': 1.1334295272827148, 'learning_rate': 0.00013349456135788298, 'epoch': 0.45}
{'loss': 0.8561, 'grad_norm': 1.2200127840042114, 'learning_rate': 0.00013344395608107031, 'epoch': 0.45}
{'loss': 0.7019, 'grad_norm': 0.9801905155181885, 'learning_rate': 0.0001333933411591818, 'epoch': 0.45}
{'loss': 0.7469, 'grad_norm': 1.9152387380599976, 'learning_rate': 0.00013334271660681455, 'epoch': 0.45}
{'loss': 0.9746, 'grad_norm': 1.445802927017212, 'learning_rate': 0.0001332920824385684, 'epoch': 0.45}
{'loss': 1.1969, 'grad_norm': 1.118241786956787, 'learning_rate': 0.00013324143866904603, 'epoch': 0.45}
{'loss': 0.994, 'grad_norm': 0.935441792011261, 'learning_rate': 0.00013319078531285285, 'epoch': 0.45}
{'loss': 0.9534, 'grad_norm': 1.0200234651565552, 'learning_rate': 0.00013314012238459707, 'epoch': 0.45}
{'loss': 0.7539, 'grad_norm': 1.04015052318573, 'learning_rate': 0.0001330894498988896, 'epoch': 0.45}
{'loss': 0.9639, 'grad_norm': 1.0799037218093872, 'learning_rate': 0.0001330387678703442, 'epoch': 0.45}
{'loss': 1.0204, 'grad_norm': 1.3136459589004517, 'learning_rate': 0.00013298807631357724, 'epoch': 0.45}
{'loss': 0.886, 'grad_norm': 1.334650993347168, 'learning_rate': 0.00013293737524320797, 'epoch': 0.45}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7129, 'grad_norm': 1.738021731376648, 'learning_rate': 0.00013288666467385833, 'epoch': 0.45}
{'loss': 1.0206, 'grad_norm': 1.2298768758773804, 'learning_rate': 0.000132835944620153, 'epoch': 0.45}
{'loss': 0.8867, 'grad_norm': 1.452933430671692, 'learning_rate': 0.0001327852150967194, 'epoch': 0.45}
{'loss': 1.1448, 'grad_norm': 0.9728876948356628, 'learning_rate': 0.00013273447611818767, 'epoch': 0.45}
{'loss': 1.1003, 'grad_norm': 1.3987199068069458, 'learning_rate': 0.00013268372769919073, 'epoch': 0.45}
{'loss': 1.1189, 'grad_norm': 1.0550369024276733, 'learning_rate': 0.00013263296985436412, 'epoch': 0.45}
{'loss': 1.0661, 'grad_norm': 1.1276553869247437, 'learning_rate': 0.00013258220259834618, 'epoch': 0.45}
{'loss': 1.2404, 'grad_norm': 0.9788698554039001, 'learning_rate': 0.00013253142594577794, 'epoch': 0.46}
{'loss': 1.004, 'grad_norm': 0.9837543368339539, 'learning_rate': 0.00013248063991130315, 'epoch': 0.46}
{'loss': 1.0864, 'grad_norm': 1.308731198310852, 'learning_rate': 0.00013242984450956828, 'epoch': 0.46}
{'loss': 1.1491, 'grad_norm': 1.1963410377502441, 'learning_rate': 0.00013237903975522243, 'epoch': 0.46}
{'loss': 1.0479, 'grad_norm': 1.0772004127502441, 'learning_rate': 0.00013232822566291747, 'epoch': 0.46}
{'loss': 0.7893, 'grad_norm': 0.8766307234764099, 'learning_rate': 0.00013227740224730798, 'epoch': 0.46}
{'loss': 0.7104, 'grad_norm': 0.8851963877677917, 'learning_rate': 0.00013222656952305113, 'epoch': 0.46}
{'loss': 1.1477, 'grad_norm': 1.4252771139144897, 'learning_rate': 0.00013217572750480686, 'epoch': 0.46}
{'loss': 1.0061, 'grad_norm': 1.0190296173095703, 'learning_rate': 0.0001321248762072377, 'epoch': 0.46}
{'loss': 0.9527, 'grad_norm': 1.048086404800415, 'learning_rate': 0.00013207401564500903, 'epoch': 0.46}
{'loss': 1.0361, 'grad_norm': 1.3622937202453613, 'learning_rate': 0.0001320231458327887, 'epoch': 0.46}
{'loss': 0.8116, 'grad_norm': 1.0734461545944214, 'learning_rate': 0.00013197226678524738, 'epoch': 0.46}
{'loss': 0.9409, 'grad_norm': 1.5012023448944092, 'learning_rate': 0.0001319213785170583, 'epoch': 0.46}
{'loss': 0.9199, 'grad_norm': 0.9214009046554565, 'learning_rate': 0.0001318704810428974, 'epoch': 0.46}
{'loss': 1.049, 'grad_norm': 1.50076162815094, 'learning_rate': 0.00013181957437744332, 'epoch': 0.46}
{'loss': 0.7387, 'grad_norm': 1.1309632062911987, 'learning_rate': 0.00013176865853537723, 'epoch': 0.46}
{'loss': 1.0826, 'grad_norm': 1.1993898153305054, 'learning_rate': 0.00013171773353138304, 'epoch': 0.46}
{'loss': 1.2563, 'grad_norm': 1.1303290128707886, 'learning_rate': 0.00013166679938014726, 'epoch': 0.46}
{'loss': 0.8167, 'grad_norm': 1.062003493309021, 'learning_rate': 0.00013161585609635908, 'epoch': 0.46}
{'loss': 1.2533, 'grad_norm': 0.9566593170166016, 'learning_rate': 0.00013156490369471027, 'epoch': 0.46}
{'loss': 1.1953, 'grad_norm': 1.186334252357483, 'learning_rate': 0.00013151394218989526, 'epoch': 0.46}
{'loss': 1.0006, 'grad_norm': 1.4528963565826416, 'learning_rate': 0.00013146297159661113, 'epoch': 0.46}
{'loss': 0.9293, 'grad_norm': 1.2330892086029053, 'learning_rate': 0.00013141199192955751, 'epoch': 0.46}
{'loss': 1.1422, 'grad_norm': 2.0457751750946045, 'learning_rate': 0.00013136100320343673, 'epoch': 0.46}
{'loss': 0.7734, 'grad_norm': 1.0354026556015015, 'learning_rate': 0.0001313100054329537, 'epoch': 0.46}
{'loss': 0.8796, 'grad_norm': 0.9663034677505493, 'learning_rate': 0.00013125899863281588, 'epoch': 0.46}
{'loss': 0.8014, 'grad_norm': 1.1586334705352783, 'learning_rate': 0.00013120798281773347, 'epoch': 0.46}
{'loss': 1.4829, 'grad_norm': 1.0971660614013672, 'learning_rate': 0.00013115695800241904, 'epoch': 0.46}
{'loss': 0.7691, 'grad_norm': 0.9778650403022766, 'learning_rate': 0.00013110592420158807, 'epoch': 0.46}
{'loss': 1.0591, 'grad_norm': 1.4836233854293823, 'learning_rate': 0.00013105488142995837, 'epoch': 0.46}
{'loss': 1.1271, 'grad_norm': 0.9535983204841614, 'learning_rate': 0.00013100382970225046, 'epoch': 0.46}
{'loss': 0.9194, 'grad_norm': 0.9211719036102295, 'learning_rate': 0.00013095276903318735, 'epoch': 0.46}
{'loss': 0.9616, 'grad_norm': 1.253770351409912, 'learning_rate': 0.00013090169943749476, 'epoch': 0.46}
{'loss': 0.9123, 'grad_norm': 1.240085244178772, 'learning_rate': 0.00013085062092990088, 'epoch': 0.46}
{'loss': 1.329, 'grad_norm': 1.0792462825775146, 'learning_rate': 0.00013079953352513655, 'epoch': 0.46}
{'loss': 1.2703, 'grad_norm': 1.038452386856079, 'learning_rate': 0.00013074843723793507, 'epoch': 0.46}
{'loss': 0.9732, 'grad_norm': 1.3746522665023804, 'learning_rate': 0.00013069733208303242, 'epoch': 0.46}
{'loss': 0.9601, 'grad_norm': 1.0492231845855713, 'learning_rate': 0.00013064621807516702, 'epoch': 0.46}
{'loss': 1.1295, 'grad_norm': 1.1032696962356567, 'learning_rate': 0.00013059509522907997, 'epoch': 0.46}
{'loss': 1.0168, 'grad_norm': 1.2089215517044067, 'learning_rate': 0.0001305439635595148, 'epoch': 0.46}
{'loss': 0.9961, 'grad_norm': 2.046762466430664, 'learning_rate': 0.0001304928230812177, 'epoch': 0.46}
{'loss': 0.8714, 'grad_norm': 1.0354230403900146, 'learning_rate': 0.00013044167380893727, 'epoch': 0.46}
{'loss': 1.0406, 'grad_norm': 1.2945137023925781, 'learning_rate': 0.0001303905157574247, 'epoch': 0.46}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9469, 'grad_norm': 1.0374541282653809, 'learning_rate': 0.0001303393489414338, 'epoch': 0.46}
{'loss': 1.0637, 'grad_norm': 1.347162127494812, 'learning_rate': 0.00013028817337572076, 'epoch': 0.46}
{'loss': 1.134, 'grad_norm': 1.009876012802124, 'learning_rate': 0.00013023698907504446, 'epoch': 0.46}
{'loss': 1.1393, 'grad_norm': 1.2581422328948975, 'learning_rate': 0.0001301857960541661, 'epoch': 0.46}
{'loss': 1.0203, 'grad_norm': 1.4168851375579834, 'learning_rate': 0.00013013459432784961, 'epoch': 0.46}
{'loss': 0.8869, 'grad_norm': 0.8180016875267029, 'learning_rate': 0.00013008338391086124, 'epoch': 0.46}
{'loss': 1.0009, 'grad_norm': 1.0766109228134155, 'learning_rate': 0.00013003216481796982, 'epoch': 0.46}
{'loss': 0.9386, 'grad_norm': 1.1858488321304321, 'learning_rate': 0.00012998093706394675, 'epoch': 0.46}
{'loss': 1.2702, 'grad_norm': 1.0798043012619019, 'learning_rate': 0.00012992970066356583, 'epoch': 0.46}
{'loss': 1.2382, 'grad_norm': 1.208904504776001, 'learning_rate': 0.0001298784556316034, 'epoch': 0.46}
{'loss': 1.1011, 'grad_norm': 1.0546966791152954, 'learning_rate': 0.00012982720198283825, 'epoch': 0.46}
{'loss': 0.7767, 'grad_norm': 1.2048611640930176, 'learning_rate': 0.00012977593973205175, 'epoch': 0.46}
{'loss': 0.8294, 'grad_norm': 1.2145041227340698, 'learning_rate': 0.00012972466889402763, 'epoch': 0.46}
{'loss': 1.097, 'grad_norm': 1.1461399793624878, 'learning_rate': 0.00012967338948355217, 'epoch': 0.46}
{'loss': 1.0709, 'grad_norm': 1.240134596824646, 'learning_rate': 0.00012962210151541413, 'epoch': 0.46}
{'loss': 0.8794, 'grad_norm': 1.104015588760376, 'learning_rate': 0.00012957080500440468, 'epoch': 0.46}
{'loss': 0.8027, 'grad_norm': 1.3317499160766602, 'learning_rate': 0.0001295194999653175, 'epoch': 0.46}
{'loss': 1.0818, 'grad_norm': 1.1499650478363037, 'learning_rate': 0.00012946818641294868, 'epoch': 0.46}
{'loss': 1.0225, 'grad_norm': 1.2761213779449463, 'learning_rate': 0.00012941686436209688, 'epoch': 0.46}
{'loss': 0.7848, 'grad_norm': 1.001186728477478, 'learning_rate': 0.0001293655338275631, 'epoch': 0.46}
{'loss': 0.9668, 'grad_norm': 1.1703132390975952, 'learning_rate': 0.00012931419482415077, 'epoch': 0.46}
{'loss': 0.9496, 'grad_norm': 1.3337353467941284, 'learning_rate': 0.00012926284736666585, 'epoch': 0.46}
{'loss': 1.0007, 'grad_norm': 0.9666364192962646, 'learning_rate': 0.0001292114914699167, 'epoch': 0.47}
{'loss': 0.8659, 'grad_norm': 1.2396951913833618, 'learning_rate': 0.0001291601271487141, 'epoch': 0.47}
{'loss': 0.7737, 'grad_norm': 1.1416720151901245, 'learning_rate': 0.00012910875441787128, 'epoch': 0.47}
{'loss': 0.9045, 'grad_norm': 1.2998473644256592, 'learning_rate': 0.00012905737329220392, 'epoch': 0.47}
{'loss': 1.342, 'grad_norm': 0.9412372708320618, 'learning_rate': 0.00012900598378653007, 'epoch': 0.47}
{'loss': 0.7757, 'grad_norm': 1.208727240562439, 'learning_rate': 0.00012895458591567015, 'epoch': 0.47}
{'loss': 0.9595, 'grad_norm': 1.0145536661148071, 'learning_rate': 0.00012890317969444716, 'epoch': 0.47}
{'loss': 1.0783, 'grad_norm': 1.110945224761963, 'learning_rate': 0.00012885176513768637, 'epoch': 0.47}
{'loss': 0.7756, 'grad_norm': 0.9323983788490295, 'learning_rate': 0.00012880034226021546, 'epoch': 0.47}
{'loss': 0.9868, 'grad_norm': 1.0930640697479248, 'learning_rate': 0.00012874891107686457, 'epoch': 0.47}
{'loss': 1.1444, 'grad_norm': 0.9506102800369263, 'learning_rate': 0.00012869747160246616, 'epoch': 0.47}
{'loss': 1.0842, 'grad_norm': 1.0785833597183228, 'learning_rate': 0.0001286460238518552, 'epoch': 0.47}
{'loss': 0.8289, 'grad_norm': 1.2418971061706543, 'learning_rate': 0.00012859456783986893, 'epoch': 0.47}
{'loss': 0.8508, 'grad_norm': 1.3781791925430298, 'learning_rate': 0.00012854310358134704, 'epoch': 0.47}
{'loss': 0.6294, 'grad_norm': 1.1181581020355225, 'learning_rate': 0.0001284916310911315, 'epoch': 0.47}
{'loss': 0.9004, 'grad_norm': 1.3915311098098755, 'learning_rate': 0.00012844015038406684, 'epoch': 0.47}
{'loss': 1.1247, 'grad_norm': 1.1453546285629272, 'learning_rate': 0.0001283886614749998, 'epoch': 0.47}
{'loss': 0.7865, 'grad_norm': 1.561699628829956, 'learning_rate': 0.0001283371643787795, 'epoch': 0.47}
{'loss': 1.1637, 'grad_norm': 1.5345677137374878, 'learning_rate': 0.0001282856591102575, 'epoch': 0.47}
{'loss': 1.0051, 'grad_norm': 0.8778609037399292, 'learning_rate': 0.00012823414568428768, 'epoch': 0.47}
{'loss': 0.8768, 'grad_norm': 0.990182101726532, 'learning_rate': 0.00012818262411572618, 'epoch': 0.47}
{'loss': 1.0723, 'grad_norm': 1.3425474166870117, 'learning_rate': 0.00012813109441943166, 'epoch': 0.47}
{'loss': 1.0033, 'grad_norm': 1.1821227073669434, 'learning_rate': 0.00012807955661026498, 'epoch': 0.47}
{'loss': 0.9401, 'grad_norm': 1.2801676988601685, 'learning_rate': 0.00012802801070308946, 'epoch': 0.47}
{'loss': 1.1273, 'grad_norm': 0.8979698419570923, 'learning_rate': 0.0001279764567127706, 'epoch': 0.47}
{'loss': 0.9922, 'grad_norm': 1.3797270059585571, 'learning_rate': 0.00012792489465417644, 'epoch': 0.47}
{'loss': 1.0727, 'grad_norm': 1.2307772636413574, 'learning_rate': 0.00012787332454217712, 'epoch': 0.47}
{'loss': 0.6752, 'grad_norm': 1.022703766822815, 'learning_rate': 0.0001278217463916453, 'epoch': 0.47}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0484, 'grad_norm': 1.448840856552124, 'learning_rate': 0.0001277701602174558, 'epoch': 0.47}
{'loss': 0.7662, 'grad_norm': 1.1951106786727905, 'learning_rate': 0.00012771856603448583, 'epoch': 0.47}
{'loss': 0.838, 'grad_norm': 1.2840090990066528, 'learning_rate': 0.00012766696385761494, 'epoch': 0.47}
{'loss': 1.1315, 'grad_norm': 1.3485625982284546, 'learning_rate': 0.00012761535370172492, 'epoch': 0.47}
{'loss': 0.9708, 'grad_norm': 1.0748887062072754, 'learning_rate': 0.0001275637355816999, 'epoch': 0.47}
{'loss': 0.987, 'grad_norm': 0.9473265409469604, 'learning_rate': 0.00012751210951242637, 'epoch': 0.47}
{'loss': 0.7374, 'grad_norm': 6.361540794372559, 'learning_rate': 0.0001274604755087929, 'epoch': 0.47}
{'loss': 0.7385, 'grad_norm': 1.382321834564209, 'learning_rate': 0.00012740883358569057, 'epoch': 0.47}
{'loss': 1.0006, 'grad_norm': 1.1236270666122437, 'learning_rate': 0.0001273571837580127, 'epoch': 0.47}
{'loss': 0.6584, 'grad_norm': 1.007182002067566, 'learning_rate': 0.00012730552604065475, 'epoch': 0.47}
{'loss': 1.3117, 'grad_norm': 1.1272112131118774, 'learning_rate': 0.00012725386044851463, 'epoch': 0.47}
{'loss': 1.0153, 'grad_norm': 1.039381980895996, 'learning_rate': 0.00012720218699649243, 'epoch': 0.47}
{'loss': 1.1372, 'grad_norm': 0.9902297854423523, 'learning_rate': 0.00012715050569949053, 'epoch': 0.47}
{'loss': 1.004, 'grad_norm': 1.1787692308425903, 'learning_rate': 0.00012709881657241355, 'epoch': 0.47}
{'loss': 0.8954, 'grad_norm': 1.1032042503356934, 'learning_rate': 0.0001270471196301684, 'epoch': 0.47}
{'loss': 1.1055, 'grad_norm': 2.265416383743286, 'learning_rate': 0.0001269954148876642, 'epoch': 0.47}
{'loss': 0.9447, 'grad_norm': 1.3871335983276367, 'learning_rate': 0.00012694370235981237, 'epoch': 0.47}
{'loss': 0.7564, 'grad_norm': 1.336187720298767, 'learning_rate': 0.00012689198206152657, 'epoch': 0.47}
{'loss': 1.274, 'grad_norm': 1.046910285949707, 'learning_rate': 0.00012684025400772268, 'epoch': 0.47}
{'loss': 1.3652, 'grad_norm': 1.430264949798584, 'learning_rate': 0.00012678851821331882, 'epoch': 0.47}
{'loss': 1.0435, 'grad_norm': 1.1193639039993286, 'learning_rate': 0.0001267367746932353, 'epoch': 0.47}
{'loss': 1.0207, 'grad_norm': 1.6010431051254272, 'learning_rate': 0.00012668502346239478, 'epoch': 0.47}
{'loss': 0.9061, 'grad_norm': 1.0764944553375244, 'learning_rate': 0.00012663326453572201, 'epoch': 0.47}
{'loss': 0.9334, 'grad_norm': 1.1846903562545776, 'learning_rate': 0.00012658149792814404, 'epoch': 0.47}
{'loss': 0.8562, 'grad_norm': 1.1215232610702515, 'learning_rate': 0.0001265297236545901, 'epoch': 0.47}
{'loss': 1.1268, 'grad_norm': 1.0287584066390991, 'learning_rate': 0.00012647794172999162, 'epoch': 0.47}
{'loss': 1.3883, 'grad_norm': 1.7151224613189697, 'learning_rate': 0.00012642615216928226, 'epoch': 0.47}
{'loss': 0.8283, 'grad_norm': 1.1027451753616333, 'learning_rate': 0.00012637435498739794, 'epoch': 0.47}
{'loss': 0.8468, 'grad_norm': 0.9591894745826721, 'learning_rate': 0.00012632255019927668, 'epoch': 0.47}
{'loss': 0.8017, 'grad_norm': 1.2710237503051758, 'learning_rate': 0.0001262707378198587, 'epoch': 0.47}
{'loss': 0.7919, 'grad_norm': 1.0640889406204224, 'learning_rate': 0.00012621891786408648, 'epoch': 0.47}
{'loss': 1.1353, 'grad_norm': 1.088454008102417, 'learning_rate': 0.00012616709034690463, 'epoch': 0.47}
{'loss': 0.8292, 'grad_norm': 1.2914787530899048, 'learning_rate': 0.00012611525528326, 'epoch': 0.47}
{'loss': 1.2891, 'grad_norm': 0.9883185625076294, 'learning_rate': 0.00012606341268810145, 'epoch': 0.47}
{'loss': 0.8985, 'grad_norm': 1.1711735725402832, 'learning_rate': 0.0001260115625763803, 'epoch': 0.47}
{'loss': 0.9206, 'grad_norm': 1.0124598741531372, 'learning_rate': 0.00012595970496304975, 'epoch': 0.47}
{'loss': 0.7872, 'grad_norm': 0.9167510867118835, 'learning_rate': 0.00012590783986306532, 'epoch': 0.47}
{'loss': 0.9043, 'grad_norm': 1.1014442443847656, 'learning_rate': 0.00012585596729138468, 'epoch': 0.48}
{'loss': 0.83, 'grad_norm': 1.23225998878479, 'learning_rate': 0.0001258040872629676, 'epoch': 0.48}
{'loss': 1.1497, 'grad_norm': 1.177923560142517, 'learning_rate': 0.00012575219979277602, 'epoch': 0.48}
{'loss': 1.2857, 'grad_norm': 1.1989567279815674, 'learning_rate': 0.0001257003048957741, 'epoch': 0.48}
{'loss': 1.0485, 'grad_norm': 1.262120008468628, 'learning_rate': 0.00012564840258692803, 'epoch': 0.48}
{'loss': 1.0059, 'grad_norm': 1.256557822227478, 'learning_rate': 0.00012559649288120618, 'epoch': 0.48}
{'loss': 1.3118, 'grad_norm': 1.483082890510559, 'learning_rate': 0.00012554457579357905, 'epoch': 0.48}
{'loss': 0.939, 'grad_norm': 1.1443760395050049, 'learning_rate': 0.00012549265133901934, 'epoch': 0.48}
{'loss': 0.9829, 'grad_norm': 1.8658397197723389, 'learning_rate': 0.00012544071953250177, 'epoch': 0.48}
{'loss': 1.1678, 'grad_norm': 1.1206655502319336, 'learning_rate': 0.0001253887803890032, 'epoch': 0.48}
{'loss': 0.9185, 'grad_norm': 0.9531997442245483, 'learning_rate': 0.00012533683392350263, 'epoch': 0.48}
{'loss': 1.3903, 'grad_norm': 0.9687807559967041, 'learning_rate': 0.00012528488015098126, 'epoch': 0.48}
{'loss': 0.7344, 'grad_norm': 1.0198386907577515, 'learning_rate': 0.00012523291908642217, 'epoch': 0.48}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.04, 'grad_norm': 1.352378010749817, 'learning_rate': 0.0001251809507448108, 'epoch': 0.48}
{'loss': 0.9591, 'grad_norm': 1.1480261087417603, 'learning_rate': 0.00012512897514113452, 'epoch': 0.48}
{'loss': 0.7836, 'grad_norm': 1.016037106513977, 'learning_rate': 0.00012507699229038284, 'epoch': 0.48}
{'loss': 0.7745, 'grad_norm': 1.1796705722808838, 'learning_rate': 0.00012502500220754737, 'epoch': 0.48}
{'loss': 1.1611, 'grad_norm': 1.0546647310256958, 'learning_rate': 0.0001249730049076218, 'epoch': 0.48}
{'loss': 0.8384, 'grad_norm': 1.6965585947036743, 'learning_rate': 0.00012492100040560188, 'epoch': 0.48}
{'loss': 0.9244, 'grad_norm': 1.2466318607330322, 'learning_rate': 0.0001248689887164855, 'epoch': 0.48}
{'loss': 1.1739, 'grad_norm': 0.985831081867218, 'learning_rate': 0.00012481696985527254, 'epoch': 0.48}
{'loss': 0.8177, 'grad_norm': 1.1284732818603516, 'learning_rate': 0.00012476494383696502, 'epoch': 0.48}
{'loss': 0.8631, 'grad_norm': 1.1150363683700562, 'learning_rate': 0.00012471291067656697, 'epoch': 0.48}
{'loss': 0.8939, 'grad_norm': 1.1011438369750977, 'learning_rate': 0.00012466087038908453, 'epoch': 0.48}
{'loss': 1.1372, 'grad_norm': 1.0231235027313232, 'learning_rate': 0.00012460882298952582, 'epoch': 0.48}
{'loss': 0.9385, 'grad_norm': 1.2197341918945312, 'learning_rate': 0.00012455676849290113, 'epoch': 0.48}
{'loss': 1.1339, 'grad_norm': 1.2566357851028442, 'learning_rate': 0.00012450470691422266, 'epoch': 0.48}
{'loss': 1.1904, 'grad_norm': 0.9885742664337158, 'learning_rate': 0.00012445263826850475, 'epoch': 0.48}
{'loss': 0.8978, 'grad_norm': 1.1276317834854126, 'learning_rate': 0.00012440056257076375, 'epoch': 0.48}
{'loss': 0.8256, 'grad_norm': 1.6444687843322754, 'learning_rate': 0.00012434847983601804, 'epoch': 0.48}
{'loss': 1.0707, 'grad_norm': 1.0153818130493164, 'learning_rate': 0.000124296390079288, 'epoch': 0.48}
{'loss': 0.7198, 'grad_norm': 1.1472710371017456, 'learning_rate': 0.0001242442933155961, 'epoch': 0.48}
{'loss': 0.9145, 'grad_norm': 1.282949447631836, 'learning_rate': 0.00012419218955996676, 'epoch': 0.48}
{'loss': 1.1929, 'grad_norm': 0.908151388168335, 'learning_rate': 0.0001241400788274265, 'epoch': 0.48}
{'loss': 0.8824, 'grad_norm': 1.0779024362564087, 'learning_rate': 0.0001240879611330038, 'epoch': 0.48}
{'loss': 1.137, 'grad_norm': 1.3101288080215454, 'learning_rate': 0.0001240358364917291, 'epoch': 0.48}
{'loss': 1.0017, 'grad_norm': 1.1573439836502075, 'learning_rate': 0.00012398370491863495, 'epoch': 0.48}
{'loss': 1.0002, 'grad_norm': 1.0897748470306396, 'learning_rate': 0.0001239315664287558, 'epoch': 0.48}
{'loss': 1.1551, 'grad_norm': 1.092637538909912, 'learning_rate': 0.00012387942103712815, 'epoch': 0.48}
{'loss': 0.8617, 'grad_norm': 1.4450856447219849, 'learning_rate': 0.00012382726875879052, 'epoch': 0.48}
{'loss': 1.1318, 'grad_norm': 1.1249810457229614, 'learning_rate': 0.00012377510960878333, 'epoch': 0.48}
{'loss': 1.1316, 'grad_norm': 1.091341495513916, 'learning_rate': 0.00012372294360214905, 'epoch': 0.48}
{'loss': 1.0181, 'grad_norm': 0.9848177433013916, 'learning_rate': 0.0001236707707539321, 'epoch': 0.48}
{'loss': 0.9835, 'grad_norm': 1.0789222717285156, 'learning_rate': 0.00012361859107917886, 'epoch': 0.48}
{'loss': 0.9586, 'grad_norm': 1.250697374343872, 'learning_rate': 0.00012356640459293771, 'epoch': 0.48}
{'loss': 0.9776, 'grad_norm': 1.477993369102478, 'learning_rate': 0.000123514211310259, 'epoch': 0.48}
{'loss': 0.8362, 'grad_norm': 1.1698344945907593, 'learning_rate': 0.00012346201124619502, 'epoch': 0.48}
{'loss': 0.8962, 'grad_norm': 0.9996578097343445, 'learning_rate': 0.0001234098044158, 'epoch': 0.48}
{'loss': 0.8958, 'grad_norm': 1.1534759998321533, 'learning_rate': 0.00012335759083413015, 'epoch': 0.48}
{'loss': 0.7525, 'grad_norm': 1.3606749773025513, 'learning_rate': 0.00012330537051624357, 'epoch': 0.48}
{'loss': 1.1321, 'grad_norm': 1.0440969467163086, 'learning_rate': 0.0001232531434772004, 'epoch': 0.48}
{'loss': 0.9393, 'grad_norm': 1.0300724506378174, 'learning_rate': 0.0001232009097320627, 'epoch': 0.48}
{'loss': 0.8894, 'grad_norm': 1.3821252584457397, 'learning_rate': 0.00012314866929589432, 'epoch': 0.48}
{'loss': 0.8869, 'grad_norm': 1.1257226467132568, 'learning_rate': 0.00012309642218376123, 'epoch': 0.48}
{'loss': 0.6751, 'grad_norm': 1.1836029291152954, 'learning_rate': 0.0001230441684107312, 'epoch': 0.48}
{'loss': 0.8966, 'grad_norm': 0.9235190749168396, 'learning_rate': 0.00012299190799187405, 'epoch': 0.48}
{'loss': 0.9257, 'grad_norm': 1.3531568050384521, 'learning_rate': 0.00012293964094226137, 'epoch': 0.48}
{'loss': 1.0021, 'grad_norm': 1.1216856241226196, 'learning_rate': 0.00012288736727696672, 'epoch': 0.48}
{'loss': 0.8591, 'grad_norm': 0.9724230766296387, 'learning_rate': 0.00012283508701106557, 'epoch': 0.48}
{'loss': 1.1073, 'grad_norm': 1.2453863620758057, 'learning_rate': 0.00012278280015963535, 'epoch': 0.48}
{'loss': 0.9222, 'grad_norm': 1.1805561780929565, 'learning_rate': 0.00012273050673775529, 'epoch': 0.48}
{'loss': 0.9912, 'grad_norm': 1.5134928226470947, 'learning_rate': 0.00012267820676050656, 'epoch': 0.48}
{'loss': 0.9799, 'grad_norm': 0.9684309959411621, 'learning_rate': 0.00012262590024297225, 'epoch': 0.48}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9679, 'grad_norm': 1.3601359128952026, 'learning_rate': 0.00012257358720023723, 'epoch': 0.48}
{'loss': 0.9803, 'grad_norm': 1.044985055923462, 'learning_rate': 0.00012252126764738844, 'epoch': 0.48}
{'loss': 1.0016, 'grad_norm': 1.2938965559005737, 'learning_rate': 0.0001224689415995145, 'epoch': 0.49}
{'loss': 0.9825, 'grad_norm': 1.1929863691329956, 'learning_rate': 0.00012241660907170608, 'epoch': 0.49}
{'loss': 0.9295, 'grad_norm': 1.019885540008545, 'learning_rate': 0.00012236427007905558, 'epoch': 0.49}
{'loss': 0.7397, 'grad_norm': 1.2411062717437744, 'learning_rate': 0.00012231192463665725, 'epoch': 0.49}
{'loss': 1.1556, 'grad_norm': 1.4655625820159912, 'learning_rate': 0.00012225957275960734, 'epoch': 0.49}
{'loss': 1.0186, 'grad_norm': 1.1393921375274658, 'learning_rate': 0.0001222072144630039, 'epoch': 0.49}
{'loss': 0.8598, 'grad_norm': 1.594628095626831, 'learning_rate': 0.00012215484976194676, 'epoch': 0.49}
{'loss': 0.8167, 'grad_norm': 1.0016835927963257, 'learning_rate': 0.00012210247867153765, 'epoch': 0.49}
{'loss': 0.828, 'grad_norm': 0.9706876873970032, 'learning_rate': 0.00012205010120688011, 'epoch': 0.49}
{'loss': 1.2225, 'grad_norm': 0.959183394908905, 'learning_rate': 0.00012199771738307963, 'epoch': 0.49}
{'loss': 0.9443, 'grad_norm': 1.1443625688552856, 'learning_rate': 0.00012194532721524341, 'epoch': 0.49}
{'loss': 0.9184, 'grad_norm': 1.0922274589538574, 'learning_rate': 0.00012189293071848051, 'epoch': 0.49}
{'loss': 0.9794, 'grad_norm': 1.0670057535171509, 'learning_rate': 0.00012184052790790184, 'epoch': 0.49}
{'loss': 0.9337, 'grad_norm': 1.0094648599624634, 'learning_rate': 0.00012178811879862013, 'epoch': 0.49}
{'loss': 0.8451, 'grad_norm': 1.351260781288147, 'learning_rate': 0.00012173570340574992, 'epoch': 0.49}
{'loss': 1.0678, 'grad_norm': 1.1426465511322021, 'learning_rate': 0.00012168328174440753, 'epoch': 0.49}
{'loss': 1.1576, 'grad_norm': 1.1538658142089844, 'learning_rate': 0.00012163085382971112, 'epoch': 0.49}
{'loss': 0.8821, 'grad_norm': 1.0409412384033203, 'learning_rate': 0.00012157841967678063, 'epoch': 0.49}
{'loss': 1.0829, 'grad_norm': 1.3946797847747803, 'learning_rate': 0.00012152597930073786, 'epoch': 0.49}
{'loss': 1.0119, 'grad_norm': 1.0111883878707886, 'learning_rate': 0.00012147353271670634, 'epoch': 0.49}
{'loss': 0.9459, 'grad_norm': 1.142560601234436, 'learning_rate': 0.0001214210799398114, 'epoch': 0.49}
{'loss': 0.8763, 'grad_norm': 1.2627791166305542, 'learning_rate': 0.00012136862098518021, 'epoch': 0.49}
{'loss': 0.9912, 'grad_norm': 0.979599118232727, 'learning_rate': 0.0001213161558679416, 'epoch': 0.49}
{'loss': 0.9892, 'grad_norm': 1.0589120388031006, 'learning_rate': 0.00012126368460322637, 'epoch': 0.49}
{'loss': 0.9023, 'grad_norm': 1.7429702281951904, 'learning_rate': 0.0001212112072061669, 'epoch': 0.49}
{'loss': 1.0106, 'grad_norm': 1.469398856163025, 'learning_rate': 0.00012115872369189742, 'epoch': 0.49}
{'loss': 1.308, 'grad_norm': 1.2495851516723633, 'learning_rate': 0.00012110623407555397, 'epoch': 0.49}
{'loss': 0.9588, 'grad_norm': 1.2439266443252563, 'learning_rate': 0.00012105373837227425, 'epoch': 0.49}
{'loss': 0.9525, 'grad_norm': 1.338539481163025, 'learning_rate': 0.0001210012365971978, 'epoch': 0.49}
{'loss': 1.0319, 'grad_norm': 0.99088054895401, 'learning_rate': 0.00012094872876546586, 'epoch': 0.49}
{'loss': 1.0176, 'grad_norm': 1.1739376783370972, 'learning_rate': 0.00012089621489222147, 'epoch': 0.49}
{'loss': 1.0286, 'grad_norm': 1.1694828271865845, 'learning_rate': 0.00012084369499260933, 'epoch': 0.49}
{'loss': 0.9553, 'grad_norm': 1.1623295545578003, 'learning_rate': 0.00012079116908177593, 'epoch': 0.49}
{'loss': 1.1483, 'grad_norm': 1.1745589971542358, 'learning_rate': 0.00012073863717486956, 'epoch': 0.49}
{'loss': 0.983, 'grad_norm': 1.4988934993743896, 'learning_rate': 0.00012068609928704009, 'epoch': 0.49}
{'loss': 0.8476, 'grad_norm': 1.1667585372924805, 'learning_rate': 0.00012063355543343924, 'epoch': 0.49}
{'loss': 1.037, 'grad_norm': 1.3411574363708496, 'learning_rate': 0.00012058100562922035, 'epoch': 0.49}
{'loss': 1.1295, 'grad_norm': 1.7170524597167969, 'learning_rate': 0.0001205284498895386, 'epoch': 0.49}
{'loss': 0.7611, 'grad_norm': 1.2334485054016113, 'learning_rate': 0.00012047588822955076, 'epoch': 0.49}
{'loss': 0.9812, 'grad_norm': 0.9464969038963318, 'learning_rate': 0.0001204233206644154, 'epoch': 0.49}
{'loss': 1.1722, 'grad_norm': 1.2606593370437622, 'learning_rate': 0.00012037074720929273, 'epoch': 0.49}
{'loss': 1.2423, 'grad_norm': 1.137963056564331, 'learning_rate': 0.00012031816787934464, 'epoch': 0.49}
{'loss': 1.0424, 'grad_norm': 1.1069811582565308, 'learning_rate': 0.00012026558268973484, 'epoch': 0.49}
{'loss': 1.0281, 'grad_norm': 1.0159825086593628, 'learning_rate': 0.00012021299165562858, 'epoch': 0.49}
{'loss': 0.9267, 'grad_norm': 1.3508909940719604, 'learning_rate': 0.00012016039479219292, 'epoch': 0.49}
{'loss': 0.8321, 'grad_norm': 1.036104679107666, 'learning_rate': 0.00012010779211459648, 'epoch': 0.49}
{'loss': 0.9474, 'grad_norm': 1.2502912282943726, 'learning_rate': 0.00012005518363800963, 'epoch': 0.49}
{'loss': 0.8376, 'grad_norm': 1.0129296779632568, 'learning_rate': 0.00012000256937760445, 'epoch': 0.49}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2431, 'grad_norm': 1.1512094736099243, 'learning_rate': 0.00011994994934855459, 'epoch': 0.49}
{'loss': 0.8829, 'grad_norm': 1.3006603717803955, 'learning_rate': 0.00011989732356603545, 'epoch': 0.49}
{'loss': 0.8715, 'grad_norm': 1.0995818376541138, 'learning_rate': 0.000119844692045224, 'epoch': 0.49}
{'loss': 0.7473, 'grad_norm': 1.208656907081604, 'learning_rate': 0.00011979205480129896, 'epoch': 0.49}
{'loss': 1.0677, 'grad_norm': 1.0948790311813354, 'learning_rate': 0.00011973941184944066, 'epoch': 0.49}
{'loss': 1.2523, 'grad_norm': 1.2532217502593994, 'learning_rate': 0.00011968676320483103, 'epoch': 0.49}
{'loss': 1.153, 'grad_norm': 0.9084911942481995, 'learning_rate': 0.00011963410888265373, 'epoch': 0.49}
{'loss': 0.8752, 'grad_norm': 1.143198013305664, 'learning_rate': 0.00011958144889809397, 'epoch': 0.49}
{'loss': 0.8188, 'grad_norm': 1.2373836040496826, 'learning_rate': 0.00011952878326633872, 'epoch': 0.49}
{'loss': 0.733, 'grad_norm': 1.199569582939148, 'learning_rate': 0.0001194761120025764, 'epoch': 0.49}
{'loss': 0.9395, 'grad_norm': 1.230650544166565, 'learning_rate': 0.0001194234351219972, 'epoch': 0.49}
{'loss': 0.6422, 'grad_norm': 1.3026412725448608, 'learning_rate': 0.00011937075263979289, 'epoch': 0.49}
{'loss': 0.8746, 'grad_norm': 1.2332202196121216, 'learning_rate': 0.00011931806457115677, 'epoch': 0.49}
{'loss': 1.3088, 'grad_norm': 1.2334520816802979, 'learning_rate': 0.00011926537093128392, 'epoch': 0.49}
{'loss': 1.0573, 'grad_norm': 1.485343098640442, 'learning_rate': 0.00011921267173537086, 'epoch': 0.49}
{'loss': 1.4349, 'grad_norm': 1.3195736408233643, 'learning_rate': 0.0001191599669986158, 'epoch': 0.49}
{'loss': 0.8523, 'grad_norm': 1.0660947561264038, 'learning_rate': 0.00011910725673621856, 'epoch': 0.49}
{'loss': 0.9723, 'grad_norm': 0.9686815142631531, 'learning_rate': 0.00011905454096338048, 'epoch': 0.5}
{'loss': 0.7942, 'grad_norm': 1.264201283454895, 'learning_rate': 0.00011900181969530461, 'epoch': 0.5}
{'loss': 0.6409, 'grad_norm': 1.6754562854766846, 'learning_rate': 0.00011894909294719547, 'epoch': 0.5}
{'loss': 0.7663, 'grad_norm': 1.1746017932891846, 'learning_rate': 0.00011889636073425918, 'epoch': 0.5}
{'loss': 1.0209, 'grad_norm': 1.2950385808944702, 'learning_rate': 0.00011884362307170344, 'epoch': 0.5}
{'loss': 0.9348, 'grad_norm': 1.0982517004013062, 'learning_rate': 0.00011879087997473759, 'epoch': 0.5}
{'loss': 1.0389, 'grad_norm': 1.2907931804656982, 'learning_rate': 0.00011873813145857249, 'epoch': 0.5}
{'loss': 0.9877, 'grad_norm': 1.380849838256836, 'learning_rate': 0.00011868537753842051, 'epoch': 0.5}
{'loss': 0.8838, 'grad_norm': 1.1372750997543335, 'learning_rate': 0.00011863261822949563, 'epoch': 0.5}
{'loss': 1.0419, 'grad_norm': 1.3833400011062622, 'learning_rate': 0.0001185798535470134, 'epoch': 0.5}
{'loss': 1.1496, 'grad_norm': 0.9381698369979858, 'learning_rate': 0.0001185270835061909, 'epoch': 0.5}
{'loss': 1.2905, 'grad_norm': 0.9131689667701721, 'learning_rate': 0.00011847430812224678, 'epoch': 0.5}
{'loss': 0.9666, 'grad_norm': 1.1964812278747559, 'learning_rate': 0.00011842152741040116, 'epoch': 0.5}
{'loss': 0.8261, 'grad_norm': 1.436515212059021, 'learning_rate': 0.00011836874138587579, 'epoch': 0.5}
{'loss': 1.0923, 'grad_norm': 1.021054983139038, 'learning_rate': 0.00011831595006389384, 'epoch': 0.5}
{'loss': 0.9651, 'grad_norm': 1.2278579473495483, 'learning_rate': 0.00011826315345968013, 'epoch': 0.5}
{'loss': 1.116, 'grad_norm': 1.2006902694702148, 'learning_rate': 0.00011821035158846095, 'epoch': 0.5}
{'loss': 0.9291, 'grad_norm': 1.1547565460205078, 'learning_rate': 0.00011815754446546405, 'epoch': 0.5}
{'loss': 0.7773, 'grad_norm': 1.1038299798965454, 'learning_rate': 0.00011810473210591881, 'epoch': 0.5}
{'loss': 1.1372, 'grad_norm': 1.2407584190368652, 'learning_rate': 0.00011805191452505602, 'epoch': 0.5}
{'loss': 1.0829, 'grad_norm': 1.2043044567108154, 'learning_rate': 0.00011799909173810802, 'epoch': 0.5}
{'loss': 0.9876, 'grad_norm': 1.4252744913101196, 'learning_rate': 0.00011794626376030866, 'epoch': 0.5}
{'loss': 1.0363, 'grad_norm': 1.4728257656097412, 'learning_rate': 0.00011789343060689329, 'epoch': 0.5}
{'loss': 1.0808, 'grad_norm': 1.0975092649459839, 'learning_rate': 0.00011784059229309869, 'epoch': 0.5}
{'loss': 0.7793, 'grad_norm': 1.2551723718643188, 'learning_rate': 0.00011778774883416323, 'epoch': 0.5}
{'loss': 1.2652, 'grad_norm': 1.2921277284622192, 'learning_rate': 0.0001177349002453267, 'epoch': 0.5}
{'loss': 1.0081, 'grad_norm': 1.3414350748062134, 'learning_rate': 0.00011768204654183033, 'epoch': 0.5}
{'loss': 0.9445, 'grad_norm': 1.2283027172088623, 'learning_rate': 0.00011762918773891691, 'epoch': 0.5}
{'loss': 1.1467, 'grad_norm': 1.811097264289856, 'learning_rate': 0.00011757632385183063, 'epoch': 0.5}
{'loss': 0.8884, 'grad_norm': 1.1672109365463257, 'learning_rate': 0.00011752345489581727, 'epoch': 0.5}
{'loss': 1.1953, 'grad_norm': 1.1040372848510742, 'learning_rate': 0.00011747058088612388, 'epoch': 0.5}
{'loss': 1.3083, 'grad_norm': 1.2102389335632324, 'learning_rate': 0.00011741770183799911, 'epoch': 0.5}
{'loss': 1.2216, 'grad_norm': 1.1398061513900757, 'learning_rate': 0.00011736481776669306, 'epoch': 0.5}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8543, 'grad_norm': 0.9943233728408813, 'learning_rate': 0.00011731192868745715, 'epoch': 0.5}
{'loss': 1.0446, 'grad_norm': 1.1223863363265991, 'learning_rate': 0.00011725903461554444, 'epoch': 0.5}
{'loss': 1.1777, 'grad_norm': 0.9209690690040588, 'learning_rate': 0.00011720613556620925, 'epoch': 0.5}
{'loss': 0.8904, 'grad_norm': 1.1752077341079712, 'learning_rate': 0.00011715323155470745, 'epoch': 0.5}
{'loss': 1.0243, 'grad_norm': 1.0490812063217163, 'learning_rate': 0.0001171003225962963, 'epoch': 0.5}
{'loss': 0.8956, 'grad_norm': 1.1520394086837769, 'learning_rate': 0.00011704740870623446, 'epoch': 0.5}
{'loss': 1.256, 'grad_norm': 1.1791619062423706, 'learning_rate': 0.00011699448989978208, 'epoch': 0.5}
{'loss': 0.6626, 'grad_norm': 1.3343751430511475, 'learning_rate': 0.00011694156619220064, 'epoch': 0.5}
{'loss': 1.0625, 'grad_norm': 1.3559411764144897, 'learning_rate': 0.00011688863759875313, 'epoch': 0.5}
{'loss': 1.0026, 'grad_norm': 1.6993106603622437, 'learning_rate': 0.00011683570413470383, 'epoch': 0.5}
{'loss': 1.0347, 'grad_norm': 1.1290751695632935, 'learning_rate': 0.00011678276581531861, 'epoch': 0.5}
{'loss': 0.9116, 'grad_norm': 1.244343876838684, 'learning_rate': 0.00011672982265586456, 'epoch': 0.5}
{'loss': 1.1165, 'grad_norm': 1.0622681379318237, 'learning_rate': 0.00011667687467161024, 'epoch': 0.5}
{'loss': 1.059, 'grad_norm': 1.0226478576660156, 'learning_rate': 0.00011662392187782558, 'epoch': 0.5}
{'loss': 1.3336, 'grad_norm': 2.126009941101074, 'learning_rate': 0.00011657096428978191, 'epoch': 0.5}
{'loss': 1.0817, 'grad_norm': 1.013622760772705, 'learning_rate': 0.00011651800192275197, 'epoch': 0.5}
{'loss': 1.1684, 'grad_norm': 1.2081800699234009, 'learning_rate': 0.00011646503479200983, 'epoch': 0.5}
{'loss': 1.0036, 'grad_norm': 1.3403902053833008, 'learning_rate': 0.00011641206291283097, 'epoch': 0.5}
{'loss': 0.8154, 'grad_norm': 1.202923059463501, 'learning_rate': 0.0001163590863004922, 'epoch': 0.5}
{'loss': 1.1182, 'grad_norm': 1.0211215019226074, 'learning_rate': 0.00011630610497027174, 'epoch': 0.5}
{'loss': 1.0472, 'grad_norm': 1.2337281703948975, 'learning_rate': 0.00011625311893744912, 'epoch': 0.5}
{'loss': 0.9164, 'grad_norm': 1.0485738515853882, 'learning_rate': 0.00011620012821730532, 'epoch': 0.5}
{'loss': 0.9444, 'grad_norm': 1.1722352504730225, 'learning_rate': 0.00011614713282512259, 'epoch': 0.5}
{'loss': 1.1503, 'grad_norm': 1.1318072080612183, 'learning_rate': 0.00011609413277618449, 'epoch': 0.5}
{'loss': 0.6866, 'grad_norm': 1.249970555305481, 'learning_rate': 0.00011604112808577603, 'epoch': 0.5}
{'loss': 1.079, 'grad_norm': 1.2153804302215576, 'learning_rate': 0.0001159881187691835, 'epoch': 0.5}
{'loss': 0.7751, 'grad_norm': 1.1293078660964966, 'learning_rate': 0.00011593510484169452, 'epoch': 0.5}
{'loss': 0.9003, 'grad_norm': 1.1308790445327759, 'learning_rate': 0.00011588208631859807, 'epoch': 0.5}
{'loss': 0.9908, 'grad_norm': 1.248658537864685, 'learning_rate': 0.00011582906321518438, 'epoch': 0.5}
{'loss': 1.1073, 'grad_norm': 1.4798823595046997, 'learning_rate': 0.00011577603554674514, 'epoch': 0.5}
{'loss': 0.7228, 'grad_norm': 1.2987277507781982, 'learning_rate': 0.00011572300332857322, 'epoch': 0.5}
{'loss': 1.0913, 'grad_norm': 0.9979141354560852, 'learning_rate': 0.00011566996657596287, 'epoch': 0.5}
{'loss': 1.1295, 'grad_norm': 1.1136530637741089, 'learning_rate': 0.00011561692530420965, 'epoch': 0.51}
{'loss': 0.9283, 'grad_norm': 0.9956265687942505, 'learning_rate': 0.00011556387952861036, 'epoch': 0.51}
{'loss': 1.1014, 'grad_norm': 1.4046409130096436, 'learning_rate': 0.00011551082926446322, 'epoch': 0.51}
{'loss': 0.8946, 'grad_norm': 1.0382517576217651, 'learning_rate': 0.0001154577745270676, 'epoch': 0.51}
{'loss': 1.0379, 'grad_norm': 1.3105206489562988, 'learning_rate': 0.0001154047153317243, 'epoch': 0.51}
{'loss': 1.0704, 'grad_norm': 1.0881108045578003, 'learning_rate': 0.00011535165169373527, 'epoch': 0.51}
{'loss': 1.1116, 'grad_norm': 1.1984000205993652, 'learning_rate': 0.00011529858362840382, 'epoch': 0.51}
{'loss': 0.6623, 'grad_norm': 1.0876612663269043, 'learning_rate': 0.00011524551115103454, 'epoch': 0.51}
{'loss': 0.9634, 'grad_norm': 0.9396663904190063, 'learning_rate': 0.0001151924342769333, 'epoch': 0.51}
{'loss': 0.9001, 'grad_norm': 1.4243332147598267, 'learning_rate': 0.00011513935302140717, 'epoch': 0.51}
{'loss': 1.0519, 'grad_norm': 1.0528275966644287, 'learning_rate': 0.00011508626739976454, 'epoch': 0.51}
{'loss': 1.034, 'grad_norm': 1.4446429014205933, 'learning_rate': 0.00011503317742731508, 'epoch': 0.51}
{'loss': 0.7994, 'grad_norm': 1.054992914199829, 'learning_rate': 0.00011498008311936965, 'epoch': 0.51}
{'loss': 0.882, 'grad_norm': 1.0923292636871338, 'learning_rate': 0.00011492698449124042, 'epoch': 0.51}
{'loss': 1.1787, 'grad_norm': 1.056217908859253, 'learning_rate': 0.00011487388155824075, 'epoch': 0.51}
{'loss': 0.9064, 'grad_norm': 1.1484489440917969, 'learning_rate': 0.00011482077433568525, 'epoch': 0.51}
{'loss': 1.3211, 'grad_norm': 0.976391077041626, 'learning_rate': 0.00011476766283888986, 'epoch': 0.51}
{'loss': 0.7388, 'grad_norm': 1.6636645793914795, 'learning_rate': 0.00011471454708317162, 'epoch': 0.51}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1961, 'grad_norm': 1.121532917022705, 'learning_rate': 0.00011466142708384888, 'epoch': 0.51}
{'loss': 0.9688, 'grad_norm': 1.204533338546753, 'learning_rate': 0.00011460830285624118, 'epoch': 0.51}
{'loss': 0.8897, 'grad_norm': 1.3114441633224487, 'learning_rate': 0.00011455517441566928, 'epoch': 0.51}
{'loss': 1.0276, 'grad_norm': 1.0340180397033691, 'learning_rate': 0.0001145020417774552, 'epoch': 0.51}
{'loss': 1.6244, 'grad_norm': 2.8303563594818115, 'learning_rate': 0.00011444890495692213, 'epoch': 0.51}
{'loss': 1.2533, 'grad_norm': 1.1252191066741943, 'learning_rate': 0.00011439576396939449, 'epoch': 0.51}
{'loss': 0.9995, 'grad_norm': 0.9524704217910767, 'learning_rate': 0.00011434261883019783, 'epoch': 0.51}
{'loss': 1.0629, 'grad_norm': 1.02586030960083, 'learning_rate': 0.00011428946955465895, 'epoch': 0.51}
{'loss': 0.9367, 'grad_norm': 1.0533196926116943, 'learning_rate': 0.00011423631615810593, 'epoch': 0.51}
{'loss': 1.2815, 'grad_norm': 1.2606220245361328, 'learning_rate': 0.00011418315865586788, 'epoch': 0.51}
{'loss': 1.2384, 'grad_norm': 0.9494072198867798, 'learning_rate': 0.00011412999706327521, 'epoch': 0.51}
{'loss': 0.7035, 'grad_norm': 0.9518283605575562, 'learning_rate': 0.00011407683139565941, 'epoch': 0.51}
{'loss': 0.9971, 'grad_norm': 0.8692876696586609, 'learning_rate': 0.00011402366166835325, 'epoch': 0.51}
{'loss': 0.7966, 'grad_norm': 1.2060885429382324, 'learning_rate': 0.0001139704878966906, 'epoch': 0.51}
{'loss': 1.1783, 'grad_norm': 0.9498081207275391, 'learning_rate': 0.00011391731009600654, 'epoch': 0.51}
{'loss': 1.1097, 'grad_norm': 1.0520703792572021, 'learning_rate': 0.0001138641282816373, 'epoch': 0.51}
{'loss': 1.0213, 'grad_norm': 1.1451196670532227, 'learning_rate': 0.00011381094246892022, 'epoch': 0.51}
{'loss': 1.0331, 'grad_norm': 1.0348684787750244, 'learning_rate': 0.00011375775267319386, 'epoch': 0.51}
{'loss': 1.4732, 'grad_norm': 1.1760176420211792, 'learning_rate': 0.00011370455890979793, 'epoch': 0.51}
{'loss': 1.059, 'grad_norm': 1.0138424634933472, 'learning_rate': 0.00011365136119407319, 'epoch': 0.51}
{'loss': 0.7225, 'grad_norm': 1.221985101699829, 'learning_rate': 0.00011359815954136164, 'epoch': 0.51}
{'loss': 0.9207, 'grad_norm': 1.5410726070404053, 'learning_rate': 0.00011354495396700632, 'epoch': 0.51}
{'loss': 0.8633, 'grad_norm': 1.3861496448516846, 'learning_rate': 0.00011349174448635158, 'epoch': 0.51}
{'loss': 0.5074, 'grad_norm': 1.1198827028274536, 'learning_rate': 0.0001134385311147427, 'epoch': 0.51}
{'loss': 1.0571, 'grad_norm': 2.50899338722229, 'learning_rate': 0.00011338531386752618, 'epoch': 0.51}
{'loss': 0.8929, 'grad_norm': 0.9212424755096436, 'learning_rate': 0.00011333209276004959, 'epoch': 0.51}
{'loss': 0.984, 'grad_norm': 1.0794774293899536, 'learning_rate': 0.00011327886780766168, 'epoch': 0.51}
{'loss': 1.057, 'grad_norm': 1.0730665922164917, 'learning_rate': 0.00011322563902571226, 'epoch': 0.51}
{'loss': 0.9324, 'grad_norm': 1.2188951969146729, 'learning_rate': 0.00011317240642955225, 'epoch': 0.51}
{'loss': 0.9348, 'grad_norm': 0.966239869594574, 'learning_rate': 0.00011311917003453365, 'epoch': 0.51}
{'loss': 1.0539, 'grad_norm': 1.239187240600586, 'learning_rate': 0.00011306592985600962, 'epoch': 0.51}
{'loss': 0.985, 'grad_norm': 1.0632340908050537, 'learning_rate': 0.00011301268590933434, 'epoch': 0.51}
{'loss': 1.1251, 'grad_norm': 1.494266152381897, 'learning_rate': 0.00011295943820986312, 'epoch': 0.51}
{'loss': 0.7833, 'grad_norm': 1.5129220485687256, 'learning_rate': 0.00011290618677295235, 'epoch': 0.51}
{'loss': 0.7118, 'grad_norm': 0.9404649138450623, 'learning_rate': 0.00011285293161395946, 'epoch': 0.51}
{'loss': 0.8694, 'grad_norm': 1.1793756484985352, 'learning_rate': 0.00011279967274824301, 'epoch': 0.51}
{'loss': 1.1423, 'grad_norm': 1.761014461517334, 'learning_rate': 0.0001127464101911626, 'epoch': 0.51}
{'loss': 0.9302, 'grad_norm': 1.3442834615707397, 'learning_rate': 0.00011269314395807888, 'epoch': 0.51}
{'loss': 0.8126, 'grad_norm': 1.4564770460128784, 'learning_rate': 0.00011263987406435359, 'epoch': 0.51}
{'loss': 0.8989, 'grad_norm': 1.5434110164642334, 'learning_rate': 0.00011258660052534951, 'epoch': 0.51}
{'loss': 0.6064, 'grad_norm': 1.1888221502304077, 'learning_rate': 0.00011253332335643043, 'epoch': 0.51}
{'loss': 1.3383, 'grad_norm': 1.0999386310577393, 'learning_rate': 0.0001124800425729613, 'epoch': 0.51}
{'loss': 1.2351, 'grad_norm': 1.164381742477417, 'learning_rate': 0.00011242675819030797, 'epoch': 0.51}
{'loss': 0.894, 'grad_norm': 1.015113353729248, 'learning_rate': 0.00011237347022383746, 'epoch': 0.51}
{'loss': 0.9229, 'grad_norm': 1.1040064096450806, 'learning_rate': 0.00011232017868891769, 'epoch': 0.51}
{'loss': 0.7303, 'grad_norm': 1.2924352884292603, 'learning_rate': 0.00011226688360091775, 'epoch': 0.51}
{'loss': 1.0511, 'grad_norm': 1.0609278678894043, 'learning_rate': 0.00011221358497520768, 'epoch': 0.51}
{'loss': 0.8475, 'grad_norm': 1.3124767541885376, 'learning_rate': 0.00011216028282715854, 'epoch': 0.52}
{'loss': 0.8019, 'grad_norm': 1.1074118614196777, 'learning_rate': 0.00011210697717214238, 'epoch': 0.52}
{'loss': 0.8591, 'grad_norm': 1.7933406829833984, 'learning_rate': 0.0001120536680255323, 'epoch': 0.52}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9382, 'grad_norm': 1.50741446018219, 'learning_rate': 0.00011200035540270246, 'epoch': 0.52}
{'loss': 1.2464, 'grad_norm': 1.4868961572647095, 'learning_rate': 0.0001119470393190279, 'epoch': 0.52}
{'loss': 1.0695, 'grad_norm': 1.3751788139343262, 'learning_rate': 0.00011189371978988477, 'epoch': 0.52}
{'loss': 1.2713, 'grad_norm': 1.1082512140274048, 'learning_rate': 0.00011184039683065013, 'epoch': 0.52}
{'loss': 0.9716, 'grad_norm': 0.9871633648872375, 'learning_rate': 0.00011178707045670204, 'epoch': 0.52}
{'loss': 0.9877, 'grad_norm': 1.3047138452529907, 'learning_rate': 0.00011173374068341962, 'epoch': 0.52}
{'loss': 1.0103, 'grad_norm': 1.0832825899124146, 'learning_rate': 0.00011168040752618291, 'epoch': 0.52}
{'loss': 0.9053, 'grad_norm': 1.1353185176849365, 'learning_rate': 0.00011162707100037295, 'epoch': 0.52}
{'loss': 0.9437, 'grad_norm': 1.1084096431732178, 'learning_rate': 0.00011157373112137171, 'epoch': 0.52}
{'loss': 0.7586, 'grad_norm': 1.0773721933364868, 'learning_rate': 0.00011152038790456211, 'epoch': 0.52}
{'loss': 1.0837, 'grad_norm': 1.4380111694335938, 'learning_rate': 0.00011146704136532818, 'epoch': 0.52}
{'loss': 1.0915, 'grad_norm': 1.0319163799285889, 'learning_rate': 0.00011141369151905475, 'epoch': 0.52}
{'loss': 1.0063, 'grad_norm': 1.1924355030059814, 'learning_rate': 0.00011136033838112765, 'epoch': 0.52}
{'loss': 0.9648, 'grad_norm': 0.9653791785240173, 'learning_rate': 0.0001113069819669337, 'epoch': 0.52}
{'loss': 1.1389, 'grad_norm': 1.0400248765945435, 'learning_rate': 0.00011125362229186057, 'epoch': 0.52}
{'loss': 1.1662, 'grad_norm': 1.1026426553726196, 'learning_rate': 0.00011120025937129699, 'epoch': 0.52}
{'loss': 1.0472, 'grad_norm': 1.229521632194519, 'learning_rate': 0.00011114689322063255, 'epoch': 0.52}
{'loss': 0.7487, 'grad_norm': 1.3412922620773315, 'learning_rate': 0.00011109352385525783, 'epoch': 0.52}
{'loss': 0.9956, 'grad_norm': 1.138908863067627, 'learning_rate': 0.0001110401512905642, 'epoch': 0.52}
{'loss': 0.8725, 'grad_norm': 1.2666593790054321, 'learning_rate': 0.00011098677554194417, 'epoch': 0.52}
{'loss': 1.1567, 'grad_norm': 1.26128089427948, 'learning_rate': 0.00011093339662479098, 'epoch': 0.52}
{'loss': 0.9772, 'grad_norm': 1.1479620933532715, 'learning_rate': 0.00011088001455449885, 'epoch': 0.52}
{'loss': 1.1386, 'grad_norm': 1.5371460914611816, 'learning_rate': 0.00011082662934646295, 'epoch': 0.52}
{'loss': 1.1018, 'grad_norm': 1.7103872299194336, 'learning_rate': 0.00011077324101607929, 'epoch': 0.52}
{'loss': 0.7723, 'grad_norm': 1.4735151529312134, 'learning_rate': 0.00011071984957874479, 'epoch': 0.52}
{'loss': 0.807, 'grad_norm': 1.5621376037597656, 'learning_rate': 0.00011066645504985733, 'epoch': 0.52}
{'loss': 1.3442, 'grad_norm': 1.3959407806396484, 'learning_rate': 0.00011061305744481558, 'epoch': 0.52}
{'loss': 1.0316, 'grad_norm': 1.302414894104004, 'learning_rate': 0.0001105596567790192, 'epoch': 0.52}
{'loss': 1.3462, 'grad_norm': 1.1250444650650024, 'learning_rate': 0.00011050625306786866, 'epoch': 0.52}
{'loss': 0.9146, 'grad_norm': 1.3191885948181152, 'learning_rate': 0.00011045284632676536, 'epoch': 0.52}
{'loss': 1.3353, 'grad_norm': 1.0459195375442505, 'learning_rate': 0.00011039943657111151, 'epoch': 0.52}
{'loss': 0.8851, 'grad_norm': 1.149712085723877, 'learning_rate': 0.00011034602381631026, 'epoch': 0.52}
{'loss': 0.9227, 'grad_norm': 1.0622773170471191, 'learning_rate': 0.00011029260807776556, 'epoch': 0.52}
{'loss': 0.956, 'grad_norm': 0.9245861172676086, 'learning_rate': 0.00011023918937088224, 'epoch': 0.52}
{'loss': 0.8787, 'grad_norm': 1.2038049697875977, 'learning_rate': 0.00011018576771106605, 'epoch': 0.52}
{'loss': 0.859, 'grad_norm': 1.2898199558258057, 'learning_rate': 0.00011013234311372353, 'epoch': 0.52}
{'loss': 0.7845, 'grad_norm': 1.1681829690933228, 'learning_rate': 0.00011007891559426203, 'epoch': 0.52}
{'loss': 0.7555, 'grad_norm': 0.8337736129760742, 'learning_rate': 0.00011002548516808983, 'epoch': 0.52}
{'loss': 1.1815, 'grad_norm': 1.17782723903656, 'learning_rate': 0.00010997205185061599, 'epoch': 0.52}
{'loss': 0.9535, 'grad_norm': 1.0296131372451782, 'learning_rate': 0.00010991861565725044, 'epoch': 0.52}
{'loss': 0.9298, 'grad_norm': 1.2147542238235474, 'learning_rate': 0.00010986517660340389, 'epoch': 0.52}
{'loss': 0.9427, 'grad_norm': 1.6142953634262085, 'learning_rate': 0.00010981173470448796, 'epoch': 0.52}
{'loss': 0.7412, 'grad_norm': 1.0992379188537598, 'learning_rate': 0.00010975828997591495, 'epoch': 0.52}
{'loss': 1.214, 'grad_norm': 0.9843120574951172, 'learning_rate': 0.00010970484243309815, 'epoch': 0.52}
{'loss': 1.2147, 'grad_norm': 1.0532087087631226, 'learning_rate': 0.00010965139209145152, 'epoch': 0.52}
{'loss': 1.0745, 'grad_norm': 1.0748220682144165, 'learning_rate': 0.00010959793896638993, 'epoch': 0.52}
{'loss': 0.7985, 'grad_norm': 1.2143454551696777, 'learning_rate': 0.00010954448307332894, 'epoch': 0.52}
{'loss': 0.7559, 'grad_norm': 1.2216434478759766, 'learning_rate': 0.000109491024427685, 'epoch': 0.52}
{'loss': 0.9691, 'grad_norm': 1.1603723764419556, 'learning_rate': 0.00010943756304487531, 'epoch': 0.52}
{'loss': 0.6455, 'grad_norm': 1.0838546752929688, 'learning_rate': 0.00010938409894031794, 'epoch': 0.52}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9939, 'grad_norm': 0.9504367113113403, 'learning_rate': 0.00010933063212943162, 'epoch': 0.52}
{'loss': 1.0155, 'grad_norm': 1.0578532218933105, 'learning_rate': 0.00010927716262763592, 'epoch': 0.52}
{'loss': 1.0217, 'grad_norm': 0.9903374314308167, 'learning_rate': 0.00010922369045035121, 'epoch': 0.52}
{'loss': 0.9166, 'grad_norm': 1.236694097518921, 'learning_rate': 0.00010917021561299863, 'epoch': 0.52}
{'loss': 0.9604, 'grad_norm': 1.003932237625122, 'learning_rate': 0.00010911673813100001, 'epoch': 0.52}
{'loss': 1.0808, 'grad_norm': 1.1642118692398071, 'learning_rate': 0.00010906325801977804, 'epoch': 0.52}
{'loss': 1.2019, 'grad_norm': 1.0213589668273926, 'learning_rate': 0.00010900977529475609, 'epoch': 0.52}
{'loss': 0.7163, 'grad_norm': 1.331730604171753, 'learning_rate': 0.00010895628997135836, 'epoch': 0.52}
{'loss': 1.0315, 'grad_norm': 0.9380477666854858, 'learning_rate': 0.00010890280206500971, 'epoch': 0.52}
{'loss': 0.7833, 'grad_norm': 1.1038756370544434, 'learning_rate': 0.00010884931159113586, 'epoch': 0.52}
{'loss': 1.1202, 'grad_norm': 1.0514158010482788, 'learning_rate': 0.00010879581856516317, 'epoch': 0.52}
{'loss': 1.0516, 'grad_norm': 1.3041554689407349, 'learning_rate': 0.00010874232300251874, 'epoch': 0.52}
{'loss': 1.1971, 'grad_norm': 1.2241657972335815, 'learning_rate': 0.00010868882491863049, 'epoch': 0.53}
{'loss': 1.3165, 'grad_norm': 1.4396610260009766, 'learning_rate': 0.00010863532432892698, 'epoch': 0.53}
{'loss': 0.999, 'grad_norm': 1.0961344242095947, 'learning_rate': 0.00010858182124883754, 'epoch': 0.53}
{'loss': 0.9776, 'grad_norm': 1.4340412616729736, 'learning_rate': 0.00010852831569379215, 'epoch': 0.53}
{'loss': 0.7511, 'grad_norm': 1.4732670783996582, 'learning_rate': 0.00010847480767922162, 'epoch': 0.53}
{'loss': 0.9629, 'grad_norm': 1.3422473669052124, 'learning_rate': 0.00010842129722055738, 'epoch': 0.53}
{'loss': 1.0812, 'grad_norm': 1.1546289920806885, 'learning_rate': 0.00010836778433323158, 'epoch': 0.53}
{'loss': 0.8574, 'grad_norm': 1.0181281566619873, 'learning_rate': 0.00010831426903267706, 'epoch': 0.53}
{'loss': 0.8932, 'grad_norm': 1.3173383474349976, 'learning_rate': 0.00010826075133432738, 'epoch': 0.53}
{'loss': 1.1491, 'grad_norm': 1.3047106266021729, 'learning_rate': 0.00010820723125361684, 'epoch': 0.53}
{'loss': 0.7743, 'grad_norm': 1.6958385705947876, 'learning_rate': 0.00010815370880598035, 'epoch': 0.53}
{'loss': 1.0481, 'grad_norm': 1.2159266471862793, 'learning_rate': 0.00010810018400685351, 'epoch': 0.53}
{'loss': 0.7199, 'grad_norm': 1.3323451280593872, 'learning_rate': 0.00010804665687167262, 'epoch': 0.53}
{'loss': 0.7653, 'grad_norm': 1.4110866785049438, 'learning_rate': 0.0001079931274158746, 'epoch': 0.53}
{'loss': 0.9982, 'grad_norm': 1.0789170265197754, 'learning_rate': 0.00010793959565489718, 'epoch': 0.53}
{'loss': 1.0558, 'grad_norm': 1.4041494131088257, 'learning_rate': 0.0001078860616041786, 'epoch': 0.53}
{'loss': 0.9827, 'grad_norm': 1.1492750644683838, 'learning_rate': 0.00010783252527915784, 'epoch': 0.53}
{'loss': 0.9711, 'grad_norm': 1.1422957181930542, 'learning_rate': 0.00010777898669527449, 'epoch': 0.53}
{'loss': 0.9727, 'grad_norm': 1.0923517942428589, 'learning_rate': 0.00010772544586796889, 'epoch': 0.53}
{'loss': 1.1084, 'grad_norm': 0.9516147971153259, 'learning_rate': 0.00010767190281268187, 'epoch': 0.53}
{'loss': 0.8586, 'grad_norm': 0.9964180588722229, 'learning_rate': 0.00010761835754485505, 'epoch': 0.53}
{'loss': 0.8813, 'grad_norm': 1.642884612083435, 'learning_rate': 0.00010756481007993063, 'epoch': 0.53}
{'loss': 0.8695, 'grad_norm': 1.0909907817840576, 'learning_rate': 0.0001075112604333514, 'epoch': 0.53}
{'loss': 0.9734, 'grad_norm': 1.3565782308578491, 'learning_rate': 0.00010745770862056081, 'epoch': 0.53}
{'loss': 0.9323, 'grad_norm': 0.9210327863693237, 'learning_rate': 0.00010740415465700301, 'epoch': 0.53}
{'loss': 1.0155, 'grad_norm': 1.1559839248657227, 'learning_rate': 0.00010735059855812268, 'epoch': 0.53}
{'loss': 0.8487, 'grad_norm': 1.8144493103027344, 'learning_rate': 0.00010729704033936512, 'epoch': 0.53}
{'loss': 1.2147, 'grad_norm': 0.9449884295463562, 'learning_rate': 0.00010724348001617625, 'epoch': 0.53}
{'loss': 0.9753, 'grad_norm': 1.0419032573699951, 'learning_rate': 0.00010718991760400265, 'epoch': 0.53}
{'loss': 1.0121, 'grad_norm': 1.182011604309082, 'learning_rate': 0.00010713635311829141, 'epoch': 0.53}
{'loss': 1.099, 'grad_norm': 1.2364466190338135, 'learning_rate': 0.00010708278657449036, 'epoch': 0.53}
{'loss': 0.8532, 'grad_norm': 1.055531620979309, 'learning_rate': 0.00010702921798804777, 'epoch': 0.53}
{'loss': 1.05, 'grad_norm': 0.9443439245223999, 'learning_rate': 0.00010697564737441252, 'epoch': 0.53}
{'loss': 0.7768, 'grad_norm': 1.1248524188995361, 'learning_rate': 0.00010692207474903422, 'epoch': 0.53}
{'loss': 0.9916, 'grad_norm': 1.340100646018982, 'learning_rate': 0.0001068685001273629, 'epoch': 0.53}
{'loss': 0.722, 'grad_norm': 1.0530179738998413, 'learning_rate': 0.00010681492352484919, 'epoch': 0.53}
{'loss': 0.9331, 'grad_norm': 0.9452279210090637, 'learning_rate': 0.00010676134495694439, 'epoch': 0.53}
{'loss': 1.0206, 'grad_norm': 1.1310045719146729, 'learning_rate': 0.00010670776443910024, 'epoch': 0.53}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9205, 'grad_norm': 1.1808393001556396, 'learning_rate': 0.00010665418198676917, 'epoch': 0.53}
{'loss': 0.8715, 'grad_norm': 1.5988825559616089, 'learning_rate': 0.00010660059761540404, 'epoch': 0.53}
{'loss': 0.8656, 'grad_norm': 0.9429972171783447, 'learning_rate': 0.00010654701134045835, 'epoch': 0.53}
{'loss': 0.9726, 'grad_norm': 1.1147325038909912, 'learning_rate': 0.00010649342317738616, 'epoch': 0.53}
{'loss': 1.2383, 'grad_norm': 1.097296118736267, 'learning_rate': 0.00010643983314164194, 'epoch': 0.53}
{'loss': 1.2009, 'grad_norm': 1.0382065773010254, 'learning_rate': 0.00010638624124868091, 'epoch': 0.53}
{'loss': 0.9409, 'grad_norm': 1.0959874391555786, 'learning_rate': 0.00010633264751395865, 'epoch': 0.53}
{'loss': 1.0961, 'grad_norm': 0.9843283295631409, 'learning_rate': 0.00010627905195293135, 'epoch': 0.53}
{'loss': 1.1408, 'grad_norm': 1.1364952325820923, 'learning_rate': 0.0001062254545810557, 'epoch': 0.53}
{'loss': 0.968, 'grad_norm': 1.0734267234802246, 'learning_rate': 0.00010617185541378895, 'epoch': 0.53}
{'loss': 1.1434, 'grad_norm': 2.0745532512664795, 'learning_rate': 0.00010611825446658884, 'epoch': 0.53}
{'loss': 0.9599, 'grad_norm': 1.14984130859375, 'learning_rate': 0.00010606465175491358, 'epoch': 0.53}
{'loss': 1.0751, 'grad_norm': 0.9506136178970337, 'learning_rate': 0.00010601104729422195, 'epoch': 0.53}
{'loss': 0.9586, 'grad_norm': 1.1549063920974731, 'learning_rate': 0.00010595744109997325, 'epoch': 0.53}
{'loss': 1.1924, 'grad_norm': 1.670876145362854, 'learning_rate': 0.00010590383318762724, 'epoch': 0.53}
{'loss': 0.968, 'grad_norm': 1.1783474683761597, 'learning_rate': 0.00010585022357264417, 'epoch': 0.53}
{'loss': 0.9193, 'grad_norm': 0.9926938414573669, 'learning_rate': 0.00010579661227048483, 'epoch': 0.53}
{'loss': 0.966, 'grad_norm': 1.0609016418457031, 'learning_rate': 0.0001057429992966104, 'epoch': 0.53}
{'loss': 0.9693, 'grad_norm': 1.0604462623596191, 'learning_rate': 0.00010568938466648264, 'epoch': 0.53}
{'loss': 0.9134, 'grad_norm': 1.0106614828109741, 'learning_rate': 0.00010563576839556374, 'epoch': 0.53}
{'loss': 0.8954, 'grad_norm': 1.132457971572876, 'learning_rate': 0.00010558215049931638, 'epoch': 0.53}
{'loss': 0.7863, 'grad_norm': 1.435784935951233, 'learning_rate': 0.00010552853099320375, 'epoch': 0.53}
{'loss': 0.9598, 'grad_norm': 1.8433382511138916, 'learning_rate': 0.00010547490989268934, 'epoch': 0.53}
{'loss': 0.7094, 'grad_norm': 1.439771056175232, 'learning_rate': 0.0001054212872132373, 'epoch': 0.53}
{'loss': 0.9884, 'grad_norm': 1.109395146369934, 'learning_rate': 0.00010536766297031215, 'epoch': 0.53}
{'loss': 1.1192, 'grad_norm': 0.8218366503715515, 'learning_rate': 0.00010531403717937887, 'epoch': 0.53}
{'loss': 1.0676, 'grad_norm': 2.865011215209961, 'learning_rate': 0.00010526040985590287, 'epoch': 0.53}
{'loss': 0.7074, 'grad_norm': 1.0604699850082397, 'learning_rate': 0.00010520678101534998, 'epoch': 0.54}
{'loss': 0.662, 'grad_norm': 1.6159297227859497, 'learning_rate': 0.00010515315067318652, 'epoch': 0.54}
{'loss': 1.0084, 'grad_norm': 1.504771113395691, 'learning_rate': 0.00010509951884487926, 'epoch': 0.54}
{'loss': 0.9097, 'grad_norm': 1.2159857749938965, 'learning_rate': 0.0001050458855458953, 'epoch': 0.54}
{'loss': 1.2314, 'grad_norm': 1.1906557083129883, 'learning_rate': 0.00010499225079170228, 'epoch': 0.54}
{'loss': 1.1431, 'grad_norm': 1.238423228263855, 'learning_rate': 0.00010493861459776812, 'epoch': 0.54}
{'loss': 0.8525, 'grad_norm': 1.1485364437103271, 'learning_rate': 0.00010488497697956135, 'epoch': 0.54}
{'loss': 0.7914, 'grad_norm': 1.2058606147766113, 'learning_rate': 0.00010483133795255071, 'epoch': 0.54}
{'loss': 0.8636, 'grad_norm': 1.2816855907440186, 'learning_rate': 0.0001047776975322055, 'epoch': 0.54}
{'loss': 0.8762, 'grad_norm': 1.4134290218353271, 'learning_rate': 0.00010472405573399534, 'epoch': 0.54}
{'loss': 1.1641, 'grad_norm': 0.9930746555328369, 'learning_rate': 0.00010467041257339023, 'epoch': 0.54}
{'loss': 1.0637, 'grad_norm': 1.1842018365859985, 'learning_rate': 0.00010461676806586064, 'epoch': 0.54}
{'loss': 0.9738, 'grad_norm': 1.1672073602676392, 'learning_rate': 0.0001045631222268774, 'epoch': 0.54}
{'loss': 1.0959, 'grad_norm': 1.007707118988037, 'learning_rate': 0.00010450947507191169, 'epoch': 0.54}
{'loss': 0.6926, 'grad_norm': 1.3238600492477417, 'learning_rate': 0.00010445582661643509, 'epoch': 0.54}
{'loss': 1.2558, 'grad_norm': 0.9082612991333008, 'learning_rate': 0.0001044021768759195, 'epoch': 0.54}
{'loss': 0.758, 'grad_norm': 1.1963518857955933, 'learning_rate': 0.00010434852586583736, 'epoch': 0.54}
{'loss': 0.8352, 'grad_norm': 1.1994954347610474, 'learning_rate': 0.00010429487360166128, 'epoch': 0.54}
{'loss': 0.7564, 'grad_norm': 1.0880635976791382, 'learning_rate': 0.00010424122009886434, 'epoch': 0.54}
{'loss': 1.0715, 'grad_norm': 0.9440018534660339, 'learning_rate': 0.00010418756537291996, 'epoch': 0.54}
{'loss': 1.0229, 'grad_norm': 1.1292388439178467, 'learning_rate': 0.0001041339094393019, 'epoch': 0.54}
{'loss': 1.0413, 'grad_norm': 0.9793809652328491, 'learning_rate': 0.00010408025231348428, 'epoch': 0.54}
{'loss': 0.7768, 'grad_norm': 1.1979336738586426, 'learning_rate': 0.00010402659401094152, 'epoch': 0.54}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.933, 'grad_norm': 1.2115652561187744, 'learning_rate': 0.00010397293454714848, 'epoch': 0.54}
{'loss': 0.9473, 'grad_norm': 1.1316951513290405, 'learning_rate': 0.00010391927393758021, 'epoch': 0.54}
{'loss': 0.8225, 'grad_norm': 1.015426516532898, 'learning_rate': 0.00010386561219771222, 'epoch': 0.54}
{'loss': 0.9714, 'grad_norm': 1.0636144876480103, 'learning_rate': 0.00010381194934302032, 'epoch': 0.54}
{'loss': 1.0649, 'grad_norm': 0.9725637435913086, 'learning_rate': 0.00010375828538898053, 'epoch': 0.54}
{'loss': 1.0175, 'grad_norm': 1.026262879371643, 'learning_rate': 0.0001037046203510694, 'epoch': 0.54}
{'loss': 1.0501, 'grad_norm': 1.4152623414993286, 'learning_rate': 0.00010365095424476357, 'epoch': 0.54}
{'loss': 1.01, 'grad_norm': 1.246403455734253, 'learning_rate': 0.00010359728708554013, 'epoch': 0.54}
{'loss': 0.8576, 'grad_norm': 1.1368873119354248, 'learning_rate': 0.00010354361888887642, 'epoch': 0.54}
{'loss': 1.2007, 'grad_norm': 0.9807187914848328, 'learning_rate': 0.00010348994967025012, 'epoch': 0.54}
{'loss': 0.8432, 'grad_norm': 1.4568482637405396, 'learning_rate': 0.00010343627944513911, 'epoch': 0.54}
{'loss': 0.8386, 'grad_norm': 0.9971973299980164, 'learning_rate': 0.00010338260822902167, 'epoch': 0.54}
{'loss': 0.7028, 'grad_norm': 1.3132842779159546, 'learning_rate': 0.0001033289360373763, 'epoch': 0.54}
{'loss': 1.0992, 'grad_norm': 1.2512223720550537, 'learning_rate': 0.00010327526288568183, 'epoch': 0.54}
{'loss': 0.8325, 'grad_norm': 0.9776089787483215, 'learning_rate': 0.00010322158878941732, 'epoch': 0.54}
{'loss': 0.7718, 'grad_norm': 0.9808298349380493, 'learning_rate': 0.0001031679137640621, 'epoch': 0.54}
{'loss': 0.8216, 'grad_norm': 1.2665919065475464, 'learning_rate': 0.00010311423782509579, 'epoch': 0.54}
{'loss': 1.1104, 'grad_norm': 0.904829204082489, 'learning_rate': 0.00010306056098799832, 'epoch': 0.54}
{'loss': 0.9025, 'grad_norm': 1.3431944847106934, 'learning_rate': 0.00010300688326824981, 'epoch': 0.54}
{'loss': 1.1488, 'grad_norm': 0.9531670808792114, 'learning_rate': 0.00010295320468133066, 'epoch': 0.54}
{'loss': 0.8896, 'grad_norm': 1.0663460493087769, 'learning_rate': 0.00010289952524272146, 'epoch': 0.54}
{'loss': 1.112, 'grad_norm': 1.4259965419769287, 'learning_rate': 0.00010284584496790318, 'epoch': 0.54}
{'loss': 1.1696, 'grad_norm': 1.3805910348892212, 'learning_rate': 0.0001027921638723569, 'epoch': 0.54}
{'loss': 1.315, 'grad_norm': 1.0897341966629028, 'learning_rate': 0.00010273848197156401, 'epoch': 0.54}
{'loss': 0.9558, 'grad_norm': 1.1492919921875, 'learning_rate': 0.00010268479928100614, 'epoch': 0.54}
{'loss': 0.9286, 'grad_norm': 1.393452525138855, 'learning_rate': 0.00010263111581616505, 'epoch': 0.54}
{'loss': 0.8797, 'grad_norm': 0.9389525651931763, 'learning_rate': 0.00010257743159252285, 'epoch': 0.54}
{'loss': 1.1542, 'grad_norm': 1.2591694593429565, 'learning_rate': 0.00010252374662556177, 'epoch': 0.54}
{'loss': 1.1137, 'grad_norm': 2.854841709136963, 'learning_rate': 0.00010247006093076434, 'epoch': 0.54}
{'loss': 0.9563, 'grad_norm': 1.0491218566894531, 'learning_rate': 0.00010241637452361323, 'epoch': 0.54}
{'loss': 1.1, 'grad_norm': 1.0268759727478027, 'learning_rate': 0.00010236268741959134, 'epoch': 0.54}
{'loss': 1.2317, 'grad_norm': 1.0714547634124756, 'learning_rate': 0.00010230899963418181, 'epoch': 0.54}
{'loss': 1.0446, 'grad_norm': 0.9036492109298706, 'learning_rate': 0.00010225531118286789, 'epoch': 0.54}
{'loss': 0.9508, 'grad_norm': 0.9572834968566895, 'learning_rate': 0.00010220162208113309, 'epoch': 0.54}
{'loss': 1.3566, 'grad_norm': 1.206791877746582, 'learning_rate': 0.00010214793234446104, 'epoch': 0.54}
{'loss': 1.0311, 'grad_norm': 0.9955031275749207, 'learning_rate': 0.0001020942419883357, 'epoch': 0.54}
{'loss': 0.7866, 'grad_norm': 1.117978811264038, 'learning_rate': 0.00010204055102824105, 'epoch': 0.54}
{'loss': 1.0723, 'grad_norm': 1.1525428295135498, 'learning_rate': 0.00010198685947966129, 'epoch': 0.54}
{'loss': 0.8364, 'grad_norm': 1.0111457109451294, 'learning_rate': 0.00010193316735808085, 'epoch': 0.54}
{'loss': 1.0761, 'grad_norm': 1.0142842531204224, 'learning_rate': 0.00010187947467898425, 'epoch': 0.54}
{'loss': 0.9444, 'grad_norm': 1.2962703704833984, 'learning_rate': 0.0001018257814578562, 'epoch': 0.54}
{'loss': 1.211, 'grad_norm': 1.2788528203964233, 'learning_rate': 0.00010177208771018158, 'epoch': 0.54}
{'loss': 0.9262, 'grad_norm': 1.5443360805511475, 'learning_rate': 0.0001017183934514454, 'epoch': 0.55}
{'loss': 0.9092, 'grad_norm': 1.092387080192566, 'learning_rate': 0.00010166469869713282, 'epoch': 0.55}
{'loss': 1.3039, 'grad_norm': 1.1173237562179565, 'learning_rate': 0.00010161100346272914, 'epoch': 0.55}
{'loss': 0.9984, 'grad_norm': 1.3620166778564453, 'learning_rate': 0.00010155730776371983, 'epoch': 0.55}
{'loss': 0.7936, 'grad_norm': 4.022112846374512, 'learning_rate': 0.00010150361161559048, 'epoch': 0.55}
{'loss': 0.752, 'grad_norm': 1.0519686937332153, 'learning_rate': 0.00010144991503382674, 'epoch': 0.55}
{'loss': 0.8654, 'grad_norm': 1.0757627487182617, 'learning_rate': 0.00010139621803391455, 'epoch': 0.55}
{'loss': 1.127, 'grad_norm': 1.063423752784729, 'learning_rate': 0.00010134252063133975, 'epoch': 0.55}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9576, 'grad_norm': 1.239291787147522, 'learning_rate': 0.0001012888228415885, 'epoch': 0.55}
{'loss': 1.0023, 'grad_norm': 1.2150444984436035, 'learning_rate': 0.00010123512468014693, 'epoch': 0.55}
{'loss': 1.0378, 'grad_norm': 1.0882641077041626, 'learning_rate': 0.00010118142616250137, 'epoch': 0.55}
{'loss': 0.8416, 'grad_norm': 1.4265141487121582, 'learning_rate': 0.00010112772730413815, 'epoch': 0.55}
{'loss': 0.8896, 'grad_norm': 1.0975768566131592, 'learning_rate': 0.00010107402812054385, 'epoch': 0.55}
{'loss': 0.7976, 'grad_norm': 1.0838881731033325, 'learning_rate': 0.00010102032862720503, 'epoch': 0.55}
{'loss': 0.8077, 'grad_norm': 1.3113895654678345, 'learning_rate': 0.00010096662883960832, 'epoch': 0.55}
{'loss': 0.9754, 'grad_norm': 1.1591804027557373, 'learning_rate': 0.0001009129287732405, 'epoch': 0.55}
{'loss': 0.9224, 'grad_norm': 1.0996110439300537, 'learning_rate': 0.00010085922844358843, 'epoch': 0.55}
{'loss': 0.9906, 'grad_norm': 1.013594388961792, 'learning_rate': 0.00010080552786613899, 'epoch': 0.55}
{'loss': 0.8477, 'grad_norm': 1.1872074604034424, 'learning_rate': 0.00010075182705637925, 'epoch': 0.55}
{'loss': 1.1549, 'grad_norm': 1.0844382047653198, 'learning_rate': 0.00010069812602979615, 'epoch': 0.55}
{'loss': 1.0306, 'grad_norm': 1.3564417362213135, 'learning_rate': 0.00010064442480187692, 'epoch': 0.55}
{'loss': 1.0066, 'grad_norm': 1.3028252124786377, 'learning_rate': 0.00010059072338810865, 'epoch': 0.55}
{'loss': 1.0648, 'grad_norm': 1.2304397821426392, 'learning_rate': 0.00010053702180397859, 'epoch': 0.55}
{'loss': 0.6043, 'grad_norm': 1.1038484573364258, 'learning_rate': 0.00010048332006497406, 'epoch': 0.55}
{'loss': 0.9139, 'grad_norm': 1.1121782064437866, 'learning_rate': 0.00010042961818658234, 'epoch': 0.55}
{'loss': 0.7942, 'grad_norm': 1.2374931573867798, 'learning_rate': 0.00010037591618429076, 'epoch': 0.55}
{'loss': 1.0022, 'grad_norm': 1.6652555465698242, 'learning_rate': 0.00010032221407358681, 'epoch': 0.55}
{'loss': 0.713, 'grad_norm': 1.2454335689544678, 'learning_rate': 0.00010026851186995785, 'epoch': 0.55}
{'loss': 0.8814, 'grad_norm': 1.3627890348434448, 'learning_rate': 0.00010021480958889138, 'epoch': 0.55}
{'loss': 0.9857, 'grad_norm': 1.0995146036148071, 'learning_rate': 0.00010016110724587487, 'epoch': 0.55}
{'loss': 1.2087, 'grad_norm': 1.5827795267105103, 'learning_rate': 0.00010010740485639579, 'epoch': 0.55}
{'loss': 0.8466, 'grad_norm': 1.1065398454666138, 'learning_rate': 0.00010005370243594166, 'epoch': 0.55}
{'loss': 0.8905, 'grad_norm': 1.5689235925674438, 'learning_rate': 0.0001, 'epoch': 0.55}
{'loss': 1.0955, 'grad_norm': 1.4009530544281006, 'learning_rate': 9.994629756405835e-05, 'epoch': 0.55}
{'loss': 1.061, 'grad_norm': 1.0869190692901611, 'learning_rate': 9.989259514360425e-05, 'epoch': 0.55}
{'loss': 1.2206, 'grad_norm': 0.9504846930503845, 'learning_rate': 9.983889275412516e-05, 'epoch': 0.55}
{'loss': 1.094, 'grad_norm': 1.5105401277542114, 'learning_rate': 9.978519041110863e-05, 'epoch': 0.55}
{'loss': 1.0993, 'grad_norm': 1.2630683183670044, 'learning_rate': 9.973148813004216e-05, 'epoch': 0.55}
{'loss': 0.8705, 'grad_norm': 1.0672646760940552, 'learning_rate': 9.96777859264132e-05, 'epoch': 0.55}
{'loss': 0.9195, 'grad_norm': 1.3015583753585815, 'learning_rate': 9.962408381570925e-05, 'epoch': 0.55}
{'loss': 1.0787, 'grad_norm': 1.0973924398422241, 'learning_rate': 9.957038181341769e-05, 'epoch': 0.55}
{'loss': 0.777, 'grad_norm': 1.0982657670974731, 'learning_rate': 9.9516679935026e-05, 'epoch': 0.55}
{'loss': 1.0697, 'grad_norm': 1.130841612815857, 'learning_rate': 9.946297819602143e-05, 'epoch': 0.55}
{'loss': 0.5764, 'grad_norm': 1.163087248802185, 'learning_rate': 9.940927661189136e-05, 'epoch': 0.55}
{'loss': 0.8767, 'grad_norm': 0.9779557585716248, 'learning_rate': 9.935557519812313e-05, 'epoch': 0.55}
{'loss': 0.9407, 'grad_norm': 1.345831036567688, 'learning_rate': 9.930187397020386e-05, 'epoch': 0.55}
{'loss': 0.8404, 'grad_norm': 1.1453269720077515, 'learning_rate': 9.924817294362079e-05, 'epoch': 0.55}
{'loss': 1.0443, 'grad_norm': 3.0605976581573486, 'learning_rate': 9.919447213386103e-05, 'epoch': 0.55}
{'loss': 0.7116, 'grad_norm': 1.0312042236328125, 'learning_rate': 9.914077155641159e-05, 'epoch': 0.55}
{'loss': 1.0584, 'grad_norm': 1.0521318912506104, 'learning_rate': 9.908707122675953e-05, 'epoch': 0.55}
{'loss': 1.239, 'grad_norm': 1.1858001947402954, 'learning_rate': 9.903337116039171e-05, 'epoch': 0.55}
{'loss': 0.9725, 'grad_norm': 1.076743721961975, 'learning_rate': 9.897967137279498e-05, 'epoch': 0.55}
{'loss': 0.9525, 'grad_norm': 1.6468498706817627, 'learning_rate': 9.892597187945616e-05, 'epoch': 0.55}
{'loss': 0.9412, 'grad_norm': 1.2270586490631104, 'learning_rate': 9.887227269586183e-05, 'epoch': 0.55}
{'loss': 0.7502, 'grad_norm': 0.8772571086883545, 'learning_rate': 9.881857383749868e-05, 'epoch': 0.55}
{'loss': 0.9531, 'grad_norm': 1.647207260131836, 'learning_rate': 9.876487531985309e-05, 'epoch': 0.55}
{'loss': 0.8121, 'grad_norm': 1.1767303943634033, 'learning_rate': 9.871117715841151e-05, 'epoch': 0.55}
{'loss': 0.8213, 'grad_norm': 1.0024282932281494, 'learning_rate': 9.865747936866027e-05, 'epoch': 0.55}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8003, 'grad_norm': 1.5863043069839478, 'learning_rate': 9.860378196608549e-05, 'epoch': 0.55}
{'loss': 1.0846, 'grad_norm': 1.092339277267456, 'learning_rate': 9.855008496617327e-05, 'epoch': 0.55}
{'loss': 0.9052, 'grad_norm': 1.3582972288131714, 'learning_rate': 9.849638838440953e-05, 'epoch': 0.55}
{'loss': 0.8225, 'grad_norm': 1.1686457395553589, 'learning_rate': 9.844269223628016e-05, 'epoch': 0.55}
{'loss': 0.8846, 'grad_norm': 0.9354205131530762, 'learning_rate': 9.838899653727088e-05, 'epoch': 0.55}
{'loss': 1.0281, 'grad_norm': 1.1998308897018433, 'learning_rate': 9.83353013028672e-05, 'epoch': 0.55}
{'loss': 1.1877, 'grad_norm': 1.271032452583313, 'learning_rate': 9.828160654855464e-05, 'epoch': 0.55}
{'loss': 0.941, 'grad_norm': 0.953069806098938, 'learning_rate': 9.822791228981844e-05, 'epoch': 0.56}
{'loss': 0.8918, 'grad_norm': 1.3374137878417969, 'learning_rate': 9.81742185421438e-05, 'epoch': 0.56}
{'loss': 0.6822, 'grad_norm': 1.1817313432693481, 'learning_rate': 9.812052532101578e-05, 'epoch': 0.56}
{'loss': 1.0384, 'grad_norm': 1.1013693809509277, 'learning_rate': 9.806683264191916e-05, 'epoch': 0.56}
{'loss': 1.0306, 'grad_norm': 1.0755586624145508, 'learning_rate': 9.801314052033872e-05, 'epoch': 0.56}
{'loss': 1.1851, 'grad_norm': 1.0277035236358643, 'learning_rate': 9.795944897175896e-05, 'epoch': 0.56}
{'loss': 0.9831, 'grad_norm': 1.0382816791534424, 'learning_rate': 9.790575801166432e-05, 'epoch': 0.56}
{'loss': 1.1702, 'grad_norm': 1.2546429634094238, 'learning_rate': 9.785206765553896e-05, 'epoch': 0.56}
{'loss': 0.9685, 'grad_norm': 1.3693767786026, 'learning_rate': 9.779837791886694e-05, 'epoch': 0.56}
{'loss': 1.0581, 'grad_norm': 1.4061424732208252, 'learning_rate': 9.774468881713216e-05, 'epoch': 0.56}
{'loss': 0.9763, 'grad_norm': 1.0415786504745483, 'learning_rate': 9.769100036581823e-05, 'epoch': 0.56}
{'loss': 0.9279, 'grad_norm': 1.3162624835968018, 'learning_rate': 9.763731258040865e-05, 'epoch': 0.56}
{'loss': 0.9569, 'grad_norm': 8.588486671447754, 'learning_rate': 9.75836254763868e-05, 'epoch': 0.56}
{'loss': 0.8907, 'grad_norm': 1.2273305654525757, 'learning_rate': 9.752993906923567e-05, 'epoch': 0.56}
{'loss': 1.302, 'grad_norm': 1.6107784509658813, 'learning_rate': 9.747625337443825e-05, 'epoch': 0.56}
{'loss': 1.0444, 'grad_norm': 1.4185383319854736, 'learning_rate': 9.742256840747718e-05, 'epoch': 0.56}
{'loss': 0.9264, 'grad_norm': 1.3428738117218018, 'learning_rate': 9.736888418383497e-05, 'epoch': 0.56}
{'loss': 1.1025, 'grad_norm': 1.06977117061615, 'learning_rate': 9.73152007189939e-05, 'epoch': 0.56}
{'loss': 0.9245, 'grad_norm': 0.9826475977897644, 'learning_rate': 9.7261518028436e-05, 'epoch': 0.56}
{'loss': 0.9016, 'grad_norm': 1.3652857542037964, 'learning_rate': 9.720783612764314e-05, 'epoch': 0.56}
{'loss': 1.1878, 'grad_norm': 1.0770621299743652, 'learning_rate': 9.715415503209685e-05, 'epoch': 0.56}
{'loss': 0.6956, 'grad_norm': 1.0800776481628418, 'learning_rate': 9.710047475727855e-05, 'epoch': 0.56}
{'loss': 0.9682, 'grad_norm': 0.884272575378418, 'learning_rate': 9.704679531866941e-05, 'epoch': 0.56}
{'loss': 0.6659, 'grad_norm': 3.3854238986968994, 'learning_rate': 9.699311673175021e-05, 'epoch': 0.56}
{'loss': 0.8587, 'grad_norm': 1.120787262916565, 'learning_rate': 9.693943901200168e-05, 'epoch': 0.56}
{'loss': 0.7891, 'grad_norm': 1.0795743465423584, 'learning_rate': 9.688576217490424e-05, 'epoch': 0.56}
{'loss': 0.6039, 'grad_norm': 1.1824873685836792, 'learning_rate': 9.683208623593793e-05, 'epoch': 0.56}
{'loss': 0.844, 'grad_norm': 1.0832278728485107, 'learning_rate': 9.677841121058273e-05, 'epoch': 0.56}
{'loss': 0.7246, 'grad_norm': 1.0775789022445679, 'learning_rate': 9.67247371143182e-05, 'epoch': 0.56}
{'loss': 1.0669, 'grad_norm': 1.274086356163025, 'learning_rate': 9.66710639626237e-05, 'epoch': 0.56}
{'loss': 1.1042, 'grad_norm': 1.2254129648208618, 'learning_rate': 9.661739177097836e-05, 'epoch': 0.56}
{'loss': 1.2033, 'grad_norm': 1.2373979091644287, 'learning_rate': 9.65637205548609e-05, 'epoch': 0.56}
{'loss': 0.9067, 'grad_norm': 1.2497822046279907, 'learning_rate': 9.651005032974994e-05, 'epoch': 0.56}
{'loss': 1.0013, 'grad_norm': 1.1336170434951782, 'learning_rate': 9.645638111112359e-05, 'epoch': 0.56}
{'loss': 0.8795, 'grad_norm': 1.1605274677276611, 'learning_rate': 9.640271291445986e-05, 'epoch': 0.56}
{'loss': 0.9721, 'grad_norm': 1.4549931287765503, 'learning_rate': 9.634904575523645e-05, 'epoch': 0.56}
{'loss': 1.0299, 'grad_norm': 1.0116785764694214, 'learning_rate': 9.629537964893063e-05, 'epoch': 0.56}
{'loss': 0.8582, 'grad_norm': 1.4215235710144043, 'learning_rate': 9.624171461101948e-05, 'epoch': 0.56}
{'loss': 1.0008, 'grad_norm': 0.8649689555168152, 'learning_rate': 9.618805065697972e-05, 'epoch': 0.56}
{'loss': 0.925, 'grad_norm': 0.9771553874015808, 'learning_rate': 9.613438780228777e-05, 'epoch': 0.56}
{'loss': 1.041, 'grad_norm': 0.9558417797088623, 'learning_rate': 9.608072606241982e-05, 'epoch': 0.56}
{'loss': 0.9192, 'grad_norm': 1.31596839427948, 'learning_rate': 9.602706545285155e-05, 'epoch': 0.56}
{'loss': 1.1179, 'grad_norm': 1.3220634460449219, 'learning_rate': 9.597340598905852e-05, 'epoch': 0.56}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.925, 'grad_norm': 1.1891753673553467, 'learning_rate': 9.591974768651574e-05, 'epoch': 0.56}
{'loss': 0.6652, 'grad_norm': 1.4584325551986694, 'learning_rate': 9.58660905606981e-05, 'epoch': 0.56}
{'loss': 0.8555, 'grad_norm': 1.3262051343917847, 'learning_rate': 9.581243462708006e-05, 'epoch': 0.56}
{'loss': 0.6929, 'grad_norm': 1.4734069108963013, 'learning_rate': 9.575877990113567e-05, 'epoch': 0.56}
{'loss': 0.9783, 'grad_norm': 1.178611159324646, 'learning_rate': 9.570512639833874e-05, 'epoch': 0.56}
{'loss': 0.7579, 'grad_norm': 1.2516382932662964, 'learning_rate': 9.565147413416266e-05, 'epoch': 0.56}
{'loss': 0.4546, 'grad_norm': 0.907698392868042, 'learning_rate': 9.559782312408048e-05, 'epoch': 0.56}
{'loss': 1.1322, 'grad_norm': 1.06540048122406, 'learning_rate': 9.554417338356496e-05, 'epoch': 0.56}
{'loss': 0.9973, 'grad_norm': 1.0444438457489014, 'learning_rate': 9.549052492808834e-05, 'epoch': 0.56}
{'loss': 0.8702, 'grad_norm': 0.9677816033363342, 'learning_rate': 9.543687777312263e-05, 'epoch': 0.56}
{'loss': 1.2267, 'grad_norm': 1.1810816526412964, 'learning_rate': 9.538323193413937e-05, 'epoch': 0.56}
{'loss': 1.0783, 'grad_norm': 1.1529685258865356, 'learning_rate': 9.532958742660976e-05, 'epoch': 0.56}
{'loss': 0.873, 'grad_norm': 0.9804080128669739, 'learning_rate': 9.52759442660047e-05, 'epoch': 0.56}
{'loss': 0.8357, 'grad_norm': 1.449520468711853, 'learning_rate': 9.522230246779452e-05, 'epoch': 0.56}
{'loss': 0.6277, 'grad_norm': 1.203003168106079, 'learning_rate': 9.516866204744931e-05, 'epoch': 0.56}
{'loss': 0.817, 'grad_norm': 1.3185162544250488, 'learning_rate': 9.511502302043868e-05, 'epoch': 0.56}
{'loss': 0.9666, 'grad_norm': 0.9032263159751892, 'learning_rate': 9.50613854022319e-05, 'epoch': 0.56}
{'loss': 1.1706, 'grad_norm': 1.5943492650985718, 'learning_rate': 9.500774920829778e-05, 'epoch': 0.56}
{'loss': 0.7955, 'grad_norm': 1.2821002006530762, 'learning_rate': 9.495411445410472e-05, 'epoch': 0.56}
{'loss': 0.9846, 'grad_norm': 0.9767237901687622, 'learning_rate': 9.490048115512074e-05, 'epoch': 0.56}
{'loss': 1.1988, 'grad_norm': 1.4605576992034912, 'learning_rate': 9.484684932681349e-05, 'epoch': 0.56}
{'loss': 1.0274, 'grad_norm': 0.9318033456802368, 'learning_rate': 9.479321898465003e-05, 'epoch': 0.56}
{'loss': 0.8844, 'grad_norm': 1.0061568021774292, 'learning_rate': 9.473959014409718e-05, 'epoch': 0.57}
{'loss': 1.2882, 'grad_norm': 1.2462334632873535, 'learning_rate': 9.468596282062114e-05, 'epoch': 0.57}
{'loss': 0.8211, 'grad_norm': 1.05498468875885, 'learning_rate': 9.463233702968783e-05, 'epoch': 0.57}
{'loss': 0.8183, 'grad_norm': 1.151785135269165, 'learning_rate': 9.457871278676272e-05, 'epoch': 0.57}
{'loss': 0.9665, 'grad_norm': 1.212110161781311, 'learning_rate': 9.452509010731069e-05, 'epoch': 0.57}
{'loss': 0.8863, 'grad_norm': 1.6503255367279053, 'learning_rate': 9.447146900679631e-05, 'epoch': 0.57}
{'loss': 0.7823, 'grad_norm': 0.9535051584243774, 'learning_rate': 9.441784950068362e-05, 'epoch': 0.57}
{'loss': 1.1412, 'grad_norm': 1.2056046724319458, 'learning_rate': 9.436423160443625e-05, 'epoch': 0.57}
{'loss': 0.7937, 'grad_norm': 1.056983470916748, 'learning_rate': 9.431061533351739e-05, 'epoch': 0.57}
{'loss': 1.2215, 'grad_norm': 1.2636449337005615, 'learning_rate': 9.42570007033896e-05, 'epoch': 0.57}
{'loss': 0.9272, 'grad_norm': 0.9489434957504272, 'learning_rate': 9.420338772951521e-05, 'epoch': 0.57}
{'loss': 0.956, 'grad_norm': 1.025517463684082, 'learning_rate': 9.414977642735584e-05, 'epoch': 0.57}
{'loss': 1.1071, 'grad_norm': 1.0286577939987183, 'learning_rate': 9.409616681237275e-05, 'epoch': 0.57}
{'loss': 1.033, 'grad_norm': 1.579801082611084, 'learning_rate': 9.404255890002677e-05, 'epoch': 0.57}
{'loss': 1.1816, 'grad_norm': 1.1556302309036255, 'learning_rate': 9.398895270577806e-05, 'epoch': 0.57}
{'loss': 1.1203, 'grad_norm': 1.308087706565857, 'learning_rate': 9.393534824508647e-05, 'epoch': 0.57}
{'loss': 1.3668, 'grad_norm': 2.1968369483947754, 'learning_rate': 9.38817455334112e-05, 'epoch': 0.57}
{'loss': 0.8651, 'grad_norm': 1.3528764247894287, 'learning_rate': 9.382814458621106e-05, 'epoch': 0.57}
{'loss': 0.8896, 'grad_norm': 1.0430710315704346, 'learning_rate': 9.377454541894433e-05, 'epoch': 0.57}
{'loss': 1.0844, 'grad_norm': 1.4212477207183838, 'learning_rate': 9.372094804706867e-05, 'epoch': 0.57}
{'loss': 0.6076, 'grad_norm': 1.1974698305130005, 'learning_rate': 9.36673524860414e-05, 'epoch': 0.57}
{'loss': 0.946, 'grad_norm': 1.094411849975586, 'learning_rate': 9.361375875131911e-05, 'epoch': 0.57}
{'loss': 0.8926, 'grad_norm': 1.2168028354644775, 'learning_rate': 9.356016685835806e-05, 'epoch': 0.57}
{'loss': 0.6469, 'grad_norm': 0.9227167367935181, 'learning_rate': 9.350657682261391e-05, 'epoch': 0.57}
{'loss': 0.8191, 'grad_norm': 1.1616584062576294, 'learning_rate': 9.345298865954166e-05, 'epoch': 0.57}
{'loss': 1.4694, 'grad_norm': 1.1354349851608276, 'learning_rate': 9.339940238459599e-05, 'epoch': 0.57}
{'loss': 0.8604, 'grad_norm': 1.060854434967041, 'learning_rate': 9.334581801323085e-05, 'epoch': 0.57}
{'loss': 1.0951, 'grad_norm': 1.0391786098480225, 'learning_rate': 9.329223556089975e-05, 'epoch': 0.57}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1011, 'grad_norm': 0.9781484603881836, 'learning_rate': 9.323865504305565e-05, 'epoch': 0.57}
{'loss': 0.9811, 'grad_norm': 0.9272205233573914, 'learning_rate': 9.318507647515082e-05, 'epoch': 0.57}
{'loss': 0.8871, 'grad_norm': 0.9670130610466003, 'learning_rate': 9.313149987263711e-05, 'epoch': 0.57}
{'loss': 0.6955, 'grad_norm': 1.163918375968933, 'learning_rate': 9.307792525096581e-05, 'epoch': 0.57}
{'loss': 0.9326, 'grad_norm': 1.182553768157959, 'learning_rate': 9.302435262558747e-05, 'epoch': 0.57}
{'loss': 1.1193, 'grad_norm': 1.1973737478256226, 'learning_rate': 9.297078201195229e-05, 'epoch': 0.57}
{'loss': 0.8431, 'grad_norm': 1.014735460281372, 'learning_rate': 9.291721342550967e-05, 'epoch': 0.57}
{'loss': 1.3952, 'grad_norm': 2.6150636672973633, 'learning_rate': 9.286364688170857e-05, 'epoch': 0.57}
{'loss': 1.0369, 'grad_norm': 1.2441072463989258, 'learning_rate': 9.281008239599736e-05, 'epoch': 0.57}
{'loss': 1.0649, 'grad_norm': 1.1280187368392944, 'learning_rate': 9.275651998382377e-05, 'epoch': 0.57}
{'loss': 0.8571, 'grad_norm': 1.2083351612091064, 'learning_rate': 9.270295966063493e-05, 'epoch': 0.57}
{'loss': 0.9374, 'grad_norm': 0.9665827751159668, 'learning_rate': 9.264940144187735e-05, 'epoch': 0.57}
{'loss': 1.3574, 'grad_norm': 1.5877379179000854, 'learning_rate': 9.2595845342997e-05, 'epoch': 0.57}
{'loss': 0.8332, 'grad_norm': 1.1740963459014893, 'learning_rate': 9.254229137943921e-05, 'epoch': 0.57}
{'loss': 1.2217, 'grad_norm': 0.9542207717895508, 'learning_rate': 9.248873956664863e-05, 'epoch': 0.57}
{'loss': 1.0601, 'grad_norm': 1.1968193054199219, 'learning_rate': 9.243518992006944e-05, 'epoch': 0.57}
{'loss': 0.8559, 'grad_norm': 0.9807433485984802, 'learning_rate': 9.238164245514497e-05, 'epoch': 0.57}
{'loss': 1.0149, 'grad_norm': 1.0801112651824951, 'learning_rate': 9.232809718731814e-05, 'epoch': 0.57}
{'loss': 1.0358, 'grad_norm': 1.2210391759872437, 'learning_rate': 9.227455413203115e-05, 'epoch': 0.57}
{'loss': 0.8653, 'grad_norm': 1.2496592998504639, 'learning_rate': 9.222101330472552e-05, 'epoch': 0.57}
{'loss': 0.7385, 'grad_norm': 1.226578712463379, 'learning_rate': 9.216747472084221e-05, 'epoch': 0.57}
{'loss': 0.7662, 'grad_norm': 1.3268067836761475, 'learning_rate': 9.211393839582142e-05, 'epoch': 0.57}
{'loss': 0.8707, 'grad_norm': 1.204947590827942, 'learning_rate': 9.206040434510282e-05, 'epoch': 0.57}
{'loss': 0.9263, 'grad_norm': 1.0914674997329712, 'learning_rate': 9.200687258412541e-05, 'epoch': 0.57}
{'loss': 0.798, 'grad_norm': 1.4620394706726074, 'learning_rate': 9.195334312832742e-05, 'epoch': 0.57}
{'loss': 0.6558, 'grad_norm': 1.1203898191452026, 'learning_rate': 9.189981599314654e-05, 'epoch': 0.57}
{'loss': 0.8572, 'grad_norm': 1.0375800132751465, 'learning_rate': 9.184629119401968e-05, 'epoch': 0.57}
{'loss': 0.97, 'grad_norm': 1.0597420930862427, 'learning_rate': 9.179276874638315e-05, 'epoch': 0.57}
{'loss': 0.7334, 'grad_norm': 1.5623292922973633, 'learning_rate': 9.173924866567263e-05, 'epoch': 0.57}
{'loss': 0.8669, 'grad_norm': 1.062956690788269, 'learning_rate': 9.168573096732297e-05, 'epoch': 0.57}
{'loss': 1.2212, 'grad_norm': 1.6989916563034058, 'learning_rate': 9.163221566676847e-05, 'epoch': 0.57}
{'loss': 0.9894, 'grad_norm': 1.1458569765090942, 'learning_rate': 9.157870277944266e-05, 'epoch': 0.57}
{'loss': 0.9577, 'grad_norm': 1.9482777118682861, 'learning_rate': 9.152519232077839e-05, 'epoch': 0.57}
{'loss': 0.9539, 'grad_norm': 1.2370752096176147, 'learning_rate': 9.147168430620787e-05, 'epoch': 0.57}
{'loss': 1.1001, 'grad_norm': 1.215907335281372, 'learning_rate': 9.141817875116248e-05, 'epoch': 0.57}
{'loss': 0.9247, 'grad_norm': 0.9600206613540649, 'learning_rate': 9.136467567107306e-05, 'epoch': 0.57}
{'loss': 0.9595, 'grad_norm': 1.187861442565918, 'learning_rate': 9.131117508136953e-05, 'epoch': 0.57}
{'loss': 1.1216, 'grad_norm': 1.2101595401763916, 'learning_rate': 9.125767699748127e-05, 'epoch': 0.58}
{'loss': 1.0699, 'grad_norm': 0.9643383026123047, 'learning_rate': 9.120418143483688e-05, 'epoch': 0.58}
{'loss': 0.9035, 'grad_norm': 1.403218150138855, 'learning_rate': 9.115068840886417e-05, 'epoch': 0.58}
{'loss': 1.1997, 'grad_norm': 1.3161191940307617, 'learning_rate': 9.10971979349903e-05, 'epoch': 0.58}
{'loss': 1.1204, 'grad_norm': 1.1888877153396606, 'learning_rate': 9.104371002864166e-05, 'epoch': 0.58}
{'loss': 0.7122, 'grad_norm': 1.428242564201355, 'learning_rate': 9.099022470524392e-05, 'epoch': 0.58}
{'loss': 1.0814, 'grad_norm': 1.2469522953033447, 'learning_rate': 9.093674198022201e-05, 'epoch': 0.58}
{'loss': 1.029, 'grad_norm': 1.2691576480865479, 'learning_rate': 9.0883261869e-05, 'epoch': 0.58}
{'loss': 0.9511, 'grad_norm': 1.199711799621582, 'learning_rate': 9.082978438700138e-05, 'epoch': 0.58}
{'loss': 0.7875, 'grad_norm': 1.069469690322876, 'learning_rate': 9.07763095496488e-05, 'epoch': 0.58}
{'loss': 0.9665, 'grad_norm': 0.919071614742279, 'learning_rate': 9.072283737236409e-05, 'epoch': 0.58}
{'loss': 1.4533, 'grad_norm': 1.2674278020858765, 'learning_rate': 9.066936787056842e-05, 'epoch': 0.58}
{'loss': 1.0669, 'grad_norm': 1.6406904458999634, 'learning_rate': 9.061590105968208e-05, 'epoch': 0.58}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2196, 'grad_norm': 1.154958724975586, 'learning_rate': 9.056243695512469e-05, 'epoch': 0.58}
{'loss': 0.7557, 'grad_norm': 1.0637012720108032, 'learning_rate': 9.050897557231504e-05, 'epoch': 0.58}
{'loss': 0.8263, 'grad_norm': 1.147680640220642, 'learning_rate': 9.045551692667108e-05, 'epoch': 0.58}
{'loss': 1.1369, 'grad_norm': 1.0805538892745972, 'learning_rate': 9.040206103361012e-05, 'epoch': 0.58}
{'loss': 0.589, 'grad_norm': 0.9890655875205994, 'learning_rate': 9.034860790854849e-05, 'epoch': 0.58}
{'loss': 0.8968, 'grad_norm': 1.3904600143432617, 'learning_rate': 9.029515756690186e-05, 'epoch': 0.58}
{'loss': 0.7681, 'grad_norm': 1.117708683013916, 'learning_rate': 9.024171002408506e-05, 'epoch': 0.58}
{'loss': 0.9398, 'grad_norm': 1.3166834115982056, 'learning_rate': 9.018826529551208e-05, 'epoch': 0.58}
{'loss': 0.834, 'grad_norm': 1.2981358766555786, 'learning_rate': 9.013482339659615e-05, 'epoch': 0.58}
{'loss': 0.7992, 'grad_norm': 1.4034206867218018, 'learning_rate': 9.00813843427496e-05, 'epoch': 0.58}
{'loss': 1.2247, 'grad_norm': 1.3518589735031128, 'learning_rate': 9.002794814938402e-05, 'epoch': 0.58}
{'loss': 1.0397, 'grad_norm': 2.799204111099243, 'learning_rate': 8.997451483191019e-05, 'epoch': 0.58}
{'loss': 1.1613, 'grad_norm': 2.7511656284332275, 'learning_rate': 8.9921084405738e-05, 'epoch': 0.58}
{'loss': 1.0407, 'grad_norm': 1.367119312286377, 'learning_rate': 8.986765688627652e-05, 'epoch': 0.58}
{'loss': 0.8514, 'grad_norm': 1.3810203075408936, 'learning_rate': 8.981423228893397e-05, 'epoch': 0.58}
{'loss': 1.2385, 'grad_norm': 1.108898639678955, 'learning_rate': 8.976081062911775e-05, 'epoch': 0.58}
{'loss': 0.8444, 'grad_norm': 1.128467321395874, 'learning_rate': 8.970739192223448e-05, 'epoch': 0.58}
{'loss': 0.9493, 'grad_norm': 2.171036958694458, 'learning_rate': 8.965397618368977e-05, 'epoch': 0.58}
{'loss': 1.1805, 'grad_norm': 0.9965960383415222, 'learning_rate': 8.960056342888854e-05, 'epoch': 0.58}
{'loss': 0.987, 'grad_norm': 0.9768473505973816, 'learning_rate': 8.954715367323468e-05, 'epoch': 0.58}
{'loss': 0.9361, 'grad_norm': 1.4887480735778809, 'learning_rate': 8.949374693213134e-05, 'epoch': 0.58}
{'loss': 0.7905, 'grad_norm': 1.0898386240005493, 'learning_rate': 8.944034322098082e-05, 'epoch': 0.58}
{'loss': 1.1825, 'grad_norm': 1.0200955867767334, 'learning_rate': 8.938694255518444e-05, 'epoch': 0.58}
{'loss': 1.0796, 'grad_norm': 1.2567291259765625, 'learning_rate': 8.933354495014272e-05, 'epoch': 0.58}
{'loss': 0.97, 'grad_norm': 1.102410078048706, 'learning_rate': 8.928015042125523e-05, 'epoch': 0.58}
{'loss': 0.7059, 'grad_norm': 1.1204357147216797, 'learning_rate': 8.922675898392072e-05, 'epoch': 0.58}
{'loss': 0.7247, 'grad_norm': 1.0948561429977417, 'learning_rate': 8.917337065353709e-05, 'epoch': 0.58}
{'loss': 0.768, 'grad_norm': 1.224696159362793, 'learning_rate': 8.911998544550116e-05, 'epoch': 0.58}
{'loss': 0.6407, 'grad_norm': 1.0694382190704346, 'learning_rate': 8.906660337520903e-05, 'epoch': 0.58}
{'loss': 1.1227, 'grad_norm': 1.1288139820098877, 'learning_rate': 8.901322445805586e-05, 'epoch': 0.58}
{'loss': 1.004, 'grad_norm': 1.2951240539550781, 'learning_rate': 8.895984870943579e-05, 'epoch': 0.58}
{'loss': 0.9409, 'grad_norm': 1.684496521949768, 'learning_rate': 8.890647614474224e-05, 'epoch': 0.58}
{'loss': 0.8218, 'grad_norm': 1.2088322639465332, 'learning_rate': 8.885310677936746e-05, 'epoch': 0.58}
{'loss': 0.8322, 'grad_norm': 1.0126231908798218, 'learning_rate': 8.879974062870302e-05, 'epoch': 0.58}
{'loss': 1.0181, 'grad_norm': 1.2309985160827637, 'learning_rate': 8.874637770813946e-05, 'epoch': 0.58}
{'loss': 0.686, 'grad_norm': 1.1063926219940186, 'learning_rate': 8.869301803306633e-05, 'epoch': 0.58}
{'loss': 1.0356, 'grad_norm': 1.068305253982544, 'learning_rate': 8.863966161887239e-05, 'epoch': 0.58}
{'loss': 0.7103, 'grad_norm': 1.0769301652908325, 'learning_rate': 8.858630848094526e-05, 'epoch': 0.58}
{'loss': 0.8467, 'grad_norm': 0.9096638560295105, 'learning_rate': 8.853295863467182e-05, 'epoch': 0.58}
{'loss': 0.9096, 'grad_norm': 1.1008810997009277, 'learning_rate': 8.84796120954379e-05, 'epoch': 0.58}
{'loss': 1.0078, 'grad_norm': 1.337239146232605, 'learning_rate': 8.842626887862833e-05, 'epoch': 0.58}
{'loss': 1.1832, 'grad_norm': 1.1553614139556885, 'learning_rate': 8.83729289996271e-05, 'epoch': 0.58}
{'loss': 1.1678, 'grad_norm': 1.0281522274017334, 'learning_rate': 8.831959247381711e-05, 'epoch': 0.58}
{'loss': 1.1111, 'grad_norm': 1.2915266752243042, 'learning_rate': 8.826625931658039e-05, 'epoch': 0.58}
{'loss': 0.9082, 'grad_norm': 0.8690693974494934, 'learning_rate': 8.821292954329798e-05, 'epoch': 0.58}
{'loss': 0.7989, 'grad_norm': 1.128554344177246, 'learning_rate': 8.81596031693499e-05, 'epoch': 0.58}
{'loss': 0.866, 'grad_norm': 0.9353204369544983, 'learning_rate': 8.810628021011528e-05, 'epoch': 0.58}
{'loss': 0.9389, 'grad_norm': 1.5941798686981201, 'learning_rate': 8.805296068097211e-05, 'epoch': 0.58}
{'loss': 1.1545, 'grad_norm': 1.0317620038986206, 'learning_rate': 8.799964459729754e-05, 'epoch': 0.58}
{'loss': 0.9047, 'grad_norm': 1.312894344329834, 'learning_rate': 8.79463319744677e-05, 'epoch': 0.58}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8142, 'grad_norm': 1.2406481504440308, 'learning_rate': 8.789302282785764e-05, 'epoch': 0.58}
{'loss': 0.8541, 'grad_norm': 1.144511103630066, 'learning_rate': 8.783971717284152e-05, 'epoch': 0.58}
{'loss': 0.8984, 'grad_norm': 1.1509919166564941, 'learning_rate': 8.778641502479234e-05, 'epoch': 0.59}
{'loss': 1.2791, 'grad_norm': 1.0767626762390137, 'learning_rate': 8.773311639908225e-05, 'epoch': 0.59}
{'loss': 0.9852, 'grad_norm': 1.2895606756210327, 'learning_rate': 8.767982131108232e-05, 'epoch': 0.59}
{'loss': 0.8638, 'grad_norm': 1.1110949516296387, 'learning_rate': 8.762652977616257e-05, 'epoch': 0.59}
{'loss': 1.1465, 'grad_norm': 1.2583754062652588, 'learning_rate': 8.757324180969207e-05, 'epoch': 0.59}
{'loss': 0.8925, 'grad_norm': 1.4204785823822021, 'learning_rate': 8.751995742703873e-05, 'epoch': 0.59}
{'loss': 0.9045, 'grad_norm': 1.3027842044830322, 'learning_rate': 8.746667664356956e-05, 'epoch': 0.59}
{'loss': 0.9766, 'grad_norm': 0.9306451678276062, 'learning_rate': 8.741339947465054e-05, 'epoch': 0.59}
{'loss': 1.1342, 'grad_norm': 0.9384389519691467, 'learning_rate': 8.736012593564642e-05, 'epoch': 0.59}
{'loss': 0.8989, 'grad_norm': 1.2868400812149048, 'learning_rate': 8.730685604192115e-05, 'epoch': 0.59}
{'loss': 1.0692, 'grad_norm': 1.2485706806182861, 'learning_rate': 8.725358980883742e-05, 'epoch': 0.59}
{'loss': 0.8448, 'grad_norm': 1.0489459037780762, 'learning_rate': 8.720032725175699e-05, 'epoch': 0.59}
{'loss': 0.9331, 'grad_norm': 0.9450671076774597, 'learning_rate': 8.714706838604055e-05, 'epoch': 0.59}
{'loss': 1.0286, 'grad_norm': 1.3706591129302979, 'learning_rate': 8.709381322704769e-05, 'epoch': 0.59}
{'loss': 1.1116, 'grad_norm': 1.3501406908035278, 'learning_rate': 8.704056179013689e-05, 'epoch': 0.59}
{'loss': 1.0622, 'grad_norm': 1.2088302373886108, 'learning_rate': 8.698731409066568e-05, 'epoch': 0.59}
{'loss': 0.8953, 'grad_norm': 1.0026750564575195, 'learning_rate': 8.693407014399039e-05, 'epoch': 0.59}
{'loss': 1.0385, 'grad_norm': 1.4069194793701172, 'learning_rate': 8.68808299654664e-05, 'epoch': 0.59}
{'loss': 0.8749, 'grad_norm': 1.240544319152832, 'learning_rate': 8.682759357044779e-05, 'epoch': 0.59}
{'loss': 0.984, 'grad_norm': 1.2839875221252441, 'learning_rate': 8.677436097428775e-05, 'epoch': 0.59}
{'loss': 0.6856, 'grad_norm': 1.2813498973846436, 'learning_rate': 8.672113219233835e-05, 'epoch': 0.59}
{'loss': 0.7761, 'grad_norm': 1.1927752494812012, 'learning_rate': 8.666790723995042e-05, 'epoch': 0.59}
{'loss': 1.1447, 'grad_norm': 1.079229712486267, 'learning_rate': 8.661468613247387e-05, 'epoch': 0.59}
{'loss': 1.038, 'grad_norm': 1.0982509851455688, 'learning_rate': 8.656146888525734e-05, 'epoch': 0.59}
{'loss': 1.1248, 'grad_norm': 1.1546759605407715, 'learning_rate': 8.650825551364843e-05, 'epoch': 0.59}
{'loss': 1.0201, 'grad_norm': 1.0051523447036743, 'learning_rate': 8.645504603299369e-05, 'epoch': 0.59}
{'loss': 1.0875, 'grad_norm': 1.1532857418060303, 'learning_rate': 8.64018404586384e-05, 'epoch': 0.59}
{'loss': 0.9075, 'grad_norm': 1.2602769136428833, 'learning_rate': 8.634863880592686e-05, 'epoch': 0.59}
{'loss': 0.7003, 'grad_norm': 1.0146452188491821, 'learning_rate': 8.629544109020211e-05, 'epoch': 0.59}
{'loss': 1.0299, 'grad_norm': 1.449354648590088, 'learning_rate': 8.624224732680612e-05, 'epoch': 0.59}
{'loss': 0.885, 'grad_norm': 1.3152329921722412, 'learning_rate': 8.618905753107979e-05, 'epoch': 0.59}
{'loss': 1.1429, 'grad_norm': 1.1789846420288086, 'learning_rate': 8.613587171836271e-05, 'epoch': 0.59}
{'loss': 0.8857, 'grad_norm': 1.413488507270813, 'learning_rate': 8.608268990399349e-05, 'epoch': 0.59}
{'loss': 0.8472, 'grad_norm': 1.215146541595459, 'learning_rate': 8.602951210330942e-05, 'epoch': 0.59}
{'loss': 0.9724, 'grad_norm': 1.1043217182159424, 'learning_rate': 8.597633833164676e-05, 'epoch': 0.59}
{'loss': 0.9218, 'grad_norm': 1.5444813966751099, 'learning_rate': 8.592316860434062e-05, 'epoch': 0.59}
{'loss': 1.1686, 'grad_norm': 1.4747915267944336, 'learning_rate': 8.587000293672481e-05, 'epoch': 0.59}
{'loss': 1.0258, 'grad_norm': 1.0642168521881104, 'learning_rate': 8.581684134413216e-05, 'epoch': 0.59}
{'loss': 0.8994, 'grad_norm': 1.1509413719177246, 'learning_rate': 8.57636838418941e-05, 'epoch': 0.59}
{'loss': 0.9671, 'grad_norm': 1.198982834815979, 'learning_rate': 8.571053044534103e-05, 'epoch': 0.59}
{'loss': 0.9769, 'grad_norm': 1.8124052286148071, 'learning_rate': 8.565738116980222e-05, 'epoch': 0.59}
{'loss': 0.9956, 'grad_norm': 1.1688932180404663, 'learning_rate': 8.560423603060554e-05, 'epoch': 0.59}
{'loss': 0.6054, 'grad_norm': 1.2044442892074585, 'learning_rate': 8.55510950430779e-05, 'epoch': 0.59}
{'loss': 1.1168, 'grad_norm': 1.0972570180892944, 'learning_rate': 8.549795822254482e-05, 'epoch': 0.59}
{'loss': 1.0384, 'grad_norm': 1.0872892141342163, 'learning_rate': 8.544482558433072e-05, 'epoch': 0.59}
{'loss': 0.8667, 'grad_norm': 1.0513534545898438, 'learning_rate': 8.539169714375885e-05, 'epoch': 0.59}
{'loss': 1.1235, 'grad_norm': 1.0458568334579468, 'learning_rate': 8.533857291615114e-05, 'epoch': 0.59}
{'loss': 1.098, 'grad_norm': 1.4015743732452393, 'learning_rate': 8.528545291682838e-05, 'epoch': 0.59}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0336, 'grad_norm': 1.0361692905426025, 'learning_rate': 8.523233716111016e-05, 'epoch': 0.59}
{'loss': 0.878, 'grad_norm': 0.9322877526283264, 'learning_rate': 8.517922566431473e-05, 'epoch': 0.59}
{'loss': 1.1438, 'grad_norm': 12.426352500915527, 'learning_rate': 8.512611844175929e-05, 'epoch': 0.59}
{'loss': 0.9936, 'grad_norm': 1.1938021183013916, 'learning_rate': 8.50730155087596e-05, 'epoch': 0.59}
{'loss': 1.1034, 'grad_norm': 1.4166713953018188, 'learning_rate': 8.501991688063033e-05, 'epoch': 0.59}
{'loss': 0.7656, 'grad_norm': 1.4146853685379028, 'learning_rate': 8.496682257268493e-05, 'epoch': 0.59}
{'loss': 0.8572, 'grad_norm': 1.4505656957626343, 'learning_rate': 8.491373260023545e-05, 'epoch': 0.59}
{'loss': 0.7005, 'grad_norm': 2.152907371520996, 'learning_rate': 8.486064697859285e-05, 'epoch': 0.59}
{'loss': 0.872, 'grad_norm': 0.9778115749359131, 'learning_rate': 8.480756572306674e-05, 'epoch': 0.59}
{'loss': 0.9739, 'grad_norm': 1.1927961111068726, 'learning_rate': 8.475448884896547e-05, 'epoch': 0.59}
{'loss': 0.9132, 'grad_norm': 1.6044974327087402, 'learning_rate': 8.47014163715962e-05, 'epoch': 0.59}
{'loss': 0.9289, 'grad_norm': 1.2471972703933716, 'learning_rate': 8.464834830626476e-05, 'epoch': 0.59}
{'loss': 0.926, 'grad_norm': 0.9654942750930786, 'learning_rate': 8.459528466827575e-05, 'epoch': 0.59}
{'loss': 1.1058, 'grad_norm': 0.9456254839897156, 'learning_rate': 8.45422254729324e-05, 'epoch': 0.59}
{'loss': 0.7971, 'grad_norm': 1.2669132947921753, 'learning_rate': 8.448917073553678e-05, 'epoch': 0.59}
{'loss': 0.677, 'grad_norm': 1.1336450576782227, 'learning_rate': 8.443612047138966e-05, 'epoch': 0.59}
{'loss': 0.7964, 'grad_norm': 2.218867778778076, 'learning_rate': 8.438307469579037e-05, 'epoch': 0.59}
{'loss': 1.2645, 'grad_norm': 1.0140677690505981, 'learning_rate': 8.433003342403715e-05, 'epoch': 0.6}
{'loss': 0.823, 'grad_norm': 1.0067893266677856, 'learning_rate': 8.427699667142682e-05, 'epoch': 0.6}
{'loss': 1.1715, 'grad_norm': 0.9639583230018616, 'learning_rate': 8.422396445325487e-05, 'epoch': 0.6}
{'loss': 0.9207, 'grad_norm': 1.513429045677185, 'learning_rate': 8.417093678481563e-05, 'epoch': 0.6}
{'loss': 1.1595, 'grad_norm': 1.0435495376586914, 'learning_rate': 8.411791368140196e-05, 'epoch': 0.6}
{'loss': 0.8689, 'grad_norm': 2.238445281982422, 'learning_rate': 8.406489515830553e-05, 'epoch': 0.6}
{'loss': 1.0489, 'grad_norm': 1.2028248310089111, 'learning_rate': 8.401188123081653e-05, 'epoch': 0.6}
{'loss': 1.1348, 'grad_norm': 1.7560070753097534, 'learning_rate': 8.395887191422397e-05, 'epoch': 0.6}
{'loss': 1.0419, 'grad_norm': 1.2889200448989868, 'learning_rate': 8.390586722381554e-05, 'epoch': 0.6}
{'loss': 1.0038, 'grad_norm': 1.1707488298416138, 'learning_rate': 8.385286717487743e-05, 'epoch': 0.6}
{'loss': 0.7944, 'grad_norm': 1.2149397134780884, 'learning_rate': 8.37998717826947e-05, 'epoch': 0.6}
{'loss': 1.0111, 'grad_norm': 1.279102087020874, 'learning_rate': 8.374688106255089e-05, 'epoch': 0.6}
{'loss': 0.9326, 'grad_norm': 1.0956765413284302, 'learning_rate': 8.369389502972828e-05, 'epoch': 0.6}
{'loss': 0.9279, 'grad_norm': 1.2779464721679688, 'learning_rate': 8.364091369950782e-05, 'epoch': 0.6}
{'loss': 0.9107, 'grad_norm': 1.5647891759872437, 'learning_rate': 8.358793708716905e-05, 'epoch': 0.6}
{'loss': 0.6602, 'grad_norm': 0.9985323548316956, 'learning_rate': 8.353496520799021e-05, 'epoch': 0.6}
{'loss': 1.2297, 'grad_norm': 1.5956523418426514, 'learning_rate': 8.348199807724806e-05, 'epoch': 0.6}
{'loss': 1.0732, 'grad_norm': 1.2540578842163086, 'learning_rate': 8.34290357102181e-05, 'epoch': 0.6}
{'loss': 0.801, 'grad_norm': 1.0223952531814575, 'learning_rate': 8.337607812217446e-05, 'epoch': 0.6}
{'loss': 0.8779, 'grad_norm': 1.2517592906951904, 'learning_rate': 8.332312532838978e-05, 'epoch': 0.6}
{'loss': 0.9797, 'grad_norm': 1.2220743894577026, 'learning_rate': 8.327017734413544e-05, 'epoch': 0.6}
{'loss': 1.0994, 'grad_norm': 1.0765749216079712, 'learning_rate': 8.32172341846814e-05, 'epoch': 0.6}
{'loss': 0.975, 'grad_norm': 1.0149085521697998, 'learning_rate': 8.316429586529615e-05, 'epoch': 0.6}
{'loss': 1.051, 'grad_norm': 1.0930230617523193, 'learning_rate': 8.311136240124691e-05, 'epoch': 0.6}
{'loss': 0.8286, 'grad_norm': 1.012805700302124, 'learning_rate': 8.305843380779938e-05, 'epoch': 0.6}
{'loss': 1.0713, 'grad_norm': 1.2803411483764648, 'learning_rate': 8.300551010021795e-05, 'epoch': 0.6}
{'loss': 0.9835, 'grad_norm': 0.9197782278060913, 'learning_rate': 8.295259129376557e-05, 'epoch': 0.6}
{'loss': 1.0478, 'grad_norm': 0.9486522078514099, 'learning_rate': 8.289967740370371e-05, 'epoch': 0.6}
{'loss': 0.8059, 'grad_norm': 1.5730012655258179, 'learning_rate': 8.284676844529257e-05, 'epoch': 0.6}
{'loss': 0.7472, 'grad_norm': 1.082261562347412, 'learning_rate': 8.279386443379076e-05, 'epoch': 0.6}
{'loss': 0.7227, 'grad_norm': 1.2126901149749756, 'learning_rate': 8.274096538445557e-05, 'epoch': 0.6}
{'loss': 1.1892, 'grad_norm': 1.2322051525115967, 'learning_rate': 8.268807131254287e-05, 'epoch': 0.6}
{'loss': 0.9269, 'grad_norm': 1.1306381225585938, 'learning_rate': 8.263518223330697e-05, 'epoch': 0.6}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0052, 'grad_norm': 1.4282253980636597, 'learning_rate': 8.25822981620009e-05, 'epoch': 0.6}
{'loss': 0.9839, 'grad_norm': 1.4647163152694702, 'learning_rate': 8.252941911387613e-05, 'epoch': 0.6}
{'loss': 0.9815, 'grad_norm': 1.070939540863037, 'learning_rate': 8.247654510418275e-05, 'epoch': 0.6}
{'loss': 0.9313, 'grad_norm': 1.6226427555084229, 'learning_rate': 8.242367614816938e-05, 'epoch': 0.6}
{'loss': 0.7487, 'grad_norm': 1.5120394229888916, 'learning_rate': 8.237081226108311e-05, 'epoch': 0.6}
{'loss': 1.007, 'grad_norm': 1.0728847980499268, 'learning_rate': 8.231795345816972e-05, 'epoch': 0.6}
{'loss': 0.8567, 'grad_norm': 1.121534824371338, 'learning_rate': 8.226509975467334e-05, 'epoch': 0.6}
{'loss': 0.7931, 'grad_norm': 1.2732468843460083, 'learning_rate': 8.221225116583678e-05, 'epoch': 0.6}
{'loss': 0.7926, 'grad_norm': 1.133029818534851, 'learning_rate': 8.215940770690133e-05, 'epoch': 0.6}
{'loss': 0.8171, 'grad_norm': 1.113806962966919, 'learning_rate': 8.210656939310672e-05, 'epoch': 0.6}
{'loss': 0.7683, 'grad_norm': 1.5739059448242188, 'learning_rate': 8.205373623969136e-05, 'epoch': 0.6}
{'loss': 0.6776, 'grad_norm': 1.1962260007858276, 'learning_rate': 8.200090826189202e-05, 'epoch': 0.6}
{'loss': 0.833, 'grad_norm': 1.6253843307495117, 'learning_rate': 8.194808547494401e-05, 'epoch': 0.6}
{'loss': 0.8669, 'grad_norm': 1.260857343673706, 'learning_rate': 8.189526789408123e-05, 'epoch': 0.6}
{'loss': 0.8339, 'grad_norm': 1.06594979763031, 'learning_rate': 8.184245553453596e-05, 'epoch': 0.6}
{'loss': 0.7779, 'grad_norm': 1.1167519092559814, 'learning_rate': 8.17896484115391e-05, 'epoch': 0.6}
{'loss': 0.8576, 'grad_norm': 1.054550051689148, 'learning_rate': 8.173684654031989e-05, 'epoch': 0.6}
{'loss': 0.9237, 'grad_norm': 1.175575613975525, 'learning_rate': 8.168404993610617e-05, 'epoch': 0.6}
{'loss': 0.8238, 'grad_norm': 0.9436169862747192, 'learning_rate': 8.163125861412426e-05, 'epoch': 0.6}
{'loss': 0.7745, 'grad_norm': 1.0819624662399292, 'learning_rate': 8.157847258959885e-05, 'epoch': 0.6}
{'loss': 0.9325, 'grad_norm': 1.1116243600845337, 'learning_rate': 8.152569187775325e-05, 'epoch': 0.6}
{'loss': 1.0757, 'grad_norm': 1.1726130247116089, 'learning_rate': 8.147291649380912e-05, 'epoch': 0.6}
{'loss': 1.0295, 'grad_norm': 0.9593953490257263, 'learning_rate': 8.142014645298661e-05, 'epoch': 0.6}
{'loss': 1.1297, 'grad_norm': 0.9347360730171204, 'learning_rate': 8.13673817705044e-05, 'epoch': 0.6}
{'loss': 0.9539, 'grad_norm': 0.9227917790412903, 'learning_rate': 8.131462246157953e-05, 'epoch': 0.6}
{'loss': 0.6875, 'grad_norm': 1.0300376415252686, 'learning_rate': 8.126186854142752e-05, 'epoch': 0.6}
{'loss': 0.7306, 'grad_norm': 1.4283592700958252, 'learning_rate': 8.120912002526242e-05, 'epoch': 0.6}
{'loss': 0.9356, 'grad_norm': 1.1699621677398682, 'learning_rate': 8.115637692829657e-05, 'epoch': 0.6}
{'loss': 0.8272, 'grad_norm': 1.3874200582504272, 'learning_rate': 8.110363926574087e-05, 'epoch': 0.6}
{'loss': 1.0007, 'grad_norm': 0.9428631663322449, 'learning_rate': 8.105090705280456e-05, 'epoch': 0.6}
{'loss': 1.0857, 'grad_norm': 1.054306149482727, 'learning_rate': 8.099818030469538e-05, 'epoch': 0.6}
{'loss': 0.8446, 'grad_norm': 1.362696647644043, 'learning_rate': 8.094545903661953e-05, 'epoch': 0.6}
{'loss': 1.03, 'grad_norm': 1.039890170097351, 'learning_rate': 8.089274326378145e-05, 'epoch': 0.61}
{'loss': 0.8322, 'grad_norm': 1.0083727836608887, 'learning_rate': 8.084003300138422e-05, 'epoch': 0.61}
{'loss': 1.1824, 'grad_norm': 0.9735260009765625, 'learning_rate': 8.078732826462915e-05, 'epoch': 0.61}
{'loss': 1.005, 'grad_norm': 1.1625269651412964, 'learning_rate': 8.07346290687161e-05, 'epoch': 0.61}
{'loss': 0.7814, 'grad_norm': 0.9861729741096497, 'learning_rate': 8.068193542884325e-05, 'epoch': 0.61}
{'loss': 0.9466, 'grad_norm': 0.9468863010406494, 'learning_rate': 8.062924736020714e-05, 'epoch': 0.61}
{'loss': 0.7891, 'grad_norm': 1.039807677268982, 'learning_rate': 8.057656487800282e-05, 'epoch': 0.61}
{'loss': 1.0623, 'grad_norm': 1.1220053434371948, 'learning_rate': 8.052388799742361e-05, 'epoch': 0.61}
{'loss': 1.036, 'grad_norm': 1.013309121131897, 'learning_rate': 8.047121673366129e-05, 'epoch': 0.61}
{'loss': 1.0542, 'grad_norm': 1.780413269996643, 'learning_rate': 8.041855110190604e-05, 'epoch': 0.61}
{'loss': 1.0941, 'grad_norm': 0.9615735411643982, 'learning_rate': 8.036589111734629e-05, 'epoch': 0.61}
{'loss': 0.9199, 'grad_norm': 0.9030556678771973, 'learning_rate': 8.0313236795169e-05, 'epoch': 0.61}
{'loss': 0.9298, 'grad_norm': 1.1157865524291992, 'learning_rate': 8.026058815055936e-05, 'epoch': 0.61}
{'loss': 0.7727, 'grad_norm': 1.3923949003219604, 'learning_rate': 8.020794519870106e-05, 'epoch': 0.61}
{'loss': 0.9393, 'grad_norm': 1.4978058338165283, 'learning_rate': 8.015530795477603e-05, 'epoch': 0.61}
{'loss': 1.0935, 'grad_norm': 1.1463139057159424, 'learning_rate': 8.010267643396457e-05, 'epoch': 0.61}
{'loss': 0.7672, 'grad_norm': 0.9167740345001221, 'learning_rate': 8.005005065144543e-05, 'epoch': 0.61}
{'loss': 0.6682, 'grad_norm': 0.9188897013664246, 'learning_rate': 7.999743062239557e-05, 'epoch': 0.61}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0818, 'grad_norm': 1.2839034795761108, 'learning_rate': 7.994481636199036e-05, 'epoch': 0.61}
{'loss': 0.6477, 'grad_norm': 1.2058035135269165, 'learning_rate': 7.989220788540355e-05, 'epoch': 0.61}
{'loss': 1.1284, 'grad_norm': 1.4379149675369263, 'learning_rate': 7.98396052078071e-05, 'epoch': 0.61}
{'loss': 0.7188, 'grad_norm': 1.1801098585128784, 'learning_rate': 7.978700834437143e-05, 'epoch': 0.61}
{'loss': 1.1244, 'grad_norm': 1.3169069290161133, 'learning_rate': 7.973441731026518e-05, 'epoch': 0.61}
{'loss': 1.1281, 'grad_norm': 1.035057783126831, 'learning_rate': 7.968183212065537e-05, 'epoch': 0.61}
{'loss': 1.4371, 'grad_norm': 1.001632809638977, 'learning_rate': 7.962925279070732e-05, 'epoch': 0.61}
{'loss': 0.6527, 'grad_norm': 1.6049941778182983, 'learning_rate': 7.957667933558461e-05, 'epoch': 0.61}
{'loss': 0.9435, 'grad_norm': 1.5052545070648193, 'learning_rate': 7.952411177044923e-05, 'epoch': 0.61}
{'loss': 0.8189, 'grad_norm': 3.6493730545043945, 'learning_rate': 7.947155011046144e-05, 'epoch': 0.61}
{'loss': 1.1622, 'grad_norm': 1.201970100402832, 'learning_rate': 7.941899437077965e-05, 'epoch': 0.61}
{'loss': 1.2239, 'grad_norm': 1.3336113691329956, 'learning_rate': 7.936644456656081e-05, 'epoch': 0.61}
{'loss': 0.9468, 'grad_norm': 1.082445502281189, 'learning_rate': 7.931390071295994e-05, 'epoch': 0.61}
{'loss': 0.9384, 'grad_norm': 1.27800452709198, 'learning_rate': 7.926136282513045e-05, 'epoch': 0.61}
{'loss': 0.674, 'grad_norm': 1.1772361993789673, 'learning_rate': 7.920883091822408e-05, 'epoch': 0.61}
{'loss': 0.9796, 'grad_norm': 1.0660239458084106, 'learning_rate': 7.915630500739071e-05, 'epoch': 0.61}
{'loss': 0.9489, 'grad_norm': 1.440322995185852, 'learning_rate': 7.910378510777857e-05, 'epoch': 0.61}
{'loss': 1.0036, 'grad_norm': 1.1765334606170654, 'learning_rate': 7.905127123453415e-05, 'epoch': 0.61}
{'loss': 1.116, 'grad_norm': 1.1214598417282104, 'learning_rate': 7.89987634028022e-05, 'epoch': 0.61}
{'loss': 1.089, 'grad_norm': 1.004150390625, 'learning_rate': 7.894626162772578e-05, 'epoch': 0.61}
{'loss': 1.1335, 'grad_norm': 1.159963607788086, 'learning_rate': 7.889376592444605e-05, 'epoch': 0.61}
{'loss': 1.1264, 'grad_norm': 1.0953502655029297, 'learning_rate': 7.88412763081026e-05, 'epoch': 0.61}
{'loss': 0.9301, 'grad_norm': 1.105299711227417, 'learning_rate': 7.878879279383314e-05, 'epoch': 0.61}
{'loss': 0.7978, 'grad_norm': 1.097252607345581, 'learning_rate': 7.873631539677364e-05, 'epoch': 0.61}
{'loss': 0.7594, 'grad_norm': 0.9918442368507385, 'learning_rate': 7.868384413205842e-05, 'epoch': 0.61}
{'loss': 1.3295, 'grad_norm': 1.476976990699768, 'learning_rate': 7.863137901481983e-05, 'epoch': 0.61}
{'loss': 0.9265, 'grad_norm': 1.0771054029464722, 'learning_rate': 7.857892006018862e-05, 'epoch': 0.61}
{'loss': 1.1771, 'grad_norm': 0.9495362639427185, 'learning_rate': 7.852646728329368e-05, 'epoch': 0.61}
{'loss': 1.1382, 'grad_norm': 1.3369951248168945, 'learning_rate': 7.847402069926215e-05, 'epoch': 0.61}
{'loss': 0.8939, 'grad_norm': 1.0676082372665405, 'learning_rate': 7.84215803232194e-05, 'epoch': 0.61}
{'loss': 0.7861, 'grad_norm': 1.5966713428497314, 'learning_rate': 7.836914617028891e-05, 'epoch': 0.61}
{'loss': 0.9699, 'grad_norm': 1.1280211210250854, 'learning_rate': 7.831671825559252e-05, 'epoch': 0.61}
{'loss': 0.8332, 'grad_norm': 0.9861366748809814, 'learning_rate': 7.82642965942501e-05, 'epoch': 0.61}
{'loss': 0.9088, 'grad_norm': 1.1613798141479492, 'learning_rate': 7.821188120137986e-05, 'epoch': 0.61}
{'loss': 1.0152, 'grad_norm': 1.2644386291503906, 'learning_rate': 7.815947209209817e-05, 'epoch': 0.61}
{'loss': 1.2203, 'grad_norm': 1.109782338142395, 'learning_rate': 7.81070692815195e-05, 'epoch': 0.61}
{'loss': 0.772, 'grad_norm': 1.2341701984405518, 'learning_rate': 7.805467278475663e-05, 'epoch': 0.61}
{'loss': 1.1797, 'grad_norm': 0.9720003604888916, 'learning_rate': 7.800228261692037e-05, 'epoch': 0.61}
{'loss': 0.8058, 'grad_norm': 1.092793583869934, 'learning_rate': 7.794989879311991e-05, 'epoch': 0.61}
{'loss': 0.9647, 'grad_norm': 1.0635789632797241, 'learning_rate': 7.789752132846239e-05, 'epoch': 0.61}
{'loss': 1.0594, 'grad_norm': 1.1562378406524658, 'learning_rate': 7.784515023805328e-05, 'epoch': 0.61}
{'loss': 0.9676, 'grad_norm': 1.1250343322753906, 'learning_rate': 7.779278553699614e-05, 'epoch': 0.61}
{'loss': 1.0949, 'grad_norm': 1.0649583339691162, 'learning_rate': 7.774042724039267e-05, 'epoch': 0.61}
{'loss': 1.0541, 'grad_norm': 1.3052127361297607, 'learning_rate': 7.768807536334276e-05, 'epoch': 0.61}
{'loss': 1.1412, 'grad_norm': 1.6781666278839111, 'learning_rate': 7.763572992094447e-05, 'epoch': 0.61}
{'loss': 1.0508, 'grad_norm': 1.1058841943740845, 'learning_rate': 7.758339092829395e-05, 'epoch': 0.61}
{'loss': 0.6501, 'grad_norm': 1.051734447479248, 'learning_rate': 7.753105840048548e-05, 'epoch': 0.61}
{'loss': 0.8955, 'grad_norm': 1.3928709030151367, 'learning_rate': 7.747873235261157e-05, 'epoch': 0.62}
{'loss': 1.0619, 'grad_norm': 0.9119873046875, 'learning_rate': 7.742641279976278e-05, 'epoch': 0.62}
{'loss': 1.2538, 'grad_norm': 0.9539655447006226, 'learning_rate': 7.73740997570278e-05, 'epoch': 0.62}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1539, 'grad_norm': 1.079582929611206, 'learning_rate': 7.732179323949347e-05, 'epoch': 0.62}
{'loss': 0.9165, 'grad_norm': 1.0850176811218262, 'learning_rate': 7.726949326224472e-05, 'epoch': 0.62}
{'loss': 0.7996, 'grad_norm': 1.0614597797393799, 'learning_rate': 7.721719984036469e-05, 'epoch': 0.62}
{'loss': 0.7413, 'grad_norm': 1.0487136840820312, 'learning_rate': 7.716491298893442e-05, 'epoch': 0.62}
{'loss': 0.8765, 'grad_norm': 1.2847033739089966, 'learning_rate': 7.711263272303331e-05, 'epoch': 0.62}
{'loss': 0.8404, 'grad_norm': 1.0257697105407715, 'learning_rate': 7.706035905773866e-05, 'epoch': 0.62}
{'loss': 0.8667, 'grad_norm': 1.13185715675354, 'learning_rate': 7.700809200812595e-05, 'epoch': 0.62}
{'loss': 0.9787, 'grad_norm': 1.5379260778427124, 'learning_rate': 7.69558315892688e-05, 'epoch': 0.62}
{'loss': 0.8417, 'grad_norm': 1.145824670791626, 'learning_rate': 7.690357781623879e-05, 'epoch': 0.62}
{'loss': 0.8746, 'grad_norm': 1.3846336603164673, 'learning_rate': 7.685133070410571e-05, 'epoch': 0.62}
{'loss': 1.178, 'grad_norm': 1.1227989196777344, 'learning_rate': 7.679909026793735e-05, 'epoch': 0.62}
{'loss': 1.2138, 'grad_norm': 1.309177279472351, 'learning_rate': 7.67468565227996e-05, 'epoch': 0.62}
{'loss': 0.7297, 'grad_norm': 1.3757374286651611, 'learning_rate': 7.669462948375646e-05, 'epoch': 0.62}
{'loss': 0.977, 'grad_norm': 0.9637895822525024, 'learning_rate': 7.664240916586989e-05, 'epoch': 0.62}
{'loss': 0.8422, 'grad_norm': 1.0271130800247192, 'learning_rate': 7.659019558420004e-05, 'epoch': 0.62}
{'loss': 0.9777, 'grad_norm': 1.2719136476516724, 'learning_rate': 7.6537988753805e-05, 'epoch': 0.62}
{'loss': 0.9451, 'grad_norm': 1.1440433263778687, 'learning_rate': 7.6485788689741e-05, 'epoch': 0.62}
{'loss': 0.8053, 'grad_norm': 1.2495355606079102, 'learning_rate': 7.643359540706232e-05, 'epoch': 0.62}
{'loss': 1.1078, 'grad_norm': 1.152216911315918, 'learning_rate': 7.638140892082117e-05, 'epoch': 0.62}
{'loss': 1.1173, 'grad_norm': 1.5468595027923584, 'learning_rate': 7.632922924606795e-05, 'epoch': 0.62}
{'loss': 0.8036, 'grad_norm': 0.9215074181556702, 'learning_rate': 7.627705639785098e-05, 'epoch': 0.62}
{'loss': 0.5576, 'grad_norm': 1.0292785167694092, 'learning_rate': 7.622489039121667e-05, 'epoch': 0.62}
{'loss': 1.0979, 'grad_norm': 1.2719358205795288, 'learning_rate': 7.617273124120951e-05, 'epoch': 0.62}
{'loss': 1.0446, 'grad_norm': 1.4079856872558594, 'learning_rate': 7.612057896287186e-05, 'epoch': 0.62}
{'loss': 0.9895, 'grad_norm': 1.2327814102172852, 'learning_rate': 7.606843357124426e-05, 'epoch': 0.62}
{'loss': 0.9533, 'grad_norm': 1.1130342483520508, 'learning_rate': 7.60162950813651e-05, 'epoch': 0.62}
{'loss': 0.9063, 'grad_norm': 1.11005699634552, 'learning_rate': 7.596416350827091e-05, 'epoch': 0.62}
{'loss': 1.1354, 'grad_norm': 1.1882946491241455, 'learning_rate': 7.591203886699625e-05, 'epoch': 0.62}
{'loss': 0.9981, 'grad_norm': 1.1217855215072632, 'learning_rate': 7.58599211725735e-05, 'epoch': 0.62}
{'loss': 1.0543, 'grad_norm': 1.2274219989776611, 'learning_rate': 7.580781044003324e-05, 'epoch': 0.62}
{'loss': 1.1845, 'grad_norm': 1.1403030157089233, 'learning_rate': 7.575570668440392e-05, 'epoch': 0.62}
{'loss': 0.8633, 'grad_norm': 1.1404160261154175, 'learning_rate': 7.570360992071199e-05, 'epoch': 0.62}
{'loss': 1.1394, 'grad_norm': 0.932776153087616, 'learning_rate': 7.5651520163982e-05, 'epoch': 0.62}
{'loss': 0.9466, 'grad_norm': 1.2688053846359253, 'learning_rate': 7.559943742923626e-05, 'epoch': 0.62}
{'loss': 1.0437, 'grad_norm': 1.0167348384857178, 'learning_rate': 7.554736173149524e-05, 'epoch': 0.62}
{'loss': 0.9131, 'grad_norm': 1.2593241930007935, 'learning_rate': 7.549529308577736e-05, 'epoch': 0.62}
{'loss': 1.2143, 'grad_norm': 0.8710053563117981, 'learning_rate': 7.54432315070989e-05, 'epoch': 0.62}
{'loss': 0.7799, 'grad_norm': 1.400874137878418, 'learning_rate': 7.53911770104742e-05, 'epoch': 0.62}
{'loss': 1.1414, 'grad_norm': 1.5323748588562012, 'learning_rate': 7.533912961091551e-05, 'epoch': 0.62}
{'loss': 1.1581, 'grad_norm': 1.210182547569275, 'learning_rate': 7.528708932343304e-05, 'epoch': 0.62}
{'loss': 0.8759, 'grad_norm': 1.1923770904541016, 'learning_rate': 7.5235056163035e-05, 'epoch': 0.62}
{'loss': 0.874, 'grad_norm': 1.0574404001235962, 'learning_rate': 7.518303014472748e-05, 'epoch': 0.62}
{'loss': 0.9567, 'grad_norm': 1.118924856185913, 'learning_rate': 7.513101128351454e-05, 'epoch': 0.62}
{'loss': 1.1843, 'grad_norm': 1.379546880722046, 'learning_rate': 7.507899959439814e-05, 'epoch': 0.62}
{'loss': 1.0163, 'grad_norm': 1.9489885568618774, 'learning_rate': 7.502699509237822e-05, 'epoch': 0.62}
{'loss': 0.9251, 'grad_norm': 1.1609827280044556, 'learning_rate': 7.497499779245267e-05, 'epoch': 0.62}
{'loss': 0.8891, 'grad_norm': 1.1970603466033936, 'learning_rate': 7.492300770961718e-05, 'epoch': 0.62}
{'loss': 0.8287, 'grad_norm': 1.2110251188278198, 'learning_rate': 7.487102485886552e-05, 'epoch': 0.62}
{'loss': 0.8109, 'grad_norm': 0.9881865382194519, 'learning_rate': 7.48190492551892e-05, 'epoch': 0.62}
{'loss': 0.9391, 'grad_norm': 0.9978121519088745, 'learning_rate': 7.476708091357782e-05, 'epoch': 0.62}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6472, 'grad_norm': 1.3382009267807007, 'learning_rate': 7.471511984901878e-05, 'epoch': 0.62}
{'loss': 0.8964, 'grad_norm': 1.2690142393112183, 'learning_rate': 7.466316607649738e-05, 'epoch': 0.62}
{'loss': 1.0903, 'grad_norm': 1.5695589780807495, 'learning_rate': 7.461121961099684e-05, 'epoch': 0.62}
{'loss': 1.1315, 'grad_norm': 1.6355842351913452, 'learning_rate': 7.455928046749827e-05, 'epoch': 0.62}
{'loss': 1.1725, 'grad_norm': 1.145909070968628, 'learning_rate': 7.450734866098066e-05, 'epoch': 0.62}
{'loss': 0.9836, 'grad_norm': 0.9916096925735474, 'learning_rate': 7.445542420642097e-05, 'epoch': 0.62}
{'loss': 1.0245, 'grad_norm': 1.0419397354125977, 'learning_rate': 7.440350711879384e-05, 'epoch': 0.62}
{'loss': 1.0119, 'grad_norm': 1.0131480693817139, 'learning_rate': 7.435159741307202e-05, 'epoch': 0.62}
{'loss': 1.122, 'grad_norm': 1.298697590827942, 'learning_rate': 7.429969510422592e-05, 'epoch': 0.62}
{'loss': 1.1686, 'grad_norm': 1.2307859659194946, 'learning_rate': 7.424780020722397e-05, 'epoch': 0.62}
{'loss': 1.163, 'grad_norm': 0.9618411660194397, 'learning_rate': 7.419591273703244e-05, 'epoch': 0.62}
{'loss': 0.8438, 'grad_norm': 1.079950213432312, 'learning_rate': 7.414403270861536e-05, 'epoch': 0.62}
{'loss': 1.0501, 'grad_norm': 1.1058536767959595, 'learning_rate': 7.409216013693471e-05, 'epoch': 0.63}
{'loss': 0.9727, 'grad_norm': 1.4426716566085815, 'learning_rate': 7.404029503695028e-05, 'epoch': 0.63}
{'loss': 0.8628, 'grad_norm': 1.187754511833191, 'learning_rate': 7.398843742361972e-05, 'epoch': 0.63}
{'loss': 0.8713, 'grad_norm': 1.0935667753219604, 'learning_rate': 7.393658731189857e-05, 'epoch': 0.63}
{'loss': 1.2429, 'grad_norm': 1.3368337154388428, 'learning_rate': 7.388474471674006e-05, 'epoch': 0.63}
{'loss': 0.9357, 'grad_norm': 1.0158061981201172, 'learning_rate': 7.383290965309541e-05, 'epoch': 0.63}
{'loss': 0.9662, 'grad_norm': 1.0087898969650269, 'learning_rate': 7.378108213591355e-05, 'epoch': 0.63}
{'loss': 1.2478, 'grad_norm': 1.3988549709320068, 'learning_rate': 7.372926218014131e-05, 'epoch': 0.63}
{'loss': 1.031, 'grad_norm': 1.1352055072784424, 'learning_rate': 7.367744980072338e-05, 'epoch': 0.63}
{'loss': 0.9413, 'grad_norm': 1.3036303520202637, 'learning_rate': 7.362564501260207e-05, 'epoch': 0.63}
{'loss': 0.7385, 'grad_norm': 1.3697502613067627, 'learning_rate': 7.357384783071772e-05, 'epoch': 0.63}
{'loss': 1.1718, 'grad_norm': 0.8539835214614868, 'learning_rate': 7.35220582700084e-05, 'epoch': 0.63}
{'loss': 1.0182, 'grad_norm': 1.4376167058944702, 'learning_rate': 7.347027634540993e-05, 'epoch': 0.63}
{'loss': 0.7814, 'grad_norm': 1.0454987287521362, 'learning_rate': 7.3418502071856e-05, 'epoch': 0.63}
{'loss': 0.6754, 'grad_norm': 1.0154755115509033, 'learning_rate': 7.336673546427801e-05, 'epoch': 0.63}
{'loss': 0.7625, 'grad_norm': 1.3812634944915771, 'learning_rate': 7.331497653760521e-05, 'epoch': 0.63}
{'loss': 0.9218, 'grad_norm': 1.358901023864746, 'learning_rate': 7.32632253067647e-05, 'epoch': 0.63}
{'loss': 0.9463, 'grad_norm': 1.015771508216858, 'learning_rate': 7.32114817866812e-05, 'epoch': 0.63}
{'loss': 0.8592, 'grad_norm': 1.1426753997802734, 'learning_rate': 7.315974599227735e-05, 'epoch': 0.63}
{'loss': 0.8435, 'grad_norm': 1.1356682777404785, 'learning_rate': 7.310801793847344e-05, 'epoch': 0.63}
{'loss': 1.0817, 'grad_norm': 1.0249156951904297, 'learning_rate': 7.305629764018764e-05, 'epoch': 0.63}
{'loss': 1.0354, 'grad_norm': 1.2522221803665161, 'learning_rate': 7.300458511233583e-05, 'epoch': 0.63}
{'loss': 0.7954, 'grad_norm': 1.0804640054702759, 'learning_rate': 7.295288036983163e-05, 'epoch': 0.63}
{'loss': 1.0973, 'grad_norm': 1.393002986907959, 'learning_rate': 7.29011834275865e-05, 'epoch': 0.63}
{'loss': 1.1565, 'grad_norm': 1.125512957572937, 'learning_rate': 7.28494943005095e-05, 'epoch': 0.63}
{'loss': 0.5879, 'grad_norm': 1.0509693622589111, 'learning_rate': 7.279781300350758e-05, 'epoch': 0.63}
{'loss': 0.8271, 'grad_norm': 0.9713096618652344, 'learning_rate': 7.27461395514854e-05, 'epoch': 0.63}
{'loss': 0.9784, 'grad_norm': 1.5028411149978638, 'learning_rate': 7.269447395934526e-05, 'epoch': 0.63}
{'loss': 1.2175, 'grad_norm': 1.1605197191238403, 'learning_rate': 7.264281624198736e-05, 'epoch': 0.63}
{'loss': 0.8278, 'grad_norm': 1.4092888832092285, 'learning_rate': 7.259116641430943e-05, 'epoch': 0.63}
{'loss': 0.9608, 'grad_norm': 1.240678071975708, 'learning_rate': 7.253952449120712e-05, 'epoch': 0.63}
{'loss': 0.8365, 'grad_norm': 0.9790722727775574, 'learning_rate': 7.248789048757368e-05, 'epoch': 0.63}
{'loss': 0.9659, 'grad_norm': 0.9717844724655151, 'learning_rate': 7.243626441830009e-05, 'epoch': 0.63}
{'loss': 0.8418, 'grad_norm': 1.4336427450180054, 'learning_rate': 7.23846462982751e-05, 'epoch': 0.63}
{'loss': 0.8045, 'grad_norm': 1.0420819520950317, 'learning_rate': 7.233303614238509e-05, 'epoch': 0.63}
{'loss': 0.7859, 'grad_norm': 1.5745548009872437, 'learning_rate': 7.22814339655142e-05, 'epoch': 0.63}
{'loss': 0.9526, 'grad_norm': 0.8802925944328308, 'learning_rate': 7.222983978254426e-05, 'epoch': 0.63}
{'loss': 0.8965, 'grad_norm': 1.4483451843261719, 'learning_rate': 7.217825360835473e-05, 'epoch': 0.63}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.5887, 'grad_norm': 1.110153317451477, 'learning_rate': 7.212667545782293e-05, 'epoch': 0.63}
{'loss': 1.4959, 'grad_norm': 1.0848884582519531, 'learning_rate': 7.207510534582361e-05, 'epoch': 0.63}
{'loss': 0.8708, 'grad_norm': 1.077889323234558, 'learning_rate': 7.202354328722938e-05, 'epoch': 0.63}
{'loss': 0.788, 'grad_norm': 0.9532608985900879, 'learning_rate': 7.197198929691059e-05, 'epoch': 0.63}
{'loss': 0.7186, 'grad_norm': 1.3720647096633911, 'learning_rate': 7.192044338973503e-05, 'epoch': 0.63}
{'loss': 0.7736, 'grad_norm': 0.9309107065200806, 'learning_rate': 7.186890558056836e-05, 'epoch': 0.63}
{'loss': 1.1336, 'grad_norm': 1.1545330286026, 'learning_rate': 7.181737588427384e-05, 'epoch': 0.63}
{'loss': 0.7985, 'grad_norm': 2.36226749420166, 'learning_rate': 7.176585431571235e-05, 'epoch': 0.63}
{'loss': 0.9528, 'grad_norm': 0.8990804553031921, 'learning_rate': 7.171434088974251e-05, 'epoch': 0.63}
{'loss': 0.9099, 'grad_norm': 1.1489636898040771, 'learning_rate': 7.166283562122049e-05, 'epoch': 0.63}
{'loss': 0.7143, 'grad_norm': 1.0858800411224365, 'learning_rate': 7.161133852500019e-05, 'epoch': 0.63}
{'loss': 0.6329, 'grad_norm': 1.1256989240646362, 'learning_rate': 7.155984961593316e-05, 'epoch': 0.63}
{'loss': 1.0559, 'grad_norm': 1.3833355903625488, 'learning_rate': 7.150836890886847e-05, 'epoch': 0.63}
{'loss': 0.7559, 'grad_norm': 1.029788851737976, 'learning_rate': 7.145689641865301e-05, 'epoch': 0.63}
{'loss': 0.7006, 'grad_norm': 0.8701285123825073, 'learning_rate': 7.14054321601311e-05, 'epoch': 0.63}
{'loss': 1.2419, 'grad_norm': 1.1372811794281006, 'learning_rate': 7.135397614814481e-05, 'epoch': 0.63}
{'loss': 0.8084, 'grad_norm': 1.1399577856063843, 'learning_rate': 7.130252839753385e-05, 'epoch': 0.63}
{'loss': 0.8775, 'grad_norm': 1.1823899745941162, 'learning_rate': 7.125108892313546e-05, 'epoch': 0.63}
{'loss': 1.1458, 'grad_norm': 1.133623480796814, 'learning_rate': 7.119965773978459e-05, 'epoch': 0.63}
{'loss': 0.7676, 'grad_norm': 1.2982487678527832, 'learning_rate': 7.114823486231366e-05, 'epoch': 0.63}
{'loss': 0.9907, 'grad_norm': 0.8886569142341614, 'learning_rate': 7.109682030555283e-05, 'epoch': 0.63}
{'loss': 1.0841, 'grad_norm': 1.4473223686218262, 'learning_rate': 7.104541408432985e-05, 'epoch': 0.63}
{'loss': 0.9267, 'grad_norm': 1.4624360799789429, 'learning_rate': 7.099401621346994e-05, 'epoch': 0.63}
{'loss': 1.0609, 'grad_norm': 1.0954817533493042, 'learning_rate': 7.094262670779612e-05, 'epoch': 0.63}
{'loss': 0.9987, 'grad_norm': 1.0701024532318115, 'learning_rate': 7.089124558212871e-05, 'epoch': 0.63}
{'loss': 0.7877, 'grad_norm': 0.8702494502067566, 'learning_rate': 7.083987285128591e-05, 'epoch': 0.63}
{'loss': 1.0115, 'grad_norm': 1.1405152082443237, 'learning_rate': 7.078850853008332e-05, 'epoch': 0.63}
{'loss': 0.7262, 'grad_norm': 1.1347399950027466, 'learning_rate': 7.073715263333416e-05, 'epoch': 0.64}
{'loss': 0.8123, 'grad_norm': 1.0529272556304932, 'learning_rate': 7.068580517584927e-05, 'epoch': 0.64}
{'loss': 0.9746, 'grad_norm': 1.0110681056976318, 'learning_rate': 7.063446617243694e-05, 'epoch': 0.64}
{'loss': 1.0441, 'grad_norm': 0.9057338237762451, 'learning_rate': 7.05831356379031e-05, 'epoch': 0.64}
{'loss': 1.082, 'grad_norm': 1.0941367149353027, 'learning_rate': 7.053181358705132e-05, 'epoch': 0.64}
{'loss': 1.0906, 'grad_norm': 1.0739132165908813, 'learning_rate': 7.048050003468251e-05, 'epoch': 0.64}
{'loss': 1.1774, 'grad_norm': 1.2009332180023193, 'learning_rate': 7.042919499559537e-05, 'epoch': 0.64}
{'loss': 0.7958, 'grad_norm': 0.8861992359161377, 'learning_rate': 7.037789848458589e-05, 'epoch': 0.64}
{'loss': 0.9757, 'grad_norm': 1.0601866245269775, 'learning_rate': 7.032661051644783e-05, 'epoch': 0.64}
{'loss': 0.8846, 'grad_norm': 1.1630182266235352, 'learning_rate': 7.027533110597239e-05, 'epoch': 0.64}
{'loss': 0.872, 'grad_norm': 1.2117868661880493, 'learning_rate': 7.022406026794828e-05, 'epoch': 0.64}
{'loss': 1.0341, 'grad_norm': 1.1531561613082886, 'learning_rate': 7.017279801716177e-05, 'epoch': 0.64}
{'loss': 0.9774, 'grad_norm': 1.163137674331665, 'learning_rate': 7.012154436839663e-05, 'epoch': 0.64}
{'loss': 0.9768, 'grad_norm': 1.955121397972107, 'learning_rate': 7.007029933643419e-05, 'epoch': 0.64}
{'loss': 1.1446, 'grad_norm': 1.3945727348327637, 'learning_rate': 7.00190629360533e-05, 'epoch': 0.64}
{'loss': 0.8179, 'grad_norm': 1.401100993156433, 'learning_rate': 6.996783518203019e-05, 'epoch': 0.64}
{'loss': 0.9357, 'grad_norm': 0.9902923107147217, 'learning_rate': 6.991661608913877e-05, 'epoch': 0.64}
{'loss': 0.8257, 'grad_norm': 0.9607477784156799, 'learning_rate': 6.986540567215044e-05, 'epoch': 0.64}
{'loss': 0.903, 'grad_norm': 0.9776608347892761, 'learning_rate': 6.981420394583389e-05, 'epoch': 0.64}
{'loss': 0.8117, 'grad_norm': 1.156683087348938, 'learning_rate': 6.976301092495556e-05, 'epoch': 0.64}
{'loss': 0.8401, 'grad_norm': 1.5244463682174683, 'learning_rate': 6.971182662427924e-05, 'epoch': 0.64}
{'loss': 0.909, 'grad_norm': 0.9756755828857422, 'learning_rate': 6.966065105856622e-05, 'epoch': 0.64}
{'loss': 0.7145, 'grad_norm': 1.4709042310714722, 'learning_rate': 6.960948424257532e-05, 'epoch': 0.64}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.63, 'grad_norm': 1.0463706254959106, 'learning_rate': 6.955832619106277e-05, 'epoch': 0.64}
{'loss': 0.7279, 'grad_norm': 1.0228172540664673, 'learning_rate': 6.950717691878236e-05, 'epoch': 0.64}
{'loss': 0.9917, 'grad_norm': 1.4275020360946655, 'learning_rate': 6.945603644048521e-05, 'epoch': 0.64}
{'loss': 0.9734, 'grad_norm': 1.3647105693817139, 'learning_rate': 6.940490477092004e-05, 'epoch': 0.64}
{'loss': 1.3155, 'grad_norm': 0.9789852499961853, 'learning_rate': 6.9353781924833e-05, 'epoch': 0.64}
{'loss': 0.7271, 'grad_norm': 1.261619210243225, 'learning_rate': 6.93026679169676e-05, 'epoch': 0.64}
{'loss': 0.9031, 'grad_norm': 1.347837209701538, 'learning_rate': 6.925156276206497e-05, 'epoch': 0.64}
{'loss': 0.8367, 'grad_norm': 1.1707191467285156, 'learning_rate': 6.920046647486349e-05, 'epoch': 0.64}
{'loss': 0.9488, 'grad_norm': 1.1460708379745483, 'learning_rate': 6.914937907009913e-05, 'epoch': 0.64}
{'loss': 1.1834, 'grad_norm': 1.2272180318832397, 'learning_rate': 6.909830056250527e-05, 'epoch': 0.64}
{'loss': 0.8888, 'grad_norm': 1.2837201356887817, 'learning_rate': 6.904723096681267e-05, 'epoch': 0.64}
{'loss': 1.047, 'grad_norm': 1.19392728805542, 'learning_rate': 6.89961702977496e-05, 'epoch': 0.64}
{'loss': 1.1784, 'grad_norm': 1.3673624992370605, 'learning_rate': 6.894511857004165e-05, 'epoch': 0.64}
{'loss': 0.9391, 'grad_norm': 1.043332576751709, 'learning_rate': 6.889407579841193e-05, 'epoch': 0.64}
{'loss': 1.1075, 'grad_norm': 1.160768747329712, 'learning_rate': 6.884304199758095e-05, 'epoch': 0.64}
{'loss': 1.006, 'grad_norm': 1.0835301876068115, 'learning_rate': 6.879201718226658e-05, 'epoch': 0.64}
{'loss': 0.9423, 'grad_norm': 1.0897070169448853, 'learning_rate': 6.874100136718416e-05, 'epoch': 0.64}
{'loss': 0.9035, 'grad_norm': 1.0238134860992432, 'learning_rate': 6.868999456704633e-05, 'epoch': 0.64}
{'loss': 0.8343, 'grad_norm': 1.0644608736038208, 'learning_rate': 6.863899679656328e-05, 'epoch': 0.64}
{'loss': 1.133, 'grad_norm': 0.9097084403038025, 'learning_rate': 6.85880080704425e-05, 'epoch': 0.64}
{'loss': 1.1857, 'grad_norm': 1.2582378387451172, 'learning_rate': 6.853702840338889e-05, 'epoch': 0.64}
{'loss': 0.7569, 'grad_norm': 1.0654922723770142, 'learning_rate': 6.848605781010477e-05, 'epoch': 0.64}
{'loss': 1.0068, 'grad_norm': 1.0723741054534912, 'learning_rate': 6.843509630528977e-05, 'epoch': 0.64}
{'loss': 0.8123, 'grad_norm': 1.3157175779342651, 'learning_rate': 6.838414390364094e-05, 'epoch': 0.64}
{'loss': 1.1171, 'grad_norm': 1.549237847328186, 'learning_rate': 6.833320061985277e-05, 'epoch': 0.64}
{'loss': 1.0427, 'grad_norm': 1.1342294216156006, 'learning_rate': 6.828226646861697e-05, 'epoch': 0.64}
{'loss': 1.2082, 'grad_norm': 1.0227605104446411, 'learning_rate': 6.82313414646228e-05, 'epoch': 0.64}
{'loss': 0.9938, 'grad_norm': 1.0923088788986206, 'learning_rate': 6.81804256225567e-05, 'epoch': 0.64}
{'loss': 1.2977, 'grad_norm': 1.8411439657211304, 'learning_rate': 6.812951895710258e-05, 'epoch': 0.64}
{'loss': 0.9619, 'grad_norm': 1.1939144134521484, 'learning_rate': 6.807862148294171e-05, 'epoch': 0.64}
{'loss': 0.758, 'grad_norm': 1.2283307313919067, 'learning_rate': 6.802773321475262e-05, 'epoch': 0.64}
{'loss': 1.0649, 'grad_norm': 1.3439245223999023, 'learning_rate': 6.79768541672113e-05, 'epoch': 0.64}
{'loss': 0.9202, 'grad_norm': 1.1528780460357666, 'learning_rate': 6.7925984354991e-05, 'epoch': 0.64}
{'loss': 1.1224, 'grad_norm': 1.5351159572601318, 'learning_rate': 6.787512379276229e-05, 'epoch': 0.64}
{'loss': 0.8253, 'grad_norm': 1.1616078615188599, 'learning_rate': 6.78242724951932e-05, 'epoch': 0.64}
{'loss': 0.8171, 'grad_norm': 0.8681745529174805, 'learning_rate': 6.77734304769489e-05, 'epoch': 0.64}
{'loss': 0.8187, 'grad_norm': 1.147071123123169, 'learning_rate': 6.772259775269203e-05, 'epoch': 0.64}
{'loss': 0.8942, 'grad_norm': 1.3446933031082153, 'learning_rate': 6.767177433708254e-05, 'epoch': 0.64}
{'loss': 0.9931, 'grad_norm': 1.2226794958114624, 'learning_rate': 6.762096024477758e-05, 'epoch': 0.64}
{'loss': 1.1073, 'grad_norm': 1.413287878036499, 'learning_rate': 6.757015549043175e-05, 'epoch': 0.64}
{'loss': 0.7214, 'grad_norm': 1.1276535987854004, 'learning_rate': 6.751936008869686e-05, 'epoch': 0.64}
{'loss': 0.9251, 'grad_norm': 1.0397557020187378, 'learning_rate': 6.746857405422207e-05, 'epoch': 0.64}
{'loss': 0.9527, 'grad_norm': 1.0801801681518555, 'learning_rate': 6.741779740165384e-05, 'epoch': 0.65}
{'loss': 0.9445, 'grad_norm': 1.193138837814331, 'learning_rate': 6.73670301456359e-05, 'epoch': 0.65}
{'loss': 1.0972, 'grad_norm': 1.188867211341858, 'learning_rate': 6.731627230080932e-05, 'epoch': 0.65}
{'loss': 0.8451, 'grad_norm': 1.1911888122558594, 'learning_rate': 6.726552388181233e-05, 'epoch': 0.65}
{'loss': 1.199, 'grad_norm': 1.240557312965393, 'learning_rate': 6.72147849032806e-05, 'epoch': 0.65}
{'loss': 0.9149, 'grad_norm': 1.3964831829071045, 'learning_rate': 6.716405537984703e-05, 'epoch': 0.65}
{'loss': 0.9171, 'grad_norm': 1.3073911666870117, 'learning_rate': 6.711333532614168e-05, 'epoch': 0.65}
{'loss': 0.9289, 'grad_norm': 1.273877739906311, 'learning_rate': 6.706262475679205e-05, 'epoch': 0.65}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9916, 'grad_norm': 1.4318138360977173, 'learning_rate': 6.70119236864228e-05, 'epoch': 0.65}
{'loss': 1.2469, 'grad_norm': 1.190220594406128, 'learning_rate': 6.696123212965583e-05, 'epoch': 0.65}
{'loss': 0.8399, 'grad_norm': 0.8547683954238892, 'learning_rate': 6.69105501011104e-05, 'epoch': 0.65}
{'loss': 0.7855, 'grad_norm': 1.2251324653625488, 'learning_rate': 6.685987761540294e-05, 'epoch': 0.65}
{'loss': 1.0736, 'grad_norm': 0.9360311627388, 'learning_rate': 6.680921468714719e-05, 'epoch': 0.65}
{'loss': 0.8123, 'grad_norm': 1.298575758934021, 'learning_rate': 6.6758561330954e-05, 'epoch': 0.65}
{'loss': 0.9236, 'grad_norm': 1.2256214618682861, 'learning_rate': 6.670791756143162e-05, 'epoch': 0.65}
{'loss': 0.8415, 'grad_norm': 0.9582660794258118, 'learning_rate': 6.665728339318551e-05, 'epoch': 0.65}
{'loss': 0.7202, 'grad_norm': 1.5766894817352295, 'learning_rate': 6.660665884081822e-05, 'epoch': 0.65}
{'loss': 0.6308, 'grad_norm': 1.1306055784225464, 'learning_rate': 6.655604391892972e-05, 'epoch': 0.65}
{'loss': 0.9486, 'grad_norm': 1.3382601737976074, 'learning_rate': 6.650543864211703e-05, 'epoch': 0.65}
{'loss': 0.8222, 'grad_norm': 1.1288387775421143, 'learning_rate': 6.64548430249745e-05, 'epoch': 0.65}
{'loss': 1.0157, 'grad_norm': 1.1326870918273926, 'learning_rate': 6.64042570820937e-05, 'epoch': 0.65}
{'loss': 1.0741, 'grad_norm': 1.0561957359313965, 'learning_rate': 6.63536808280633e-05, 'epoch': 0.65}
{'loss': 0.8558, 'grad_norm': 1.2071826457977295, 'learning_rate': 6.630311427746932e-05, 'epoch': 0.65}
{'loss': 1.2052, 'grad_norm': 0.9801191091537476, 'learning_rate': 6.625255744489489e-05, 'epoch': 0.65}
{'loss': 0.9603, 'grad_norm': 1.262799859046936, 'learning_rate': 6.620201034492032e-05, 'epoch': 0.65}
{'loss': 0.8817, 'grad_norm': 1.1322073936462402, 'learning_rate': 6.615147299212321e-05, 'epoch': 0.65}
{'loss': 0.7297, 'grad_norm': 1.1847612857818604, 'learning_rate': 6.61009454010782e-05, 'epoch': 0.65}
{'loss': 0.842, 'grad_norm': 1.1074004173278809, 'learning_rate': 6.605042758635729e-05, 'epoch': 0.65}
{'loss': 1.2056, 'grad_norm': 1.5223729610443115, 'learning_rate': 6.599991956252956e-05, 'epoch': 0.65}
{'loss': 0.9871, 'grad_norm': 1.2422559261322021, 'learning_rate': 6.594942134416122e-05, 'epoch': 0.65}
{'loss': 0.8527, 'grad_norm': 1.0063809156417847, 'learning_rate': 6.58989329458158e-05, 'epoch': 0.65}
{'loss': 1.1823, 'grad_norm': 1.122086524963379, 'learning_rate': 6.584845438205383e-05, 'epoch': 0.65}
{'loss': 1.2097, 'grad_norm': 1.2962900400161743, 'learning_rate': 6.579798566743314e-05, 'epoch': 0.65}
{'loss': 0.6801, 'grad_norm': 1.036046028137207, 'learning_rate': 6.574752681650864e-05, 'epoch': 0.65}
{'loss': 0.9174, 'grad_norm': 1.0006874799728394, 'learning_rate': 6.56970778438324e-05, 'epoch': 0.65}
{'loss': 1.5053, 'grad_norm': 1.1688886880874634, 'learning_rate': 6.564663876395376e-05, 'epoch': 0.65}
{'loss': 0.7003, 'grad_norm': 1.1295905113220215, 'learning_rate': 6.559620959141897e-05, 'epoch': 0.65}
{'loss': 0.7627, 'grad_norm': 1.2048207521438599, 'learning_rate': 6.554579034077164e-05, 'epoch': 0.65}
{'loss': 0.9019, 'grad_norm': 1.3340263366699219, 'learning_rate': 6.549538102655245e-05, 'epoch': 0.65}
{'loss': 0.8467, 'grad_norm': 0.9121869802474976, 'learning_rate': 6.544498166329913e-05, 'epoch': 0.65}
{'loss': 0.943, 'grad_norm': 1.0678805112838745, 'learning_rate': 6.53945922655467e-05, 'epoch': 0.65}
{'loss': 1.0165, 'grad_norm': 1.4075982570648193, 'learning_rate': 6.534421284782715e-05, 'epoch': 0.65}
{'loss': 0.7069, 'grad_norm': 0.9170325398445129, 'learning_rate': 6.52938434246697e-05, 'epoch': 0.65}
{'loss': 0.629, 'grad_norm': 1.4462742805480957, 'learning_rate': 6.524348401060067e-05, 'epoch': 0.65}
{'loss': 1.1182, 'grad_norm': 1.0441781282424927, 'learning_rate': 6.51931346201434e-05, 'epoch': 0.65}
{'loss': 0.9351, 'grad_norm': 1.34212064743042, 'learning_rate': 6.51427952678185e-05, 'epoch': 0.65}
{'loss': 1.0123, 'grad_norm': 0.9596336483955383, 'learning_rate': 6.509246596814351e-05, 'epoch': 0.65}
{'loss': 0.6905, 'grad_norm': 1.1957695484161377, 'learning_rate': 6.50421467356332e-05, 'epoch': 0.65}
{'loss': 0.8171, 'grad_norm': 1.0452117919921875, 'learning_rate': 6.499183758479944e-05, 'epoch': 0.65}
{'loss': 0.7468, 'grad_norm': 1.7057862281799316, 'learning_rate': 6.494153853015106e-05, 'epoch': 0.65}
{'loss': 0.9736, 'grad_norm': 1.5181759595870972, 'learning_rate': 6.489124958619412e-05, 'epoch': 0.65}
{'loss': 0.9707, 'grad_norm': 1.173455834388733, 'learning_rate': 6.48409707674317e-05, 'epoch': 0.65}
{'loss': 0.9958, 'grad_norm': 1.1743507385253906, 'learning_rate': 6.479070208836394e-05, 'epoch': 0.65}
{'loss': 0.8517, 'grad_norm': 1.0236903429031372, 'learning_rate': 6.474044356348811e-05, 'epoch': 0.65}
{'loss': 1.1341, 'grad_norm': 1.0167884826660156, 'learning_rate': 6.469019520729853e-05, 'epoch': 0.65}
{'loss': 1.2691, 'grad_norm': 1.0117453336715698, 'learning_rate': 6.463995703428662e-05, 'epoch': 0.65}
{'loss': 0.8902, 'grad_norm': 1.2274103164672852, 'learning_rate': 6.458972905894074e-05, 'epoch': 0.65}
{'loss': 0.9408, 'grad_norm': 1.1574031114578247, 'learning_rate': 6.453951129574644e-05, 'epoch': 0.65}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.727, 'grad_norm': 1.3031681776046753, 'learning_rate': 6.448930375918631e-05, 'epoch': 0.65}
{'loss': 0.7688, 'grad_norm': 1.1341438293457031, 'learning_rate': 6.443910646373992e-05, 'epoch': 0.65}
{'loss': 0.9843, 'grad_norm': 1.0095821619033813, 'learning_rate': 6.438891942388393e-05, 'epoch': 0.65}
{'loss': 0.8483, 'grad_norm': 1.4126312732696533, 'learning_rate': 6.43387426540921e-05, 'epoch': 0.65}
{'loss': 1.0431, 'grad_norm': 1.3866523504257202, 'learning_rate': 6.428857616883507e-05, 'epoch': 0.65}
{'loss': 1.0791, 'grad_norm': 1.3504743576049805, 'learning_rate': 6.423841998258069e-05, 'epoch': 0.65}
{'loss': 1.0343, 'grad_norm': 1.101386547088623, 'learning_rate': 6.418827410979373e-05, 'epoch': 0.65}
{'loss': 0.9565, 'grad_norm': 1.102746605873108, 'learning_rate': 6.413813856493602e-05, 'epoch': 0.66}
{'loss': 0.9069, 'grad_norm': 1.1924768686294556, 'learning_rate': 6.408801336246645e-05, 'epoch': 0.66}
{'loss': 1.0007, 'grad_norm': 1.2578437328338623, 'learning_rate': 6.403789851684082e-05, 'epoch': 0.66}
{'loss': 0.7749, 'grad_norm': 1.1706018447875977, 'learning_rate': 6.398779404251208e-05, 'epoch': 0.66}
{'loss': 0.9182, 'grad_norm': 0.9101699590682983, 'learning_rate': 6.393769995393003e-05, 'epoch': 0.66}
{'loss': 0.9123, 'grad_norm': 1.2345517873764038, 'learning_rate': 6.388761626554163e-05, 'epoch': 0.66}
{'loss': 1.2672, 'grad_norm': 1.0633224248886108, 'learning_rate': 6.383754299179079e-05, 'epoch': 0.66}
{'loss': 0.9431, 'grad_norm': 1.7637138366699219, 'learning_rate': 6.378748014711834e-05, 'epoch': 0.66}
{'loss': 0.7663, 'grad_norm': 1.2562953233718872, 'learning_rate': 6.37374277459622e-05, 'epoch': 0.66}
{'loss': 0.8312, 'grad_norm': 1.299480676651001, 'learning_rate': 6.368738580275724e-05, 'epoch': 0.66}
{'loss': 0.6983, 'grad_norm': 1.0123440027236938, 'learning_rate': 6.36373543319353e-05, 'epoch': 0.66}
{'loss': 1.0928, 'grad_norm': 1.5229798555374146, 'learning_rate': 6.358733334792526e-05, 'epoch': 0.66}
{'loss': 0.9117, 'grad_norm': 1.0968241691589355, 'learning_rate': 6.353732286515286e-05, 'epoch': 0.66}
{'loss': 1.0711, 'grad_norm': 1.4048645496368408, 'learning_rate': 6.348732289804096e-05, 'epoch': 0.66}
{'loss': 1.0414, 'grad_norm': 0.9556204080581665, 'learning_rate': 6.343733346100924e-05, 'epoch': 0.66}
{'loss': 0.9813, 'grad_norm': 1.3503671884536743, 'learning_rate': 6.338735456847442e-05, 'epoch': 0.66}
{'loss': 0.7672, 'grad_norm': 1.100125789642334, 'learning_rate': 6.333738623485025e-05, 'epoch': 0.66}
{'loss': 1.1282, 'grad_norm': 1.1776642799377441, 'learning_rate': 6.328742847454724e-05, 'epoch': 0.66}
{'loss': 1.0008, 'grad_norm': 1.137098789215088, 'learning_rate': 6.323748130197308e-05, 'epoch': 0.66}
{'loss': 0.9751, 'grad_norm': 0.9900568127632141, 'learning_rate': 6.318754473153221e-05, 'epoch': 0.66}
{'loss': 1.1083, 'grad_norm': 1.4026445150375366, 'learning_rate': 6.313761877762615e-05, 'epoch': 0.66}
{'loss': 0.8243, 'grad_norm': 0.9904361963272095, 'learning_rate': 6.308770345465327e-05, 'epoch': 0.66}
{'loss': 0.6756, 'grad_norm': 0.8734514117240906, 'learning_rate': 6.30377987770089e-05, 'epoch': 0.66}
{'loss': 0.7266, 'grad_norm': 1.0089678764343262, 'learning_rate': 6.298790475908539e-05, 'epoch': 0.66}
{'loss': 1.0201, 'grad_norm': 1.0266282558441162, 'learning_rate': 6.293802141527183e-05, 'epoch': 0.66}
{'loss': 0.8975, 'grad_norm': 1.666123628616333, 'learning_rate': 6.288814875995437e-05, 'epoch': 0.66}
{'loss': 1.0554, 'grad_norm': 1.0914312601089478, 'learning_rate': 6.283828680751612e-05, 'epoch': 0.66}
{'loss': 0.9822, 'grad_norm': 1.1318708658218384, 'learning_rate': 6.278843557233689e-05, 'epoch': 0.66}
{'loss': 0.8502, 'grad_norm': 1.2831380367279053, 'learning_rate': 6.273859506879365e-05, 'epoch': 0.66}
{'loss': 0.9745, 'grad_norm': 1.147022008895874, 'learning_rate': 6.268876531126012e-05, 'epoch': 0.66}
{'loss': 1.0941, 'grad_norm': 0.9323557615280151, 'learning_rate': 6.263894631410692e-05, 'epoch': 0.66}
{'loss': 0.927, 'grad_norm': 0.9881642460823059, 'learning_rate': 6.258913809170168e-05, 'epoch': 0.66}
{'loss': 1.2286, 'grad_norm': 1.1682243347167969, 'learning_rate': 6.25393406584088e-05, 'epoch': 0.66}
{'loss': 0.8437, 'grad_norm': 1.0760785341262817, 'learning_rate': 6.248955402858965e-05, 'epoch': 0.66}
{'loss': 0.8048, 'grad_norm': 1.1341718435287476, 'learning_rate': 6.243977821660248e-05, 'epoch': 0.66}
{'loss': 1.0243, 'grad_norm': 1.0619481801986694, 'learning_rate': 6.239001323680231e-05, 'epoch': 0.66}
{'loss': 0.9828, 'grad_norm': 1.1519418954849243, 'learning_rate': 6.234025910354122e-05, 'epoch': 0.66}
{'loss': 0.9674, 'grad_norm': 1.236713171005249, 'learning_rate': 6.229051583116796e-05, 'epoch': 0.66}
{'loss': 0.9931, 'grad_norm': 1.0672874450683594, 'learning_rate': 6.224078343402833e-05, 'epoch': 0.66}
{'loss': 0.8223, 'grad_norm': 1.2429441213607788, 'learning_rate': 6.21910619264649e-05, 'epoch': 0.66}
{'loss': 1.0123, 'grad_norm': 0.9262089729309082, 'learning_rate': 6.214135132281707e-05, 'epoch': 0.66}
{'loss': 0.917, 'grad_norm': 1.0240520238876343, 'learning_rate': 6.209165163742119e-05, 'epoch': 0.66}
{'loss': 1.0116, 'grad_norm': 1.0497277975082397, 'learning_rate': 6.204196288461037e-05, 'epoch': 0.66}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9161, 'grad_norm': 1.2161551713943481, 'learning_rate': 6.199228507871462e-05, 'epoch': 0.66}
{'loss': 0.8956, 'grad_norm': 1.0584241151809692, 'learning_rate': 6.194261823406083e-05, 'epoch': 0.66}
{'loss': 1.1029, 'grad_norm': 1.2141003608703613, 'learning_rate': 6.18929623649726e-05, 'epoch': 0.66}
{'loss': 1.0871, 'grad_norm': 1.0866578817367554, 'learning_rate': 6.18433174857705e-05, 'epoch': 0.66}
{'loss': 0.9441, 'grad_norm': 1.1039718389511108, 'learning_rate': 6.179368361077183e-05, 'epoch': 0.66}
{'loss': 1.0282, 'grad_norm': 1.065474271774292, 'learning_rate': 6.174406075429076e-05, 'epoch': 0.66}
{'loss': 0.9822, 'grad_norm': 1.0984457731246948, 'learning_rate': 6.169444893063836e-05, 'epoch': 0.66}
{'loss': 0.9577, 'grad_norm': 1.1224526166915894, 'learning_rate': 6.164484815412234e-05, 'epoch': 0.66}
{'loss': 1.1372, 'grad_norm': 1.0813312530517578, 'learning_rate': 6.159525843904741e-05, 'epoch': 0.66}
{'loss': 1.0472, 'grad_norm': 1.0877346992492676, 'learning_rate': 6.154567979971493e-05, 'epoch': 0.66}
{'loss': 0.8143, 'grad_norm': 1.3549312353134155, 'learning_rate': 6.149611225042317e-05, 'epoch': 0.66}
{'loss': 1.004, 'grad_norm': 1.4226605892181396, 'learning_rate': 6.144655580546724e-05, 'epoch': 0.66}
{'loss': 0.9867, 'grad_norm': 1.0770177841186523, 'learning_rate': 6.139701047913885e-05, 'epoch': 0.66}
{'loss': 1.1825, 'grad_norm': 0.9246991276741028, 'learning_rate': 6.134747628572677e-05, 'epoch': 0.66}
{'loss': 1.1238, 'grad_norm': 0.9166575074195862, 'learning_rate': 6.12979532395163e-05, 'epoch': 0.66}
{'loss': 0.8923, 'grad_norm': 1.1442369222640991, 'learning_rate': 6.12484413547897e-05, 'epoch': 0.66}
{'loss': 0.7247, 'grad_norm': 1.2891790866851807, 'learning_rate': 6.119894064582601e-05, 'epoch': 0.66}
{'loss': 0.9023, 'grad_norm': 1.157639980316162, 'learning_rate': 6.114945112690091e-05, 'epoch': 0.66}
{'loss': 1.0179, 'grad_norm': 1.1082757711410522, 'learning_rate': 6.1099972812287e-05, 'epoch': 0.66}
{'loss': 0.868, 'grad_norm': 1.1070400476455688, 'learning_rate': 6.105050571625353e-05, 'epoch': 0.66}
{'loss': 0.7482, 'grad_norm': 1.2050063610076904, 'learning_rate': 6.100104985306665e-05, 'epoch': 0.66}
{'loss': 0.7263, 'grad_norm': 0.903127133846283, 'learning_rate': 6.095160523698913e-05, 'epoch': 0.66}
{'loss': 1.2047, 'grad_norm': 1.0601016283035278, 'learning_rate': 6.090217188228058e-05, 'epoch': 0.67}
{'loss': 0.857, 'grad_norm': 1.1304048299789429, 'learning_rate': 6.0852749803197354e-05, 'epoch': 0.67}
{'loss': 0.8767, 'grad_norm': 1.5052242279052734, 'learning_rate': 6.080333901399251e-05, 'epoch': 0.67}
{'loss': 1.0871, 'grad_norm': 1.2894022464752197, 'learning_rate': 6.075393952891588e-05, 'epoch': 0.67}
{'loss': 1.1619, 'grad_norm': 1.1110713481903076, 'learning_rate': 6.070455136221411e-05, 'epoch': 0.67}
{'loss': 1.1305, 'grad_norm': 0.8664016723632812, 'learning_rate': 6.065517452813042e-05, 'epoch': 0.67}
{'loss': 0.8632, 'grad_norm': 1.653653621673584, 'learning_rate': 6.0605809040904894e-05, 'epoch': 0.67}
{'loss': 0.9268, 'grad_norm': 1.3964451551437378, 'learning_rate': 6.0556454914774295e-05, 'epoch': 0.67}
{'loss': 0.7529, 'grad_norm': 1.0629910230636597, 'learning_rate': 6.050711216397212e-05, 'epoch': 0.67}
{'loss': 0.8662, 'grad_norm': 1.3310219049453735, 'learning_rate': 6.045778080272859e-05, 'epoch': 0.67}
{'loss': 0.9676, 'grad_norm': 0.9641475081443787, 'learning_rate': 6.040846084527059e-05, 'epoch': 0.67}
{'loss': 1.0335, 'grad_norm': 1.0447577238082886, 'learning_rate': 6.0359152305821766e-05, 'epoch': 0.67}
{'loss': 0.8849, 'grad_norm': 1.3195469379425049, 'learning_rate': 6.030985519860255e-05, 'epoch': 0.67}
{'loss': 0.7572, 'grad_norm': 1.4035624265670776, 'learning_rate': 6.026056953782987e-05, 'epoch': 0.67}
{'loss': 0.984, 'grad_norm': 1.0194568634033203, 'learning_rate': 6.021129533771757e-05, 'epoch': 0.67}
{'loss': 1.034, 'grad_norm': 0.9911234974861145, 'learning_rate': 6.0162032612476004e-05, 'epoch': 0.67}
{'loss': 1.1185, 'grad_norm': 1.2648166418075562, 'learning_rate': 6.011278137631236e-05, 'epoch': 0.67}
{'loss': 0.5409, 'grad_norm': 0.9935958385467529, 'learning_rate': 6.006354164343046e-05, 'epoch': 0.67}
{'loss': 0.6814, 'grad_norm': 1.0982674360275269, 'learning_rate': 6.0014313428030774e-05, 'epoch': 0.67}
{'loss': 0.9043, 'grad_norm': 1.1154015064239502, 'learning_rate': 5.9965096744310526e-05, 'epoch': 0.67}
{'loss': 0.7237, 'grad_norm': 1.2261549234390259, 'learning_rate': 5.991589160646352e-05, 'epoch': 0.67}
{'loss': 0.8856, 'grad_norm': 1.0898157358169556, 'learning_rate': 5.9866698028680293e-05, 'epoch': 0.67}
{'loss': 0.8674, 'grad_norm': 1.025867223739624, 'learning_rate': 5.981751602514809e-05, 'epoch': 0.67}
{'loss': 1.2932, 'grad_norm': 1.0925663709640503, 'learning_rate': 5.976834561005069e-05, 'epoch': 0.67}
{'loss': 1.0467, 'grad_norm': 0.953840434551239, 'learning_rate': 5.9719186797568674e-05, 'epoch': 0.67}
{'loss': 0.8112, 'grad_norm': 1.1681796312332153, 'learning_rate': 5.967003960187914e-05, 'epoch': 0.67}
{'loss': 0.9382, 'grad_norm': 1.3160035610198975, 'learning_rate': 5.962090403715592e-05, 'epoch': 0.67}
{'loss': 0.7402, 'grad_norm': 1.125962495803833, 'learning_rate': 5.957178011756952e-05, 'epoch': 0.67}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1041, 'grad_norm': 1.2748115062713623, 'learning_rate': 5.9522667857286974e-05, 'epoch': 0.67}
{'loss': 0.7533, 'grad_norm': 1.2331981658935547, 'learning_rate': 5.947356727047208e-05, 'epoch': 0.67}
{'loss': 1.0408, 'grad_norm': 1.3054473400115967, 'learning_rate': 5.942447837128516e-05, 'epoch': 0.67}
{'loss': 0.9134, 'grad_norm': 1.0369945764541626, 'learning_rate': 5.937540117388325e-05, 'epoch': 0.67}
{'loss': 1.2239, 'grad_norm': 0.9859575629234314, 'learning_rate': 5.9326335692419995e-05, 'epoch': 0.67}
{'loss': 1.2344, 'grad_norm': 1.0632787942886353, 'learning_rate': 5.927728194104558e-05, 'epoch': 0.67}
{'loss': 1.0129, 'grad_norm': 0.9690666198730469, 'learning_rate': 5.922823993390696e-05, 'epoch': 0.67}
{'loss': 0.8867, 'grad_norm': 1.1350926160812378, 'learning_rate': 5.917920968514752e-05, 'epoch': 0.67}
{'loss': 1.0559, 'grad_norm': 1.0616579055786133, 'learning_rate': 5.913019120890737e-05, 'epoch': 0.67}
{'loss': 0.9107, 'grad_norm': 1.0695576667785645, 'learning_rate': 5.9081184519323275e-05, 'epoch': 0.67}
{'loss': 0.6435, 'grad_norm': 1.175328016281128, 'learning_rate': 5.9032189630528436e-05, 'epoch': 0.67}
{'loss': 0.8785, 'grad_norm': 1.3316543102264404, 'learning_rate': 5.898320655665279e-05, 'epoch': 0.67}
{'loss': 1.0302, 'grad_norm': 1.2163898944854736, 'learning_rate': 5.8934235311822824e-05, 'epoch': 0.67}
{'loss': 0.9578, 'grad_norm': 1.2525948286056519, 'learning_rate': 5.8885275910161576e-05, 'epoch': 0.67}
{'loss': 1.132, 'grad_norm': 1.0299049615859985, 'learning_rate': 5.883632836578876e-05, 'epoch': 0.67}
{'loss': 0.9044, 'grad_norm': 0.9253410696983337, 'learning_rate': 5.8787392692820533e-05, 'epoch': 0.67}
{'loss': 0.9434, 'grad_norm': 1.2141317129135132, 'learning_rate': 5.873846890536976e-05, 'epoch': 0.67}
{'loss': 0.9807, 'grad_norm': 1.3989582061767578, 'learning_rate': 5.868955701754584e-05, 'epoch': 0.67}
{'loss': 0.5021, 'grad_norm': 0.9797281622886658, 'learning_rate': 5.864065704345467e-05, 'epoch': 0.67}
{'loss': 1.1396, 'grad_norm': 1.2145450115203857, 'learning_rate': 5.859176899719883e-05, 'epoch': 0.67}
{'loss': 0.9766, 'grad_norm': 0.9835371971130371, 'learning_rate': 5.8542892892877335e-05, 'epoch': 0.67}
{'loss': 0.9226, 'grad_norm': 1.1498024463653564, 'learning_rate': 5.8494028744585826e-05, 'epoch': 0.67}
{'loss': 1.1601, 'grad_norm': 1.0228406190872192, 'learning_rate': 5.844517656641655e-05, 'epoch': 0.67}
{'loss': 0.7073, 'grad_norm': 1.1939290761947632, 'learning_rate': 5.8396336372458195e-05, 'epoch': 0.67}
{'loss': 0.8011, 'grad_norm': 1.2515668869018555, 'learning_rate': 5.834750817679606e-05, 'epoch': 0.67}
{'loss': 0.9118, 'grad_norm': 1.2204740047454834, 'learning_rate': 5.829869199351188e-05, 'epoch': 0.67}
{'loss': 1.4141, 'grad_norm': 1.8295317888259888, 'learning_rate': 5.8249887836684126e-05, 'epoch': 0.67}
{'loss': 1.1495, 'grad_norm': 1.1833868026733398, 'learning_rate': 5.820109572038761e-05, 'epoch': 0.67}
{'loss': 0.7996, 'grad_norm': 1.301000714302063, 'learning_rate': 5.8152315658693765e-05, 'epoch': 0.67}
{'loss': 1.2207, 'grad_norm': 1.0740079879760742, 'learning_rate': 5.810354766567052e-05, 'epoch': 0.67}
{'loss': 1.0953, 'grad_norm': 1.05247962474823, 'learning_rate': 5.805479175538229e-05, 'epoch': 0.67}
{'loss': 1.1551, 'grad_norm': 1.322238802909851, 'learning_rate': 5.8006047941890116e-05, 'epoch': 0.67}
{'loss': 0.8378, 'grad_norm': 1.112416386604309, 'learning_rate': 5.795731623925146e-05, 'epoch': 0.67}
{'loss': 1.0968, 'grad_norm': 1.1462419033050537, 'learning_rate': 5.790859666152029e-05, 'epoch': 0.67}
{'loss': 0.9669, 'grad_norm': 1.4614458084106445, 'learning_rate': 5.785988922274711e-05, 'epoch': 0.67}
{'loss': 1.2967, 'grad_norm': 0.9262072443962097, 'learning_rate': 5.78111939369789e-05, 'epoch': 0.67}
{'loss': 0.9227, 'grad_norm': 0.9627888798713684, 'learning_rate': 5.776251081825912e-05, 'epoch': 0.67}
{'loss': 1.283, 'grad_norm': 1.2390377521514893, 'learning_rate': 5.771383988062786e-05, 'epoch': 0.68}
{'loss': 1.0951, 'grad_norm': 1.3104184865951538, 'learning_rate': 5.766518113812141e-05, 'epoch': 0.68}
{'loss': 0.8009, 'grad_norm': 1.247174859046936, 'learning_rate': 5.761653460477286e-05, 'epoch': 0.68}
{'loss': 0.9359, 'grad_norm': 1.0187519788742065, 'learning_rate': 5.7567900294611586e-05, 'epoch': 0.68}
{'loss': 1.0684, 'grad_norm': 1.082369327545166, 'learning_rate': 5.751927822166345e-05, 'epoch': 0.68}
{'loss': 1.2409, 'grad_norm': 1.51760995388031, 'learning_rate': 5.747066839995093e-05, 'epoch': 0.68}
{'loss': 0.9623, 'grad_norm': 1.2030563354492188, 'learning_rate': 5.7422070843492734e-05, 'epoch': 0.68}
{'loss': 1.0455, 'grad_norm': 1.1472249031066895, 'learning_rate': 5.737348556630429e-05, 'epoch': 0.68}
{'loss': 0.951, 'grad_norm': 1.431368350982666, 'learning_rate': 5.7324912582397225e-05, 'epoch': 0.68}
{'loss': 0.7499, 'grad_norm': 0.8497730493545532, 'learning_rate': 5.727635190577986e-05, 'epoch': 0.68}
{'loss': 0.8446, 'grad_norm': 1.2259694337844849, 'learning_rate': 5.722780355045682e-05, 'epoch': 0.68}
{'loss': 1.17, 'grad_norm': 1.0834540128707886, 'learning_rate': 5.7179267530429216e-05, 'epoch': 0.68}
{'loss': 1.1611, 'grad_norm': 1.0609180927276611, 'learning_rate': 5.713074385969457e-05, 'epoch': 0.68}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1273, 'grad_norm': 1.2080981731414795, 'learning_rate': 5.708223255224695e-05, 'epoch': 0.68}
{'loss': 0.7972, 'grad_norm': 1.0821607112884521, 'learning_rate': 5.7033733622076744e-05, 'epoch': 0.68}
{'loss': 0.8811, 'grad_norm': 1.072568655014038, 'learning_rate': 5.698524708317081e-05, 'epoch': 0.68}
{'loss': 0.8591, 'grad_norm': 1.5485235452651978, 'learning_rate': 5.693677294951244e-05, 'epoch': 0.68}
{'loss': 1.0123, 'grad_norm': 1.1377559900283813, 'learning_rate': 5.6888311235081295e-05, 'epoch': 0.68}
{'loss': 0.8966, 'grad_norm': 1.2029820680618286, 'learning_rate': 5.6839861953853605e-05, 'epoch': 0.68}
{'loss': 0.7916, 'grad_norm': 1.0506324768066406, 'learning_rate': 5.679142511980175e-05, 'epoch': 0.68}
{'loss': 1.1896, 'grad_norm': 0.9624496698379517, 'learning_rate': 5.674300074689488e-05, 'epoch': 0.68}
{'loss': 1.2012, 'grad_norm': 1.069108486175537, 'learning_rate': 5.6694588849098154e-05, 'epoch': 0.68}
{'loss': 1.0641, 'grad_norm': 1.0738177299499512, 'learning_rate': 5.6646189440373456e-05, 'epoch': 0.68}
{'loss': 0.8743, 'grad_norm': 1.1691075563430786, 'learning_rate': 5.65978025346789e-05, 'epoch': 0.68}
{'loss': 0.9555, 'grad_norm': 1.1730210781097412, 'learning_rate': 5.654942814596902e-05, 'epoch': 0.68}
{'loss': 0.8674, 'grad_norm': 1.392767310142517, 'learning_rate': 5.650106628819485e-05, 'epoch': 0.68}
{'loss': 1.0073, 'grad_norm': 1.2009804248809814, 'learning_rate': 5.645271697530356e-05, 'epoch': 0.68}
{'loss': 1.0648, 'grad_norm': 1.03876531124115, 'learning_rate': 5.6404380221238985e-05, 'epoch': 0.68}
{'loss': 1.1355, 'grad_norm': 0.9003477096557617, 'learning_rate': 5.6356056039941175e-05, 'epoch': 0.68}
{'loss': 0.8302, 'grad_norm': 1.0110300779342651, 'learning_rate': 5.630774444534659e-05, 'epoch': 0.68}
{'loss': 1.1118, 'grad_norm': 1.3180807828903198, 'learning_rate': 5.625944545138805e-05, 'epoch': 0.68}
{'loss': 0.7533, 'grad_norm': 1.334959864616394, 'learning_rate': 5.621115907199477e-05, 'epoch': 0.68}
{'loss': 0.6938, 'grad_norm': 1.0735135078430176, 'learning_rate': 5.616288532109225e-05, 'epoch': 0.68}
{'loss': 0.8993, 'grad_norm': 1.6487911939620972, 'learning_rate': 5.6114624212602504e-05, 'epoch': 0.68}
{'loss': 1.0237, 'grad_norm': 1.2435979843139648, 'learning_rate': 5.606637576044376e-05, 'epoch': 0.68}
{'loss': 0.9461, 'grad_norm': 1.2964907884597778, 'learning_rate': 5.601813997853062e-05, 'epoch': 0.68}
{'loss': 0.6626, 'grad_norm': 0.9678516387939453, 'learning_rate': 5.596991688077409e-05, 'epoch': 0.68}
{'loss': 0.6885, 'grad_norm': 1.0072184801101685, 'learning_rate': 5.59217064810814e-05, 'epoch': 0.68}
{'loss': 1.1947, 'grad_norm': 1.1099716424942017, 'learning_rate': 5.587350879335634e-05, 'epoch': 0.68}
{'loss': 0.8364, 'grad_norm': 1.0147968530654907, 'learning_rate': 5.582532383149872e-05, 'epoch': 0.68}
{'loss': 1.0647, 'grad_norm': 1.0332860946655273, 'learning_rate': 5.577715160940502e-05, 'epoch': 0.68}
{'loss': 0.9786, 'grad_norm': 1.3186179399490356, 'learning_rate': 5.5728992140967715e-05, 'epoch': 0.68}
{'loss': 1.0044, 'grad_norm': 1.0956637859344482, 'learning_rate': 5.568084544007588e-05, 'epoch': 0.68}
{'loss': 0.8698, 'grad_norm': 1.0437613725662231, 'learning_rate': 5.563271152061476e-05, 'epoch': 0.68}
{'loss': 0.8091, 'grad_norm': 1.0844981670379639, 'learning_rate': 5.5584590396465916e-05, 'epoch': 0.68}
{'loss': 1.1518, 'grad_norm': 1.2770652770996094, 'learning_rate': 5.553648208150728e-05, 'epoch': 0.68}
{'loss': 0.695, 'grad_norm': 1.0297317504882812, 'learning_rate': 5.548838658961302e-05, 'epoch': 0.68}
{'loss': 1.08, 'grad_norm': 1.1469292640686035, 'learning_rate': 5.5440303934653694e-05, 'epoch': 0.68}
{'loss': 1.1165, 'grad_norm': 1.0154578685760498, 'learning_rate': 5.53922341304961e-05, 'epoch': 0.68}
{'loss': 0.9446, 'grad_norm': 1.1723190546035767, 'learning_rate': 5.534417719100332e-05, 'epoch': 0.68}
{'loss': 1.1501, 'grad_norm': 1.2050880193710327, 'learning_rate': 5.5296133130034713e-05, 'epoch': 0.68}
{'loss': 0.9026, 'grad_norm': 1.7140172719955444, 'learning_rate': 5.5248101961446065e-05, 'epoch': 0.68}
{'loss': 1.0521, 'grad_norm': 1.33744215965271, 'learning_rate': 5.520008369908918e-05, 'epoch': 0.68}
{'loss': 0.8713, 'grad_norm': 1.0069057941436768, 'learning_rate': 5.5152078356812456e-05, 'epoch': 0.68}
{'loss': 1.0177, 'grad_norm': 1.2361520528793335, 'learning_rate': 5.5104085948460235e-05, 'epoch': 0.68}
{'loss': 1.218, 'grad_norm': 1.0645856857299805, 'learning_rate': 5.5056106487873426e-05, 'epoch': 0.68}
{'loss': 0.8791, 'grad_norm': 1.0903059244155884, 'learning_rate': 5.500813998888903e-05, 'epoch': 0.68}
{'loss': 1.0992, 'grad_norm': 1.1571910381317139, 'learning_rate': 5.4960186465340316e-05, 'epoch': 0.68}
{'loss': 0.796, 'grad_norm': 1.3137484788894653, 'learning_rate': 5.491224593105695e-05, 'epoch': 0.68}
{'loss': 0.8661, 'grad_norm': 1.0234500169754028, 'learning_rate': 5.486431839986462e-05, 'epoch': 0.68}
{'loss': 0.7634, 'grad_norm': 1.017075777053833, 'learning_rate': 5.481640388558551e-05, 'epoch': 0.68}
{'loss': 0.7875, 'grad_norm': 1.1572293043136597, 'learning_rate': 5.476850240203788e-05, 'epoch': 0.68}
{'loss': 1.1822, 'grad_norm': 1.0339158773422241, 'learning_rate': 5.472061396303629e-05, 'epoch': 0.68}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8477, 'grad_norm': 1.2438249588012695, 'learning_rate': 5.467273858239155e-05, 'epoch': 0.68}
{'loss': 1.1636, 'grad_norm': 1.1144026517868042, 'learning_rate': 5.462487627391066e-05, 'epoch': 0.68}
{'loss': 0.9531, 'grad_norm': 1.0582294464111328, 'learning_rate': 5.457702705139689e-05, 'epoch': 0.69}
{'loss': 0.7148, 'grad_norm': 1.2427566051483154, 'learning_rate': 5.4529190928649754e-05, 'epoch': 0.69}
{'loss': 1.2124, 'grad_norm': 1.488732099533081, 'learning_rate': 5.448136791946494e-05, 'epoch': 0.69}
{'loss': 0.9597, 'grad_norm': 1.1702189445495605, 'learning_rate': 5.443355803763438e-05, 'epoch': 0.69}
{'loss': 0.6465, 'grad_norm': 1.2000278234481812, 'learning_rate': 5.43857612969462e-05, 'epoch': 0.69}
{'loss': 0.6273, 'grad_norm': 1.3727612495422363, 'learning_rate': 5.4337977711184717e-05, 'epoch': 0.69}
{'loss': 0.913, 'grad_norm': 1.378131628036499, 'learning_rate': 5.4290207294130615e-05, 'epoch': 0.69}
{'loss': 1.1316, 'grad_norm': 1.2142903804779053, 'learning_rate': 5.424245005956048e-05, 'epoch': 0.69}
{'loss': 1.1021, 'grad_norm': 1.3397315740585327, 'learning_rate': 5.419470602124742e-05, 'epoch': 0.69}
{'loss': 0.9169, 'grad_norm': 1.0217889547348022, 'learning_rate': 5.414697519296046e-05, 'epoch': 0.69}
{'loss': 1.1747, 'grad_norm': 0.9140220284461975, 'learning_rate': 5.4099257588465015e-05, 'epoch': 0.69}
{'loss': 1.1684, 'grad_norm': 1.1307717561721802, 'learning_rate': 5.405155322152261e-05, 'epoch': 0.69}
{'loss': 0.8173, 'grad_norm': 1.2202250957489014, 'learning_rate': 5.40038621058909e-05, 'epoch': 0.69}
{'loss': 0.8656, 'grad_norm': 1.2994800806045532, 'learning_rate': 5.395618425532389e-05, 'epoch': 0.69}
{'loss': 1.0502, 'grad_norm': 1.0925891399383545, 'learning_rate': 5.390851968357149e-05, 'epoch': 0.69}
{'loss': 1.1506, 'grad_norm': 1.07676362991333, 'learning_rate': 5.386086840438004e-05, 'epoch': 0.69}
{'loss': 0.7041, 'grad_norm': 0.9253450036048889, 'learning_rate': 5.381323043149191e-05, 'epoch': 0.69}
{'loss': 1.1729, 'grad_norm': 1.193995714187622, 'learning_rate': 5.376560577864567e-05, 'epoch': 0.69}
{'loss': 1.0209, 'grad_norm': 1.1325793266296387, 'learning_rate': 5.371799445957598e-05, 'epoch': 0.69}
{'loss': 0.8084, 'grad_norm': 1.2577199935913086, 'learning_rate': 5.3670396488013854e-05, 'epoch': 0.69}
{'loss': 1.0404, 'grad_norm': 0.9690232276916504, 'learning_rate': 5.362281187768614e-05, 'epoch': 0.69}
{'loss': 0.998, 'grad_norm': 1.0238802433013916, 'learning_rate': 5.357524064231616e-05, 'epoch': 0.69}
{'loss': 0.8165, 'grad_norm': 1.4250370264053345, 'learning_rate': 5.3527682795623146e-05, 'epoch': 0.69}
{'loss': 1.0829, 'grad_norm': 1.5290660858154297, 'learning_rate': 5.3480138351322596e-05, 'epoch': 0.69}
{'loss': 0.9117, 'grad_norm': 1.2303287982940674, 'learning_rate': 5.343260732312606e-05, 'epoch': 0.69}
{'loss': 0.7613, 'grad_norm': 1.0567926168441772, 'learning_rate': 5.3385089724741246e-05, 'epoch': 0.69}
{'loss': 0.8618, 'grad_norm': 1.300662875175476, 'learning_rate': 5.3337585569872106e-05, 'epoch': 0.69}
{'loss': 0.997, 'grad_norm': 1.1236176490783691, 'learning_rate': 5.329009487221845e-05, 'epoch': 0.69}
{'loss': 1.0769, 'grad_norm': 1.1586272716522217, 'learning_rate': 5.32426176454765e-05, 'epoch': 0.69}
{'loss': 1.126, 'grad_norm': 0.9200392365455627, 'learning_rate': 5.31951539033384e-05, 'epoch': 0.69}
{'loss': 1.1093, 'grad_norm': 0.8729624152183533, 'learning_rate': 5.314770365949249e-05, 'epoch': 0.69}
{'loss': 0.942, 'grad_norm': 1.092537760734558, 'learning_rate': 5.3100266927623156e-05, 'epoch': 0.69}
{'loss': 0.6388, 'grad_norm': 0.9500986933708191, 'learning_rate': 5.305284372141095e-05, 'epoch': 0.69}
{'loss': 0.806, 'grad_norm': 1.0399194955825806, 'learning_rate': 5.300543405453244e-05, 'epoch': 0.69}
{'loss': 0.9878, 'grad_norm': 1.048484206199646, 'learning_rate': 5.295803794066046e-05, 'epoch': 0.69}
{'loss': 0.9136, 'grad_norm': 1.061665654182434, 'learning_rate': 5.2910655393463736e-05, 'epoch': 0.69}
{'loss': 0.8913, 'grad_norm': 1.573467493057251, 'learning_rate': 5.286328642660718e-05, 'epoch': 0.69}
{'loss': 1.0108, 'grad_norm': 1.7210099697113037, 'learning_rate': 5.28159310537518e-05, 'epoch': 0.69}
{'loss': 0.9279, 'grad_norm': 1.2315717935562134, 'learning_rate': 5.276858928855458e-05, 'epoch': 0.69}
{'loss': 1.0975, 'grad_norm': 1.0790424346923828, 'learning_rate': 5.27212611446688e-05, 'epoch': 0.69}
{'loss': 0.9742, 'grad_norm': 1.0321730375289917, 'learning_rate': 5.267394663574351e-05, 'epoch': 0.69}
{'loss': 1.0359, 'grad_norm': 1.1874172687530518, 'learning_rate': 5.262664577542413e-05, 'epoch': 0.69}
{'loss': 1.0377, 'grad_norm': 0.8939710259437561, 'learning_rate': 5.257935857735186e-05, 'epoch': 0.69}
{'loss': 1.1099, 'grad_norm': 1.0268529653549194, 'learning_rate': 5.2532085055164204e-05, 'epoch': 0.69}
{'loss': 1.0047, 'grad_norm': 1.1126177310943604, 'learning_rate': 5.248482522249458e-05, 'epoch': 0.69}
{'loss': 0.7965, 'grad_norm': 1.1894195079803467, 'learning_rate': 5.243757909297247e-05, 'epoch': 0.69}
{'loss': 1.0496, 'grad_norm': 1.2405692338943481, 'learning_rate': 5.2390346680223535e-05, 'epoch': 0.69}
{'loss': 0.7543, 'grad_norm': 1.0561273097991943, 'learning_rate': 5.234312799786921e-05, 'epoch': 0.69}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8363, 'grad_norm': 1.4345223903656006, 'learning_rate': 5.2295923059527285e-05, 'epoch': 0.69}
{'loss': 0.938, 'grad_norm': 1.318491816520691, 'learning_rate': 5.2248731878811365e-05, 'epoch': 0.69}
{'loss': 1.2791, 'grad_norm': 1.2538939714431763, 'learning_rate': 5.220155446933117e-05, 'epoch': 0.69}
{'loss': 0.8607, 'grad_norm': 1.3482106924057007, 'learning_rate': 5.215439084469245e-05, 'epoch': 0.69}
{'loss': 0.6744, 'grad_norm': 0.9440274834632874, 'learning_rate': 5.210724101849696e-05, 'epoch': 0.69}
{'loss': 0.9901, 'grad_norm': 1.146162748336792, 'learning_rate': 5.206010500434243e-05, 'epoch': 0.69}
{'loss': 0.8685, 'grad_norm': 0.9768868684768677, 'learning_rate': 5.201298281582277e-05, 'epoch': 0.69}
{'loss': 0.9675, 'grad_norm': 1.3371307849884033, 'learning_rate': 5.1965874466527694e-05, 'epoch': 0.69}
{'loss': 1.1397, 'grad_norm': 1.1483205556869507, 'learning_rate': 5.191877997004309e-05, 'epoch': 0.69}
{'loss': 0.9326, 'grad_norm': 1.5084854364395142, 'learning_rate': 5.1871699339950755e-05, 'epoch': 0.69}
{'loss': 1.0302, 'grad_norm': 1.2175586223602295, 'learning_rate': 5.182463258982846e-05, 'epoch': 0.69}
{'loss': 0.9143, 'grad_norm': 0.9766502976417542, 'learning_rate': 5.177757973325017e-05, 'epoch': 0.69}
{'loss': 1.0208, 'grad_norm': 1.1715402603149414, 'learning_rate': 5.1730540783785544e-05, 'epoch': 0.69}
{'loss': 0.7169, 'grad_norm': 1.1629246473312378, 'learning_rate': 5.168351575500049e-05, 'epoch': 0.69}
{'loss': 1.0268, 'grad_norm': 1.0424574613571167, 'learning_rate': 5.163650466045677e-05, 'epoch': 0.69}
{'loss': 1.0397, 'grad_norm': 1.2391140460968018, 'learning_rate': 5.1589507513712164e-05, 'epoch': 0.69}
{'loss': 1.0309, 'grad_norm': 1.4546153545379639, 'learning_rate': 5.154252432832043e-05, 'epoch': 0.69}
{'loss': 0.8964, 'grad_norm': 1.4512214660644531, 'learning_rate': 5.149555511783126e-05, 'epoch': 0.7}
{'loss': 1.0451, 'grad_norm': 1.1779437065124512, 'learning_rate': 5.144859989579034e-05, 'epoch': 0.7}
{'loss': 1.0161, 'grad_norm': 1.2193782329559326, 'learning_rate': 5.14016586757394e-05, 'epoch': 0.7}
{'loss': 0.8866, 'grad_norm': 1.1779816150665283, 'learning_rate': 5.135473147121601e-05, 'epoch': 0.7}
{'loss': 1.0937, 'grad_norm': 1.4007585048675537, 'learning_rate': 5.130781829575375e-05, 'epoch': 0.7}
{'loss': 0.9699, 'grad_norm': 1.1996641159057617, 'learning_rate': 5.126091916288218e-05, 'epoch': 0.7}
{'loss': 1.2822, 'grad_norm': 1.0724678039550781, 'learning_rate': 5.121403408612672e-05, 'epoch': 0.7}
{'loss': 0.696, 'grad_norm': 1.2453404664993286, 'learning_rate': 5.116716307900893e-05, 'epoch': 0.7}
{'loss': 0.6072, 'grad_norm': 1.3436520099639893, 'learning_rate': 5.1120306155046015e-05, 'epoch': 0.7}
{'loss': 0.945, 'grad_norm': 1.065246343612671, 'learning_rate': 5.10734633277514e-05, 'epoch': 0.7}
{'loss': 0.845, 'grad_norm': 0.9775194525718689, 'learning_rate': 5.102663461063433e-05, 'epoch': 0.7}
{'loss': 1.3473, 'grad_norm': 0.9114504456520081, 'learning_rate': 5.097982001719993e-05, 'epoch': 0.7}
{'loss': 0.9875, 'grad_norm': 1.0747978687286377, 'learning_rate': 5.093301956094934e-05, 'epoch': 0.7}
{'loss': 0.8253, 'grad_norm': 1.7609272003173828, 'learning_rate': 5.088623325537952e-05, 'epoch': 0.7}
{'loss': 0.7766, 'grad_norm': 1.1734739542007446, 'learning_rate': 5.083946111398356e-05, 'epoch': 0.7}
{'loss': 0.9438, 'grad_norm': 1.3143073320388794, 'learning_rate': 5.0792703150250134e-05, 'epoch': 0.7}
{'loss': 0.9705, 'grad_norm': 1.1714586019515991, 'learning_rate': 5.0745959377664154e-05, 'epoch': 0.7}
{'loss': 0.8675, 'grad_norm': 0.9921011328697205, 'learning_rate': 5.069922980970626e-05, 'epoch': 0.7}
{'loss': 1.0747, 'grad_norm': 1.1772023439407349, 'learning_rate': 5.0652514459853016e-05, 'epoch': 0.7}
{'loss': 0.9005, 'grad_norm': 0.9361208081245422, 'learning_rate': 5.0605813341576924e-05, 'epoch': 0.7}
{'loss': 0.8189, 'grad_norm': 1.126083254814148, 'learning_rate': 5.055912646834635e-05, 'epoch': 0.7}
{'loss': 1.0476, 'grad_norm': 1.0547492504119873, 'learning_rate': 5.051245385362553e-05, 'epoch': 0.7}
{'loss': 0.7448, 'grad_norm': 0.9577327966690063, 'learning_rate': 5.046579551087469e-05, 'epoch': 0.7}
{'loss': 0.8737, 'grad_norm': 1.1140207052230835, 'learning_rate': 5.041915145354983e-05, 'epoch': 0.7}
{'loss': 0.8855, 'grad_norm': 1.0824319124221802, 'learning_rate': 5.0372521695102894e-05, 'epoch': 0.7}
{'loss': 1.08, 'grad_norm': 0.9358720183372498, 'learning_rate': 5.032590624898166e-05, 'epoch': 0.7}
{'loss': 0.8867, 'grad_norm': 1.0749917030334473, 'learning_rate': 5.027930512862976e-05, 'epoch': 0.7}
{'loss': 0.833, 'grad_norm': 1.3837840557098389, 'learning_rate': 5.0232718347486864e-05, 'epoch': 0.7}
{'loss': 1.0385, 'grad_norm': 1.1324342489242554, 'learning_rate': 5.018614591898821e-05, 'epoch': 0.7}
{'loss': 0.7677, 'grad_norm': 1.222429871559143, 'learning_rate': 5.013958785656516e-05, 'epoch': 0.7}
{'loss': 1.0907, 'grad_norm': 1.207631230354309, 'learning_rate': 5.0093044173644834e-05, 'epoch': 0.7}
{'loss': 0.7122, 'grad_norm': 1.2041739225387573, 'learning_rate': 5.0046514883650176e-05, 'epoch': 0.7}
{'loss': 0.9322, 'grad_norm': 1.1451423168182373, 'learning_rate': 5.000000000000002e-05, 'epoch': 0.7}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1765, 'grad_norm': 1.6635050773620605, 'learning_rate': 4.9953499536109005e-05, 'epoch': 0.7}
{'loss': 0.777, 'grad_norm': 1.3906309604644775, 'learning_rate': 4.99070135053877e-05, 'epoch': 0.7}
{'loss': 0.7896, 'grad_norm': 1.2595510482788086, 'learning_rate': 4.986054192124242e-05, 'epoch': 0.7}
{'loss': 1.0802, 'grad_norm': 1.2513833045959473, 'learning_rate': 4.9814084797075355e-05, 'epoch': 0.7}
{'loss': 0.9018, 'grad_norm': 0.9809983968734741, 'learning_rate': 4.97676421462845e-05, 'epoch': 0.7}
{'loss': 0.6325, 'grad_norm': 0.8154441118240356, 'learning_rate': 4.972121398226371e-05, 'epoch': 0.7}
{'loss': 0.7493, 'grad_norm': 0.8014841675758362, 'learning_rate': 4.967480031840259e-05, 'epoch': 0.7}
{'loss': 1.041, 'grad_norm': 1.0289851427078247, 'learning_rate': 4.9628401168086746e-05, 'epoch': 0.7}
{'loss': 0.7597, 'grad_norm': 1.2114266157150269, 'learning_rate': 4.958201654469731e-05, 'epoch': 0.7}
{'loss': 1.0285, 'grad_norm': 1.1645383834838867, 'learning_rate': 4.953564646161148e-05, 'epoch': 0.7}
{'loss': 1.0422, 'grad_norm': 1.1618752479553223, 'learning_rate': 4.948929093220216e-05, 'epoch': 0.7}
{'loss': 0.8495, 'grad_norm': 0.9207674264907837, 'learning_rate': 4.944294996983805e-05, 'epoch': 0.7}
{'loss': 1.0254, 'grad_norm': 1.5765767097473145, 'learning_rate': 4.939662358788364e-05, 'epoch': 0.7}
{'loss': 0.8594, 'grad_norm': 1.230002760887146, 'learning_rate': 4.9350311799699226e-05, 'epoch': 0.7}
{'loss': 0.7883, 'grad_norm': 1.244377613067627, 'learning_rate': 4.9304014618640995e-05, 'epoch': 0.7}
{'loss': 0.8567, 'grad_norm': 1.3323965072631836, 'learning_rate': 4.925773205806069e-05, 'epoch': 0.7}
{'loss': 0.9997, 'grad_norm': 1.5826716423034668, 'learning_rate': 4.9211464131306095e-05, 'epoch': 0.7}
{'loss': 0.7121, 'grad_norm': 1.4739112854003906, 'learning_rate': 4.916521085172062e-05, 'epoch': 0.7}
{'loss': 0.7403, 'grad_norm': 1.2806683778762817, 'learning_rate': 4.911897223264348e-05, 'epoch': 0.7}
{'loss': 1.0155, 'grad_norm': 1.3548974990844727, 'learning_rate': 4.9072748287409677e-05, 'epoch': 0.7}
{'loss': 0.7505, 'grad_norm': 0.9637598991394043, 'learning_rate': 4.902653902934997e-05, 'epoch': 0.7}
{'loss': 0.7632, 'grad_norm': 0.98841392993927, 'learning_rate': 4.898034447179085e-05, 'epoch': 0.7}
{'loss': 1.0425, 'grad_norm': 1.0904122591018677, 'learning_rate': 4.893416462805469e-05, 'epoch': 0.7}
{'loss': 0.6924, 'grad_norm': 1.117181658744812, 'learning_rate': 4.888799951145948e-05, 'epoch': 0.7}
{'loss': 1.0145, 'grad_norm': 1.119041085243225, 'learning_rate': 4.884184913531902e-05, 'epoch': 0.7}
{'loss': 1.0311, 'grad_norm': 2.4107444286346436, 'learning_rate': 4.8795713512942865e-05, 'epoch': 0.7}
{'loss': 0.7232, 'grad_norm': 1.0412623882293701, 'learning_rate': 4.874959265763627e-05, 'epoch': 0.7}
{'loss': 1.1143, 'grad_norm': 1.2932802438735962, 'learning_rate': 4.8703486582700365e-05, 'epoch': 0.7}
{'loss': 1.1903, 'grad_norm': 1.0755637884140015, 'learning_rate': 4.8657395301431784e-05, 'epoch': 0.7}
{'loss': 0.9264, 'grad_norm': 1.0345062017440796, 'learning_rate': 4.861131882712314e-05, 'epoch': 0.7}
{'loss': 1.0877, 'grad_norm': 1.6558341979980469, 'learning_rate': 4.856525717306263e-05, 'epoch': 0.7}
{'loss': 0.7886, 'grad_norm': 0.8906362056732178, 'learning_rate': 4.8519210352534207e-05, 'epoch': 0.7}
{'loss': 0.8633, 'grad_norm': 1.0290054082870483, 'learning_rate': 4.8473178378817564e-05, 'epoch': 0.71}
{'loss': 0.9496, 'grad_norm': 0.9931199550628662, 'learning_rate': 4.8427161265188034e-05, 'epoch': 0.71}
{'loss': 1.003, 'grad_norm': 1.1762845516204834, 'learning_rate': 4.838115902491685e-05, 'epoch': 0.71}
{'loss': 0.9467, 'grad_norm': 1.1553668975830078, 'learning_rate': 4.833517167127076e-05, 'epoch': 0.71}
{'loss': 0.8818, 'grad_norm': 1.106890320777893, 'learning_rate': 4.8289199217512324e-05, 'epoch': 0.71}
{'loss': 1.0983, 'grad_norm': 1.1367244720458984, 'learning_rate': 4.824324167689975e-05, 'epoch': 0.71}
{'loss': 1.1627, 'grad_norm': 1.1810494661331177, 'learning_rate': 4.8197299062686995e-05, 'epoch': 0.71}
{'loss': 0.9086, 'grad_norm': 1.108350396156311, 'learning_rate': 4.8151371388123644e-05, 'epoch': 0.71}
{'loss': 1.0012, 'grad_norm': 0.9477580785751343, 'learning_rate': 4.810545866645512e-05, 'epoch': 0.71}
{'loss': 0.8742, 'grad_norm': 1.0653406381607056, 'learning_rate': 4.805956091092227e-05, 'epoch': 0.71}
{'loss': 0.9252, 'grad_norm': 1.1227378845214844, 'learning_rate': 4.801367813476194e-05, 'epoch': 0.71}
{'loss': 1.1717, 'grad_norm': 0.8560318946838379, 'learning_rate': 4.796781035120642e-05, 'epoch': 0.71}
{'loss': 0.74, 'grad_norm': 1.375367522239685, 'learning_rate': 4.7921957573483754e-05, 'epoch': 0.71}
{'loss': 1.1256, 'grad_norm': 1.0232021808624268, 'learning_rate': 4.7876119814817756e-05, 'epoch': 0.71}
{'loss': 0.8752, 'grad_norm': 0.8790760040283203, 'learning_rate': 4.783029708842766e-05, 'epoch': 0.71}
{'loss': 0.9372, 'grad_norm': 1.2374340295791626, 'learning_rate': 4.7784489407528686e-05, 'epoch': 0.71}
{'loss': 0.9808, 'grad_norm': 1.1945858001708984, 'learning_rate': 4.773869678533138e-05, 'epoch': 0.71}
{'loss': 0.8197, 'grad_norm': 0.9580907821655273, 'learning_rate': 4.7692919235042255e-05, 'epoch': 0.71}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7865, 'grad_norm': 1.1718658208847046, 'learning_rate': 4.7647156769863265e-05, 'epoch': 0.71}
{'loss': 0.9595, 'grad_norm': 1.0224430561065674, 'learning_rate': 4.7601409402992106e-05, 'epoch': 0.71}
{'loss': 0.776, 'grad_norm': 0.9824160933494568, 'learning_rate': 4.755567714762209e-05, 'epoch': 0.71}
{'loss': 0.8124, 'grad_norm': 1.062918782234192, 'learning_rate': 4.7509960016942144e-05, 'epoch': 0.71}
{'loss': 0.6867, 'grad_norm': 1.0150586366653442, 'learning_rate': 4.746425802413694e-05, 'epoch': 0.71}
{'loss': 0.9331, 'grad_norm': 1.2331750392913818, 'learning_rate': 4.7418571182386686e-05, 'epoch': 0.71}
{'loss': 0.7155, 'grad_norm': 1.0458688735961914, 'learning_rate': 4.737289950486723e-05, 'epoch': 0.71}
{'loss': 1.2673, 'grad_norm': 1.3848434686660767, 'learning_rate': 4.732724300475008e-05, 'epoch': 0.71}
{'loss': 1.0242, 'grad_norm': 1.1674473285675049, 'learning_rate': 4.7281601695202347e-05, 'epoch': 0.71}
{'loss': 0.8713, 'grad_norm': 1.2742955684661865, 'learning_rate': 4.723597558938672e-05, 'epoch': 0.71}
{'loss': 0.7726, 'grad_norm': 1.8365135192871094, 'learning_rate': 4.7190364700461664e-05, 'epoch': 0.71}
{'loss': 0.9944, 'grad_norm': 1.248939037322998, 'learning_rate': 4.714476904158098e-05, 'epoch': 0.71}
{'loss': 0.9087, 'grad_norm': 1.1622480154037476, 'learning_rate': 4.7099188625894375e-05, 'epoch': 0.71}
{'loss': 0.9504, 'grad_norm': 1.1519322395324707, 'learning_rate': 4.7053623466546956e-05, 'epoch': 0.71}
{'loss': 1.074, 'grad_norm': 1.2103701829910278, 'learning_rate': 4.700807357667952e-05, 'epoch': 0.71}
{'loss': 1.1117, 'grad_norm': 1.1681350469589233, 'learning_rate': 4.6962538969428416e-05, 'epoch': 0.71}
{'loss': 1.0457, 'grad_norm': 1.184069275856018, 'learning_rate': 4.691701965792559e-05, 'epoch': 0.71}
{'loss': 0.914, 'grad_norm': 1.6097379922866821, 'learning_rate': 4.687151565529864e-05, 'epoch': 0.71}
{'loss': 0.6448, 'grad_norm': 1.3965660333633423, 'learning_rate': 4.682602697467067e-05, 'epoch': 0.71}
{'loss': 0.9459, 'grad_norm': 0.9625170230865479, 'learning_rate': 4.678055362916041e-05, 'epoch': 0.71}
{'loss': 0.8717, 'grad_norm': 1.3919785022735596, 'learning_rate': 4.673509563188214e-05, 'epoch': 0.71}
{'loss': 0.838, 'grad_norm': 0.9678134322166443, 'learning_rate': 4.668965299594574e-05, 'epoch': 0.71}
{'loss': 0.7399, 'grad_norm': 0.9545056223869324, 'learning_rate': 4.66442257344566e-05, 'epoch': 0.71}
{'loss': 0.8924, 'grad_norm': 1.0570346117019653, 'learning_rate': 4.6598813860515856e-05, 'epoch': 0.71}
{'loss': 0.864, 'grad_norm': 0.9207924008369446, 'learning_rate': 4.6553417387219886e-05, 'epoch': 0.71}
{'loss': 0.897, 'grad_norm': 1.237805962562561, 'learning_rate': 4.650803632766096e-05, 'epoch': 0.71}
{'loss': 0.8342, 'grad_norm': 0.9516456723213196, 'learning_rate': 4.6462670694926704e-05, 'epoch': 0.71}
{'loss': 1.0219, 'grad_norm': 1.0055140256881714, 'learning_rate': 4.6417320502100316e-05, 'epoch': 0.71}
{'loss': 1.196, 'grad_norm': 1.273537516593933, 'learning_rate': 4.6371985762260685e-05, 'epoch': 0.71}
{'loss': 0.6633, 'grad_norm': 1.4474705457687378, 'learning_rate': 4.6326666488481975e-05, 'epoch': 0.71}
{'loss': 0.7425, 'grad_norm': 1.264170527458191, 'learning_rate': 4.628136269383421e-05, 'epoch': 0.71}
{'loss': 1.0012, 'grad_norm': 1.1770294904708862, 'learning_rate': 4.6236074391382624e-05, 'epoch': 0.71}
{'loss': 0.8767, 'grad_norm': 1.1732714176177979, 'learning_rate': 4.619080159418826e-05, 'epoch': 0.71}
{'loss': 0.8831, 'grad_norm': 1.3064783811569214, 'learning_rate': 4.6145544315307534e-05, 'epoch': 0.71}
{'loss': 1.1263, 'grad_norm': 1.2508372068405151, 'learning_rate': 4.6100302567792444e-05, 'epoch': 0.71}
{'loss': 0.8438, 'grad_norm': 1.1347609758377075, 'learning_rate': 4.6055076364690476e-05, 'epoch': 0.71}
{'loss': 0.8523, 'grad_norm': 1.0804392099380493, 'learning_rate': 4.600986571904461e-05, 'epoch': 0.71}
{'loss': 0.9503, 'grad_norm': 1.1332088708877563, 'learning_rate': 4.596467064389346e-05, 'epoch': 0.71}
{'loss': 0.9638, 'grad_norm': 0.8729414343833923, 'learning_rate': 4.5919491152271033e-05, 'epoch': 0.71}
{'loss': 1.1553, 'grad_norm': 1.0465377569198608, 'learning_rate': 4.587432725720687e-05, 'epoch': 0.71}
{'loss': 0.9317, 'grad_norm': 1.287711262702942, 'learning_rate': 4.582917897172603e-05, 'epoch': 0.71}
{'loss': 1.0323, 'grad_norm': 1.0706013441085815, 'learning_rate': 4.578404630884905e-05, 'epoch': 0.71}
{'loss': 0.7893, 'grad_norm': 1.5874429941177368, 'learning_rate': 4.5738929281591946e-05, 'epoch': 0.71}
{'loss': 0.9126, 'grad_norm': 1.3184354305267334, 'learning_rate': 4.5693827902966355e-05, 'epoch': 0.71}
{'loss': 0.89, 'grad_norm': 1.1362063884735107, 'learning_rate': 4.564874218597915e-05, 'epoch': 0.71}
{'loss': 0.8461, 'grad_norm': 1.4429243803024292, 'learning_rate': 4.5603672143632944e-05, 'epoch': 0.71}
{'loss': 0.7636, 'grad_norm': 1.0049912929534912, 'learning_rate': 4.5558617788925695e-05, 'epoch': 0.71}
{'loss': 0.9736, 'grad_norm': 1.0245941877365112, 'learning_rate': 4.5513579134850816e-05, 'epoch': 0.72}
{'loss': 0.9053, 'grad_norm': 0.8455104827880859, 'learning_rate': 4.546855619439734e-05, 'epoch': 0.72}
{'loss': 1.0012, 'grad_norm': 1.4159369468688965, 'learning_rate': 4.542354898054953e-05, 'epoch': 0.72}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0826, 'grad_norm': 1.0446181297302246, 'learning_rate': 4.5378557506287404e-05, 'epoch': 0.72}
{'loss': 1.1093, 'grad_norm': 1.1799827814102173, 'learning_rate': 4.533358178458612e-05, 'epoch': 0.72}
{'loss': 1.0308, 'grad_norm': 0.9536666870117188, 'learning_rate': 4.528862182841659e-05, 'epoch': 0.72}
{'loss': 0.8239, 'grad_norm': 0.8801926970481873, 'learning_rate': 4.524367765074499e-05, 'epoch': 0.72}
{'loss': 0.7867, 'grad_norm': 1.0625388622283936, 'learning_rate': 4.519874926453302e-05, 'epoch': 0.72}
{'loss': 1.0065, 'grad_norm': 0.9829503893852234, 'learning_rate': 4.5153836682737774e-05, 'epoch': 0.72}
{'loss': 1.1582, 'grad_norm': 0.8508511185646057, 'learning_rate': 4.5108939918311886e-05, 'epoch': 0.72}
{'loss': 1.0054, 'grad_norm': 1.233385682106018, 'learning_rate': 4.506405898420335e-05, 'epoch': 0.72}
{'loss': 0.8192, 'grad_norm': 0.9823245406150818, 'learning_rate': 4.5019193893355585e-05, 'epoch': 0.72}
{'loss': 0.5855, 'grad_norm': 1.033517599105835, 'learning_rate': 4.497434465870749e-05, 'epoch': 0.72}
{'loss': 1.1633, 'grad_norm': 1.1914736032485962, 'learning_rate': 4.492951129319331e-05, 'epoch': 0.72}
{'loss': 1.0385, 'grad_norm': 1.0763583183288574, 'learning_rate': 4.4884693809742906e-05, 'epoch': 0.72}
{'loss': 0.7143, 'grad_norm': 1.0560184717178345, 'learning_rate': 4.483989222128127e-05, 'epoch': 0.72}
{'loss': 1.0238, 'grad_norm': 0.9901595115661621, 'learning_rate': 4.479510654072909e-05, 'epoch': 0.72}
{'loss': 0.8708, 'grad_norm': 1.129955530166626, 'learning_rate': 4.4750336781002225e-05, 'epoch': 0.72}
{'loss': 0.9485, 'grad_norm': 1.4770580530166626, 'learning_rate': 4.4705582955012137e-05, 'epoch': 0.72}
{'loss': 0.9082, 'grad_norm': 1.1331936120986938, 'learning_rate': 4.46608450756656e-05, 'epoch': 0.72}
{'loss': 0.9127, 'grad_norm': 1.2617474794387817, 'learning_rate': 4.461612315586481e-05, 'epoch': 0.72}
{'loss': 0.8824, 'grad_norm': 1.0755032300949097, 'learning_rate': 4.457141720850733e-05, 'epoch': 0.72}
{'loss': 0.699, 'grad_norm': 1.3225574493408203, 'learning_rate': 4.452672724648611e-05, 'epoch': 0.72}
{'loss': 0.7533, 'grad_norm': 1.2011560201644897, 'learning_rate': 4.448205328268959e-05, 'epoch': 0.72}
{'loss': 0.8182, 'grad_norm': 1.1555182933807373, 'learning_rate': 4.443739533000151e-05, 'epoch': 0.72}
{'loss': 0.7281, 'grad_norm': 1.245017409324646, 'learning_rate': 4.439275340130099e-05, 'epoch': 0.72}
{'loss': 0.9145, 'grad_norm': 0.9727174639701843, 'learning_rate': 4.434812750946256e-05, 'epoch': 0.72}
{'loss': 1.3013, 'grad_norm': 1.1786738634109497, 'learning_rate': 4.430351766735609e-05, 'epoch': 0.72}
{'loss': 0.8952, 'grad_norm': 0.9871300458908081, 'learning_rate': 4.425892388784682e-05, 'epoch': 0.72}
{'loss': 1.2996, 'grad_norm': 1.3404923677444458, 'learning_rate': 4.42143461837955e-05, 'epoch': 0.72}
{'loss': 0.7013, 'grad_norm': 1.1254476308822632, 'learning_rate': 4.416978456805796e-05, 'epoch': 0.72}
{'loss': 1.1456, 'grad_norm': 1.1264487504959106, 'learning_rate': 4.412523905348568e-05, 'epoch': 0.72}
{'loss': 1.069, 'grad_norm': 1.6967520713806152, 'learning_rate': 4.4080709652925336e-05, 'epoch': 0.72}
{'loss': 0.9313, 'grad_norm': 0.8475579023361206, 'learning_rate': 4.403619637921894e-05, 'epoch': 0.72}
{'loss': 0.8392, 'grad_norm': 1.3189928531646729, 'learning_rate': 4.399169924520403e-05, 'epoch': 0.72}
{'loss': 1.1214, 'grad_norm': 1.3133810758590698, 'learning_rate': 4.394721826371322e-05, 'epoch': 0.72}
{'loss': 0.9561, 'grad_norm': 1.1764169931411743, 'learning_rate': 4.390275344757475e-05, 'epoch': 0.72}
{'loss': 1.21, 'grad_norm': 1.1364580392837524, 'learning_rate': 4.385830480961192e-05, 'epoch': 0.72}
{'loss': 0.8655, 'grad_norm': 1.3753732442855835, 'learning_rate': 4.381387236264359e-05, 'epoch': 0.72}
{'loss': 0.7228, 'grad_norm': 1.1396375894546509, 'learning_rate': 4.376945611948385e-05, 'epoch': 0.72}
{'loss': 0.9269, 'grad_norm': 0.9789468050003052, 'learning_rate': 4.372505609294214e-05, 'epoch': 0.72}
{'loss': 0.984, 'grad_norm': 1.2768012285232544, 'learning_rate': 4.368067229582319e-05, 'epoch': 0.72}
{'loss': 0.7396, 'grad_norm': 1.2465877532958984, 'learning_rate': 4.3636304740927046e-05, 'epoch': 0.72}
{'loss': 1.0233, 'grad_norm': 0.9475207328796387, 'learning_rate': 4.359195344104916e-05, 'epoch': 0.72}
{'loss': 0.7971, 'grad_norm': 0.8891913890838623, 'learning_rate': 4.3547618408980215e-05, 'epoch': 0.72}
{'loss': 0.7844, 'grad_norm': 1.0089689493179321, 'learning_rate': 4.350329965750621e-05, 'epoch': 0.72}
{'loss': 0.9571, 'grad_norm': 0.8992099761962891, 'learning_rate': 4.345899719940843e-05, 'epoch': 0.72}
{'loss': 0.9245, 'grad_norm': 1.46466863155365, 'learning_rate': 4.34147110474636e-05, 'epoch': 0.72}
{'loss': 1.0514, 'grad_norm': 1.3555352687835693, 'learning_rate': 4.3370441214443466e-05, 'epoch': 0.72}
{'loss': 0.822, 'grad_norm': 1.0801931619644165, 'learning_rate': 4.3326187713115415e-05, 'epoch': 0.72}
{'loss': 0.9381, 'grad_norm': 1.0996403694152832, 'learning_rate': 4.3281950556241766e-05, 'epoch': 0.72}
{'loss': 1.1728, 'grad_norm': 1.598555088043213, 'learning_rate': 4.3237729756580434e-05, 'epoch': 0.72}
{'loss': 1.1961, 'grad_norm': 0.9761494994163513, 'learning_rate': 4.3193525326884435e-05, 'epoch': 0.72}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8123, 'grad_norm': 1.2317845821380615, 'learning_rate': 4.314933727990211e-05, 'epoch': 0.72}
{'loss': 0.7935, 'grad_norm': 1.197156548500061, 'learning_rate': 4.31051656283771e-05, 'epoch': 0.72}
{'loss': 0.837, 'grad_norm': 1.1945626735687256, 'learning_rate': 4.306101038504824e-05, 'epoch': 0.72}
{'loss': 1.2346, 'grad_norm': 1.5328184366226196, 'learning_rate': 4.301687156264977e-05, 'epoch': 0.72}
{'loss': 0.9178, 'grad_norm': 1.8305504322052002, 'learning_rate': 4.2972749173911074e-05, 'epoch': 0.72}
{'loss': 0.8467, 'grad_norm': 1.2155978679656982, 'learning_rate': 4.2928643231556844e-05, 'epoch': 0.72}
{'loss': 0.8957, 'grad_norm': 1.190366506576538, 'learning_rate': 4.288455374830701e-05, 'epoch': 0.72}
{'loss': 1.165, 'grad_norm': 1.079038381576538, 'learning_rate': 4.2840480736876776e-05, 'epoch': 0.72}
{'loss': 0.8896, 'grad_norm': 1.5495035648345947, 'learning_rate': 4.279642420997655e-05, 'epoch': 0.72}
{'loss': 0.9415, 'grad_norm': 1.6189924478530884, 'learning_rate': 4.275238418031209e-05, 'epoch': 0.72}
{'loss': 0.9377, 'grad_norm': 1.1606498956680298, 'learning_rate': 4.270836066058429e-05, 'epoch': 0.72}
{'loss': 1.0175, 'grad_norm': 1.104920744895935, 'learning_rate': 4.266435366348932e-05, 'epoch': 0.72}
{'loss': 0.6856, 'grad_norm': 1.0251606702804565, 'learning_rate': 4.2620363201718594e-05, 'epoch': 0.73}
{'loss': 0.9605, 'grad_norm': 0.9927231669425964, 'learning_rate': 4.257638928795872e-05, 'epoch': 0.73}
{'loss': 1.1281, 'grad_norm': 1.2242705821990967, 'learning_rate': 4.253243193489165e-05, 'epoch': 0.73}
{'loss': 0.6479, 'grad_norm': 1.0209784507751465, 'learning_rate': 4.2488491155194346e-05, 'epoch': 0.73}
{'loss': 1.4086, 'grad_norm': 1.0532591342926025, 'learning_rate': 4.244456696153924e-05, 'epoch': 0.73}
{'loss': 0.8936, 'grad_norm': 1.313480257987976, 'learning_rate': 4.240065936659374e-05, 'epoch': 0.73}
{'loss': 0.9816, 'grad_norm': 0.8290253281593323, 'learning_rate': 4.235676838302068e-05, 'epoch': 0.73}
{'loss': 1.0352, 'grad_norm': 1.397216558456421, 'learning_rate': 4.231289402347798e-05, 'epoch': 0.73}
{'loss': 1.0748, 'grad_norm': 1.2295230627059937, 'learning_rate': 4.226903630061878e-05, 'epoch': 0.73}
{'loss': 0.7508, 'grad_norm': 1.0423932075500488, 'learning_rate': 4.2225195227091454e-05, 'epoch': 0.73}
{'loss': 0.8928, 'grad_norm': 1.3337572813034058, 'learning_rate': 4.2181370815539504e-05, 'epoch': 0.73}
{'loss': 1.1755, 'grad_norm': 0.9680938720703125, 'learning_rate': 4.213756307860175e-05, 'epoch': 0.73}
{'loss': 0.7763, 'grad_norm': 1.0683444738388062, 'learning_rate': 4.209377202891212e-05, 'epoch': 0.73}
{'loss': 0.7542, 'grad_norm': 0.9937630891799927, 'learning_rate': 4.204999767909972e-05, 'epoch': 0.73}
{'loss': 1.1334, 'grad_norm': 0.9947593212127686, 'learning_rate': 4.2006240041788825e-05, 'epoch': 0.73}
{'loss': 0.8901, 'grad_norm': 1.0641757249832153, 'learning_rate': 4.1962499129599044e-05, 'epoch': 0.73}
{'loss': 0.9126, 'grad_norm': 1.1662834882736206, 'learning_rate': 4.191877495514489e-05, 'epoch': 0.73}
{'loss': 1.0687, 'grad_norm': 1.1617482900619507, 'learning_rate': 4.1875067531036374e-05, 'epoch': 0.73}
{'loss': 0.9332, 'grad_norm': 1.315812110900879, 'learning_rate': 4.1831376869878336e-05, 'epoch': 0.73}
{'loss': 0.8686, 'grad_norm': 1.4163661003112793, 'learning_rate': 4.1787702984271074e-05, 'epoch': 0.73}
{'loss': 0.9235, 'grad_norm': 1.0723060369491577, 'learning_rate': 4.174404588680988e-05, 'epoch': 0.73}
{'loss': 0.9212, 'grad_norm': 0.8907224535942078, 'learning_rate': 4.170040559008522e-05, 'epoch': 0.73}
{'loss': 0.9407, 'grad_norm': 1.3502918481826782, 'learning_rate': 4.165678210668286e-05, 'epoch': 0.73}
{'loss': 0.9145, 'grad_norm': 1.111531138420105, 'learning_rate': 4.161317544918345e-05, 'epoch': 0.73}
{'loss': 0.8187, 'grad_norm': 1.320590615272522, 'learning_rate': 4.1569585630163044e-05, 'epoch': 0.73}
{'loss': 0.9217, 'grad_norm': 1.3005841970443726, 'learning_rate': 4.1526012662192714e-05, 'epoch': 0.73}
{'loss': 0.8071, 'grad_norm': 1.1151281595230103, 'learning_rate': 4.148245655783869e-05, 'epoch': 0.73}
{'loss': 0.8967, 'grad_norm': 1.4066945314407349, 'learning_rate': 4.143891732966233e-05, 'epoch': 0.73}
{'loss': 1.0474, 'grad_norm': 1.0769797563552856, 'learning_rate': 4.139539499022015e-05, 'epoch': 0.73}
{'loss': 0.77, 'grad_norm': 1.070968747138977, 'learning_rate': 4.135188955206375e-05, 'epoch': 0.73}
{'loss': 0.9504, 'grad_norm': 1.7240879535675049, 'learning_rate': 4.130840102773995e-05, 'epoch': 0.73}
{'loss': 0.7241, 'grad_norm': 1.0567387342453003, 'learning_rate': 4.12649294297906e-05, 'epoch': 0.73}
{'loss': 1.1068, 'grad_norm': 1.443070411682129, 'learning_rate': 4.12214747707527e-05, 'epoch': 0.73}
{'loss': 0.8655, 'grad_norm': 0.9475365281105042, 'learning_rate': 4.117803706315836e-05, 'epoch': 0.73}
{'loss': 0.824, 'grad_norm': 0.9759372472763062, 'learning_rate': 4.113461631953477e-05, 'epoch': 0.73}
{'loss': 0.7998, 'grad_norm': 1.086422085762024, 'learning_rate': 4.109121255240438e-05, 'epoch': 0.73}
{'loss': 0.8531, 'grad_norm': 1.1416754722595215, 'learning_rate': 4.104782577428448e-05, 'epoch': 0.73}
{'loss': 1.0347, 'grad_norm': 1.5912976264953613, 'learning_rate': 4.100445599768774e-05, 'epoch': 0.73}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2982, 'grad_norm': 1.223315715789795, 'learning_rate': 4.096110323512167e-05, 'epoch': 0.73}
{'loss': 1.341, 'grad_norm': 0.9298465847969055, 'learning_rate': 4.09177674990891e-05, 'epoch': 0.73}
{'loss': 1.3093, 'grad_norm': 1.0670366287231445, 'learning_rate': 4.0874448802087794e-05, 'epoch': 0.73}
{'loss': 0.6434, 'grad_norm': 0.891732931137085, 'learning_rate': 4.0831147156610684e-05, 'epoch': 0.73}
{'loss': 0.5673, 'grad_norm': 1.0596650838851929, 'learning_rate': 4.078786257514573e-05, 'epoch': 0.73}
{'loss': 0.8665, 'grad_norm': 1.255043625831604, 'learning_rate': 4.074459507017597e-05, 'epoch': 0.73}
{'loss': 1.0242, 'grad_norm': 1.162729024887085, 'learning_rate': 4.070134465417963e-05, 'epoch': 0.73}
{'loss': 0.6514, 'grad_norm': 1.0119320154190063, 'learning_rate': 4.065811133962987e-05, 'epoch': 0.73}
{'loss': 1.1925, 'grad_norm': 1.0802189111709595, 'learning_rate': 4.0614895138994965e-05, 'epoch': 0.73}
{'loss': 0.9161, 'grad_norm': 0.9857537746429443, 'learning_rate': 4.057169606473827e-05, 'epoch': 0.73}
{'loss': 0.9714, 'grad_norm': 1.3184434175491333, 'learning_rate': 4.0528514129318195e-05, 'epoch': 0.73}
{'loss': 0.9913, 'grad_norm': 1.1550482511520386, 'learning_rate': 4.048534934518816e-05, 'epoch': 0.73}
{'loss': 0.8635, 'grad_norm': 1.0878571271896362, 'learning_rate': 4.044220172479675e-05, 'epoch': 0.73}
{'loss': 0.7111, 'grad_norm': 1.0211831331253052, 'learning_rate': 4.039907128058749e-05, 'epoch': 0.73}
{'loss': 0.7782, 'grad_norm': 5.506045341491699, 'learning_rate': 4.0355958024998995e-05, 'epoch': 0.73}
{'loss': 1.0476, 'grad_norm': 1.2946555614471436, 'learning_rate': 4.031286197046493e-05, 'epoch': 0.73}
{'loss': 0.9366, 'grad_norm': 1.6058316230773926, 'learning_rate': 4.0269783129413955e-05, 'epoch': 0.73}
{'loss': 0.6885, 'grad_norm': 0.9380848407745361, 'learning_rate': 4.022672151426989e-05, 'epoch': 0.73}
{'loss': 0.8156, 'grad_norm': 1.1906964778900146, 'learning_rate': 4.018367713745137e-05, 'epoch': 0.73}
{'loss': 0.9295, 'grad_norm': 1.0087476968765259, 'learning_rate': 4.0140650011372295e-05, 'epoch': 0.73}
{'loss': 0.8736, 'grad_norm': 1.1122571229934692, 'learning_rate': 4.009764014844143e-05, 'epoch': 0.73}
{'loss': 0.6506, 'grad_norm': 0.9748991131782532, 'learning_rate': 4.005464756106262e-05, 'epoch': 0.73}
{'loss': 1.0959, 'grad_norm': 1.4262505769729614, 'learning_rate': 4.001167226163471e-05, 'epoch': 0.73}
{'loss': 0.8809, 'grad_norm': 1.4090601205825806, 'learning_rate': 3.99687142625516e-05, 'epoch': 0.73}
{'loss': 0.7664, 'grad_norm': 1.0463064908981323, 'learning_rate': 3.99257735762021e-05, 'epoch': 0.73}
{'loss': 1.0279, 'grad_norm': 1.1504696607589722, 'learning_rate': 3.988285021497019e-05, 'epoch': 0.73}
{'loss': 1.2268, 'grad_norm': 2.8304872512817383, 'learning_rate': 3.9839944191234715e-05, 'epoch': 0.73}
{'loss': 1.0756, 'grad_norm': 1.1834847927093506, 'learning_rate': 3.9797055517369577e-05, 'epoch': 0.74}
{'loss': 1.2695, 'grad_norm': 0.8553813099861145, 'learning_rate': 3.975418420574364e-05, 'epoch': 0.74}
{'loss': 0.8955, 'grad_norm': 1.0300358533859253, 'learning_rate': 3.971133026872077e-05, 'epoch': 0.74}
{'loss': 0.8179, 'grad_norm': 1.112053394317627, 'learning_rate': 3.966849371865993e-05, 'epoch': 0.74}
{'loss': 1.0195, 'grad_norm': 1.125980257987976, 'learning_rate': 3.962567456791484e-05, 'epoch': 0.74}
{'loss': 0.9876, 'grad_norm': 1.5442147254943848, 'learning_rate': 3.9582872828834485e-05, 'epoch': 0.74}
{'loss': 0.7959, 'grad_norm': 0.9984935522079468, 'learning_rate': 3.954008851376252e-05, 'epoch': 0.74}
{'loss': 0.9524, 'grad_norm': 1.1083483695983887, 'learning_rate': 3.9497321635037856e-05, 'epoch': 0.74}
{'loss': 1.1562, 'grad_norm': 1.0895990133285522, 'learning_rate': 3.9454572204994215e-05, 'epoch': 0.74}
{'loss': 0.8811, 'grad_norm': 0.9743421077728271, 'learning_rate': 3.9411840235960296e-05, 'epoch': 0.74}
{'loss': 0.6359, 'grad_norm': 1.2239913940429688, 'learning_rate': 3.936912574025988e-05, 'epoch': 0.74}
{'loss': 1.0523, 'grad_norm': 1.1952438354492188, 'learning_rate': 3.9326428730211505e-05, 'epoch': 0.74}
{'loss': 0.8686, 'grad_norm': 1.6525793075561523, 'learning_rate': 3.9283749218128885e-05, 'epoch': 0.74}
{'loss': 0.9396, 'grad_norm': 1.2708690166473389, 'learning_rate': 3.9241087216320536e-05, 'epoch': 0.74}
{'loss': 0.9, 'grad_norm': 1.3808497190475464, 'learning_rate': 3.919844273708999e-05, 'epoch': 0.74}
{'loss': 1.1693, 'grad_norm': 1.4739423990249634, 'learning_rate': 3.915581579273569e-05, 'epoch': 0.74}
{'loss': 1.0359, 'grad_norm': 1.0936620235443115, 'learning_rate': 3.9113206395551064e-05, 'epoch': 0.74}
{'loss': 0.6981, 'grad_norm': 0.9537941813468933, 'learning_rate': 3.9070614557824406e-05, 'epoch': 0.74}
{'loss': 1.2257, 'grad_norm': 1.2721803188323975, 'learning_rate': 3.902804029183907e-05, 'epoch': 0.74}
{'loss': 0.8435, 'grad_norm': 0.9800212979316711, 'learning_rate': 3.8985483609873244e-05, 'epoch': 0.74}
{'loss': 0.6012, 'grad_norm': 1.2644494771957397, 'learning_rate': 3.894294452420005e-05, 'epoch': 0.74}
{'loss': 0.8604, 'grad_norm': 1.2327405214309692, 'learning_rate': 3.8900423047087585e-05, 'epoch': 0.74}
{'loss': 0.9512, 'grad_norm': 1.05731999874115, 'learning_rate': 3.885791919079878e-05, 'epoch': 0.74}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7739, 'grad_norm': 1.0494016408920288, 'learning_rate': 3.881543296759165e-05, 'epoch': 0.74}
{'loss': 1.1244, 'grad_norm': 1.2429441213607788, 'learning_rate': 3.877296438971888e-05, 'epoch': 0.74}
{'loss': 1.0032, 'grad_norm': 1.0376472473144531, 'learning_rate': 3.873051346942831e-05, 'epoch': 0.74}
{'loss': 1.1196, 'grad_norm': 1.2238123416900635, 'learning_rate': 3.868808021896254e-05, 'epoch': 0.74}
{'loss': 1.0039, 'grad_norm': 1.0412007570266724, 'learning_rate': 3.864566465055912e-05, 'epoch': 0.74}
{'loss': 1.1082, 'grad_norm': 1.3312387466430664, 'learning_rate': 3.860326677645051e-05, 'epoch': 0.74}
{'loss': 0.849, 'grad_norm': 1.3111618757247925, 'learning_rate': 3.856088660886402e-05, 'epoch': 0.74}
{'loss': 1.1206, 'grad_norm': 1.1686413288116455, 'learning_rate': 3.851852416002187e-05, 'epoch': 0.74}
{'loss': 0.8736, 'grad_norm': 1.1245925426483154, 'learning_rate': 3.847617944214127e-05, 'epoch': 0.74}
{'loss': 1.058, 'grad_norm': 1.566369891166687, 'learning_rate': 3.843385246743417e-05, 'epoch': 0.74}
{'loss': 1.4016, 'grad_norm': 1.286389708518982, 'learning_rate': 3.839154324810749e-05, 'epoch': 0.74}
{'loss': 0.9276, 'grad_norm': 1.2939027547836304, 'learning_rate': 3.8349251796363e-05, 'epoch': 0.74}
{'loss': 0.9938, 'grad_norm': 1.1836639642715454, 'learning_rate': 3.830697812439729e-05, 'epoch': 0.74}
{'loss': 1.0971, 'grad_norm': 1.4057021141052246, 'learning_rate': 3.826472224440202e-05, 'epoch': 0.74}
{'loss': 1.033, 'grad_norm': 1.3203983306884766, 'learning_rate': 3.8222484168563424e-05, 'epoch': 0.74}
{'loss': 1.0546, 'grad_norm': 1.2733458280563354, 'learning_rate': 3.818026390906291e-05, 'epoch': 0.74}
{'loss': 1.0053, 'grad_norm': 0.883047878742218, 'learning_rate': 3.813806147807645e-05, 'epoch': 0.74}
{'loss': 0.563, 'grad_norm': 1.4290456771850586, 'learning_rate': 3.809587688777514e-05, 'epoch': 0.74}
{'loss': 1.0511, 'grad_norm': 1.0860209465026855, 'learning_rate': 3.805371015032476e-05, 'epoch': 0.74}
{'loss': 0.9052, 'grad_norm': 1.0627204179763794, 'learning_rate': 3.8011561277885964e-05, 'epoch': 0.74}
{'loss': 1.1346, 'grad_norm': 1.1185309886932373, 'learning_rate': 3.7969430282614404e-05, 'epoch': 0.74}
{'loss': 0.8673, 'grad_norm': 1.0565824508666992, 'learning_rate': 3.792731717666029e-05, 'epoch': 0.74}
{'loss': 0.8355, 'grad_norm': 1.963822364807129, 'learning_rate': 3.788522197216897e-05, 'epoch': 0.74}
{'loss': 0.8146, 'grad_norm': 1.1589068174362183, 'learning_rate': 3.784314468128044e-05, 'epoch': 0.74}
{'loss': 1.0423, 'grad_norm': 1.0740944147109985, 'learning_rate': 3.7801085316129615e-05, 'epoch': 0.74}
{'loss': 1.001, 'grad_norm': 1.0519229173660278, 'learning_rate': 3.775904388884618e-05, 'epoch': 0.74}
{'loss': 0.7286, 'grad_norm': 1.098748803138733, 'learning_rate': 3.77170204115547e-05, 'epoch': 0.74}
{'loss': 1.092, 'grad_norm': 1.0889613628387451, 'learning_rate': 3.767501489637452e-05, 'epoch': 0.74}
{'loss': 0.7482, 'grad_norm': 1.0354405641555786, 'learning_rate': 3.763302735541987e-05, 'epoch': 0.74}
{'loss': 0.9215, 'grad_norm': 1.1429665088653564, 'learning_rate': 3.759105780079974e-05, 'epoch': 0.74}
{'loss': 0.9059, 'grad_norm': 1.1164374351501465, 'learning_rate': 3.754910624461795e-05, 'epoch': 0.74}
{'loss': 0.5851, 'grad_norm': 1.4972939491271973, 'learning_rate': 3.75071726989731e-05, 'epoch': 0.74}
{'loss': 0.7886, 'grad_norm': 0.9996748566627502, 'learning_rate': 3.7465257175958624e-05, 'epoch': 0.74}
{'loss': 0.9095, 'grad_norm': 0.9866017699241638, 'learning_rate': 3.742335968766283e-05, 'epoch': 0.74}
{'loss': 0.7551, 'grad_norm': 1.1725598573684692, 'learning_rate': 3.738148024616863e-05, 'epoch': 0.74}
{'loss': 0.5741, 'grad_norm': 1.2141152620315552, 'learning_rate': 3.733961886355398e-05, 'epoch': 0.74}
{'loss': 0.813, 'grad_norm': 0.9578588604927063, 'learning_rate': 3.729777555189139e-05, 'epoch': 0.74}
{'loss': 1.0888, 'grad_norm': 1.0514404773712158, 'learning_rate': 3.725595032324833e-05, 'epoch': 0.74}
{'loss': 0.8368, 'grad_norm': 1.0080901384353638, 'learning_rate': 3.7214143189687e-05, 'epoch': 0.74}
{'loss': 0.7677, 'grad_norm': 1.0474661588668823, 'learning_rate': 3.7172354163264324e-05, 'epoch': 0.74}
{'loss': 0.8915, 'grad_norm': 1.1932777166366577, 'learning_rate': 3.713058325603213e-05, 'epoch': 0.74}
{'loss': 0.8428, 'grad_norm': 1.0216697454452515, 'learning_rate': 3.7088830480036894e-05, 'epoch': 0.74}
{'loss': 0.9057, 'grad_norm': 1.1031816005706787, 'learning_rate': 3.7047095847319935e-05, 'epoch': 0.75}
{'loss': 0.9912, 'grad_norm': 1.2946840524673462, 'learning_rate': 3.7005379369917325e-05, 'epoch': 0.75}
{'loss': 0.9676, 'grad_norm': 1.1027337312698364, 'learning_rate': 3.696368105985988e-05, 'epoch': 0.75}
{'loss': 0.9936, 'grad_norm': 1.0036516189575195, 'learning_rate': 3.692200092917315e-05, 'epoch': 0.75}
{'loss': 1.0821, 'grad_norm': 1.1126335859298706, 'learning_rate': 3.6880338989877604e-05, 'epoch': 0.75}
{'loss': 0.98, 'grad_norm': 1.322643518447876, 'learning_rate': 3.6838695253988206e-05, 'epoch': 0.75}
{'loss': 0.8052, 'grad_norm': 1.1697572469711304, 'learning_rate': 3.679706973351491e-05, 'epoch': 0.75}
{'loss': 0.8744, 'grad_norm': 1.0030288696289062, 'learning_rate': 3.675546244046228e-05, 'epoch': 0.75}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0398, 'grad_norm': 1.1417781114578247, 'learning_rate': 3.6713873386829654e-05, 'epoch': 0.75}
{'loss': 1.2443, 'grad_norm': 1.1784206628799438, 'learning_rate': 3.667230258461113e-05, 'epoch': 0.75}
{'loss': 0.7585, 'grad_norm': 1.1705186367034912, 'learning_rate': 3.663075004579547e-05, 'epoch': 0.75}
{'loss': 0.6798, 'grad_norm': 1.3477822542190552, 'learning_rate': 3.6589215782366345e-05, 'epoch': 0.75}
{'loss': 1.0122, 'grad_norm': 1.7695770263671875, 'learning_rate': 3.65476998063019e-05, 'epoch': 0.75}
{'loss': 0.9395, 'grad_norm': 1.1717514991760254, 'learning_rate': 3.650620212957524e-05, 'epoch': 0.75}
{'loss': 0.9769, 'grad_norm': 1.185416579246521, 'learning_rate': 3.646472276415406e-05, 'epoch': 0.75}
{'loss': 0.8171, 'grad_norm': 1.267071008682251, 'learning_rate': 3.64232617220008e-05, 'epoch': 0.75}
{'loss': 0.9679, 'grad_norm': 1.3522204160690308, 'learning_rate': 3.638181901507265e-05, 'epoch': 0.75}
{'loss': 1.2264, 'grad_norm': 0.9378558397293091, 'learning_rate': 3.6340394655321465e-05, 'epoch': 0.75}
{'loss': 1.0858, 'grad_norm': 1.1068248748779297, 'learning_rate': 3.629898865469381e-05, 'epoch': 0.75}
{'loss': 0.9615, 'grad_norm': 1.5091845989227295, 'learning_rate': 3.6257601025131026e-05, 'epoch': 0.75}
{'loss': 0.9223, 'grad_norm': 1.0614105463027954, 'learning_rate': 3.6216231778569096e-05, 'epoch': 0.75}
{'loss': 0.8776, 'grad_norm': 1.2875500917434692, 'learning_rate': 3.6174880926938705e-05, 'epoch': 0.75}
{'loss': 0.7628, 'grad_norm': 1.2839902639389038, 'learning_rate': 3.6133548482165225e-05, 'epoch': 0.75}
{'loss': 0.9546, 'grad_norm': 1.4288420677185059, 'learning_rate': 3.6092234456168706e-05, 'epoch': 0.75}
{'loss': 1.1373, 'grad_norm': 1.163216233253479, 'learning_rate': 3.605093886086403e-05, 'epoch': 0.75}
{'loss': 1.0408, 'grad_norm': 1.2173662185668945, 'learning_rate': 3.60096617081605e-05, 'epoch': 0.75}
{'loss': 0.9227, 'grad_norm': 1.1311073303222656, 'learning_rate': 3.596840300996238e-05, 'epoch': 0.75}
{'loss': 0.9753, 'grad_norm': 1.2425079345703125, 'learning_rate': 3.5927162778168355e-05, 'epoch': 0.75}
{'loss': 1.0321, 'grad_norm': 1.3342498540878296, 'learning_rate': 3.5885941024671996e-05, 'epoch': 0.75}
{'loss': 0.8139, 'grad_norm': 0.940864622592926, 'learning_rate': 3.584473776136145e-05, 'epoch': 0.75}
{'loss': 0.6563, 'grad_norm': 0.9910532832145691, 'learning_rate': 3.5803553000119474e-05, 'epoch': 0.75}
{'loss': 0.8703, 'grad_norm': 1.2572839260101318, 'learning_rate': 3.576238675282364e-05, 'epoch': 0.75}
{'loss': 1.0673, 'grad_norm': 1.3207242488861084, 'learning_rate': 3.5721239031346066e-05, 'epoch': 0.75}
{'loss': 0.7223, 'grad_norm': 1.0749309062957764, 'learning_rate': 3.568010984755354e-05, 'epoch': 0.75}
{'loss': 0.8724, 'grad_norm': 1.0347867012023926, 'learning_rate': 3.563899921330755e-05, 'epoch': 0.75}
{'loss': 1.0218, 'grad_norm': 1.1575424671173096, 'learning_rate': 3.559790714046417e-05, 'epoch': 0.75}
{'loss': 0.6945, 'grad_norm': 1.1216944456100464, 'learning_rate': 3.555683364087414e-05, 'epoch': 0.75}
{'loss': 0.761, 'grad_norm': 0.9954620003700256, 'learning_rate': 3.5515778726382966e-05, 'epoch': 0.75}
{'loss': 1.5236, 'grad_norm': 2.1873722076416016, 'learning_rate': 3.5474742408830544e-05, 'epoch': 0.75}
{'loss': 0.8596, 'grad_norm': 1.0911403894424438, 'learning_rate': 3.543372470005164e-05, 'epoch': 0.75}
{'loss': 0.9289, 'grad_norm': 1.0810635089874268, 'learning_rate': 3.539272561187555e-05, 'epoch': 0.75}
{'loss': 0.9652, 'grad_norm': 1.3910267353057861, 'learning_rate': 3.5351745156126216e-05, 'epoch': 0.75}
{'loss': 1.0072, 'grad_norm': 1.1135334968566895, 'learning_rate': 3.531078334462218e-05, 'epoch': 0.75}
{'loss': 0.8724, 'grad_norm': 1.1116372346878052, 'learning_rate': 3.526984018917662e-05, 'epoch': 0.75}
{'loss': 0.9044, 'grad_norm': 0.9375227093696594, 'learning_rate': 3.522891570159743e-05, 'epoch': 0.75}
{'loss': 0.8747, 'grad_norm': 1.1852604150772095, 'learning_rate': 3.518800989368691e-05, 'epoch': 0.75}
{'loss': 0.7439, 'grad_norm': 1.0586942434310913, 'learning_rate': 3.5147122777242204e-05, 'epoch': 0.75}
{'loss': 1.0344, 'grad_norm': 1.479283332824707, 'learning_rate': 3.510625436405491e-05, 'epoch': 0.75}
{'loss': 0.8732, 'grad_norm': 1.001090407371521, 'learning_rate': 3.506540466591129e-05, 'epoch': 0.75}
{'loss': 1.2959, 'grad_norm': 1.2036384344100952, 'learning_rate': 3.5024573694592214e-05, 'epoch': 0.75}
{'loss': 1.0078, 'grad_norm': 0.9395120739936829, 'learning_rate': 3.4983761461873075e-05, 'epoch': 0.75}
{'loss': 0.9641, 'grad_norm': 1.0771794319152832, 'learning_rate': 3.494296797952401e-05, 'epoch': 0.75}
{'loss': 0.8701, 'grad_norm': 0.9488399624824524, 'learning_rate': 3.490219325930962e-05, 'epoch': 0.75}
{'loss': 1.1603, 'grad_norm': 2.434741735458374, 'learning_rate': 3.486143731298916e-05, 'epoch': 0.75}
{'loss': 0.9146, 'grad_norm': 1.0725855827331543, 'learning_rate': 3.482070015231642e-05, 'epoch': 0.75}
{'loss': 1.0577, 'grad_norm': 1.129770278930664, 'learning_rate': 3.477998178903982e-05, 'epoch': 0.75}
{'loss': 0.8618, 'grad_norm': 1.0912705659866333, 'learning_rate': 3.4739282234902306e-05, 'epoch': 0.75}
{'loss': 0.7575, 'grad_norm': 0.994801938533783, 'learning_rate': 3.469860150164152e-05, 'epoch': 0.75}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.3015, 'grad_norm': 1.1242804527282715, 'learning_rate': 3.465793960098945e-05, 'epoch': 0.75}
{'loss': 0.9423, 'grad_norm': 1.1717736721038818, 'learning_rate': 3.461729654467293e-05, 'epoch': 0.75}
{'loss': 0.9511, 'grad_norm': 1.3390352725982666, 'learning_rate': 3.4576672344413145e-05, 'epoch': 0.75}
{'loss': 1.3088, 'grad_norm': 1.118086338043213, 'learning_rate': 3.4536067011925945e-05, 'epoch': 0.75}
{'loss': 0.7982, 'grad_norm': 1.4388574361801147, 'learning_rate': 3.449548055892171e-05, 'epoch': 0.75}
{'loss': 0.7657, 'grad_norm': 1.183617353439331, 'learning_rate': 3.445491299710534e-05, 'epoch': 0.75}
{'loss': 0.9193, 'grad_norm': 1.4835320711135864, 'learning_rate': 3.441436433817641e-05, 'epoch': 0.75}
{'loss': 0.7606, 'grad_norm': 1.3561935424804688, 'learning_rate': 3.4373834593828845e-05, 'epoch': 0.76}
{'loss': 1.0002, 'grad_norm': 1.084836721420288, 'learning_rate': 3.4333323775751325e-05, 'epoch': 0.76}
{'loss': 0.9872, 'grad_norm': 1.1341185569763184, 'learning_rate': 3.429283189562694e-05, 'epoch': 0.76}
{'loss': 0.7044, 'grad_norm': 1.2653875350952148, 'learning_rate': 3.4252358965133344e-05, 'epoch': 0.76}
{'loss': 1.0162, 'grad_norm': 1.2902202606201172, 'learning_rate': 3.421190499594271e-05, 'epoch': 0.76}
{'loss': 0.9634, 'grad_norm': 1.1621134281158447, 'learning_rate': 3.417146999972187e-05, 'epoch': 0.76}
{'loss': 0.8216, 'grad_norm': 0.9396746754646301, 'learning_rate': 3.413105398813195e-05, 'epoch': 0.76}
{'loss': 0.7308, 'grad_norm': 1.0037078857421875, 'learning_rate': 3.409065697282883e-05, 'epoch': 0.76}
{'loss': 0.8213, 'grad_norm': 1.4913543462753296, 'learning_rate': 3.4050278965462764e-05, 'epoch': 0.76}
{'loss': 0.9939, 'grad_norm': 1.0931214094161987, 'learning_rate': 3.400991997767857e-05, 'epoch': 0.76}
{'loss': 0.9074, 'grad_norm': 1.0658890008926392, 'learning_rate': 3.396958002111568e-05, 'epoch': 0.76}
{'loss': 0.7644, 'grad_norm': 1.2893197536468506, 'learning_rate': 3.3929259107407784e-05, 'epoch': 0.76}
{'loss': 0.8123, 'grad_norm': 1.1505399942398071, 'learning_rate': 3.388895724818341e-05, 'epoch': 0.76}
{'loss': 1.0835, 'grad_norm': 1.2764232158660889, 'learning_rate': 3.3848674455065255e-05, 'epoch': 0.76}
{'loss': 0.9624, 'grad_norm': 1.1344401836395264, 'learning_rate': 3.38084107396708e-05, 'epoch': 0.76}
{'loss': 1.3774, 'grad_norm': 1.133087158203125, 'learning_rate': 3.3768166113611865e-05, 'epoch': 0.76}
{'loss': 0.7377, 'grad_norm': 1.0219019651412964, 'learning_rate': 3.3727940588494824e-05, 'epoch': 0.76}
{'loss': 1.0575, 'grad_norm': 1.2611366510391235, 'learning_rate': 3.36877341759205e-05, 'epoch': 0.76}
{'loss': 1.2707, 'grad_norm': 1.0449546575546265, 'learning_rate': 3.364754688748422e-05, 'epoch': 0.76}
{'loss': 1.3925, 'grad_norm': 1.0181761980056763, 'learning_rate': 3.360737873477584e-05, 'epoch': 0.76}
{'loss': 1.2587, 'grad_norm': 1.084628939628601, 'learning_rate': 3.3567229729379656e-05, 'epoch': 0.76}
{'loss': 1.2558, 'grad_norm': 1.0589429140090942, 'learning_rate': 3.352709988287444e-05, 'epoch': 0.76}
{'loss': 0.8015, 'grad_norm': 0.9600821137428284, 'learning_rate': 3.348698920683343e-05, 'epoch': 0.76}
{'loss': 1.0581, 'grad_norm': 0.9484320878982544, 'learning_rate': 3.3446897712824377e-05, 'epoch': 0.76}
{'loss': 1.1328, 'grad_norm': 1.261976957321167, 'learning_rate': 3.340682541240943e-05, 'epoch': 0.76}
{'loss': 0.9213, 'grad_norm': 1.3485560417175293, 'learning_rate': 3.336677231714535e-05, 'epoch': 0.76}
{'loss': 0.7527, 'grad_norm': 1.2764501571655273, 'learning_rate': 3.3326738438583114e-05, 'epoch': 0.76}
{'loss': 0.8715, 'grad_norm': 1.3500007390975952, 'learning_rate': 3.3286723788268414e-05, 'epoch': 0.76}
{'loss': 1.1189, 'grad_norm': 1.0376805067062378, 'learning_rate': 3.3246728377741246e-05, 'epoch': 0.76}
{'loss': 0.7217, 'grad_norm': 1.1085788011550903, 'learning_rate': 3.320675221853608e-05, 'epoch': 0.76}
{'loss': 1.1179, 'grad_norm': 1.1773067712783813, 'learning_rate': 3.3166795322181865e-05, 'epoch': 0.76}
{'loss': 0.8583, 'grad_norm': 0.9314283728599548, 'learning_rate': 3.312685770020193e-05, 'epoch': 0.76}
{'loss': 1.1915, 'grad_norm': 1.3644770383834839, 'learning_rate': 3.308693936411421e-05, 'epoch': 0.76}
{'loss': 0.9178, 'grad_norm': 0.9968345761299133, 'learning_rate': 3.304704032543081e-05, 'epoch': 0.76}
{'loss': 0.7312, 'grad_norm': 1.0818170309066772, 'learning_rate': 3.300716059565853e-05, 'epoch': 0.76}
{'loss': 0.8699, 'grad_norm': 1.1250839233398438, 'learning_rate': 3.296730018629846e-05, 'epoch': 0.76}
{'loss': 0.7396, 'grad_norm': 1.0838953256607056, 'learning_rate': 3.292745910884614e-05, 'epoch': 0.76}
{'loss': 1.1222, 'grad_norm': 0.9704375267028809, 'learning_rate': 3.288763737479155e-05, 'epoch': 0.76}
{'loss': 0.8816, 'grad_norm': 1.2853986024856567, 'learning_rate': 3.284783499561907e-05, 'epoch': 0.76}
{'loss': 0.7205, 'grad_norm': 0.9075764417648315, 'learning_rate': 3.280805198280754e-05, 'epoch': 0.76}
{'loss': 0.7226, 'grad_norm': 1.0961129665374756, 'learning_rate': 3.27682883478302e-05, 'epoch': 0.76}
{'loss': 0.6819, 'grad_norm': 1.1109777688980103, 'learning_rate': 3.272854410215467e-05, 'epoch': 0.76}
{'loss': 1.1475, 'grad_norm': 1.0044045448303223, 'learning_rate': 3.268881925724297e-05, 'epoch': 0.76}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9945, 'grad_norm': 1.149493932723999, 'learning_rate': 3.264911382455164e-05, 'epoch': 0.76}
{'loss': 1.1584, 'grad_norm': 1.309234857559204, 'learning_rate': 3.2609427815531426e-05, 'epoch': 0.76}
{'loss': 0.7979, 'grad_norm': 0.8791096210479736, 'learning_rate': 3.2569761241627696e-05, 'epoch': 0.76}
{'loss': 1.0125, 'grad_norm': 1.0800760984420776, 'learning_rate': 3.253011411427998e-05, 'epoch': 0.76}
{'loss': 0.8472, 'grad_norm': 1.2724138498306274, 'learning_rate': 3.24904864449224e-05, 'epoch': 0.76}
{'loss': 1.0406, 'grad_norm': 0.9910820126533508, 'learning_rate': 3.245087824498336e-05, 'epoch': 0.76}
{'loss': 1.0816, 'grad_norm': 1.4281623363494873, 'learning_rate': 3.241128952588569e-05, 'epoch': 0.76}
{'loss': 0.8913, 'grad_norm': 1.1070998907089233, 'learning_rate': 3.237172029904657e-05, 'epoch': 0.76}
{'loss': 0.9549, 'grad_norm': 1.0457450151443481, 'learning_rate': 3.233217057587754e-05, 'epoch': 0.76}
{'loss': 1.1335, 'grad_norm': 1.1806941032409668, 'learning_rate': 3.229264036778462e-05, 'epoch': 0.76}
{'loss': 0.8585, 'grad_norm': 1.1159329414367676, 'learning_rate': 3.2253129686168106e-05, 'epoch': 0.76}
{'loss': 0.8521, 'grad_norm': 1.3177088499069214, 'learning_rate': 3.221363854242268e-05, 'epoch': 0.76}
{'loss': 0.9477, 'grad_norm': 1.0599461793899536, 'learning_rate': 3.217416694793739e-05, 'epoch': 0.76}
{'loss': 0.5984, 'grad_norm': 1.115297794342041, 'learning_rate': 3.213471491409568e-05, 'epoch': 0.76}
{'loss': 0.8473, 'grad_norm': 1.136354684829712, 'learning_rate': 3.2095282452275256e-05, 'epoch': 0.76}
{'loss': 1.2934, 'grad_norm': 1.1531108617782593, 'learning_rate': 3.205586957384838e-05, 'epoch': 0.76}
{'loss': 1.1005, 'grad_norm': 1.1196174621582031, 'learning_rate': 3.201647629018139e-05, 'epoch': 0.76}
{'loss': 1.0181, 'grad_norm': 1.0936790704727173, 'learning_rate': 3.1977102612635215e-05, 'epoch': 0.76}
{'loss': 0.9306, 'grad_norm': 1.1060324907302856, 'learning_rate': 3.1937748552565015e-05, 'epoch': 0.76}
{'loss': 1.1501, 'grad_norm': 1.4106948375701904, 'learning_rate': 3.1898414121320276e-05, 'epoch': 0.76}
{'loss': 0.8662, 'grad_norm': 1.4070584774017334, 'learning_rate': 3.185909933024494e-05, 'epoch': 0.76}
{'loss': 1.0205, 'grad_norm': 0.9963929057121277, 'learning_rate': 3.1819804190677085e-05, 'epoch': 0.76}
{'loss': 0.8391, 'grad_norm': 1.3191144466400146, 'learning_rate': 3.1780528713949375e-05, 'epoch': 0.77}
{'loss': 0.9647, 'grad_norm': 1.384582757949829, 'learning_rate': 3.174127291138853e-05, 'epoch': 0.77}
{'loss': 0.8232, 'grad_norm': 1.082323431968689, 'learning_rate': 3.170203679431584e-05, 'epoch': 0.77}
{'loss': 0.8683, 'grad_norm': 1.199823021888733, 'learning_rate': 3.1662820374046774e-05, 'epoch': 0.77}
{'loss': 1.0945, 'grad_norm': 1.3756377696990967, 'learning_rate': 3.162362366189116e-05, 'epoch': 0.77}
{'loss': 0.8445, 'grad_norm': 1.0207629203796387, 'learning_rate': 3.1584446669153136e-05, 'epoch': 0.77}
{'loss': 0.8275, 'grad_norm': 1.3552287817001343, 'learning_rate': 3.154528940713113e-05, 'epoch': 0.77}
{'loss': 1.1909, 'grad_norm': 1.0550081729888916, 'learning_rate': 3.1506151887117974e-05, 'epoch': 0.77}
{'loss': 1.0374, 'grad_norm': 1.037949800491333, 'learning_rate': 3.146703412040071e-05, 'epoch': 0.77}
{'loss': 0.7963, 'grad_norm': 1.1861222982406616, 'learning_rate': 3.142793611826069e-05, 'epoch': 0.77}
{'loss': 0.9394, 'grad_norm': 1.0550698041915894, 'learning_rate': 3.13888578919736e-05, 'epoch': 0.77}
{'loss': 0.9054, 'grad_norm': 1.2670609951019287, 'learning_rate': 3.134979945280948e-05, 'epoch': 0.77}
{'loss': 0.9442, 'grad_norm': 1.3130897283554077, 'learning_rate': 3.131076081203247e-05, 'epoch': 0.77}
{'loss': 0.7987, 'grad_norm': 0.9876207113265991, 'learning_rate': 3.127174198090126e-05, 'epoch': 0.77}
{'loss': 1.1142, 'grad_norm': 1.1315281391143799, 'learning_rate': 3.123274297066856e-05, 'epoch': 0.77}
{'loss': 0.9788, 'grad_norm': 1.2188769578933716, 'learning_rate': 3.11937637925816e-05, 'epoch': 0.77}
{'loss': 1.0077, 'grad_norm': 0.9829255938529968, 'learning_rate': 3.115480445788174e-05, 'epoch': 0.77}
{'loss': 1.041, 'grad_norm': 1.0402593612670898, 'learning_rate': 3.1115864977804676e-05, 'epoch': 0.77}
{'loss': 0.9025, 'grad_norm': 1.0660873651504517, 'learning_rate': 3.1076945363580365e-05, 'epoch': 0.77}
{'loss': 0.7003, 'grad_norm': 1.127841591835022, 'learning_rate': 3.103804562643302e-05, 'epoch': 0.77}
{'loss': 0.7956, 'grad_norm': 1.0516185760498047, 'learning_rate': 3.0999165777581176e-05, 'epoch': 0.77}
{'loss': 1.1723, 'grad_norm': 1.2681652307510376, 'learning_rate': 3.096030582823757e-05, 'epoch': 0.77}
{'loss': 1.0633, 'grad_norm': 0.8889829516410828, 'learning_rate': 3.0921465789609226e-05, 'epoch': 0.77}
{'loss': 1.1508, 'grad_norm': 1.1651787757873535, 'learning_rate': 3.0882645672897425e-05, 'epoch': 0.77}
{'loss': 0.751, 'grad_norm': 0.9192309379577637, 'learning_rate': 3.08438454892977e-05, 'epoch': 0.77}
{'loss': 1.0516, 'grad_norm': 1.39454984664917, 'learning_rate': 3.080506524999981e-05, 'epoch': 0.77}
{'loss': 0.7854, 'grad_norm': 1.1198439598083496, 'learning_rate': 3.076630496618788e-05, 'epoch': 0.77}
{'loss': 1.0638, 'grad_norm': 1.0738290548324585, 'learning_rate': 3.072756464904006e-05, 'epoch': 0.77}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9068, 'grad_norm': 1.3252702951431274, 'learning_rate': 3.068884430972897e-05, 'epoch': 0.77}
{'loss': 0.9146, 'grad_norm': 1.098677635192871, 'learning_rate': 3.065014395942134e-05, 'epoch': 0.77}
{'loss': 0.9176, 'grad_norm': 1.077390432357788, 'learning_rate': 3.061146360927813e-05, 'epoch': 0.77}
{'loss': 0.8577, 'grad_norm': 1.118481993675232, 'learning_rate': 3.057280327045467e-05, 'epoch': 0.77}
{'loss': 0.6362, 'grad_norm': 0.9907476902008057, 'learning_rate': 3.053416295410026e-05, 'epoch': 0.77}
{'loss': 0.9219, 'grad_norm': 0.9395198822021484, 'learning_rate': 3.0495542671358746e-05, 'epoch': 0.77}
{'loss': 0.532, 'grad_norm': 1.2987241744995117, 'learning_rate': 3.0456942433367875e-05, 'epoch': 0.77}
{'loss': 0.9282, 'grad_norm': 0.9991110563278198, 'learning_rate': 3.0418362251259892e-05, 'epoch': 0.77}
{'loss': 0.9696, 'grad_norm': 1.2280858755111694, 'learning_rate': 3.0379802136161074e-05, 'epoch': 0.77}
{'loss': 1.099, 'grad_norm': 1.1017560958862305, 'learning_rate': 3.0341262099191993e-05, 'epoch': 0.77}
{'loss': 0.8629, 'grad_norm': 0.9914207458496094, 'learning_rate': 3.03027421514674e-05, 'epoch': 0.77}
{'loss': 1.0489, 'grad_norm': 1.117848515510559, 'learning_rate': 3.0264242304096235e-05, 'epoch': 0.77}
{'loss': 1.2033, 'grad_norm': 1.1248950958251953, 'learning_rate': 3.0225762568181736e-05, 'epoch': 0.77}
{'loss': 0.9377, 'grad_norm': 1.1315972805023193, 'learning_rate': 3.018730295482124e-05, 'epoch': 0.77}
{'loss': 0.9272, 'grad_norm': 1.1656793355941772, 'learning_rate': 3.0148863475106314e-05, 'epoch': 0.77}
{'loss': 0.7507, 'grad_norm': 0.9954193234443665, 'learning_rate': 3.0110444140122708e-05, 'epoch': 0.77}
{'loss': 0.7323, 'grad_norm': 1.361474871635437, 'learning_rate': 3.0072044960950384e-05, 'epoch': 0.77}
{'loss': 1.0343, 'grad_norm': 1.2952370643615723, 'learning_rate': 3.0033665948663448e-05, 'epoch': 0.77}
{'loss': 1.0068, 'grad_norm': 1.0211964845657349, 'learning_rate': 2.9995307114330318e-05, 'epoch': 0.77}
{'loss': 1.0219, 'grad_norm': 0.9422801733016968, 'learning_rate': 2.9956968469013368e-05, 'epoch': 0.77}
{'loss': 1.0625, 'grad_norm': 1.0995882749557495, 'learning_rate': 2.991865002376937e-05, 'epoch': 0.77}
{'loss': 0.8107, 'grad_norm': 1.2847709655761719, 'learning_rate': 2.9880351789649154e-05, 'epoch': 0.77}
{'loss': 0.9358, 'grad_norm': 1.1104676723480225, 'learning_rate': 2.984207377769771e-05, 'epoch': 0.77}
{'loss': 1.0811, 'grad_norm': 1.1196175813674927, 'learning_rate': 2.9803815998954332e-05, 'epoch': 0.77}
{'loss': 1.0257, 'grad_norm': 1.005244493484497, 'learning_rate': 2.976557846445225e-05, 'epoch': 0.77}
{'loss': 1.0777, 'grad_norm': 1.182486891746521, 'learning_rate': 2.972736118521908e-05, 'epoch': 0.77}
{'loss': 1.0045, 'grad_norm': 1.0143297910690308, 'learning_rate': 2.968916417227646e-05, 'epoch': 0.77}
{'loss': 1.1607, 'grad_norm': 1.370512843132019, 'learning_rate': 2.9650987436640242e-05, 'epoch': 0.77}
{'loss': 0.7144, 'grad_norm': 0.9965731501579285, 'learning_rate': 2.9612830989320406e-05, 'epoch': 0.77}
{'loss': 0.9855, 'grad_norm': 0.9587777256965637, 'learning_rate': 2.9574694841321082e-05, 'epoch': 0.77}
{'loss': 0.9414, 'grad_norm': 1.172918438911438, 'learning_rate': 2.953657900364053e-05, 'epoch': 0.77}
{'loss': 0.6725, 'grad_norm': 0.9989815950393677, 'learning_rate': 2.9498483487271223e-05, 'epoch': 0.77}
{'loss': 1.0945, 'grad_norm': 0.7925596833229065, 'learning_rate': 2.9460408303199694e-05, 'epoch': 0.77}
{'loss': 0.6726, 'grad_norm': 1.2884196043014526, 'learning_rate': 2.942235346240666e-05, 'epoch': 0.77}
{'loss': 0.942, 'grad_norm': 1.0847963094711304, 'learning_rate': 2.938431897586694e-05, 'epoch': 0.77}
{'loss': 1.2664, 'grad_norm': 1.0092639923095703, 'learning_rate': 2.934630485454948e-05, 'epoch': 0.77}
{'loss': 0.9713, 'grad_norm': 1.0995441675186157, 'learning_rate': 2.930831110941743e-05, 'epoch': 0.77}
{'loss': 1.0061, 'grad_norm': 1.1470493078231812, 'learning_rate': 2.9270337751427913e-05, 'epoch': 0.78}
{'loss': 1.0466, 'grad_norm': 1.183637261390686, 'learning_rate': 2.9232384791532375e-05, 'epoch': 0.78}
{'loss': 0.7087, 'grad_norm': 1.0852965116500854, 'learning_rate': 2.919445224067614e-05, 'epoch': 0.78}
{'loss': 0.8551, 'grad_norm': 1.2070868015289307, 'learning_rate': 2.915654010979887e-05, 'epoch': 0.78}
{'loss': 0.7622, 'grad_norm': 0.9097945690155029, 'learning_rate': 2.9118648409834205e-05, 'epoch': 0.78}
{'loss': 1.0775, 'grad_norm': 1.1712590456008911, 'learning_rate': 2.9080777151709925e-05, 'epoch': 0.78}
{'loss': 1.1673, 'grad_norm': 1.2784103155136108, 'learning_rate': 2.904292634634793e-05, 'epoch': 0.78}
{'loss': 0.8056, 'grad_norm': 1.3220657110214233, 'learning_rate': 2.9005096004664177e-05, 'epoch': 0.78}
{'loss': 0.8982, 'grad_norm': 1.7070982456207275, 'learning_rate': 2.8967286137568804e-05, 'epoch': 0.78}
{'loss': 1.0465, 'grad_norm': 0.934569776058197, 'learning_rate': 2.892949675596599e-05, 'epoch': 0.78}
{'loss': 1.0057, 'grad_norm': 1.0141501426696777, 'learning_rate': 2.8891727870753983e-05, 'epoch': 0.78}
{'loss': 0.925, 'grad_norm': 1.0385230779647827, 'learning_rate': 2.8853979492825156e-05, 'epoch': 0.78}
{'loss': 1.0526, 'grad_norm': 0.9319545030593872, 'learning_rate': 2.881625163306596e-05, 'epoch': 0.78}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.934, 'grad_norm': 1.2159961462020874, 'learning_rate': 2.8778544302356904e-05, 'epoch': 0.78}
{'loss': 0.8665, 'grad_norm': 1.0176430940628052, 'learning_rate': 2.8740857511572684e-05, 'epoch': 0.78}
{'loss': 1.0049, 'grad_norm': 1.2513203620910645, 'learning_rate': 2.8703191271581875e-05, 'epoch': 0.78}
{'loss': 0.8742, 'grad_norm': 1.4094855785369873, 'learning_rate': 2.8665545593247312e-05, 'epoch': 0.78}
{'loss': 1.027, 'grad_norm': 0.9591007828712463, 'learning_rate': 2.862792048742583e-05, 'epoch': 0.78}
{'loss': 0.8271, 'grad_norm': 1.1659921407699585, 'learning_rate': 2.8590315964968262e-05, 'epoch': 0.78}
{'loss': 0.8448, 'grad_norm': 1.1565115451812744, 'learning_rate': 2.8552732036719687e-05, 'epoch': 0.78}
{'loss': 1.2229, 'grad_norm': 0.9305652976036072, 'learning_rate': 2.8515168713518993e-05, 'epoch': 0.78}
{'loss': 0.9322, 'grad_norm': 1.2646530866622925, 'learning_rate': 2.8477626006199364e-05, 'epoch': 0.78}
{'loss': 1.1519, 'grad_norm': 1.0509452819824219, 'learning_rate': 2.84401039255879e-05, 'epoch': 0.78}
{'loss': 0.9156, 'grad_norm': 1.0114773511886597, 'learning_rate': 2.840260248250578e-05, 'epoch': 0.78}
{'loss': 0.8104, 'grad_norm': 0.8806667327880859, 'learning_rate': 2.836512168776826e-05, 'epoch': 0.78}
{'loss': 0.9048, 'grad_norm': 1.337633728981018, 'learning_rate': 2.8327661552184603e-05, 'epoch': 0.78}
{'loss': 0.8697, 'grad_norm': 0.8086103200912476, 'learning_rate': 2.8290222086558106e-05, 'epoch': 0.78}
{'loss': 0.9834, 'grad_norm': 1.4443260431289673, 'learning_rate': 2.8252803301686193e-05, 'epoch': 0.78}
{'loss': 0.7125, 'grad_norm': 1.2992490530014038, 'learning_rate': 2.8215405208360233e-05, 'epoch': 0.78}
{'loss': 1.1304, 'grad_norm': 0.9400638937950134, 'learning_rate': 2.817802781736565e-05, 'epoch': 0.78}
{'loss': 0.8058, 'grad_norm': 1.2048022747039795, 'learning_rate': 2.8140671139481912e-05, 'epoch': 0.78}
{'loss': 0.8442, 'grad_norm': 1.5011521577835083, 'learning_rate': 2.810333518548246e-05, 'epoch': 0.78}
{'loss': 1.0314, 'grad_norm': 1.0373576879501343, 'learning_rate': 2.8066019966134904e-05, 'epoch': 0.78}
{'loss': 0.9574, 'grad_norm': 1.4451638460159302, 'learning_rate': 2.8028725492200657e-05, 'epoch': 0.78}
{'loss': 0.9751, 'grad_norm': 1.0792062282562256, 'learning_rate': 2.7991451774435384e-05, 'epoch': 0.78}
{'loss': 1.5199, 'grad_norm': 1.1011608839035034, 'learning_rate': 2.7954198823588517e-05, 'epoch': 0.78}
{'loss': 0.9979, 'grad_norm': 1.0544377565383911, 'learning_rate': 2.791696665040373e-05, 'epoch': 0.78}
{'loss': 0.9785, 'grad_norm': 0.8797280192375183, 'learning_rate': 2.7879755265618555e-05, 'epoch': 0.78}
{'loss': 0.9023, 'grad_norm': 1.1212879419326782, 'learning_rate': 2.7842564679964567e-05, 'epoch': 0.78}
{'loss': 0.6383, 'grad_norm': 1.0772240161895752, 'learning_rate': 2.7805394904167437e-05, 'epoch': 0.78}
{'loss': 0.8427, 'grad_norm': 0.8802421689033508, 'learning_rate': 2.7768245948946612e-05, 'epoch': 0.78}
{'loss': 0.7412, 'grad_norm': 1.0922901630401611, 'learning_rate': 2.7731117825015772e-05, 'epoch': 0.78}
{'loss': 1.0739, 'grad_norm': 1.0964967012405396, 'learning_rate': 2.7694010543082472e-05, 'epoch': 0.78}
{'loss': 0.7524, 'grad_norm': 1.0749688148498535, 'learning_rate': 2.765692411384826e-05, 'epoch': 0.78}
{'loss': 0.6875, 'grad_norm': 1.0900179147720337, 'learning_rate': 2.761985854800868e-05, 'epoch': 0.78}
{'loss': 0.7331, 'grad_norm': 1.0449130535125732, 'learning_rate': 2.7582813856253275e-05, 'epoch': 0.78}
{'loss': 1.0438, 'grad_norm': 1.6152760982513428, 'learning_rate': 2.754579004926551e-05, 'epoch': 0.78}
{'loss': 1.0166, 'grad_norm': 1.0345629453659058, 'learning_rate': 2.7508787137722947e-05, 'epoch': 0.78}
{'loss': 0.7737, 'grad_norm': 1.0321491956710815, 'learning_rate': 2.7471805132297013e-05, 'epoch': 0.78}
{'loss': 1.0326, 'grad_norm': 1.2549101114273071, 'learning_rate': 2.743484404365314e-05, 'epoch': 0.78}
{'loss': 1.0422, 'grad_norm': 1.072062611579895, 'learning_rate': 2.7397903882450728e-05, 'epoch': 0.78}
{'loss': 0.5877, 'grad_norm': 0.9174677133560181, 'learning_rate': 2.73609846593431e-05, 'epoch': 0.78}
{'loss': 0.9176, 'grad_norm': 1.7922065258026123, 'learning_rate': 2.7324086384977698e-05, 'epoch': 0.78}
{'loss': 1.1223, 'grad_norm': 1.2759881019592285, 'learning_rate': 2.728720906999567e-05, 'epoch': 0.78}
{'loss': 1.0803, 'grad_norm': 1.1467056274414062, 'learning_rate': 2.725035272503238e-05, 'epoch': 0.78}
{'loss': 0.9516, 'grad_norm': 1.0827910900115967, 'learning_rate': 2.7213517360716888e-05, 'epoch': 0.78}
{'loss': 0.9202, 'grad_norm': 1.8449920415878296, 'learning_rate': 2.7176702987672443e-05, 'epoch': 0.78}
{'loss': 0.8953, 'grad_norm': 1.3317545652389526, 'learning_rate': 2.713990961651609e-05, 'epoch': 0.78}
{'loss': 0.918, 'grad_norm': 1.212971806526184, 'learning_rate': 2.7103137257858868e-05, 'epoch': 0.78}
{'loss': 1.0966, 'grad_norm': 1.073983073234558, 'learning_rate': 2.7066385922305714e-05, 'epoch': 0.78}
{'loss': 1.1197, 'grad_norm': 1.2919883728027344, 'learning_rate': 2.7029655620455608e-05, 'epoch': 0.78}
{'loss': 0.8332, 'grad_norm': 1.4746657609939575, 'learning_rate': 2.699294636290134e-05, 'epoch': 0.78}
{'loss': 1.0126, 'grad_norm': 1.1959118843078613, 'learning_rate': 2.6956258160229695e-05, 'epoch': 0.78}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1688, 'grad_norm': 1.0097838640213013, 'learning_rate': 2.6919591023021384e-05, 'epoch': 0.78}
{'loss': 1.0632, 'grad_norm': 1.2242343425750732, 'learning_rate': 2.6882944961850987e-05, 'epoch': 0.78}
{'loss': 1.1075, 'grad_norm': 1.0253257751464844, 'learning_rate': 2.684631998728715e-05, 'epoch': 0.79}
{'loss': 0.8673, 'grad_norm': 1.1429246664047241, 'learning_rate': 2.6809716109892214e-05, 'epoch': 0.79}
{'loss': 0.8669, 'grad_norm': 1.4898639917373657, 'learning_rate': 2.677313334022268e-05, 'epoch': 0.79}
{'loss': 1.1975, 'grad_norm': 0.9650387763977051, 'learning_rate': 2.6736571688828726e-05, 'epoch': 0.79}
{'loss': 0.8881, 'grad_norm': 1.1687003374099731, 'learning_rate': 2.6700031166254658e-05, 'epoch': 0.79}
{'loss': 0.8155, 'grad_norm': 1.217747688293457, 'learning_rate': 2.666351178303853e-05, 'epoch': 0.79}
{'loss': 1.0849, 'grad_norm': 1.1644134521484375, 'learning_rate': 2.6627013549712355e-05, 'epoch': 0.79}
{'loss': 1.0056, 'grad_norm': 1.4793576002120972, 'learning_rate': 2.6590536476802118e-05, 'epoch': 0.79}
{'loss': 0.9736, 'grad_norm': 1.0729949474334717, 'learning_rate': 2.655408057482751e-05, 'epoch': 0.79}
{'loss': 0.7433, 'grad_norm': 1.0415719747543335, 'learning_rate': 2.651764585430234e-05, 'epoch': 0.79}
{'loss': 1.0361, 'grad_norm': 1.115120530128479, 'learning_rate': 2.6481232325734174e-05, 'epoch': 0.79}
{'loss': 0.9454, 'grad_norm': 0.9827365875244141, 'learning_rate': 2.6444839999624494e-05, 'epoch': 0.79}
{'loss': 0.9264, 'grad_norm': 1.0884380340576172, 'learning_rate': 2.6408468886468672e-05, 'epoch': 0.79}
{'loss': 1.351, 'grad_norm': 0.9242231845855713, 'learning_rate': 2.637211899675596e-05, 'epoch': 0.79}
{'loss': 0.616, 'grad_norm': 1.1598310470581055, 'learning_rate': 2.6335790340969456e-05, 'epoch': 0.79}
{'loss': 0.8456, 'grad_norm': 1.2955833673477173, 'learning_rate': 2.629948292958625e-05, 'epoch': 0.79}
{'loss': 1.009, 'grad_norm': 1.035661220550537, 'learning_rate': 2.626319677307718e-05, 'epoch': 0.79}
{'loss': 1.0011, 'grad_norm': 1.235561728477478, 'learning_rate': 2.622693188190699e-05, 'epoch': 0.79}
{'loss': 0.8501, 'grad_norm': 0.9845936894416809, 'learning_rate': 2.6190688266534313e-05, 'epoch': 0.79}
{'loss': 0.8613, 'grad_norm': 1.0173717737197876, 'learning_rate': 2.615446593741161e-05, 'epoch': 0.79}
{'loss': 1.1769, 'grad_norm': 1.0700210332870483, 'learning_rate': 2.61182649049853e-05, 'epoch': 0.79}
{'loss': 0.9135, 'grad_norm': 1.0956021547317505, 'learning_rate': 2.608208517969547e-05, 'epoch': 0.79}
{'loss': 1.088, 'grad_norm': 1.007124900817871, 'learning_rate': 2.6045926771976303e-05, 'epoch': 0.79}
{'loss': 0.7659, 'grad_norm': 0.9903222918510437, 'learning_rate': 2.6009789692255582e-05, 'epoch': 0.79}
{'loss': 1.0259, 'grad_norm': 1.024275302886963, 'learning_rate': 2.597367395095517e-05, 'epoch': 0.79}
{'loss': 0.9505, 'grad_norm': 0.936144232749939, 'learning_rate': 2.593757955849063e-05, 'epoch': 0.79}
{'loss': 1.1938, 'grad_norm': 1.1111520528793335, 'learning_rate': 2.5901506525271425e-05, 'epoch': 0.79}
{'loss': 0.6139, 'grad_norm': 1.0182609558105469, 'learning_rate': 2.58654548617008e-05, 'epoch': 0.79}
{'loss': 0.946, 'grad_norm': 1.1597506999969482, 'learning_rate': 2.582942457817594e-05, 'epoch': 0.79}
{'loss': 0.9997, 'grad_norm': 1.102185606956482, 'learning_rate': 2.5793415685087797e-05, 'epoch': 0.79}
{'loss': 1.0963, 'grad_norm': 1.0663325786590576, 'learning_rate': 2.5757428192821133e-05, 'epoch': 0.79}
{'loss': 0.9476, 'grad_norm': 1.0806037187576294, 'learning_rate': 2.572146211175458e-05, 'epoch': 0.79}
{'loss': 0.9144, 'grad_norm': 1.4265782833099365, 'learning_rate': 2.5685517452260567e-05, 'epoch': 0.79}
{'loss': 1.1065, 'grad_norm': 1.1094255447387695, 'learning_rate': 2.564959422470543e-05, 'epoch': 0.79}
{'loss': 1.0578, 'grad_norm': 0.8409968614578247, 'learning_rate': 2.5613692439449143e-05, 'epoch': 0.79}
{'loss': 0.7805, 'grad_norm': 1.1258467435836792, 'learning_rate': 2.55778121068457e-05, 'epoch': 0.79}
{'loss': 0.8154, 'grad_norm': 1.0482831001281738, 'learning_rate': 2.5541953237242777e-05, 'epoch': 0.79}
{'loss': 1.1986, 'grad_norm': 1.225399374961853, 'learning_rate': 2.5506115840981904e-05, 'epoch': 0.79}
{'loss': 0.9756, 'grad_norm': 1.0785892009735107, 'learning_rate': 2.5470299928398424e-05, 'epoch': 0.79}
{'loss': 1.0389, 'grad_norm': 1.5339242219924927, 'learning_rate': 2.543450550982143e-05, 'epoch': 0.79}
{'loss': 0.9584, 'grad_norm': 1.1986407041549683, 'learning_rate': 2.539873259557396e-05, 'epoch': 0.79}
{'loss': 1.0015, 'grad_norm': 1.1792502403259277, 'learning_rate': 2.5362981195972625e-05, 'epoch': 0.79}
{'loss': 0.7922, 'grad_norm': 1.0421515703201294, 'learning_rate': 2.5327251321328037e-05, 'epoch': 0.79}
{'loss': 0.9104, 'grad_norm': 1.2176655530929565, 'learning_rate': 2.5291542981944504e-05, 'epoch': 0.79}
{'loss': 0.8152, 'grad_norm': 1.3720903396606445, 'learning_rate': 2.5255856188120132e-05, 'epoch': 0.79}
{'loss': 0.9523, 'grad_norm': 1.0584423542022705, 'learning_rate': 2.5220190950146827e-05, 'epoch': 0.79}
{'loss': 0.7243, 'grad_norm': 1.044278621673584, 'learning_rate': 2.5184547278310267e-05, 'epoch': 0.79}
{'loss': 1.1963, 'grad_norm': 1.3011705875396729, 'learning_rate': 2.514892518288988e-05, 'epoch': 0.79}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7432, 'grad_norm': 1.0532666444778442, 'learning_rate': 2.511332467415899e-05, 'epoch': 0.79}
{'loss': 0.9738, 'grad_norm': 1.0414307117462158, 'learning_rate': 2.5077745762384542e-05, 'epoch': 0.79}
{'loss': 1.1056, 'grad_norm': 1.2390549182891846, 'learning_rate': 2.5042188457827365e-05, 'epoch': 0.79}
{'loss': 1.1354, 'grad_norm': 1.5336651802062988, 'learning_rate': 2.5006652770741978e-05, 'epoch': 0.79}
{'loss': 0.9486, 'grad_norm': 1.2360080480575562, 'learning_rate': 2.4971138711376697e-05, 'epoch': 0.79}
{'loss': 0.8224, 'grad_norm': 1.1403151750564575, 'learning_rate': 2.493564628997369e-05, 'epoch': 0.79}
{'loss': 1.1304, 'grad_norm': 1.6431083679199219, 'learning_rate': 2.4900175516768676e-05, 'epoch': 0.79}
{'loss': 0.9897, 'grad_norm': 1.102325439453125, 'learning_rate': 2.4864726401991378e-05, 'epoch': 0.79}
{'loss': 0.9736, 'grad_norm': 1.2928649187088013, 'learning_rate': 2.482929895586502e-05, 'epoch': 0.79}
{'loss': 0.8457, 'grad_norm': 0.8595825433731079, 'learning_rate': 2.479389318860682e-05, 'epoch': 0.79}
{'loss': 0.8808, 'grad_norm': 1.0030198097229004, 'learning_rate': 2.4758509110427575e-05, 'epoch': 0.79}
{'loss': 0.9907, 'grad_norm': 1.0549285411834717, 'learning_rate': 2.4723146731531876e-05, 'epoch': 0.79}
{'loss': 1.0659, 'grad_norm': 1.1974135637283325, 'learning_rate': 2.4687806062118134e-05, 'epoch': 0.79}
{'loss': 0.9585, 'grad_norm': 1.1480755805969238, 'learning_rate': 2.465248711237831e-05, 'epoch': 0.79}
{'loss': 0.9025, 'grad_norm': 1.5538873672485352, 'learning_rate': 2.4617189892498327e-05, 'epoch': 0.79}
{'loss': 1.0572, 'grad_norm': 1.5138660669326782, 'learning_rate': 2.458191441265768e-05, 'epoch': 0.79}
{'loss': 0.9428, 'grad_norm': 1.1832311153411865, 'learning_rate': 2.4546660683029653e-05, 'epoch': 0.79}
{'loss': 0.8532, 'grad_norm': 1.1482641696929932, 'learning_rate': 2.4511428713781238e-05, 'epoch': 0.8}
{'loss': 1.0936, 'grad_norm': 1.241231918334961, 'learning_rate': 2.4476218515073234e-05, 'epoch': 0.8}
{'loss': 0.849, 'grad_norm': 1.2553694248199463, 'learning_rate': 2.444103009705999e-05, 'epoch': 0.8}
{'loss': 1.4364, 'grad_norm': 1.043647289276123, 'learning_rate': 2.4405863469889757e-05, 'epoch': 0.8}
{'loss': 1.0303, 'grad_norm': 1.241693377494812, 'learning_rate': 2.4370718643704392e-05, 'epoch': 0.8}
{'loss': 0.8916, 'grad_norm': 1.4856312274932861, 'learning_rate': 2.4335595628639496e-05, 'epoch': 0.8}
{'loss': 0.9876, 'grad_norm': 1.079475998878479, 'learning_rate': 2.4300494434824373e-05, 'epoch': 0.8}
{'loss': 0.9009, 'grad_norm': 1.0563985109329224, 'learning_rate': 2.4265415072382016e-05, 'epoch': 0.8}
{'loss': 0.8511, 'grad_norm': 0.9680880904197693, 'learning_rate': 2.4230357551429216e-05, 'epoch': 0.8}
{'loss': 1.1642, 'grad_norm': 1.0505143404006958, 'learning_rate': 2.4195321882076295e-05, 'epoch': 0.8}
{'loss': 1.046, 'grad_norm': 0.9857138991355896, 'learning_rate': 2.4160308074427452e-05, 'epoch': 0.8}
{'loss': 1.0599, 'grad_norm': 1.3941998481750488, 'learning_rate': 2.4125316138580467e-05, 'epoch': 0.8}
{'loss': 1.1245, 'grad_norm': 1.3126519918441772, 'learning_rate': 2.409034608462686e-05, 'epoch': 0.8}
{'loss': 1.1192, 'grad_norm': 0.8999510407447815, 'learning_rate': 2.4055397922651802e-05, 'epoch': 0.8}
{'loss': 0.9732, 'grad_norm': 0.8875332474708557, 'learning_rate': 2.40204716627342e-05, 'epoch': 0.8}
{'loss': 0.9655, 'grad_norm': 0.9132203459739685, 'learning_rate': 2.3985567314946578e-05, 'epoch': 0.8}
{'loss': 1.0131, 'grad_norm': 1.1453258991241455, 'learning_rate': 2.3950684889355234e-05, 'epoch': 0.8}
{'loss': 0.7276, 'grad_norm': 1.0360187292099, 'learning_rate': 2.391582439602007e-05, 'epoch': 0.8}
{'loss': 1.2565, 'grad_norm': 0.984568178653717, 'learning_rate': 2.3880985844994674e-05, 'epoch': 0.8}
{'loss': 0.6889, 'grad_norm': 1.0373677015304565, 'learning_rate': 2.3846169246326343e-05, 'epoch': 0.8}
{'loss': 1.1719, 'grad_norm': 1.3442203998565674, 'learning_rate': 2.3811374610055957e-05, 'epoch': 0.8}
{'loss': 1.1524, 'grad_norm': 1.0766429901123047, 'learning_rate': 2.3776601946218223e-05, 'epoch': 0.8}
{'loss': 0.6226, 'grad_norm': 0.9844877123832703, 'learning_rate': 2.3741851264841297e-05, 'epoch': 0.8}
{'loss': 0.7542, 'grad_norm': 1.426228404045105, 'learning_rate': 2.3707122575947194e-05, 'epoch': 0.8}
{'loss': 0.8695, 'grad_norm': 0.8815398216247559, 'learning_rate': 2.367241588955146e-05, 'epoch': 0.8}
{'loss': 0.661, 'grad_norm': 1.0648752450942993, 'learning_rate': 2.363773121566334e-05, 'epoch': 0.8}
{'loss': 0.6356, 'grad_norm': 1.0940107107162476, 'learning_rate': 2.3603068564285747e-05, 'epoch': 0.8}
{'loss': 0.8967, 'grad_norm': 1.021836757659912, 'learning_rate': 2.356842794541516e-05, 'epoch': 0.8}
{'loss': 0.695, 'grad_norm': 1.2930688858032227, 'learning_rate': 2.353380936904188e-05, 'epoch': 0.8}
{'loss': 0.8539, 'grad_norm': 1.2576539516448975, 'learning_rate': 2.349921284514961e-05, 'epoch': 0.8}
{'loss': 0.7965, 'grad_norm': 1.2701225280761719, 'learning_rate': 2.346463838371591e-05, 'epoch': 0.8}
{'loss': 1.0889, 'grad_norm': 1.1472251415252686, 'learning_rate': 2.343008599471187e-05, 'epoch': 0.8}
{'loss': 0.8802, 'grad_norm': 1.1065171957015991, 'learning_rate': 2.339555568810221e-05, 'epoch': 0.8}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9807, 'grad_norm': 1.522199034690857, 'learning_rate': 2.3361047473845322e-05, 'epoch': 0.8}
{'loss': 0.8482, 'grad_norm': 1.1387676000595093, 'learning_rate': 2.3326561361893205e-05, 'epoch': 0.8}
{'loss': 1.2046, 'grad_norm': 1.3878862857818604, 'learning_rate': 2.3292097362191456e-05, 'epoch': 0.8}
{'loss': 1.0232, 'grad_norm': 1.048671007156372, 'learning_rate': 2.3257655484679374e-05, 'epoch': 0.8}
{'loss': 0.9681, 'grad_norm': 1.2965478897094727, 'learning_rate': 2.322323573928983e-05, 'epoch': 0.8}
{'loss': 0.8396, 'grad_norm': 1.2352280616760254, 'learning_rate': 2.3188838135949253e-05, 'epoch': 0.8}
{'loss': 0.6327, 'grad_norm': 1.6055623292922974, 'learning_rate': 2.315446268457786e-05, 'epoch': 0.8}
{'loss': 0.8755, 'grad_norm': 1.2371383905410767, 'learning_rate': 2.312010939508923e-05, 'epoch': 0.8}
{'loss': 0.848, 'grad_norm': 0.9375468492507935, 'learning_rate': 2.3085778277390803e-05, 'epoch': 0.8}
{'loss': 0.6826, 'grad_norm': 0.9801722168922424, 'learning_rate': 2.3051469341383402e-05, 'epoch': 0.8}
{'loss': 1.1343, 'grad_norm': 1.260136604309082, 'learning_rate': 2.301718259696165e-05, 'epoch': 0.8}
{'loss': 0.7515, 'grad_norm': 1.4072455167770386, 'learning_rate': 2.2982918054013637e-05, 'epoch': 0.8}
{'loss': 0.8478, 'grad_norm': 1.2929843664169312, 'learning_rate': 2.2948675722421086e-05, 'epoch': 0.8}
{'loss': 0.8696, 'grad_norm': 0.839023768901825, 'learning_rate': 2.291445561205934e-05, 'epoch': 0.8}
{'loss': 0.845, 'grad_norm': 1.140649676322937, 'learning_rate': 2.2880257732797284e-05, 'epoch': 0.8}
{'loss': 0.9738, 'grad_norm': 1.367234230041504, 'learning_rate': 2.2846082094497458e-05, 'epoch': 0.8}
{'loss': 1.025, 'grad_norm': 1.115759253501892, 'learning_rate': 2.281192870701594e-05, 'epoch': 0.8}
{'loss': 1.1761, 'grad_norm': 1.227392554283142, 'learning_rate': 2.277779758020241e-05, 'epoch': 0.8}
{'loss': 1.2729, 'grad_norm': 1.211256504058838, 'learning_rate': 2.274368872390009e-05, 'epoch': 0.8}
{'loss': 1.0756, 'grad_norm': 1.4433928728103638, 'learning_rate': 2.270960214794584e-05, 'epoch': 0.8}
{'loss': 1.0208, 'grad_norm': 1.0285786390304565, 'learning_rate': 2.2675537862170027e-05, 'epoch': 0.8}
{'loss': 1.1116, 'grad_norm': 0.970646858215332, 'learning_rate': 2.2641495876396713e-05, 'epoch': 0.8}
{'loss': 0.5671, 'grad_norm': 1.037764072418213, 'learning_rate': 2.2607476200443324e-05, 'epoch': 0.8}
{'loss': 0.871, 'grad_norm': 1.4499951601028442, 'learning_rate': 2.2573478844121054e-05, 'epoch': 0.8}
{'loss': 0.9209, 'grad_norm': 1.4072661399841309, 'learning_rate': 2.2539503817234553e-05, 'epoch': 0.8}
{'loss': 0.8672, 'grad_norm': 1.1160533428192139, 'learning_rate': 2.2505551129582047e-05, 'epoch': 0.8}
{'loss': 0.6234, 'grad_norm': 1.1082379817962646, 'learning_rate': 2.247162079095535e-05, 'epoch': 0.8}
{'loss': 0.9688, 'grad_norm': 1.0532506704330444, 'learning_rate': 2.2437712811139767e-05, 'epoch': 0.8}
{'loss': 0.6871, 'grad_norm': 1.1081430912017822, 'learning_rate': 2.240382719991426e-05, 'epoch': 0.8}
{'loss': 0.9138, 'grad_norm': 0.9967716932296753, 'learning_rate': 2.23699639670512e-05, 'epoch': 0.8}
{'loss': 1.161, 'grad_norm': 1.4685899019241333, 'learning_rate': 2.2336123122316644e-05, 'epoch': 0.8}
{'loss': 0.7625, 'grad_norm': 1.2665646076202393, 'learning_rate': 2.23023046754701e-05, 'epoch': 0.8}
{'loss': 1.1991, 'grad_norm': 1.5959097146987915, 'learning_rate': 2.2268508636264652e-05, 'epoch': 0.81}
{'loss': 0.9975, 'grad_norm': 1.0620782375335693, 'learning_rate': 2.2234735014446907e-05, 'epoch': 0.81}
{'loss': 0.99, 'grad_norm': 1.2441399097442627, 'learning_rate': 2.2200983819757026e-05, 'epoch': 0.81}
{'loss': 0.8828, 'grad_norm': 0.9763342142105103, 'learning_rate': 2.216725506192865e-05, 'epoch': 0.81}
{'loss': 1.0725, 'grad_norm': 1.2508387565612793, 'learning_rate': 2.213354875068906e-05, 'epoch': 0.81}
{'loss': 1.1488, 'grad_norm': 1.1050752401351929, 'learning_rate': 2.2099864895758948e-05, 'epoch': 0.81}
{'loss': 0.9025, 'grad_norm': 0.9461698532104492, 'learning_rate': 2.2066203506852566e-05, 'epoch': 0.81}
{'loss': 0.9783, 'grad_norm': 1.0425318479537964, 'learning_rate': 2.2032564593677774e-05, 'epoch': 0.81}
{'loss': 0.8476, 'grad_norm': 0.927873969078064, 'learning_rate': 2.1998948165935762e-05, 'epoch': 0.81}
{'loss': 0.9574, 'grad_norm': 1.1491152048110962, 'learning_rate': 2.1965354233321445e-05, 'epoch': 0.81}
{'loss': 0.4961, 'grad_norm': 0.8330757021903992, 'learning_rate': 2.193178280552306e-05, 'epoch': 0.81}
{'loss': 1.0719, 'grad_norm': 1.1467338800430298, 'learning_rate': 2.1898233892222507e-05, 'epoch': 0.81}
{'loss': 0.8718, 'grad_norm': 1.1152153015136719, 'learning_rate': 2.1864707503095128e-05, 'epoch': 0.81}
{'loss': 1.1566, 'grad_norm': 1.012574553489685, 'learning_rate': 2.183120364780975e-05, 'epoch': 0.81}
{'loss': 0.8129, 'grad_norm': 1.127121090888977, 'learning_rate': 2.1797722336028724e-05, 'epoch': 0.81}
{'loss': 0.8634, 'grad_norm': 1.2429413795471191, 'learning_rate': 2.1764263577407896e-05, 'epoch': 0.81}
{'loss': 1.0173, 'grad_norm': 1.1913626194000244, 'learning_rate': 2.1730827381596643e-05, 'epoch': 0.81}
{'loss': 0.7894, 'grad_norm': 1.2849916219711304, 'learning_rate': 2.1697413758237784e-05, 'epoch': 0.81}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2146, 'grad_norm': 1.0442860126495361, 'learning_rate': 2.1664022716967635e-05, 'epoch': 0.81}
{'loss': 0.7857, 'grad_norm': 1.0707764625549316, 'learning_rate': 2.163065426741603e-05, 'epoch': 0.81}
{'loss': 0.9652, 'grad_norm': 1.243783950805664, 'learning_rate': 2.1597308419206264e-05, 'epoch': 0.81}
{'loss': 0.8935, 'grad_norm': 1.286211371421814, 'learning_rate': 2.1563985181955082e-05, 'epoch': 0.81}
{'loss': 1.0359, 'grad_norm': 1.169190764427185, 'learning_rate': 2.153068456527283e-05, 'epoch': 0.81}
{'loss': 1.0122, 'grad_norm': 1.3537973165512085, 'learning_rate': 2.1497406578763147e-05, 'epoch': 0.81}
{'loss': 1.1196, 'grad_norm': 0.9186873435974121, 'learning_rate': 2.146415123202331e-05, 'epoch': 0.81}
{'loss': 0.9326, 'grad_norm': 0.9375715255737305, 'learning_rate': 2.1430918534643996e-05, 'epoch': 0.81}
{'loss': 1.053, 'grad_norm': 0.9492185711860657, 'learning_rate': 2.13977084962093e-05, 'epoch': 0.81}
{'loss': 1.0356, 'grad_norm': 1.4297914505004883, 'learning_rate': 2.136452112629693e-05, 'epoch': 0.81}
{'loss': 1.0617, 'grad_norm': 1.4077509641647339, 'learning_rate': 2.1331356434477866e-05, 'epoch': 0.81}
{'loss': 0.8473, 'grad_norm': 0.9655101299285889, 'learning_rate': 2.1298214430316743e-05, 'epoch': 0.81}
{'loss': 0.7672, 'grad_norm': 0.9501218199729919, 'learning_rate': 2.1265095123371447e-05, 'epoch': 0.81}
{'loss': 0.899, 'grad_norm': 1.0972172021865845, 'learning_rate': 2.123199852319352e-05, 'epoch': 0.81}
{'loss': 1.0451, 'grad_norm': 1.058922529220581, 'learning_rate': 2.119892463932781e-05, 'epoch': 0.81}
{'loss': 0.9362, 'grad_norm': 1.0370858907699585, 'learning_rate': 2.1165873481312692e-05, 'epoch': 0.81}
{'loss': 0.8911, 'grad_norm': 1.2456984519958496, 'learning_rate': 2.113284505867994e-05, 'epoch': 0.81}
{'loss': 1.1455, 'grad_norm': 1.4232097864151, 'learning_rate': 2.109983938095479e-05, 'epoch': 0.81}
{'loss': 1.0664, 'grad_norm': 1.27346932888031, 'learning_rate': 2.1066856457655946e-05, 'epoch': 0.81}
{'loss': 1.0695, 'grad_norm': 1.095651626586914, 'learning_rate': 2.1033896298295508e-05, 'epoch': 0.81}
{'loss': 1.0986, 'grad_norm': 0.9799562096595764, 'learning_rate': 2.100095891237903e-05, 'epoch': 0.81}
{'loss': 1.3198, 'grad_norm': 1.0050824880599976, 'learning_rate': 2.096804430940551e-05, 'epoch': 0.81}
{'loss': 0.8278, 'grad_norm': 1.5612772703170776, 'learning_rate': 2.0935152498867326e-05, 'epoch': 0.81}
{'loss': 0.6908, 'grad_norm': 1.1259727478027344, 'learning_rate': 2.090228349025032e-05, 'epoch': 0.81}
{'loss': 1.0252, 'grad_norm': 1.0592617988586426, 'learning_rate': 2.0869437293033835e-05, 'epoch': 0.81}
{'loss': 0.7559, 'grad_norm': 0.9620485305786133, 'learning_rate': 2.0836613916690428e-05, 'epoch': 0.81}
{'loss': 0.6379, 'grad_norm': 1.1120933294296265, 'learning_rate': 2.0803813370686298e-05, 'epoch': 0.81}
{'loss': 1.1945, 'grad_norm': 1.2423158884048462, 'learning_rate': 2.0771035664480942e-05, 'epoch': 0.81}
{'loss': 0.9204, 'grad_norm': 1.2082701921463013, 'learning_rate': 2.0738280807527276e-05, 'epoch': 0.81}
{'loss': 1.1846, 'grad_norm': 1.1285430192947388, 'learning_rate': 2.0705548809271658e-05, 'epoch': 0.81}
{'loss': 0.7114, 'grad_norm': 1.1167540550231934, 'learning_rate': 2.0672839679153812e-05, 'epoch': 0.81}
{'loss': 0.8725, 'grad_norm': 1.0636039972305298, 'learning_rate': 2.0640153426606935e-05, 'epoch': 0.81}
{'loss': 0.8585, 'grad_norm': 1.0992933511734009, 'learning_rate': 2.0607490061057565e-05, 'epoch': 0.81}
{'loss': 1.0265, 'grad_norm': 1.5294591188430786, 'learning_rate': 2.057484959192567e-05, 'epoch': 0.81}
{'loss': 0.5581, 'grad_norm': 1.084324836730957, 'learning_rate': 2.0542232028624586e-05, 'epoch': 0.81}
{'loss': 0.9494, 'grad_norm': 1.0097366571426392, 'learning_rate': 2.0509637380561063e-05, 'epoch': 0.81}
{'loss': 0.746, 'grad_norm': 1.1950123310089111, 'learning_rate': 2.047706565713523e-05, 'epoch': 0.81}
{'loss': 1.1354, 'grad_norm': 1.0475552082061768, 'learning_rate': 2.044451686774068e-05, 'epoch': 0.81}
{'loss': 0.8103, 'grad_norm': 1.0868926048278809, 'learning_rate': 2.041199102176422e-05, 'epoch': 0.81}
{'loss': 0.612, 'grad_norm': 1.1800161600112915, 'learning_rate': 2.0379488128586243e-05, 'epoch': 0.81}
{'loss': 0.7112, 'grad_norm': 1.4095345735549927, 'learning_rate': 2.0347008197580374e-05, 'epoch': 0.81}
{'loss': 0.7504, 'grad_norm': 1.113680362701416, 'learning_rate': 2.0314551238113677e-05, 'epoch': 0.81}
{'loss': 0.9109, 'grad_norm': 1.1701080799102783, 'learning_rate': 2.028211725954663e-05, 'epoch': 0.81}
{'loss': 0.7826, 'grad_norm': 1.014362096786499, 'learning_rate': 2.024970627123295e-05, 'epoch': 0.81}
{'loss': 1.1933, 'grad_norm': 1.0494415760040283, 'learning_rate': 2.0217318282519904e-05, 'epoch': 0.81}
{'loss': 0.9209, 'grad_norm': 1.3465219736099243, 'learning_rate': 2.0184953302747934e-05, 'epoch': 0.81}
{'loss': 1.0193, 'grad_norm': 1.138959527015686, 'learning_rate': 2.0152611341251015e-05, 'epoch': 0.81}
{'loss': 0.9161, 'grad_norm': 1.7118821144104004, 'learning_rate': 2.01202924073564e-05, 'epoch': 0.82}
{'loss': 0.7637, 'grad_norm': 1.1926474571228027, 'learning_rate': 2.00879965103847e-05, 'epoch': 0.82}
{'loss': 1.0792, 'grad_norm': 1.232686996459961, 'learning_rate': 2.0055723659649904e-05, 'epoch': 0.82}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8398, 'grad_norm': 1.2004810571670532, 'learning_rate': 2.002347386445932e-05, 'epoch': 0.82}
{'loss': 1.217, 'grad_norm': 1.1991623640060425, 'learning_rate': 1.999124713411368e-05, 'epoch': 0.82}
{'loss': 0.7111, 'grad_norm': 1.038623332977295, 'learning_rate': 1.9959043477907e-05, 'epoch': 0.82}
{'loss': 0.9675, 'grad_norm': 0.9699398875236511, 'learning_rate': 1.9926862905126665e-05, 'epoch': 0.82}
{'loss': 1.0926, 'grad_norm': 1.331504225730896, 'learning_rate': 1.9894705425053372e-05, 'epoch': 0.82}
{'loss': 0.8069, 'grad_norm': 1.0186344385147095, 'learning_rate': 1.986257104696121e-05, 'epoch': 0.82}
{'loss': 1.0844, 'grad_norm': 1.1399859189987183, 'learning_rate': 1.9830459780117538e-05, 'epoch': 0.82}
{'loss': 0.8154, 'grad_norm': 1.5158069133758545, 'learning_rate': 1.9798371633783174e-05, 'epoch': 0.82}
{'loss': 0.9575, 'grad_norm': 1.2628681659698486, 'learning_rate': 1.9766306617212072e-05, 'epoch': 0.82}
{'loss': 0.8937, 'grad_norm': 1.0754845142364502, 'learning_rate': 1.973426473965172e-05, 'epoch': 0.82}
{'loss': 0.8047, 'grad_norm': 1.1899054050445557, 'learning_rate': 1.9702246010342816e-05, 'epoch': 0.82}
{'loss': 0.7257, 'grad_norm': 1.2393858432769775, 'learning_rate': 1.967025043851939e-05, 'epoch': 0.82}
{'loss': 1.0353, 'grad_norm': 0.9871886372566223, 'learning_rate': 1.963827803340883e-05, 'epoch': 0.82}
{'loss': 0.8715, 'grad_norm': 1.2459402084350586, 'learning_rate': 1.960632880423178e-05, 'epoch': 0.82}
{'loss': 0.8798, 'grad_norm': 0.9934430718421936, 'learning_rate': 1.9574402760202315e-05, 'epoch': 0.82}
{'loss': 1.0669, 'grad_norm': 2.5861542224884033, 'learning_rate': 1.95424999105277e-05, 'epoch': 0.82}
{'loss': 0.7479, 'grad_norm': 1.0876591205596924, 'learning_rate': 1.9510620264408596e-05, 'epoch': 0.82}
{'loss': 0.7855, 'grad_norm': 1.4349632263183594, 'learning_rate': 1.9478763831038915e-05, 'epoch': 0.82}
{'loss': 0.661, 'grad_norm': 1.1160354614257812, 'learning_rate': 1.944693061960591e-05, 'epoch': 0.82}
{'loss': 0.7168, 'grad_norm': 1.1548343896865845, 'learning_rate': 1.9415120639290085e-05, 'epoch': 0.82}
{'loss': 0.7828, 'grad_norm': 1.0960568189620972, 'learning_rate': 1.9383333899265364e-05, 'epoch': 0.82}
{'loss': 0.887, 'grad_norm': 1.1006568670272827, 'learning_rate': 1.935157040869884e-05, 'epoch': 0.82}
{'loss': 0.9993, 'grad_norm': 1.3823682069778442, 'learning_rate': 1.9319830176750964e-05, 'epoch': 0.82}
{'loss': 0.6787, 'grad_norm': 1.2445552349090576, 'learning_rate': 1.9288113212575452e-05, 'epoch': 0.82}
{'loss': 0.6851, 'grad_norm': 1.25824773311615, 'learning_rate': 1.9256419525319313e-05, 'epoch': 0.82}
{'loss': 0.9621, 'grad_norm': 1.3274277448654175, 'learning_rate': 1.922474912412292e-05, 'epoch': 0.82}
{'loss': 0.8336, 'grad_norm': 1.261628270149231, 'learning_rate': 1.919310201811977e-05, 'epoch': 0.82}
{'loss': 0.879, 'grad_norm': 1.1633613109588623, 'learning_rate': 1.9161478216436836e-05, 'epoch': 0.82}
{'loss': 0.9246, 'grad_norm': 1.2704836130142212, 'learning_rate': 1.912987772819417e-05, 'epoch': 0.82}
{'loss': 1.1586, 'grad_norm': 1.1457445621490479, 'learning_rate': 1.9098300562505266e-05, 'epoch': 0.82}
{'loss': 1.4382, 'grad_norm': 1.3607312440872192, 'learning_rate': 1.9066746728476813e-05, 'epoch': 0.82}
{'loss': 1.016, 'grad_norm': 1.0397582054138184, 'learning_rate': 1.9035216235208775e-05, 'epoch': 0.82}
{'loss': 0.9093, 'grad_norm': 1.2888119220733643, 'learning_rate': 1.9003709091794408e-05, 'epoch': 0.82}
{'loss': 1.1207, 'grad_norm': 1.5521864891052246, 'learning_rate': 1.8972225307320178e-05, 'epoch': 0.82}
{'loss': 1.1237, 'grad_norm': 1.3253028392791748, 'learning_rate': 1.8940764890865914e-05, 'epoch': 0.82}
{'loss': 1.0935, 'grad_norm': 1.0099905729293823, 'learning_rate': 1.8909327851504633e-05, 'epoch': 0.82}
{'loss': 0.9403, 'grad_norm': 1.1774810552597046, 'learning_rate': 1.8877914198302626e-05, 'epoch': 0.82}
{'loss': 1.1875, 'grad_norm': 1.035961389541626, 'learning_rate': 1.8846523940319416e-05, 'epoch': 0.82}
{'loss': 1.0735, 'grad_norm': 1.1060956716537476, 'learning_rate': 1.8815157086607826e-05, 'epoch': 0.82}
{'loss': 1.139, 'grad_norm': 0.9892654418945312, 'learning_rate': 1.8783813646213867e-05, 'epoch': 0.82}
{'loss': 0.7726, 'grad_norm': 1.1028467416763306, 'learning_rate': 1.8752493628176925e-05, 'epoch': 0.82}
{'loss': 1.048, 'grad_norm': 1.1592159271240234, 'learning_rate': 1.8721197041529427e-05, 'epoch': 0.82}
{'loss': 1.0093, 'grad_norm': 1.1637053489685059, 'learning_rate': 1.8689923895297245e-05, 'epoch': 0.82}
{'loss': 0.7297, 'grad_norm': 1.0262913703918457, 'learning_rate': 1.865867419849937e-05, 'epoch': 0.82}
{'loss': 0.969, 'grad_norm': 1.0865997076034546, 'learning_rate': 1.8627447960148037e-05, 'epoch': 0.82}
{'loss': 1.0201, 'grad_norm': 1.3342729806900024, 'learning_rate': 1.8596245189248827e-05, 'epoch': 0.82}
{'loss': 0.6684, 'grad_norm': 1.1711698770523071, 'learning_rate': 1.856506589480037e-05, 'epoch': 0.82}
{'loss': 0.6873, 'grad_norm': 0.9027471542358398, 'learning_rate': 1.8533910085794713e-05, 'epoch': 0.82}
{'loss': 0.6958, 'grad_norm': 1.2895690202713013, 'learning_rate': 1.850277777121696e-05, 'epoch': 0.82}
{'loss': 1.0691, 'grad_norm': 1.0062222480773926, 'learning_rate': 1.8471668960045574e-05, 'epoch': 0.82}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.821, 'grad_norm': 1.0656489133834839, 'learning_rate': 1.8440583661252165e-05, 'epoch': 0.82}
{'loss': 0.7289, 'grad_norm': 1.203047275543213, 'learning_rate': 1.8409521883801595e-05, 'epoch': 0.82}
{'loss': 0.8762, 'grad_norm': 1.020654559135437, 'learning_rate': 1.83784836366519e-05, 'epoch': 0.82}
{'loss': 0.5922, 'grad_norm': 1.2095504999160767, 'learning_rate': 1.8347468928754407e-05, 'epoch': 0.82}
{'loss': 0.9482, 'grad_norm': 1.0959711074829102, 'learning_rate': 1.8316477769053597e-05, 'epoch': 0.82}
{'loss': 1.4042, 'grad_norm': 1.1066941022872925, 'learning_rate': 1.8285510166487152e-05, 'epoch': 0.82}
{'loss': 0.9662, 'grad_norm': 0.989316463470459, 'learning_rate': 1.8254566129985995e-05, 'epoch': 0.82}
{'loss': 0.8912, 'grad_norm': 1.6480748653411865, 'learning_rate': 1.82236456684742e-05, 'epoch': 0.82}
{'loss': 0.8527, 'grad_norm': 1.2561641931533813, 'learning_rate': 1.819274879086916e-05, 'epoch': 0.82}
{'loss': 0.7567, 'grad_norm': 1.103035569190979, 'learning_rate': 1.8161875506081293e-05, 'epoch': 0.82}
{'loss': 0.8161, 'grad_norm': 1.0482053756713867, 'learning_rate': 1.81310258230144e-05, 'epoch': 0.82}
{'loss': 0.9155, 'grad_norm': 1.08591628074646, 'learning_rate': 1.8100199750565273e-05, 'epoch': 0.82}
{'loss': 1.2188, 'grad_norm': 1.2012401819229126, 'learning_rate': 1.806939729762409e-05, 'epoch': 0.83}
{'loss': 1.2176, 'grad_norm': 1.0489174127578735, 'learning_rate': 1.8038618473074087e-05, 'epoch': 0.83}
{'loss': 0.8109, 'grad_norm': 1.3112329244613647, 'learning_rate': 1.800786328579176e-05, 'epoch': 0.83}
{'loss': 0.8162, 'grad_norm': 1.3594025373458862, 'learning_rate': 1.7977131744646724e-05, 'epoch': 0.83}
{'loss': 1.3289, 'grad_norm': 1.2426249980926514, 'learning_rate': 1.794642385850179e-05, 'epoch': 0.83}
{'loss': 0.8699, 'grad_norm': 1.2766993045806885, 'learning_rate': 1.7915739636213035e-05, 'epoch': 0.83}
{'loss': 0.9343, 'grad_norm': 1.002447247505188, 'learning_rate': 1.78850790866296e-05, 'epoch': 0.83}
{'loss': 1.0486, 'grad_norm': 1.2111543416976929, 'learning_rate': 1.7854442218593838e-05, 'epoch': 0.83}
{'loss': 0.8946, 'grad_norm': 0.9090064764022827, 'learning_rate': 1.7823829040941286e-05, 'epoch': 0.83}
{'loss': 0.7974, 'grad_norm': 1.011540412902832, 'learning_rate': 1.779323956250062e-05, 'epoch': 0.83}
{'loss': 1.189, 'grad_norm': 1.6426666975021362, 'learning_rate': 1.7762673792093687e-05, 'epoch': 0.83}
{'loss': 0.935, 'grad_norm': 1.2312045097351074, 'learning_rate': 1.773213173853554e-05, 'epoch': 0.83}
{'loss': 1.0798, 'grad_norm': 1.2490650415420532, 'learning_rate': 1.7701613410634365e-05, 'epoch': 0.83}
{'loss': 1.1072, 'grad_norm': 1.1522510051727295, 'learning_rate': 1.767111881719148e-05, 'epoch': 0.83}
{'loss': 0.9114, 'grad_norm': 1.3649097681045532, 'learning_rate': 1.7640647967001378e-05, 'epoch': 0.83}
{'loss': 1.0887, 'grad_norm': 1.1220901012420654, 'learning_rate': 1.761020086885169e-05, 'epoch': 0.83}
{'loss': 1.1399, 'grad_norm': 1.131824254989624, 'learning_rate': 1.757977753152328e-05, 'epoch': 0.83}
{'loss': 1.0076, 'grad_norm': 1.0636274814605713, 'learning_rate': 1.7549377963789994e-05, 'epoch': 0.83}
{'loss': 1.1596, 'grad_norm': 1.1269659996032715, 'learning_rate': 1.751900217441902e-05, 'epoch': 0.83}
{'loss': 0.9141, 'grad_norm': 1.2317874431610107, 'learning_rate': 1.7488650172170496e-05, 'epoch': 0.83}
{'loss': 1.1188, 'grad_norm': 1.171270489692688, 'learning_rate': 1.7458321965797852e-05, 'epoch': 0.83}
{'loss': 0.9921, 'grad_norm': 1.1631807088851929, 'learning_rate': 1.7428017564047594e-05, 'epoch': 0.83}
{'loss': 0.6955, 'grad_norm': 0.9634861350059509, 'learning_rate': 1.7397736975659352e-05, 'epoch': 0.83}
{'loss': 0.8455, 'grad_norm': 1.1596996784210205, 'learning_rate': 1.736748020936587e-05, 'epoch': 0.83}
{'loss': 1.0079, 'grad_norm': 1.0796180963516235, 'learning_rate': 1.733724727389313e-05, 'epoch': 0.83}
{'loss': 0.8555, 'grad_norm': 1.0292339324951172, 'learning_rate': 1.730703817796011e-05, 'epoch': 0.83}
{'loss': 0.6172, 'grad_norm': 0.8106105327606201, 'learning_rate': 1.7276852930278985e-05, 'epoch': 0.83}
{'loss': 0.69, 'grad_norm': 0.9971617460250854, 'learning_rate': 1.7246691539555028e-05, 'epoch': 0.83}
{'loss': 0.8877, 'grad_norm': 1.1225378513336182, 'learning_rate': 1.721655401448662e-05, 'epoch': 0.83}
{'loss': 0.8265, 'grad_norm': 1.1829876899719238, 'learning_rate': 1.7186440363765355e-05, 'epoch': 0.83}
{'loss': 0.8705, 'grad_norm': 0.9991326928138733, 'learning_rate': 1.7156350596075744e-05, 'epoch': 0.83}
{'loss': 0.8269, 'grad_norm': 1.1090891361236572, 'learning_rate': 1.7126284720095653e-05, 'epoch': 0.83}
{'loss': 0.8305, 'grad_norm': 0.8848057985305786, 'learning_rate': 1.7096242744495837e-05, 'epoch': 0.83}
{'loss': 0.9442, 'grad_norm': 1.4062362909317017, 'learning_rate': 1.7066224677940314e-05, 'epoch': 0.83}
{'loss': 0.9918, 'grad_norm': 1.1042207479476929, 'learning_rate': 1.703623052908614e-05, 'epoch': 0.83}
{'loss': 1.1836, 'grad_norm': 1.142360806465149, 'learning_rate': 1.7006260306583454e-05, 'epoch': 0.83}
{'loss': 0.7531, 'grad_norm': 1.399367094039917, 'learning_rate': 1.697631401907559e-05, 'epoch': 0.83}
{'loss': 0.9224, 'grad_norm': 0.9244447946548462, 'learning_rate': 1.6946391675198836e-05, 'epoch': 0.83}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.027, 'grad_norm': 1.0385327339172363, 'learning_rate': 1.69164932835827e-05, 'epoch': 0.83}
{'loss': 0.9948, 'grad_norm': 0.9753585457801819, 'learning_rate': 1.6886618852849724e-05, 'epoch': 0.83}
{'loss': 1.0882, 'grad_norm': 1.3154653310775757, 'learning_rate': 1.685676839161554e-05, 'epoch': 0.83}
{'loss': 1.1296, 'grad_norm': 1.0686540603637695, 'learning_rate': 1.6826941908488893e-05, 'epoch': 0.83}
{'loss': 1.1467, 'grad_norm': 0.9824878573417664, 'learning_rate': 1.6797139412071584e-05, 'epoch': 0.83}
{'loss': 0.9529, 'grad_norm': 1.0016449689865112, 'learning_rate': 1.6767360910958495e-05, 'epoch': 0.83}
{'loss': 1.0608, 'grad_norm': 0.9316709637641907, 'learning_rate': 1.6737606413737638e-05, 'epoch': 0.83}
{'loss': 0.9941, 'grad_norm': 1.108903408050537, 'learning_rate': 1.6707875928990058e-05, 'epoch': 0.83}
{'loss': 0.9028, 'grad_norm': 1.4280147552490234, 'learning_rate': 1.6678169465289863e-05, 'epoch': 0.83}
{'loss': 0.9226, 'grad_norm': 1.138482689857483, 'learning_rate': 1.6648487031204273e-05, 'epoch': 0.83}
{'loss': 1.0503, 'grad_norm': 1.1618475914001465, 'learning_rate': 1.661882863529354e-05, 'epoch': 0.83}
{'loss': 1.0646, 'grad_norm': 1.231332540512085, 'learning_rate': 1.6589194286111044e-05, 'epoch': 0.83}
{'loss': 0.993, 'grad_norm': 1.266053318977356, 'learning_rate': 1.6559583992203122e-05, 'epoch': 0.83}
{'loss': 1.0878, 'grad_norm': 1.366180181503296, 'learning_rate': 1.6529997762109317e-05, 'epoch': 0.83}
{'loss': 1.0953, 'grad_norm': 1.2631539106369019, 'learning_rate': 1.6500435604362075e-05, 'epoch': 0.83}
{'loss': 0.8175, 'grad_norm': 1.1816203594207764, 'learning_rate': 1.647089752748704e-05, 'epoch': 0.83}
{'loss': 0.9242, 'grad_norm': 1.3151698112487793, 'learning_rate': 1.644138354000284e-05, 'epoch': 0.83}
{'loss': 1.0991, 'grad_norm': 1.2586013078689575, 'learning_rate': 1.641189365042115e-05, 'epoch': 0.83}
{'loss': 0.5653, 'grad_norm': 1.5044697523117065, 'learning_rate': 1.638242786724672e-05, 'epoch': 0.83}
{'loss': 1.1695, 'grad_norm': 0.9832028746604919, 'learning_rate': 1.6352986198977325e-05, 'epoch': 0.83}
{'loss': 0.8572, 'grad_norm': 1.1262000799179077, 'learning_rate': 1.632356865410384e-05, 'epoch': 0.83}
{'loss': 0.9689, 'grad_norm': 1.0199403762817383, 'learning_rate': 1.629417524111011e-05, 'epoch': 0.83}
{'loss': 0.9665, 'grad_norm': 1.0291593074798584, 'learning_rate': 1.626480596847306e-05, 'epoch': 0.83}
{'loss': 1.0461, 'grad_norm': 1.158734917640686, 'learning_rate': 1.6235460844662642e-05, 'epoch': 0.83}
{'loss': 0.8945, 'grad_norm': 0.9937501549720764, 'learning_rate': 1.620613987814189e-05, 'epoch': 0.83}
{'loss': 0.9022, 'grad_norm': 1.1136316061019897, 'learning_rate': 1.6176843077366754e-05, 'epoch': 0.83}
{'loss': 0.9382, 'grad_norm': 1.2871776819229126, 'learning_rate': 1.6147570450786387e-05, 'epoch': 0.83}
{'loss': 0.9636, 'grad_norm': 1.3793259859085083, 'learning_rate': 1.6118322006842757e-05, 'epoch': 0.84}
{'loss': 1.0146, 'grad_norm': 1.190679907798767, 'learning_rate': 1.608909775397106e-05, 'epoch': 0.84}
{'loss': 0.7768, 'grad_norm': 1.0195921659469604, 'learning_rate': 1.6059897700599413e-05, 'epoch': 0.84}
{'loss': 0.909, 'grad_norm': 1.0384092330932617, 'learning_rate': 1.6030721855148932e-05, 'epoch': 0.84}
{'loss': 0.8226, 'grad_norm': 1.433174729347229, 'learning_rate': 1.600157022603388e-05, 'epoch': 0.84}
{'loss': 0.7746, 'grad_norm': 0.9553965926170349, 'learning_rate': 1.5972442821661337e-05, 'epoch': 0.84}
{'loss': 0.9252, 'grad_norm': 1.4848213195800781, 'learning_rate': 1.5943339650431576e-05, 'epoch': 0.84}
{'loss': 1.0438, 'grad_norm': 1.1601330041885376, 'learning_rate': 1.5914260720737795e-05, 'epoch': 0.84}
{'loss': 0.9076, 'grad_norm': 1.1835110187530518, 'learning_rate': 1.5885206040966215e-05, 'epoch': 0.84}
{'loss': 0.6384, 'grad_norm': 1.042151927947998, 'learning_rate': 1.585617561949606e-05, 'epoch': 0.84}
{'loss': 0.9735, 'grad_norm': 1.0131117105484009, 'learning_rate': 1.5827169464699576e-05, 'epoch': 0.84}
{'loss': 0.9625, 'grad_norm': 1.0220052003860474, 'learning_rate': 1.579818758494197e-05, 'epoch': 0.84}
{'loss': 0.8969, 'grad_norm': 1.1822086572647095, 'learning_rate': 1.576922998858151e-05, 'epoch': 0.84}
{'loss': 0.8099, 'grad_norm': 1.056361436843872, 'learning_rate': 1.5740296683969423e-05, 'epoch': 0.84}
{'loss': 1.0646, 'grad_norm': 1.1361591815948486, 'learning_rate': 1.571138767944993e-05, 'epoch': 0.84}
{'loss': 0.9596, 'grad_norm': 1.2906253337860107, 'learning_rate': 1.5682502983360247e-05, 'epoch': 0.84}
{'loss': 0.9298, 'grad_norm': 0.9824836850166321, 'learning_rate': 1.565364260403055e-05, 'epoch': 0.84}
{'loss': 0.8765, 'grad_norm': 1.1008598804473877, 'learning_rate': 1.562480654978411e-05, 'epoch': 0.84}
{'loss': 1.0165, 'grad_norm': 1.2554075717926025, 'learning_rate': 1.559599482893701e-05, 'epoch': 0.84}
{'loss': 0.8905, 'grad_norm': 1.237209677696228, 'learning_rate': 1.5567207449798515e-05, 'epoch': 0.84}
{'loss': 1.1394, 'grad_norm': 1.032808780670166, 'learning_rate': 1.553844442067066e-05, 'epoch': 0.84}
{'loss': 0.8448, 'grad_norm': 1.0476714372634888, 'learning_rate': 1.5509705749848648e-05, 'epoch': 0.84}
{'loss': 0.9523, 'grad_norm': 0.9480348825454712, 'learning_rate': 1.5480991445620542e-05, 'epoch': 0.84}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.95, 'grad_norm': 1.1392297744750977, 'learning_rate': 1.5452301516267375e-05, 'epoch': 0.84}
{'loss': 0.8757, 'grad_norm': 1.2809062004089355, 'learning_rate': 1.5423635970063265e-05, 'epoch': 0.84}
{'loss': 0.7907, 'grad_norm': 0.9801517128944397, 'learning_rate': 1.5394994815275132e-05, 'epoch': 0.84}
{'loss': 0.7998, 'grad_norm': 1.1764743328094482, 'learning_rate': 1.536637806016301e-05, 'epoch': 0.84}
{'loss': 0.8732, 'grad_norm': 1.6119812726974487, 'learning_rate': 1.5337785712979802e-05, 'epoch': 0.84}
{'loss': 0.9775, 'grad_norm': 1.121838092803955, 'learning_rate': 1.530921778197142e-05, 'epoch': 0.84}
{'loss': 0.9418, 'grad_norm': 1.2334680557250977, 'learning_rate': 1.5280674275376693e-05, 'epoch': 0.84}
{'loss': 0.9755, 'grad_norm': 1.0727614164352417, 'learning_rate': 1.5252155201427453e-05, 'epoch': 0.84}
{'loss': 1.0386, 'grad_norm': 1.0039455890655518, 'learning_rate': 1.5223660568348442e-05, 'epoch': 0.84}
{'loss': 0.8571, 'grad_norm': 1.1847002506256104, 'learning_rate': 1.5195190384357404e-05, 'epoch': 0.84}
{'loss': 0.8498, 'grad_norm': 0.8891138434410095, 'learning_rate': 1.5166744657664988e-05, 'epoch': 0.84}
{'loss': 0.9922, 'grad_norm': 0.9896225333213806, 'learning_rate': 1.5138323396474808e-05, 'epoch': 0.84}
{'loss': 0.8852, 'grad_norm': 1.0830868482589722, 'learning_rate': 1.5109926608983416e-05, 'epoch': 0.84}
{'loss': 1.1343, 'grad_norm': 0.9838510751724243, 'learning_rate': 1.5081554303380285e-05, 'epoch': 0.84}
{'loss': 0.9854, 'grad_norm': 1.338606357574463, 'learning_rate': 1.5053206487847914e-05, 'epoch': 0.84}
{'loss': 1.1539, 'grad_norm': 1.118274211883545, 'learning_rate': 1.50248831705616e-05, 'epoch': 0.84}
{'loss': 1.0183, 'grad_norm': 1.1659822463989258, 'learning_rate': 1.4996584359689703e-05, 'epoch': 0.84}
{'loss': 0.9742, 'grad_norm': 1.1497806310653687, 'learning_rate': 1.4968310063393453e-05, 'epoch': 0.84}
{'loss': 0.9284, 'grad_norm': 1.292350172996521, 'learning_rate': 1.4940060289827017e-05, 'epoch': 0.84}
{'loss': 0.7126, 'grad_norm': 1.069235920906067, 'learning_rate': 1.4911835047137501e-05, 'epoch': 0.84}
{'loss': 0.8616, 'grad_norm': 1.0670286417007446, 'learning_rate': 1.4883634343464913e-05, 'epoch': 0.84}
{'loss': 0.8453, 'grad_norm': 1.3094713687896729, 'learning_rate': 1.4855458186942184e-05, 'epoch': 0.84}
{'loss': 0.9381, 'grad_norm': 1.0008317232131958, 'learning_rate': 1.4827306585695234e-05, 'epoch': 0.84}
{'loss': 0.9076, 'grad_norm': 0.9602997303009033, 'learning_rate': 1.4799179547842822e-05, 'epoch': 0.84}
{'loss': 1.0874, 'grad_norm': 1.2816437482833862, 'learning_rate': 1.4771077081496654e-05, 'epoch': 0.84}
{'loss': 0.9847, 'grad_norm': 4.48665189743042, 'learning_rate': 1.4742999194761342e-05, 'epoch': 0.84}
{'loss': 0.7723, 'grad_norm': 1.0361238718032837, 'learning_rate': 1.4714945895734377e-05, 'epoch': 0.84}
{'loss': 0.998, 'grad_norm': 1.1303601264953613, 'learning_rate': 1.4686917192506289e-05, 'epoch': 0.84}
{'loss': 0.9895, 'grad_norm': 1.280385971069336, 'learning_rate': 1.465891309316032e-05, 'epoch': 0.84}
{'loss': 0.9921, 'grad_norm': 1.2553757429122925, 'learning_rate': 1.4630933605772801e-05, 'epoch': 0.84}
{'loss': 1.0109, 'grad_norm': 1.0948352813720703, 'learning_rate': 1.4602978738412798e-05, 'epoch': 0.84}
{'loss': 0.8145, 'grad_norm': 1.2214505672454834, 'learning_rate': 1.457504849914242e-05, 'epoch': 0.84}
{'loss': 1.1139, 'grad_norm': 1.0728682279586792, 'learning_rate': 1.4547142896016608e-05, 'epoch': 0.84}
{'loss': 0.9064, 'grad_norm': 1.01600182056427, 'learning_rate': 1.4519261937083161e-05, 'epoch': 0.84}
{'loss': 0.8753, 'grad_norm': 1.2216635942459106, 'learning_rate': 1.4491405630382892e-05, 'epoch': 0.84}
{'loss': 0.8206, 'grad_norm': 1.0660419464111328, 'learning_rate': 1.4463573983949341e-05, 'epoch': 0.84}
{'loss': 0.935, 'grad_norm': 1.2352135181427002, 'learning_rate': 1.4435767005809075e-05, 'epoch': 0.84}
{'loss': 0.6789, 'grad_norm': 1.1259443759918213, 'learning_rate': 1.4407984703981469e-05, 'epoch': 0.84}
{'loss': 1.0994, 'grad_norm': 1.1219433546066284, 'learning_rate': 1.4380227086478815e-05, 'epoch': 0.84}
{'loss': 1.0114, 'grad_norm': 1.0131051540374756, 'learning_rate': 1.4352494161306285e-05, 'epoch': 0.84}
{'loss': 0.9008, 'grad_norm': 0.8711072206497192, 'learning_rate': 1.4324785936461892e-05, 'epoch': 0.84}
{'loss': 1.1028, 'grad_norm': 1.1282857656478882, 'learning_rate': 1.429710241993656e-05, 'epoch': 0.84}
{'loss': 0.8467, 'grad_norm': 1.4757483005523682, 'learning_rate': 1.4269443619714117e-05, 'epoch': 0.85}
{'loss': 0.8079, 'grad_norm': 1.1350454092025757, 'learning_rate': 1.42418095437712e-05, 'epoch': 0.85}
{'loss': 1.0356, 'grad_norm': 1.4977704286575317, 'learning_rate': 1.4214200200077343e-05, 'epoch': 0.85}
{'loss': 0.9849, 'grad_norm': 1.5650919675827026, 'learning_rate': 1.4186615596594966e-05, 'epoch': 0.85}
{'loss': 0.8508, 'grad_norm': 1.3227492570877075, 'learning_rate': 1.4159055741279281e-05, 'epoch': 0.85}
{'loss': 0.8822, 'grad_norm': 1.1154487133026123, 'learning_rate': 1.4131520642078521e-05, 'epoch': 0.85}
{'loss': 1.04, 'grad_norm': 1.4962468147277832, 'learning_rate': 1.4104010306933557e-05, 'epoch': 0.85}
{'loss': 1.0911, 'grad_norm': 1.1118125915527344, 'learning_rate': 1.4076524743778319e-05, 'epoch': 0.85}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0977, 'grad_norm': 0.9364922642707825, 'learning_rate': 1.4049063960539488e-05, 'epoch': 0.85}
{'loss': 0.7776, 'grad_norm': 1.1915264129638672, 'learning_rate': 1.4021627965136618e-05, 'epoch': 0.85}
{'loss': 0.8156, 'grad_norm': 1.2313278913497925, 'learning_rate': 1.3994216765482127e-05, 'epoch': 0.85}
{'loss': 1.1247, 'grad_norm': 1.1905715465545654, 'learning_rate': 1.3966830369481232e-05, 'epoch': 0.85}
{'loss': 1.075, 'grad_norm': 1.044653058052063, 'learning_rate': 1.3939468785032106e-05, 'epoch': 0.85}
{'loss': 0.9316, 'grad_norm': 1.2160742282867432, 'learning_rate': 1.3912132020025647e-05, 'epoch': 0.85}
{'loss': 0.8554, 'grad_norm': 1.0739761590957642, 'learning_rate': 1.3884820082345673e-05, 'epoch': 0.85}
{'loss': 0.9768, 'grad_norm': 1.0172216892242432, 'learning_rate': 1.3857532979868804e-05, 'epoch': 0.85}
{'loss': 0.9314, 'grad_norm': 1.3269166946411133, 'learning_rate': 1.383027072046451e-05, 'epoch': 0.85}
{'loss': 0.9135, 'grad_norm': 1.1329975128173828, 'learning_rate': 1.3803033311995072e-05, 'epoch': 0.85}
{'loss': 0.8129, 'grad_norm': 0.9622637033462524, 'learning_rate': 1.3775820762315694e-05, 'epoch': 0.85}
{'loss': 0.9551, 'grad_norm': 1.1052528619766235, 'learning_rate': 1.3748633079274253e-05, 'epoch': 0.85}
{'loss': 0.6676, 'grad_norm': 1.1339170932769775, 'learning_rate': 1.3721470270711623e-05, 'epoch': 0.85}
{'loss': 0.9522, 'grad_norm': 0.9441165924072266, 'learning_rate': 1.369433234446139e-05, 'epoch': 0.85}
{'loss': 0.6598, 'grad_norm': 1.011189579963684, 'learning_rate': 1.366721930835001e-05, 'epoch': 0.85}
{'loss': 1.0505, 'grad_norm': 1.1120635271072388, 'learning_rate': 1.3640131170196758e-05, 'epoch': 0.85}
{'loss': 0.9965, 'grad_norm': 0.8667186498641968, 'learning_rate': 1.3613067937813685e-05, 'epoch': 0.85}
{'loss': 0.7968, 'grad_norm': 1.0245940685272217, 'learning_rate': 1.358602961900578e-05, 'epoch': 0.85}
{'loss': 0.9855, 'grad_norm': 1.6785082817077637, 'learning_rate': 1.3559016221570663e-05, 'epoch': 0.85}
{'loss': 0.7369, 'grad_norm': 1.2567299604415894, 'learning_rate': 1.3532027753298937e-05, 'epoch': 0.85}
{'loss': 0.8064, 'grad_norm': 1.22374427318573, 'learning_rate': 1.3505064221973929e-05, 'epoch': 0.85}
{'loss': 1.0856, 'grad_norm': 1.1495625972747803, 'learning_rate': 1.3478125635371786e-05, 'epoch': 0.85}
{'loss': 0.9921, 'grad_norm': 1.0861942768096924, 'learning_rate': 1.345121200126146e-05, 'epoch': 0.85}
{'loss': 0.8983, 'grad_norm': 0.9999683499336243, 'learning_rate': 1.342432332740473e-05, 'epoch': 0.85}
{'loss': 0.7928, 'grad_norm': 0.9744926691055298, 'learning_rate': 1.339745962155613e-05, 'epoch': 0.85}
{'loss': 0.858, 'grad_norm': 1.031121015548706, 'learning_rate': 1.3370620891463071e-05, 'epoch': 0.85}
{'loss': 1.0688, 'grad_norm': 1.1367844343185425, 'learning_rate': 1.3343807144865683e-05, 'epoch': 0.85}
{'loss': 0.7519, 'grad_norm': 1.2168997526168823, 'learning_rate': 1.3317018389496927e-05, 'epoch': 0.85}
{'loss': 0.6965, 'grad_norm': 0.9712713956832886, 'learning_rate': 1.3290254633082555e-05, 'epoch': 0.85}
{'loss': 0.6333, 'grad_norm': 1.1055117845535278, 'learning_rate': 1.326351588334107e-05, 'epoch': 0.85}
{'loss': 1.192, 'grad_norm': 1.3369851112365723, 'learning_rate': 1.3236802147983873e-05, 'epoch': 0.85}
{'loss': 1.0261, 'grad_norm': 1.4302517175674438, 'learning_rate': 1.3210113434715e-05, 'epoch': 0.85}
{'loss': 0.9737, 'grad_norm': 1.186275601387024, 'learning_rate': 1.3183449751231392e-05, 'epoch': 0.85}
{'loss': 0.9212, 'grad_norm': 0.9650299549102783, 'learning_rate': 1.3156811105222721e-05, 'epoch': 0.85}
{'loss': 0.9183, 'grad_norm': 1.0488440990447998, 'learning_rate': 1.3130197504371434e-05, 'epoch': 0.85}
{'loss': 0.7069, 'grad_norm': 1.007550835609436, 'learning_rate': 1.3103608956352764e-05, 'epoch': 0.85}
{'loss': 1.1058, 'grad_norm': 0.9251585006713867, 'learning_rate': 1.3077045468834715e-05, 'epoch': 0.85}
{'loss': 1.0553, 'grad_norm': 1.1042897701263428, 'learning_rate': 1.30505070494781e-05, 'epoch': 0.85}
{'loss': 1.004, 'grad_norm': 0.9666301608085632, 'learning_rate': 1.3023993705936444e-05, 'epoch': 0.85}
{'loss': 1.0795, 'grad_norm': 1.007746934890747, 'learning_rate': 1.2997505445856084e-05, 'epoch': 0.85}
{'loss': 1.1356, 'grad_norm': 1.2243813276290894, 'learning_rate': 1.2971042276876088e-05, 'epoch': 0.85}
{'loss': 1.0395, 'grad_norm': 1.0129666328430176, 'learning_rate': 1.294460420662832e-05, 'epoch': 0.85}
{'loss': 0.8683, 'grad_norm': 1.205586552619934, 'learning_rate': 1.2918191242737366e-05, 'epoch': 0.85}
{'loss': 0.6689, 'grad_norm': 1.0877790451049805, 'learning_rate': 1.2891803392820668e-05, 'epoch': 0.85}
{'loss': 1.1078, 'grad_norm': 1.1766061782836914, 'learning_rate': 1.2865440664488248e-05, 'epoch': 0.85}
{'loss': 0.9682, 'grad_norm': 1.1445279121398926, 'learning_rate': 1.2839103065343083e-05, 'epoch': 0.85}
{'loss': 0.8456, 'grad_norm': 1.2824316024780273, 'learning_rate': 1.2812790602980773e-05, 'epoch': 0.85}
{'loss': 1.1053, 'grad_norm': 1.0968669652938843, 'learning_rate': 1.2786503284989704e-05, 'epoch': 0.85}
{'loss': 1.0232, 'grad_norm': 1.064012885093689, 'learning_rate': 1.2760241118951011e-05, 'epoch': 0.85}
{'loss': 0.9516, 'grad_norm': 1.0851874351501465, 'learning_rate': 1.2734004112438568e-05, 'epoch': 0.85}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8265, 'grad_norm': 1.3842190504074097, 'learning_rate': 1.2707792273019048e-05, 'epoch': 0.85}
{'loss': 1.064, 'grad_norm': 1.0709396600723267, 'learning_rate': 1.2681605608251745e-05, 'epoch': 0.85}
{'loss': 0.7334, 'grad_norm': 1.1040247678756714, 'learning_rate': 1.2655444125688832e-05, 'epoch': 0.85}
{'loss': 1.128, 'grad_norm': 1.112627387046814, 'learning_rate': 1.2629307832875126e-05, 'epoch': 0.85}
{'loss': 0.9928, 'grad_norm': 1.01871657371521, 'learning_rate': 1.260319673734821e-05, 'epoch': 0.85}
{'loss': 0.8833, 'grad_norm': 1.1374467611312866, 'learning_rate': 1.2577110846638407e-05, 'epoch': 0.85}
{'loss': 0.9166, 'grad_norm': 1.1036864519119263, 'learning_rate': 1.255105016826873e-05, 'epoch': 0.85}
{'loss': 0.9241, 'grad_norm': 1.41224205493927, 'learning_rate': 1.252501470975499e-05, 'epoch': 0.86}
{'loss': 1.3135, 'grad_norm': 1.1188501119613647, 'learning_rate': 1.249900447860568e-05, 'epoch': 0.86}
{'loss': 0.6892, 'grad_norm': 0.9654076099395752, 'learning_rate': 1.2473019482322023e-05, 'epoch': 0.86}
{'loss': 0.9454, 'grad_norm': 1.3000690937042236, 'learning_rate': 1.244705972839797e-05, 'epoch': 0.86}
{'loss': 0.955, 'grad_norm': 1.078080415725708, 'learning_rate': 1.2421125224320185e-05, 'epoch': 0.86}
{'loss': 0.7532, 'grad_norm': 1.2837867736816406, 'learning_rate': 1.239521597756802e-05, 'epoch': 0.86}
{'loss': 0.9882, 'grad_norm': 1.2933926582336426, 'learning_rate': 1.2369331995613665e-05, 'epoch': 0.86}
{'loss': 1.0011, 'grad_norm': 1.3002978563308716, 'learning_rate': 1.2343473285921825e-05, 'epoch': 0.86}
{'loss': 0.9589, 'grad_norm': 1.1841888427734375, 'learning_rate': 1.2317639855950114e-05, 'epoch': 0.86}
{'loss': 0.9295, 'grad_norm': 1.018803596496582, 'learning_rate': 1.2291831713148727e-05, 'epoch': 0.86}
{'loss': 1.1317, 'grad_norm': 1.1455274820327759, 'learning_rate': 1.2266048864960634e-05, 'epoch': 0.86}
{'loss': 0.9376, 'grad_norm': 0.9983971118927002, 'learning_rate': 1.2240291318821462e-05, 'epoch': 0.86}
{'loss': 0.995, 'grad_norm': 1.0427907705307007, 'learning_rate': 1.2214559082159537e-05, 'epoch': 0.86}
{'loss': 0.7645, 'grad_norm': 1.2569727897644043, 'learning_rate': 1.218885216239598e-05, 'epoch': 0.86}
{'loss': 1.0772, 'grad_norm': 1.0117390155792236, 'learning_rate': 1.2163170566944504e-05, 'epoch': 0.86}
{'loss': 0.8328, 'grad_norm': 0.9345887899398804, 'learning_rate': 1.2137514303211561e-05, 'epoch': 0.86}
{'loss': 0.842, 'grad_norm': 1.1146379709243774, 'learning_rate': 1.21118833785963e-05, 'epoch': 0.86}
{'loss': 0.9211, 'grad_norm': 1.0292786359786987, 'learning_rate': 1.2086277800490554e-05, 'epoch': 0.86}
{'loss': 0.866, 'grad_norm': 1.369214415550232, 'learning_rate': 1.2060697576278813e-05, 'epoch': 0.86}
{'loss': 0.9836, 'grad_norm': 1.0847140550613403, 'learning_rate': 1.2035142713338366e-05, 'epoch': 0.86}
{'loss': 0.7793, 'grad_norm': 1.2375417947769165, 'learning_rate': 1.2009613219039029e-05, 'epoch': 0.86}
{'loss': 0.9977, 'grad_norm': 1.2647558450698853, 'learning_rate': 1.1984109100743446e-05, 'epoch': 0.86}
{'loss': 1.0339, 'grad_norm': 1.2677897214889526, 'learning_rate': 1.1958630365806867e-05, 'epoch': 0.86}
{'loss': 0.909, 'grad_norm': 1.6895432472229004, 'learning_rate': 1.1933177021577202e-05, 'epoch': 0.86}
{'loss': 0.8509, 'grad_norm': 1.501187801361084, 'learning_rate': 1.1907749075395147e-05, 'epoch': 0.86}
{'loss': 1.009, 'grad_norm': 1.3389208316802979, 'learning_rate': 1.1882346534593902e-05, 'epoch': 0.86}
{'loss': 1.1785, 'grad_norm': 1.260840892791748, 'learning_rate': 1.1856969406499541e-05, 'epoch': 0.86}
{'loss': 0.7682, 'grad_norm': 1.2150589227676392, 'learning_rate': 1.1831617698430609e-05, 'epoch': 0.86}
{'loss': 0.9818, 'grad_norm': 1.0360817909240723, 'learning_rate': 1.180629141769849e-05, 'epoch': 0.86}
{'loss': 0.9965, 'grad_norm': 0.9683235287666321, 'learning_rate': 1.178099057160712e-05, 'epoch': 0.86}
{'loss': 1.0597, 'grad_norm': 1.3812744617462158, 'learning_rate': 1.1755715167453151e-05, 'epoch': 0.86}
{'loss': 1.1205, 'grad_norm': 1.077568769454956, 'learning_rate': 1.17304652125259e-05, 'epoch': 0.86}
{'loss': 1.1566, 'grad_norm': 1.325296401977539, 'learning_rate': 1.1705240714107302e-05, 'epoch': 0.86}
{'loss': 0.7416, 'grad_norm': 0.9772194623947144, 'learning_rate': 1.1680041679472021e-05, 'epoch': 0.86}
{'loss': 0.9097, 'grad_norm': 1.12653386592865, 'learning_rate': 1.165486811588732e-05, 'epoch': 0.86}
{'loss': 0.9573, 'grad_norm': 0.9615524411201477, 'learning_rate': 1.1629720030613122e-05, 'epoch': 0.86}
{'loss': 0.8514, 'grad_norm': 1.4074772596359253, 'learning_rate': 1.160459743090203e-05, 'epoch': 0.86}
{'loss': 1.1258, 'grad_norm': 1.273960828781128, 'learning_rate': 1.157950032399927e-05, 'epoch': 0.86}
{'loss': 1.0722, 'grad_norm': 1.1049537658691406, 'learning_rate': 1.1554428717142707e-05, 'epoch': 0.86}
{'loss': 0.8591, 'grad_norm': 1.2627135515213013, 'learning_rate': 1.1529382617562944e-05, 'epoch': 0.86}
{'loss': 0.9062, 'grad_norm': 0.9696463346481323, 'learning_rate': 1.1504362032483062e-05, 'epoch': 0.86}
{'loss': 0.7502, 'grad_norm': 1.3906785249710083, 'learning_rate': 1.1479366969118943e-05, 'epoch': 0.86}
{'loss': 0.6964, 'grad_norm': 1.152621865272522, 'learning_rate': 1.1454397434679021e-05, 'epoch': 0.86}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8398, 'grad_norm': 1.0861177444458008, 'learning_rate': 1.142945343636439e-05, 'epoch': 0.86}
{'loss': 0.8632, 'grad_norm': 1.253468632698059, 'learning_rate': 1.1404534981368775e-05, 'epoch': 0.86}
{'loss': 1.0348, 'grad_norm': 0.9776506423950195, 'learning_rate': 1.1379642076878527e-05, 'epoch': 0.86}
{'loss': 1.0251, 'grad_norm': 0.9217299222946167, 'learning_rate': 1.1354774730072692e-05, 'epoch': 0.86}
{'loss': 0.7602, 'grad_norm': 1.0449488162994385, 'learning_rate': 1.1329932948122823e-05, 'epoch': 0.86}
{'loss': 0.7313, 'grad_norm': 1.3429467678070068, 'learning_rate': 1.1305116738193212e-05, 'epoch': 0.86}
{'loss': 0.8703, 'grad_norm': 1.03490149974823, 'learning_rate': 1.1280326107440731e-05, 'epoch': 0.86}
{'loss': 0.8439, 'grad_norm': 1.0347952842712402, 'learning_rate': 1.1255561063014875e-05, 'epoch': 0.86}
{'loss': 0.8941, 'grad_norm': 1.2226530313491821, 'learning_rate': 1.1230821612057751e-05, 'epoch': 0.86}
{'loss': 0.6426, 'grad_norm': 1.0882768630981445, 'learning_rate': 1.1206107761704132e-05, 'epoch': 0.86}
{'loss': 1.0066, 'grad_norm': 1.5587867498397827, 'learning_rate': 1.1181419519081316e-05, 'epoch': 0.86}
{'loss': 0.8934, 'grad_norm': 1.1414587497711182, 'learning_rate': 1.1156756891309327e-05, 'epoch': 0.86}
{'loss': 1.1121, 'grad_norm': 1.1107560396194458, 'learning_rate': 1.1132119885500735e-05, 'epoch': 0.86}
{'loss': 0.8642, 'grad_norm': 1.0664732456207275, 'learning_rate': 1.11075085087607e-05, 'epoch': 0.86}
{'loss': 0.5728, 'grad_norm': 1.1989885568618774, 'learning_rate': 1.10829227681871e-05, 'epoch': 0.86}
{'loss': 1.3288, 'grad_norm': 0.9051955938339233, 'learning_rate': 1.1058362670870249e-05, 'epoch': 0.86}
{'loss': 1.1384, 'grad_norm': 0.9779497385025024, 'learning_rate': 1.1033828223893238e-05, 'epoch': 0.86}
{'loss': 0.869, 'grad_norm': 1.2385896444320679, 'learning_rate': 1.1009319434331622e-05, 'epoch': 0.86}
{'loss': 0.9227, 'grad_norm': 1.3788139820098877, 'learning_rate': 1.0984836309253666e-05, 'epoch': 0.86}
{'loss': 0.8976, 'grad_norm': 1.0646610260009766, 'learning_rate': 1.0960378855720177e-05, 'epoch': 0.86}
{'loss': 1.1508, 'grad_norm': 1.1864534616470337, 'learning_rate': 1.0935947080784548e-05, 'epoch': 0.86}
{'loss': 0.7327, 'grad_norm': 1.2318698167800903, 'learning_rate': 1.0911540991492797e-05, 'epoch': 0.86}
{'loss': 1.1063, 'grad_norm': 0.944301426410675, 'learning_rate': 1.0887160594883506e-05, 'epoch': 0.87}
{'loss': 1.0154, 'grad_norm': 1.1049984693527222, 'learning_rate': 1.0862805897987894e-05, 'epoch': 0.87}
{'loss': 0.9351, 'grad_norm': 1.1160430908203125, 'learning_rate': 1.083847690782972e-05, 'epoch': 0.87}
{'loss': 1.1587, 'grad_norm': 1.1023331880569458, 'learning_rate': 1.081417363142535e-05, 'epoch': 0.87}
{'loss': 0.8744, 'grad_norm': 1.0421167612075806, 'learning_rate': 1.0789896075783733e-05, 'epoch': 0.87}
{'loss': 0.8415, 'grad_norm': 1.1070926189422607, 'learning_rate': 1.0765644247906404e-05, 'epoch': 0.87}
{'loss': 0.839, 'grad_norm': 0.9431021213531494, 'learning_rate': 1.0741418154787442e-05, 'epoch': 0.87}
{'loss': 0.9214, 'grad_norm': 1.0147672891616821, 'learning_rate': 1.0717217803413604e-05, 'epoch': 0.87}
{'loss': 0.9848, 'grad_norm': 1.111709713935852, 'learning_rate': 1.0693043200764074e-05, 'epoch': 0.87}
{'loss': 1.194, 'grad_norm': 1.1080437898635864, 'learning_rate': 1.0668894353810743e-05, 'epoch': 0.87}
{'loss': 0.9347, 'grad_norm': 1.186486840248108, 'learning_rate': 1.0644771269518017e-05, 'epoch': 0.87}
{'loss': 0.5951, 'grad_norm': 1.15872323513031, 'learning_rate': 1.0620673954842841e-05, 'epoch': 0.87}
{'loss': 0.9553, 'grad_norm': 1.4534263610839844, 'learning_rate': 1.0596602416734835e-05, 'epoch': 0.87}
{'loss': 0.7125, 'grad_norm': 1.1535439491271973, 'learning_rate': 1.0572556662136035e-05, 'epoch': 0.87}
{'loss': 0.8281, 'grad_norm': 1.178917407989502, 'learning_rate': 1.0548536697981192e-05, 'epoch': 0.87}
{'loss': 0.6631, 'grad_norm': 1.047175407409668, 'learning_rate': 1.0524542531197467e-05, 'epoch': 0.87}
{'loss': 0.6772, 'grad_norm': 1.14658522605896, 'learning_rate': 1.0500574168704746e-05, 'epoch': 0.87}
{'loss': 0.9574, 'grad_norm': 1.4414010047912598, 'learning_rate': 1.0476631617415333e-05, 'epoch': 0.87}
{'loss': 1.0348, 'grad_norm': 1.1571919918060303, 'learning_rate': 1.045271488423417e-05, 'epoch': 0.87}
{'loss': 0.8493, 'grad_norm': 1.0404613018035889, 'learning_rate': 1.042882397605871e-05, 'epoch': 0.87}
{'loss': 0.9314, 'grad_norm': 1.3240790367126465, 'learning_rate': 1.0404958899778993e-05, 'epoch': 0.87}
{'loss': 1.2824, 'grad_norm': 1.1363184452056885, 'learning_rate': 1.0381119662277594e-05, 'epoch': 0.87}
{'loss': 0.8798, 'grad_norm': 0.917210042476654, 'learning_rate': 1.0357306270429624e-05, 'epoch': 0.87}
{'loss': 0.8207, 'grad_norm': 1.1073282957077026, 'learning_rate': 1.0333518731102754e-05, 'epoch': 0.87}
{'loss': 0.7336, 'grad_norm': 1.3719632625579834, 'learning_rate': 1.0309757051157176e-05, 'epoch': 0.87}
{'loss': 0.769, 'grad_norm': 1.4871748685836792, 'learning_rate': 1.0286021237445697e-05, 'epoch': 0.87}
{'loss': 1.0291, 'grad_norm': 1.153242588043213, 'learning_rate': 1.0262311296813543e-05, 'epoch': 0.87}
{'loss': 0.8628, 'grad_norm': 1.046321153640747, 'learning_rate': 1.0238627236098619e-05, 'epoch': 0.87}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9008, 'grad_norm': 1.1274430751800537, 'learning_rate': 1.0214969062131208e-05, 'epoch': 0.87}
{'loss': 0.6165, 'grad_norm': 1.0045967102050781, 'learning_rate': 1.0191336781734285e-05, 'epoch': 0.87}
{'loss': 0.8964, 'grad_norm': 1.3762818574905396, 'learning_rate': 1.0167730401723263e-05, 'epoch': 0.87}
{'loss': 1.0274, 'grad_norm': 1.18532395362854, 'learning_rate': 1.014414992890611e-05, 'epoch': 0.87}
{'loss': 0.937, 'grad_norm': 1.090800404548645, 'learning_rate': 1.0120595370083318e-05, 'epoch': 0.87}
{'loss': 0.8154, 'grad_norm': 1.1318212747573853, 'learning_rate': 1.0097066732047878e-05, 'epoch': 0.87}
{'loss': 0.8854, 'grad_norm': 0.9659507870674133, 'learning_rate': 1.007356402158539e-05, 'epoch': 0.87}
{'loss': 0.9552, 'grad_norm': 1.3771108388900757, 'learning_rate': 1.0050087245473906e-05, 'epoch': 0.87}
{'loss': 1.2314, 'grad_norm': 1.318886160850525, 'learning_rate': 1.0026636410484003e-05, 'epoch': 0.87}
{'loss': 0.9723, 'grad_norm': 1.2352715730667114, 'learning_rate': 1.0003211523378796e-05, 'epoch': 0.87}
{'loss': 1.4637, 'grad_norm': 1.3391395807266235, 'learning_rate': 9.979812590913906e-06, 'epoch': 0.87}
{'loss': 1.0559, 'grad_norm': 1.1481579542160034, 'learning_rate': 9.956439619837454e-06, 'epoch': 0.87}
{'loss': 0.8871, 'grad_norm': 1.0632703304290771, 'learning_rate': 9.93309261689015e-06, 'epoch': 0.87}
{'loss': 0.8216, 'grad_norm': 1.2424852848052979, 'learning_rate': 9.909771588805072e-06, 'epoch': 0.87}
{'loss': 0.9792, 'grad_norm': 1.0112192630767822, 'learning_rate': 9.886476542307966e-06, 'epoch': 0.87}
{'loss': 0.8244, 'grad_norm': 1.3628267049789429, 'learning_rate': 9.863207484116988e-06, 'epoch': 0.87}
{'loss': 1.1394, 'grad_norm': 0.8961866497993469, 'learning_rate': 9.839964420942793e-06, 'epoch': 0.87}
{'loss': 0.7769, 'grad_norm': 1.29092276096344, 'learning_rate': 9.816747359488632e-06, 'epoch': 0.87}
{'loss': 0.9289, 'grad_norm': 0.8974554538726807, 'learning_rate': 9.793556306450125e-06, 'epoch': 0.87}
{'loss': 0.9137, 'grad_norm': 1.083659291267395, 'learning_rate': 9.770391268515522e-06, 'epoch': 0.87}
{'loss': 0.7863, 'grad_norm': 1.443052887916565, 'learning_rate': 9.747252252365446e-06, 'epoch': 0.87}
{'loss': 0.8738, 'grad_norm': 1.1166558265686035, 'learning_rate': 9.724139264673116e-06, 'epoch': 0.87}
{'loss': 0.8091, 'grad_norm': 1.0696911811828613, 'learning_rate': 9.701052312104209e-06, 'epoch': 0.87}
{'loss': 1.2088, 'grad_norm': 1.115776777267456, 'learning_rate': 9.677991401316877e-06, 'epoch': 0.87}
{'loss': 0.9373, 'grad_norm': 1.0624147653579712, 'learning_rate': 9.65495653896179e-06, 'epoch': 0.87}
{'loss': 0.6704, 'grad_norm': 1.1892114877700806, 'learning_rate': 9.631947731682056e-06, 'epoch': 0.87}
{'loss': 1.0709, 'grad_norm': 1.8457365036010742, 'learning_rate': 9.608964986113345e-06, 'epoch': 0.87}
{'loss': 0.8873, 'grad_norm': 1.2529059648513794, 'learning_rate': 9.586008308883754e-06, 'epoch': 0.87}
{'loss': 0.9869, 'grad_norm': 1.2902923822402954, 'learning_rate': 9.563077706613877e-06, 'epoch': 0.87}
{'loss': 0.919, 'grad_norm': 1.052751064300537, 'learning_rate': 9.540173185916778e-06, 'epoch': 0.87}
{'loss': 1.0078, 'grad_norm': 0.8924198746681213, 'learning_rate': 9.517294753398064e-06, 'epoch': 0.87}
{'loss': 0.7287, 'grad_norm': 1.5727417469024658, 'learning_rate': 9.49444241565568e-06, 'epoch': 0.87}
{'loss': 1.0855, 'grad_norm': 1.4627320766448975, 'learning_rate': 9.471616179280207e-06, 'epoch': 0.87}
{'loss': 0.8176, 'grad_norm': 1.1477710008621216, 'learning_rate': 9.44881605085456e-06, 'epoch': 0.87}
{'loss': 1.1049, 'grad_norm': 1.0340397357940674, 'learning_rate': 9.42604203695423e-06, 'epoch': 0.87}
{'loss': 0.9241, 'grad_norm': 1.082902193069458, 'learning_rate': 9.403294144147124e-06, 'epoch': 0.87}
{'loss': 0.8826, 'grad_norm': 1.118237853050232, 'learning_rate': 9.38057237899359e-06, 'epoch': 0.87}
{'loss': 0.9067, 'grad_norm': 0.915457010269165, 'learning_rate': 9.357876748046546e-06, 'epoch': 0.88}
{'loss': 0.9346, 'grad_norm': 1.4296501874923706, 'learning_rate': 9.335207257851231e-06, 'epoch': 0.88}
{'loss': 1.0815, 'grad_norm': 1.0981693267822266, 'learning_rate': 9.31256391494546e-06, 'epoch': 0.88}
{'loss': 1.0385, 'grad_norm': 1.0320039987564087, 'learning_rate': 9.289946725859444e-06, 'epoch': 0.88}
{'loss': 1.0722, 'grad_norm': 1.071211814880371, 'learning_rate': 9.267355697115887e-06, 'epoch': 0.88}
{'loss': 1.1411, 'grad_norm': 1.0916167497634888, 'learning_rate': 9.244790835229922e-06, 'epoch': 0.88}
{'loss': 0.9944, 'grad_norm': 0.9456688761711121, 'learning_rate': 9.222252146709142e-06, 'epoch': 0.88}
{'loss': 1.0369, 'grad_norm': 1.0182095766067505, 'learning_rate': 9.199739638053595e-06, 'epoch': 0.88}
{'loss': 0.9888, 'grad_norm': 1.2155787944793701, 'learning_rate': 9.177253315755796e-06, 'epoch': 0.88}
{'loss': 0.8644, 'grad_norm': 1.103620171546936, 'learning_rate': 9.154793186300692e-06, 'epoch': 0.88}
{'loss': 0.7881, 'grad_norm': 0.9139638543128967, 'learning_rate': 9.132359256165667e-06, 'epoch': 0.88}
{'loss': 0.8259, 'grad_norm': 0.9941396713256836, 'learning_rate': 9.10995153182056e-06, 'epoch': 0.88}
{'loss': 1.0084, 'grad_norm': 1.0294257402420044, 'learning_rate': 9.08757001972762e-06, 'epoch': 0.88}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6616, 'grad_norm': 1.0405174493789673, 'learning_rate': 9.065214726341653e-06, 'epoch': 0.88}
{'loss': 1.1477, 'grad_norm': 1.8821876049041748, 'learning_rate': 9.042885658109712e-06, 'epoch': 0.88}
{'loss': 0.9566, 'grad_norm': 1.3245666027069092, 'learning_rate': 9.020582821471479e-06, 'epoch': 0.88}
{'loss': 0.8189, 'grad_norm': 1.176256775856018, 'learning_rate': 8.998306222858922e-06, 'epoch': 0.88}
{'loss': 0.8337, 'grad_norm': 1.084118127822876, 'learning_rate': 8.976055868696542e-06, 'epoch': 0.88}
{'loss': 0.7017, 'grad_norm': 1.270389199256897, 'learning_rate': 8.953831765401233e-06, 'epoch': 0.88}
{'loss': 0.9875, 'grad_norm': 1.3057222366333008, 'learning_rate': 8.931633919382298e-06, 'epoch': 0.88}
{'loss': 0.987, 'grad_norm': 1.0467497110366821, 'learning_rate': 8.909462337041507e-06, 'epoch': 0.88}
{'loss': 1.122, 'grad_norm': 1.3247467279434204, 'learning_rate': 8.887317024773012e-06, 'epoch': 0.88}
{'loss': 0.8268, 'grad_norm': 1.2647148370742798, 'learning_rate': 8.865197988963458e-06, 'epoch': 0.88}
{'loss': 1.0178, 'grad_norm': 1.0007351636886597, 'learning_rate': 8.843105235991833e-06, 'epoch': 0.88}
{'loss': 1.0244, 'grad_norm': 1.2631456851959229, 'learning_rate': 8.821038772229595e-06, 'epoch': 0.88}
{'loss': 0.6418, 'grad_norm': 1.8651763200759888, 'learning_rate': 8.798998604040609e-06, 'epoch': 0.88}
{'loss': 0.8811, 'grad_norm': 1.2304757833480835, 'learning_rate': 8.776984737781135e-06, 'epoch': 0.88}
{'loss': 0.7909, 'grad_norm': 1.0735645294189453, 'learning_rate': 8.754997179799874e-06, 'epoch': 0.88}
{'loss': 0.8358, 'grad_norm': 0.9797933101654053, 'learning_rate': 8.733035936437961e-06, 'epoch': 0.88}
{'loss': 0.9978, 'grad_norm': 1.0792412757873535, 'learning_rate': 8.711101014028856e-06, 'epoch': 0.88}
{'loss': 1.0366, 'grad_norm': 0.862453043460846, 'learning_rate': 8.689192418898529e-06, 'epoch': 0.88}
{'loss': 0.9241, 'grad_norm': 1.2086111307144165, 'learning_rate': 8.667310157365304e-06, 'epoch': 0.88}
{'loss': 0.616, 'grad_norm': 1.2159994840621948, 'learning_rate': 8.645454235739903e-06, 'epoch': 0.88}
{'loss': 0.7674, 'grad_norm': 1.2078964710235596, 'learning_rate': 8.623624660325514e-06, 'epoch': 0.88}
{'loss': 1.1223, 'grad_norm': 1.2701773643493652, 'learning_rate': 8.601821437417613e-06, 'epoch': 0.88}
{'loss': 0.8077, 'grad_norm': 1.1567047834396362, 'learning_rate': 8.580044573304202e-06, 'epoch': 0.88}
{'loss': 1.1067, 'grad_norm': 0.9654984474182129, 'learning_rate': 8.558294074265605e-06, 'epoch': 0.88}
{'loss': 0.9008, 'grad_norm': 1.0914784669876099, 'learning_rate': 8.536569946574546e-06, 'epoch': 0.88}
{'loss': 0.974, 'grad_norm': 1.0291131734848022, 'learning_rate': 8.514872196496183e-06, 'epoch': 0.88}
{'loss': 0.9094, 'grad_norm': 1.258496642112732, 'learning_rate': 8.493200830288028e-06, 'epoch': 0.88}
{'loss': 1.0499, 'grad_norm': 1.1944162845611572, 'learning_rate': 8.471555854199987e-06, 'epoch': 0.88}
{'loss': 0.9775, 'grad_norm': 1.1149647235870361, 'learning_rate': 8.449937274474396e-06, 'epoch': 0.88}
{'loss': 0.8127, 'grad_norm': 1.0791230201721191, 'learning_rate': 8.428345097345936e-06, 'epoch': 0.88}
{'loss': 1.0559, 'grad_norm': 1.0445079803466797, 'learning_rate': 8.406779329041681e-06, 'epoch': 0.88}
{'loss': 0.8134, 'grad_norm': 1.1495671272277832, 'learning_rate': 8.385239975781112e-06, 'epoch': 0.88}
{'loss': 0.8148, 'grad_norm': 1.1312823295593262, 'learning_rate': 8.363727043776038e-06, 'epoch': 0.88}
{'loss': 0.7806, 'grad_norm': 1.0329934358596802, 'learning_rate': 8.34224053923074e-06, 'epoch': 0.88}
{'loss': 1.0854, 'grad_norm': 1.0493295192718506, 'learning_rate': 8.32078046834176e-06, 'epoch': 0.88}
{'loss': 0.939, 'grad_norm': 1.1774898767471313, 'learning_rate': 8.29934683729814e-06, 'epoch': 0.88}
{'loss': 0.6429, 'grad_norm': 0.9455947279930115, 'learning_rate': 8.277939652281186e-06, 'epoch': 0.88}
{'loss': 0.9428, 'grad_norm': 1.126968264579773, 'learning_rate': 8.256558919464652e-06, 'epoch': 0.88}
{'loss': 1.0773, 'grad_norm': 1.1014002561569214, 'learning_rate': 8.235204645014639e-06, 'epoch': 0.88}
{'loss': 0.9678, 'grad_norm': 1.1667864322662354, 'learning_rate': 8.213876835089607e-06, 'epoch': 0.88}
{'loss': 1.0877, 'grad_norm': 1.2684959173202515, 'learning_rate': 8.192575495840404e-06, 'epoch': 0.88}
{'loss': 0.7842, 'grad_norm': 1.1308709383010864, 'learning_rate': 8.171300633410217e-06, 'epoch': 0.88}
{'loss': 1.0138, 'grad_norm': 1.1297675371170044, 'learning_rate': 8.150052253934637e-06, 'epoch': 0.88}
{'loss': 0.8871, 'grad_norm': 1.1873239278793335, 'learning_rate': 8.128830363541574e-06, 'epoch': 0.88}
{'loss': 1.1162, 'grad_norm': 1.1725084781646729, 'learning_rate': 8.107634968351342e-06, 'epoch': 0.88}
{'loss': 0.7105, 'grad_norm': 1.4463862180709839, 'learning_rate': 8.086466074476563e-06, 'epoch': 0.88}
{'loss': 1.2054, 'grad_norm': 1.023283839225769, 'learning_rate': 8.065323688022264e-06, 'epoch': 0.88}
{'loss': 0.8403, 'grad_norm': 1.0845190286636353, 'learning_rate': 8.044207815085791e-06, 'epoch': 0.88}
{'loss': 1.1521, 'grad_norm': 1.0817469358444214, 'learning_rate': 8.023118461756872e-06, 'epoch': 0.88}
{'loss': 0.9357, 'grad_norm': 1.9163209199905396, 'learning_rate': 8.002055634117578e-06, 'epoch': 0.88}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7596, 'grad_norm': 1.5563485622406006, 'learning_rate': 7.981019338242323e-06, 'epoch': 0.88}
{'loss': 0.8365, 'grad_norm': 1.0666062831878662, 'learning_rate': 7.96000958019788e-06, 'epoch': 0.88}
{'loss': 0.9271, 'grad_norm': 1.0374616384506226, 'learning_rate': 7.939026366043322e-06, 'epoch': 0.89}
{'loss': 1.105, 'grad_norm': 1.1218270063400269, 'learning_rate': 7.918069701830189e-06, 'epoch': 0.89}
{'loss': 1.0337, 'grad_norm': 1.3785018920898438, 'learning_rate': 7.897139593602188e-06, 'epoch': 0.89}
{'loss': 0.8166, 'grad_norm': 1.369474172592163, 'learning_rate': 7.876236047395525e-06, 'epoch': 0.89}
{'loss': 0.8653, 'grad_norm': 1.1208857297897339, 'learning_rate': 7.855359069238666e-06, 'epoch': 0.89}
{'loss': 0.789, 'grad_norm': 1.1996543407440186, 'learning_rate': 7.834508665152418e-06, 'epoch': 0.89}
{'loss': 0.5913, 'grad_norm': 1.4139082431793213, 'learning_rate': 7.81368484114996e-06, 'epoch': 0.89}
{'loss': 0.9038, 'grad_norm': 1.3911081552505493, 'learning_rate': 7.792887603236754e-06, 'epoch': 0.89}
{'loss': 0.9014, 'grad_norm': 0.9755268692970276, 'learning_rate': 7.772116957410636e-06, 'epoch': 0.89}
{'loss': 0.8307, 'grad_norm': 1.2649574279785156, 'learning_rate': 7.751372909661769e-06, 'epoch': 0.89}
{'loss': 1.127, 'grad_norm': 1.1189252138137817, 'learning_rate': 7.730655465972635e-06, 'epoch': 0.89}
{'loss': 1.1364, 'grad_norm': 1.0959372520446777, 'learning_rate': 7.709964632318034e-06, 'epoch': 0.89}
{'loss': 1.0408, 'grad_norm': 1.2358559370040894, 'learning_rate': 7.689300414665124e-06, 'epoch': 0.89}
{'loss': 0.6085, 'grad_norm': 1.0321849584579468, 'learning_rate': 7.668662818973315e-06, 'epoch': 0.89}
{'loss': 0.8714, 'grad_norm': 1.0921250581741333, 'learning_rate': 7.64805185119447e-06, 'epoch': 0.89}
{'loss': 1.0301, 'grad_norm': 1.082554817199707, 'learning_rate': 7.6274675172726125e-06, 'epoch': 0.89}
{'loss': 1.1357, 'grad_norm': 1.0668799877166748, 'learning_rate': 7.606909823144237e-06, 'epoch': 0.89}
{'loss': 0.6968, 'grad_norm': 1.0402872562408447, 'learning_rate': 7.586378774738012e-06, 'epoch': 0.89}
{'loss': 0.9253, 'grad_norm': 1.031248927116394, 'learning_rate': 7.565874377975046e-06, 'epoch': 0.89}
{'loss': 1.171, 'grad_norm': 1.1829475164413452, 'learning_rate': 7.545396638768698e-06, 'epoch': 0.89}
{'loss': 0.9753, 'grad_norm': 1.0721479654312134, 'learning_rate': 7.5249455630246215e-06, 'epoch': 0.89}
{'loss': 0.7749, 'grad_norm': 1.3276914358139038, 'learning_rate': 7.504521156640853e-06, 'epoch': 0.89}
{'loss': 0.7598, 'grad_norm': 1.1809120178222656, 'learning_rate': 7.4841234255076495e-06, 'epoch': 0.89}
{'loss': 0.8694, 'grad_norm': 1.0544520616531372, 'learning_rate': 7.463752375507649e-06, 'epoch': 0.89}
{'loss': 0.9672, 'grad_norm': 1.1644837856292725, 'learning_rate': 7.443408012515751e-06, 'epoch': 0.89}
{'loss': 0.913, 'grad_norm': 1.1903935670852661, 'learning_rate': 7.4230903423991745e-06, 'epoch': 0.89}
{'loss': 0.7007, 'grad_norm': 1.1542993783950806, 'learning_rate': 7.402799371017444e-06, 'epoch': 0.89}
{'loss': 1.0382, 'grad_norm': 1.2051048278808594, 'learning_rate': 7.382535104222366e-06, 'epoch': 0.89}
{'loss': 1.1288, 'grad_norm': 1.12295663356781, 'learning_rate': 7.362297547858044e-06, 'epoch': 0.89}
{'loss': 1.0207, 'grad_norm': 0.9874807000160217, 'learning_rate': 7.342086707760931e-06, 'epoch': 0.89}
{'loss': 0.9195, 'grad_norm': 1.0985281467437744, 'learning_rate': 7.321902589759711e-06, 'epoch': 0.89}
{'loss': 0.965, 'grad_norm': 1.1704386472702026, 'learning_rate': 7.301745199675392e-06, 'epoch': 0.89}
{'loss': 1.1014, 'grad_norm': 1.1248462200164795, 'learning_rate': 7.281614543321269e-06, 'epoch': 0.89}
{'loss': 1.2234, 'grad_norm': 1.1634794473648071, 'learning_rate': 7.26151062650291e-06, 'epoch': 0.89}
{'loss': 0.7494, 'grad_norm': 1.4055639505386353, 'learning_rate': 7.241433455018231e-06, 'epoch': 0.89}
{'loss': 0.9725, 'grad_norm': 1.1763688325881958, 'learning_rate': 7.221383034657326e-06, 'epoch': 0.89}
{'loss': 0.9004, 'grad_norm': 1.4906377792358398, 'learning_rate': 7.201359371202699e-06, 'epoch': 0.89}
{'loss': 0.9515, 'grad_norm': 1.0002660751342773, 'learning_rate': 7.1813624704290535e-06, 'epoch': 0.89}
{'loss': 0.9809, 'grad_norm': 1.2363111972808838, 'learning_rate': 7.161392338103401e-06, 'epoch': 0.89}
{'loss': 0.9306, 'grad_norm': 1.196730375289917, 'learning_rate': 7.141448979985032e-06, 'epoch': 0.89}
{'loss': 1.131, 'grad_norm': 1.0729948282241821, 'learning_rate': 7.1215324018255145e-06, 'epoch': 0.89}
{'loss': 0.8092, 'grad_norm': 1.1101069450378418, 'learning_rate': 7.101642609368675e-06, 'epoch': 0.89}
{'loss': 0.9588, 'grad_norm': 1.4090616703033447, 'learning_rate': 7.08177960835068e-06, 'epoch': 0.89}
{'loss': 0.8829, 'grad_norm': 1.3749395608901978, 'learning_rate': 7.061943404499882e-06, 'epoch': 0.89}
{'loss': 0.8363, 'grad_norm': 1.71791672706604, 'learning_rate': 7.042134003536971e-06, 'epoch': 0.89}
{'loss': 1.1049, 'grad_norm': 0.9834370613098145, 'learning_rate': 7.022351411174866e-06, 'epoch': 0.89}
{'loss': 0.9868, 'grad_norm': 2.0087783336639404, 'learning_rate': 7.0025956331187695e-06, 'epoch': 0.89}
{'loss': 0.9463, 'grad_norm': 1.253505825996399, 'learning_rate': 6.9828666750661795e-06, 'epoch': 0.89}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9172, 'grad_norm': 1.1349806785583496, 'learning_rate': 6.9631645427067906e-06, 'epoch': 0.89}
{'loss': 0.8966, 'grad_norm': 1.1314074993133545, 'learning_rate': 6.943489241722645e-06, 'epoch': 0.89}
{'loss': 0.9675, 'grad_norm': 1.2199923992156982, 'learning_rate': 6.923840777787982e-06, 'epoch': 0.89}
{'loss': 0.9843, 'grad_norm': 1.4733397960662842, 'learning_rate': 6.904219156569325e-06, 'epoch': 0.89}
{'loss': 0.9793, 'grad_norm': 1.2327923774719238, 'learning_rate': 6.884624383725469e-06, 'epoch': 0.89}
{'loss': 0.8067, 'grad_norm': 1.119202733039856, 'learning_rate': 6.8650564649074156e-06, 'epoch': 0.89}
{'loss': 1.006, 'grad_norm': 1.1168791055679321, 'learning_rate': 6.845515405758518e-06, 'epoch': 0.89}
{'loss': 0.8909, 'grad_norm': 0.9798017740249634, 'learning_rate': 6.826001211914257e-06, 'epoch': 0.89}
{'loss': 1.0019, 'grad_norm': 1.2897074222564697, 'learning_rate': 6.806513889002486e-06, 'epoch': 0.89}
{'loss': 0.834, 'grad_norm': 1.1952133178710938, 'learning_rate': 6.787053442643232e-06, 'epoch': 0.89}
{'loss': 0.8587, 'grad_norm': 1.1153032779693604, 'learning_rate': 6.767619878448783e-06, 'epoch': 0.89}
{'loss': 0.9343, 'grad_norm': 1.1168298721313477, 'learning_rate': 6.748213202023712e-06, 'epoch': 0.89}
{'loss': 1.1379, 'grad_norm': 0.9712492227554321, 'learning_rate': 6.728833418964797e-06, 'epoch': 0.89}
{'loss': 0.6768, 'grad_norm': 1.3847501277923584, 'learning_rate': 6.709480534861046e-06, 'epoch': 0.89}
{'loss': 0.8879, 'grad_norm': 1.0043257474899292, 'learning_rate': 6.690154555293793e-06, 'epoch': 0.89}
{'loss': 0.815, 'grad_norm': 1.36334228515625, 'learning_rate': 6.670855485836525e-06, 'epoch': 0.89}
{'loss': 0.8105, 'grad_norm': 1.2394229173660278, 'learning_rate': 6.6515833320550115e-06, 'epoch': 0.89}
{'loss': 0.9653, 'grad_norm': 1.341386079788208, 'learning_rate': 6.63233809950724e-06, 'epoch': 0.9}
{'loss': 0.9804, 'grad_norm': 1.1801445484161377, 'learning_rate': 6.613119793743428e-06, 'epoch': 0.9}
{'loss': 0.9584, 'grad_norm': 1.2101496458053589, 'learning_rate': 6.593928420306084e-06, 'epoch': 0.9}
{'loss': 1.1392, 'grad_norm': 1.3386101722717285, 'learning_rate': 6.5747639847298594e-06, 'epoch': 0.9}
{'loss': 0.7603, 'grad_norm': 1.3472501039505005, 'learning_rate': 6.555626492541744e-06, 'epoch': 0.9}
{'loss': 0.7174, 'grad_norm': 1.4277613162994385, 'learning_rate': 6.5365159492608355e-06, 'epoch': 0.9}
{'loss': 0.7693, 'grad_norm': 1.270613193511963, 'learning_rate': 6.517432360398556e-06, 'epoch': 0.9}
{'loss': 0.7254, 'grad_norm': 1.4253993034362793, 'learning_rate': 6.498375731458528e-06, 'epoch': 0.9}
{'loss': 0.9163, 'grad_norm': 1.1267507076263428, 'learning_rate': 6.479346067936554e-06, 'epoch': 0.9}
{'loss': 0.99, 'grad_norm': 1.4880824089050293, 'learning_rate': 6.4603433753207435e-06, 'epoch': 0.9}
{'loss': 0.9462, 'grad_norm': 1.1861830949783325, 'learning_rate': 6.441367659091357e-06, 'epoch': 0.9}
{'loss': 1.3625, 'grad_norm': 1.1406469345092773, 'learning_rate': 6.422418924720907e-06, 'epoch': 0.9}
{'loss': 0.7413, 'grad_norm': 1.1678283214569092, 'learning_rate': 6.403497177674111e-06, 'epoch': 0.9}
{'loss': 0.7895, 'grad_norm': 1.0606322288513184, 'learning_rate': 6.384602423407904e-06, 'epoch': 0.9}
{'loss': 1.1581, 'grad_norm': 0.9085732102394104, 'learning_rate': 6.365734667371448e-06, 'epoch': 0.9}
{'loss': 0.6304, 'grad_norm': 1.1825929880142212, 'learning_rate': 6.346893915006136e-06, 'epoch': 0.9}
{'loss': 0.9276, 'grad_norm': 1.0901787281036377, 'learning_rate': 6.32808017174551e-06, 'epoch': 0.9}
{'loss': 0.9195, 'grad_norm': 1.0133849382400513, 'learning_rate': 6.309293443015385e-06, 'epoch': 0.9}
{'loss': 0.9769, 'grad_norm': 1.109724998474121, 'learning_rate': 6.2905337342337615e-06, 'epoch': 0.9}
{'loss': 0.7955, 'grad_norm': 1.092162847518921, 'learning_rate': 6.2718010508108545e-06, 'epoch': 0.9}
{'loss': 1.087, 'grad_norm': 1.1792330741882324, 'learning_rate': 6.253095398149067e-06, 'epoch': 0.9}
{'loss': 1.0038, 'grad_norm': 1.1262089014053345, 'learning_rate': 6.2344167816430155e-06, 'epoch': 0.9}
{'loss': 0.9253, 'grad_norm': 1.5135310888290405, 'learning_rate': 6.215765206679569e-06, 'epoch': 0.9}
{'loss': 0.8092, 'grad_norm': 1.0583082437515259, 'learning_rate': 6.197140678637669e-06, 'epoch': 0.9}
{'loss': 0.7601, 'grad_norm': 1.2962815761566162, 'learning_rate': 6.178543202888621e-06, 'epoch': 0.9}
{'loss': 0.9711, 'grad_norm': 1.1249234676361084, 'learning_rate': 6.1599727847957975e-06, 'epoch': 0.9}
{'loss': 0.9268, 'grad_norm': 0.9991788864135742, 'learning_rate': 6.14142942971484e-06, 'epoch': 0.9}
{'loss': 1.0001, 'grad_norm': 1.1766413450241089, 'learning_rate': 6.1229131429935475e-06, 'epoch': 0.9}
{'loss': 0.8128, 'grad_norm': 1.111130952835083, 'learning_rate': 6.104423929971948e-06, 'epoch': 0.9}
{'loss': 0.7614, 'grad_norm': 1.2852048873901367, 'learning_rate': 6.085961795982209e-06, 'epoch': 0.9}
{'loss': 0.9077, 'grad_norm': 1.1036593914031982, 'learning_rate': 6.067526746348751e-06, 'epoch': 0.9}
{'loss': 0.9912, 'grad_norm': 1.3953319787979126, 'learning_rate': 6.049118786388152e-06, 'epoch': 0.9}
{'loss': 1.0017, 'grad_norm': 1.2736899852752686, 'learning_rate': 6.030737921409169e-06, 'epoch': 0.9}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0939, 'grad_norm': 1.329535722732544, 'learning_rate': 6.012384156712758e-06, 'epoch': 0.9}
{'loss': 0.8383, 'grad_norm': 1.013991355895996, 'learning_rate': 5.994057497592031e-06, 'epoch': 0.9}
{'loss': 1.1717, 'grad_norm': 1.0893869400024414, 'learning_rate': 5.975757949332373e-06, 'epoch': 0.9}
{'loss': 1.0946, 'grad_norm': 1.0600929260253906, 'learning_rate': 5.9574855172112145e-06, 'epoch': 0.9}
{'loss': 0.987, 'grad_norm': 1.0143895149230957, 'learning_rate': 5.939240206498287e-06, 'epoch': 0.9}
{'loss': 1.0637, 'grad_norm': 1.0272259712219238, 'learning_rate': 5.921022022455424e-06, 'epoch': 0.9}
{'loss': 0.9314, 'grad_norm': 1.4764875173568726, 'learning_rate': 5.90283097033667e-06, 'epoch': 0.9}
{'loss': 0.9481, 'grad_norm': 1.1463009119033813, 'learning_rate': 5.884667055388227e-06, 'epoch': 0.9}
{'loss': 1.0263, 'grad_norm': 1.2982341051101685, 'learning_rate': 5.866530282848492e-06, 'epoch': 0.9}
{'loss': 0.9073, 'grad_norm': 1.2813595533370972, 'learning_rate': 5.8484206579480396e-06, 'epoch': 0.9}
{'loss': 0.9243, 'grad_norm': 0.9577407240867615, 'learning_rate': 5.8303381859095455e-06, 'epoch': 0.9}
{'loss': 1.0071, 'grad_norm': 1.2415581941604614, 'learning_rate': 5.812282871947961e-06, 'epoch': 0.9}
{'loss': 0.9896, 'grad_norm': 1.2759599685668945, 'learning_rate': 5.7942547212703315e-06, 'epoch': 0.9}
{'loss': 0.9233, 'grad_norm': 1.1002733707427979, 'learning_rate': 5.7762537390758875e-06, 'epoch': 0.9}
{'loss': 0.9627, 'grad_norm': 1.3850866556167603, 'learning_rate': 5.758279930556021e-06, 'epoch': 0.9}
{'loss': 0.7844, 'grad_norm': 0.9505792856216431, 'learning_rate': 5.7403333008943186e-06, 'epoch': 0.9}
{'loss': 0.9123, 'grad_norm': 1.235578179359436, 'learning_rate': 5.722413855266451e-06, 'epoch': 0.9}
{'loss': 0.625, 'grad_norm': 1.0750954151153564, 'learning_rate': 5.70452159884034e-06, 'epoch': 0.9}
{'loss': 1.0358, 'grad_norm': 1.269134283065796, 'learning_rate': 5.686656536776025e-06, 'epoch': 0.9}
{'loss': 0.6296, 'grad_norm': 1.0850261449813843, 'learning_rate': 5.668818674225685e-06, 'epoch': 0.9}
{'loss': 0.9287, 'grad_norm': 1.1948461532592773, 'learning_rate': 5.651008016333703e-06, 'epoch': 0.9}
{'loss': 1.1172, 'grad_norm': 0.984585702419281, 'learning_rate': 5.633224568236539e-06, 'epoch': 0.9}
{'loss': 1.0417, 'grad_norm': 1.1842820644378662, 'learning_rate': 5.615468335062912e-06, 'epoch': 0.9}
{'loss': 0.9257, 'grad_norm': 1.0038281679153442, 'learning_rate': 5.5977393219335815e-06, 'epoch': 0.9}
{'loss': 0.8714, 'grad_norm': 1.333398461341858, 'learning_rate': 5.580037533961546e-06, 'epoch': 0.9}
{'loss': 0.9639, 'grad_norm': 1.0463571548461914, 'learning_rate': 5.562362976251901e-06, 'epoch': 0.9}
{'loss': 0.9724, 'grad_norm': 1.3709498643875122, 'learning_rate': 5.5447156539019e-06, 'epoch': 0.9}
{'loss': 0.9593, 'grad_norm': 1.0063066482543945, 'learning_rate': 5.527095572000962e-06, 'epoch': 0.9}
{'loss': 1.0404, 'grad_norm': 1.1796315908432007, 'learning_rate': 5.509502735630601e-06, 'epoch': 0.9}
{'loss': 0.8381, 'grad_norm': 1.1626935005187988, 'learning_rate': 5.491937149864545e-06, 'epoch': 0.9}
{'loss': 1.0303, 'grad_norm': 0.9970130324363708, 'learning_rate': 5.4743988197686e-06, 'epoch': 0.9}
{'loss': 0.8905, 'grad_norm': 1.02409827709198, 'learning_rate': 5.456887750400752e-06, 'epoch': 0.9}
{'loss': 0.6707, 'grad_norm': 0.8439059257507324, 'learning_rate': 5.439403946811095e-06, 'epoch': 0.91}
{'loss': 0.6586, 'grad_norm': 1.0647387504577637, 'learning_rate': 5.421947414041883e-06, 'epoch': 0.91}
{'loss': 0.6561, 'grad_norm': 1.1254000663757324, 'learning_rate': 5.40451815712748e-06, 'epoch': 0.91}
{'loss': 0.853, 'grad_norm': 1.1893829107284546, 'learning_rate': 5.387116181094431e-06, 'epoch': 0.91}
{'loss': 0.5973, 'grad_norm': 0.9019837975502014, 'learning_rate': 5.369741490961344e-06, 'epoch': 0.91}
{'loss': 0.91, 'grad_norm': 1.4916577339172363, 'learning_rate': 5.3523940917390215e-06, 'epoch': 0.91}
{'loss': 1.0705, 'grad_norm': 1.1166749000549316, 'learning_rate': 5.335073988430372e-06, 'epoch': 0.91}
{'loss': 0.6584, 'grad_norm': 1.0440096855163574, 'learning_rate': 5.317781186030413e-06, 'epoch': 0.91}
{'loss': 1.101, 'grad_norm': 1.2664715051651, 'learning_rate': 5.30051568952632e-06, 'epoch': 0.91}
{'loss': 0.7775, 'grad_norm': 1.008286476135254, 'learning_rate': 5.2832775038973544e-06, 'epoch': 0.91}
{'loss': 0.9544, 'grad_norm': 1.0332372188568115, 'learning_rate': 5.266066634114975e-06, 'epoch': 0.91}
{'loss': 0.796, 'grad_norm': 0.9533077478408813, 'learning_rate': 5.248883085142653e-06, 'epoch': 0.91}
{'loss': 1.1492, 'grad_norm': 1.0078941583633423, 'learning_rate': 5.231726861936082e-06, 'epoch': 0.91}
{'loss': 0.911, 'grad_norm': 1.4167685508728027, 'learning_rate': 5.214597969443025e-06, 'epoch': 0.91}
{'loss': 0.7413, 'grad_norm': 1.191422700881958, 'learning_rate': 5.197496412603364e-06, 'epoch': 0.91}
{'loss': 0.9986, 'grad_norm': 1.1839452981948853, 'learning_rate': 5.180422196349111e-06, 'epoch': 0.91}
{'loss': 1.014, 'grad_norm': 1.0594958066940308, 'learning_rate': 5.163375325604414e-06, 'epoch': 0.91}
{'loss': 0.6625, 'grad_norm': 1.1947598457336426, 'learning_rate': 5.146355805285452e-06, 'epoch': 0.91}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1907, 'grad_norm': 1.0097781419754028, 'learning_rate': 5.129363640300611e-06, 'epoch': 0.91}
{'loss': 0.9988, 'grad_norm': 1.6377395391464233, 'learning_rate': 5.1123988355503475e-06, 'epoch': 0.91}
{'loss': 1.0313, 'grad_norm': 0.9868049621582031, 'learning_rate': 5.095461395927214e-06, 'epoch': 0.91}
{'loss': 0.9918, 'grad_norm': 1.2050586938858032, 'learning_rate': 5.078551326315917e-06, 'epoch': 0.91}
{'loss': 0.6165, 'grad_norm': 1.1222035884857178, 'learning_rate': 5.061668631593197e-06, 'epoch': 0.91}
{'loss': 0.8551, 'grad_norm': 1.2261884212493896, 'learning_rate': 5.0448133166279944e-06, 'epoch': 0.91}
{'loss': 1.2297, 'grad_norm': 1.071529507637024, 'learning_rate': 5.027985386281231e-06, 'epoch': 0.91}
{'loss': 0.6872, 'grad_norm': 1.0890847444534302, 'learning_rate': 5.01118484540607e-06, 'epoch': 0.91}
{'loss': 0.9781, 'grad_norm': 1.037010908126831, 'learning_rate': 4.994411698847667e-06, 'epoch': 0.91}
{'loss': 1.2066, 'grad_norm': 1.2237383127212524, 'learning_rate': 4.977665951443322e-06, 'epoch': 0.91}
{'loss': 1.1007, 'grad_norm': 0.9621210694313049, 'learning_rate': 4.960947608022437e-06, 'epoch': 0.91}
{'loss': 0.9666, 'grad_norm': 1.1855125427246094, 'learning_rate': 4.944256673406478e-06, 'epoch': 0.91}
{'loss': 1.0799, 'grad_norm': 0.9464963674545288, 'learning_rate': 4.927593152409071e-06, 'epoch': 0.91}
{'loss': 0.6171, 'grad_norm': 1.0081887245178223, 'learning_rate': 4.910957049835863e-06, 'epoch': 0.91}
{'loss': 1.0115, 'grad_norm': 1.2057324647903442, 'learning_rate': 4.8943483704846475e-06, 'epoch': 0.91}
{'loss': 0.7553, 'grad_norm': 1.1133995056152344, 'learning_rate': 4.877767119145271e-06, 'epoch': 0.91}
{'loss': 0.7283, 'grad_norm': 1.1563084125518799, 'learning_rate': 4.861213300599687e-06, 'epoch': 0.91}
{'loss': 1.1363, 'grad_norm': 1.3349415063858032, 'learning_rate': 4.844686919621932e-06, 'epoch': 0.91}
{'loss': 0.814, 'grad_norm': 1.1498403549194336, 'learning_rate': 4.828187980978171e-06, 'epoch': 0.91}
{'loss': 0.7258, 'grad_norm': 1.089647889137268, 'learning_rate': 4.811716489426566e-06, 'epoch': 0.91}
{'loss': 0.9247, 'grad_norm': 1.4953835010528564, 'learning_rate': 4.795272449717458e-06, 'epoch': 0.91}
{'loss': 0.9458, 'grad_norm': 0.998211681842804, 'learning_rate': 4.778855866593202e-06, 'epoch': 0.91}
{'loss': 0.9909, 'grad_norm': 1.1728817224502563, 'learning_rate': 4.7624667447882855e-06, 'epoch': 0.91}
{'loss': 1.0027, 'grad_norm': 1.2514876127243042, 'learning_rate': 4.746105089029229e-06, 'epoch': 0.91}
{'loss': 0.9827, 'grad_norm': 1.2350467443466187, 'learning_rate': 4.729770904034647e-06, 'epoch': 0.91}
{'loss': 0.7015, 'grad_norm': 0.9457681775093079, 'learning_rate': 4.7134641945152935e-06, 'epoch': 0.91}
{'loss': 0.826, 'grad_norm': 1.0010372400283813, 'learning_rate': 4.697184965173884e-06, 'epoch': 0.91}
{'loss': 1.0365, 'grad_norm': 1.2392067909240723, 'learning_rate': 4.680933220705308e-06, 'epoch': 0.91}
{'loss': 0.6091, 'grad_norm': 1.0153437852859497, 'learning_rate': 4.664708965796472e-06, 'epoch': 0.91}
{'loss': 0.78, 'grad_norm': 1.1298993825912476, 'learning_rate': 4.648512205126376e-06, 'epoch': 0.91}
{'loss': 0.9599, 'grad_norm': 0.9741724729537964, 'learning_rate': 4.632342943366097e-06, 'epoch': 0.91}
{'loss': 0.9738, 'grad_norm': 1.3691821098327637, 'learning_rate': 4.616201185178748e-06, 'epoch': 0.91}
{'loss': 0.9196, 'grad_norm': 1.1540749073028564, 'learning_rate': 4.600086935219561e-06, 'epoch': 0.91}
{'loss': 0.7462, 'grad_norm': 1.2577438354492188, 'learning_rate': 4.584000198135807e-06, 'epoch': 0.91}
{'loss': 0.7528, 'grad_norm': 1.0550357103347778, 'learning_rate': 4.567940978566809e-06, 'epoch': 0.91}
{'loss': 1.0113, 'grad_norm': 1.280421495437622, 'learning_rate': 4.551909281143962e-06, 'epoch': 0.91}
{'loss': 0.68, 'grad_norm': 0.99638831615448, 'learning_rate': 4.535905110490779e-06, 'epoch': 0.91}
{'loss': 1.121, 'grad_norm': 1.16923987865448, 'learning_rate': 4.519928471222712e-06, 'epoch': 0.91}
{'loss': 0.8975, 'grad_norm': 1.0600003004074097, 'learning_rate': 4.50397936794742e-06, 'epoch': 0.91}
{'loss': 0.9128, 'grad_norm': 0.9908778071403503, 'learning_rate': 4.488057805264478e-06, 'epoch': 0.91}
{'loss': 0.9112, 'grad_norm': 0.9698162078857422, 'learning_rate': 4.4721637877656375e-06, 'epoch': 0.91}
{'loss': 0.9725, 'grad_norm': 1.0418727397918701, 'learning_rate': 4.4562973200346416e-06, 'epoch': 0.91}
{'loss': 0.6231, 'grad_norm': 0.9685057997703552, 'learning_rate': 4.440458406647297e-06, 'epoch': 0.91}
{'loss': 1.0502, 'grad_norm': 1.0312386751174927, 'learning_rate': 4.424647052171483e-06, 'epoch': 0.91}
{'loss': 0.9919, 'grad_norm': 1.3038913011550903, 'learning_rate': 4.408863261167096e-06, 'epoch': 0.91}
{'loss': 1.0347, 'grad_norm': 0.9947027564048767, 'learning_rate': 4.3931070381861396e-06, 'epoch': 0.91}
{'loss': 0.8406, 'grad_norm': 1.0938186645507812, 'learning_rate': 4.377378387772602e-06, 'epoch': 0.91}
{'loss': 0.8242, 'grad_norm': 1.225302815437317, 'learning_rate': 4.3616773144625644e-06, 'epoch': 0.92}
{'loss': 0.9468, 'grad_norm': 1.1680651903152466, 'learning_rate': 4.346003822784139e-06, 'epoch': 0.92}
{'loss': 0.9418, 'grad_norm': 1.399591326713562, 'learning_rate': 4.3303579172574885e-06, 'epoch': 0.92}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7379, 'grad_norm': 1.2142618894577026, 'learning_rate': 4.314739602394791e-06, 'epoch': 0.92}
{'loss': 0.8408, 'grad_norm': 1.0718038082122803, 'learning_rate': 4.299148882700355e-06, 'epoch': 0.92}
{'loss': 0.9117, 'grad_norm': 1.0704833269119263, 'learning_rate': 4.283585762670383e-06, 'epoch': 0.92}
{'loss': 0.9053, 'grad_norm': 1.1423587799072266, 'learning_rate': 4.268050246793276e-06, 'epoch': 0.92}
{'loss': 0.9198, 'grad_norm': 1.0136884450912476, 'learning_rate': 4.25254233954937e-06, 'epoch': 0.92}
{'loss': 1.0989, 'grad_norm': 1.137961745262146, 'learning_rate': 4.237062045411067e-06, 'epoch': 0.92}
{'loss': 0.9728, 'grad_norm': 1.1089732646942139, 'learning_rate': 4.221609368842838e-06, 'epoch': 0.92}
{'loss': 1.2095, 'grad_norm': 1.0771048069000244, 'learning_rate': 4.20618431430112e-06, 'epoch': 0.92}
{'loss': 1.082, 'grad_norm': 1.3579492568969727, 'learning_rate': 4.190786886234465e-06, 'epoch': 0.92}
{'loss': 0.863, 'grad_norm': 1.2727571725845337, 'learning_rate': 4.175417089083378e-06, 'epoch': 0.92}
{'loss': 1.0287, 'grad_norm': 1.0690382719039917, 'learning_rate': 4.1600749272804666e-06, 'epoch': 0.92}
{'loss': 0.8874, 'grad_norm': 1.19107985496521, 'learning_rate': 4.144760405250325e-06, 'epoch': 0.92}
{'loss': 1.2272, 'grad_norm': 1.2357261180877686, 'learning_rate': 4.129473527409577e-06, 'epoch': 0.92}
{'loss': 1.067, 'grad_norm': 1.1779696941375732, 'learning_rate': 4.114214298166896e-06, 'epoch': 0.92}
{'loss': 0.9177, 'grad_norm': 0.9985581040382385, 'learning_rate': 4.098982721922961e-06, 'epoch': 0.92}
{'loss': 0.9714, 'grad_norm': 1.5834261178970337, 'learning_rate': 4.083778803070504e-06, 'epoch': 0.92}
{'loss': 1.0718, 'grad_norm': 1.0520730018615723, 'learning_rate': 4.068602545994249e-06, 'epoch': 0.92}
{'loss': 0.7487, 'grad_norm': 0.9466564655303955, 'learning_rate': 4.053453955070952e-06, 'epoch': 0.92}
{'loss': 0.9084, 'grad_norm': 1.1481496095657349, 'learning_rate': 4.038333034669406e-06, 'epoch': 0.92}
{'loss': 1.0305, 'grad_norm': 1.2501271963119507, 'learning_rate': 4.023239789150402e-06, 'epoch': 0.92}
{'loss': 0.8802, 'grad_norm': 1.1150164604187012, 'learning_rate': 4.008174222866745e-06, 'epoch': 0.92}
{'loss': 0.7555, 'grad_norm': 1.0140907764434814, 'learning_rate': 3.993136340163317e-06, 'epoch': 0.92}
{'loss': 0.9409, 'grad_norm': 1.0871450901031494, 'learning_rate': 3.978126145376915e-06, 'epoch': 0.92}
{'loss': 0.8985, 'grad_norm': 1.1497360467910767, 'learning_rate': 3.963143642836442e-06, 'epoch': 0.92}
{'loss': 1.1823, 'grad_norm': 1.1447603702545166, 'learning_rate': 3.948188836862776e-06, 'epoch': 0.92}
{'loss': 0.8041, 'grad_norm': 1.3457844257354736, 'learning_rate': 3.9332617317688004e-06, 'epoch': 0.92}
{'loss': 1.2006, 'grad_norm': 1.222808599472046, 'learning_rate': 3.918362331859437e-06, 'epoch': 0.92}
{'loss': 0.7512, 'grad_norm': 0.983152449131012, 'learning_rate': 3.903490641431573e-06, 'epoch': 0.92}
{'loss': 0.9661, 'grad_norm': 1.2465648651123047, 'learning_rate': 3.888646664774154e-06, 'epoch': 0.92}
{'loss': 0.9865, 'grad_norm': 1.0026122331619263, 'learning_rate': 3.873830406168111e-06, 'epoch': 0.92}
{'loss': 1.2405, 'grad_norm': 0.9862821698188782, 'learning_rate': 3.859041869886382e-06, 'epoch': 0.92}
{'loss': 1.1013, 'grad_norm': 1.332146167755127, 'learning_rate': 3.8442810601939105e-06, 'epoch': 0.92}
{'loss': 0.8863, 'grad_norm': 1.9251853227615356, 'learning_rate': 3.8295479813476255e-06, 'epoch': 0.92}
{'loss': 0.974, 'grad_norm': 1.546015977859497, 'learning_rate': 3.814842637596483e-06, 'epoch': 0.92}
{'loss': 1.0894, 'grad_norm': 1.04617440700531, 'learning_rate': 3.8001650331814465e-06, 'epoch': 0.92}
{'loss': 0.907, 'grad_norm': 1.0736892223358154, 'learning_rate': 3.785515172335463e-06, 'epoch': 0.92}
{'loss': 0.9997, 'grad_norm': 1.2164186239242554, 'learning_rate': 3.7708930592834648e-06, 'epoch': 0.92}
{'loss': 0.8805, 'grad_norm': 1.2569036483764648, 'learning_rate': 3.756298698242422e-06, 'epoch': 0.92}
{'loss': 0.9913, 'grad_norm': 1.0207706689834595, 'learning_rate': 3.7417320934212574e-06, 'epoch': 0.92}
{'loss': 0.9474, 'grad_norm': 0.9363391399383545, 'learning_rate': 3.7271932490209328e-06, 'epoch': 0.92}
{'loss': 0.6938, 'grad_norm': 1.3965462446212769, 'learning_rate': 3.7126821692343605e-06, 'epoch': 0.92}
{'loss': 0.8825, 'grad_norm': 1.1532173156738281, 'learning_rate': 3.6981988582464822e-06, 'epoch': 0.92}
{'loss': 0.9484, 'grad_norm': 1.3109631538391113, 'learning_rate': 3.68374332023419e-06, 'epoch': 0.92}
{'loss': 1.0325, 'grad_norm': 1.303541660308838, 'learning_rate': 3.6693155593664154e-06, 'epoch': 0.92}
{'loss': 0.9067, 'grad_norm': 1.2137341499328613, 'learning_rate': 3.654915579804041e-06, 'epoch': 0.92}
{'loss': 0.705, 'grad_norm': 1.4484455585479736, 'learning_rate': 3.6405433856999684e-06, 'epoch': 0.92}
{'loss': 0.6567, 'grad_norm': 1.25338876247406, 'learning_rate': 3.626198981199047e-06, 'epoch': 0.92}
{'loss': 0.5586, 'grad_norm': 1.1582374572753906, 'learning_rate': 3.6118823704381467e-06, 'epoch': 0.92}
{'loss': 0.7928, 'grad_norm': 1.1268428564071655, 'learning_rate': 3.5975935575461083e-06, 'epoch': 0.92}
{'loss': 0.8592, 'grad_norm': 1.3363348245620728, 'learning_rate': 3.5833325466437694e-06, 'epoch': 0.92}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8794, 'grad_norm': 1.5117322206497192, 'learning_rate': 3.569099341843907e-06, 'epoch': 0.92}
{'loss': 0.7902, 'grad_norm': 0.9444639086723328, 'learning_rate': 3.5548939472513365e-06, 'epoch': 0.92}
{'loss': 0.9845, 'grad_norm': 1.5820374488830566, 'learning_rate': 3.5407163669628153e-06, 'epoch': 0.92}
{'loss': 0.6947, 'grad_norm': 1.163107991218567, 'learning_rate': 3.526566605067072e-06, 'epoch': 0.92}
{'loss': 1.0484, 'grad_norm': 0.9667262434959412, 'learning_rate': 3.512444665644865e-06, 'epoch': 0.92}
{'loss': 1.0336, 'grad_norm': 1.4308587312698364, 'learning_rate': 3.4983505527688586e-06, 'epoch': 0.92}
{'loss': 0.8589, 'grad_norm': 1.1950911283493042, 'learning_rate': 3.484284270503746e-06, 'epoch': 0.92}
{'loss': 0.804, 'grad_norm': 1.0703405141830444, 'learning_rate': 3.470245822906182e-06, 'epoch': 0.92}
{'loss': 0.8099, 'grad_norm': 1.4337986707687378, 'learning_rate': 3.456235214024761e-06, 'epoch': 0.92}
{'loss': 0.7526, 'grad_norm': 1.3985157012939453, 'learning_rate': 3.4422524479001182e-06, 'epoch': 0.92}
{'loss': 0.8277, 'grad_norm': 1.1048520803451538, 'learning_rate': 3.428297528564761e-06, 'epoch': 0.92}
{'loss': 0.8197, 'grad_norm': 1.1609495878219604, 'learning_rate': 3.414370460043259e-06, 'epoch': 0.92}
{'loss': 0.8488, 'grad_norm': 0.9191293716430664, 'learning_rate': 3.4004712463521103e-06, 'epoch': 0.93}
{'loss': 0.8412, 'grad_norm': 1.3167216777801514, 'learning_rate': 3.3865998914997643e-06, 'epoch': 0.93}
{'loss': 1.0098, 'grad_norm': 1.164811134338379, 'learning_rate': 3.372756399486665e-06, 'epoch': 0.93}
{'loss': 0.6246, 'grad_norm': 1.2672462463378906, 'learning_rate': 3.358940774305208e-06, 'epoch': 0.93}
{'loss': 0.8259, 'grad_norm': 1.056843876838684, 'learning_rate': 3.3451530199397395e-06, 'epoch': 0.93}
{'loss': 0.7378, 'grad_norm': 1.3379865884780884, 'learning_rate': 3.331393140366601e-06, 'epoch': 0.93}
{'loss': 0.8518, 'grad_norm': 1.3143928050994873, 'learning_rate': 3.3176611395540626e-06, 'epoch': 0.93}
{'loss': 0.9589, 'grad_norm': 1.6943384408950806, 'learning_rate': 3.3039570214623782e-06, 'epoch': 0.93}
{'loss': 0.9988, 'grad_norm': 1.1117981672286987, 'learning_rate': 3.2902807900437315e-06, 'epoch': 0.93}
{'loss': 0.8637, 'grad_norm': 0.8948540091514587, 'learning_rate': 3.2766324492422895e-06, 'epoch': 0.93}
{'loss': 1.0725, 'grad_norm': 1.1792229413986206, 'learning_rate': 3.2630120029942037e-06, 'epoch': 0.93}
{'loss': 0.9319, 'grad_norm': 1.5794121026992798, 'learning_rate': 3.249419455227476e-06, 'epoch': 0.93}
{'loss': 0.8743, 'grad_norm': 1.7622092962265015, 'learning_rate': 3.2358548098621932e-06, 'epoch': 0.93}
{'loss': 1.0641, 'grad_norm': 1.0456730127334595, 'learning_rate': 3.222318070810293e-06, 'epoch': 0.93}
{'loss': 0.7725, 'grad_norm': 1.2835749387741089, 'learning_rate': 3.20880924197573e-06, 'epoch': 0.93}
{'loss': 0.9727, 'grad_norm': 1.1188632249832153, 'learning_rate': 3.1953283272543768e-06, 'epoch': 0.93}
{'loss': 0.6539, 'grad_norm': 1.1620575189590454, 'learning_rate': 3.1818753305340565e-06, 'epoch': 0.93}
{'loss': 0.8935, 'grad_norm': 1.1633557081222534, 'learning_rate': 3.168450255694566e-06, 'epoch': 0.93}
{'loss': 0.8653, 'grad_norm': 0.947587251663208, 'learning_rate': 3.1550531066076085e-06, 'epoch': 0.93}
{'loss': 0.9362, 'grad_norm': 1.1381632089614868, 'learning_rate': 3.1416838871368924e-06, 'epoch': 0.93}
{'loss': 0.9687, 'grad_norm': 1.1699708700180054, 'learning_rate': 3.128342601138001e-06, 'epoch': 0.93}
{'loss': 0.8837, 'grad_norm': 1.3938342332839966, 'learning_rate': 3.1150292524585236e-06, 'epoch': 0.93}
{'loss': 0.8246, 'grad_norm': 1.1160728931427002, 'learning_rate': 3.1017438449379434e-06, 'epoch': 0.93}
{'loss': 1.0929, 'grad_norm': 1.1039061546325684, 'learning_rate': 3.088486382407718e-06, 'epoch': 0.93}
{'loss': 0.7408, 'grad_norm': 1.16408109664917, 'learning_rate': 3.075256868691234e-06, 'epoch': 0.93}
{'loss': 0.8655, 'grad_norm': 1.1199604272842407, 'learning_rate': 3.0620553076038393e-06, 'epoch': 0.93}
{'loss': 0.6487, 'grad_norm': 1.2312583923339844, 'learning_rate': 3.048881702952755e-06, 'epoch': 0.93}
{'loss': 0.8703, 'grad_norm': 1.0488842725753784, 'learning_rate': 3.035736058537231e-06, 'epoch': 0.93}
{'loss': 1.0077, 'grad_norm': 1.199380874633789, 'learning_rate': 3.0226183781483896e-06, 'epoch': 0.93}
{'loss': 1.1149, 'grad_norm': 1.1580486297607422, 'learning_rate': 3.0095286655692945e-06, 'epoch': 0.93}
{'loss': 0.8633, 'grad_norm': 1.18837571144104, 'learning_rate': 2.996466924574992e-06, 'epoch': 0.93}
{'loss': 0.8078, 'grad_norm': 1.0708740949630737, 'learning_rate': 2.9834331589323693e-06, 'epoch': 0.93}
{'loss': 1.1122, 'grad_norm': 1.1810328960418701, 'learning_rate': 2.970427372400353e-06, 'epoch': 0.93}
{'loss': 0.9323, 'grad_norm': 1.1588276624679565, 'learning_rate': 2.957449568729731e-06, 'epoch': 0.93}
{'loss': 0.8362, 'grad_norm': 0.9374893307685852, 'learning_rate': 2.9444997516632323e-06, 'epoch': 0.93}
{'loss': 0.7781, 'grad_norm': 1.36787748336792, 'learning_rate': 2.9315779249355356e-06, 'epoch': 0.93}
{'loss': 0.9098, 'grad_norm': 1.0532190799713135, 'learning_rate': 2.918684092273216e-06, 'epoch': 0.93}
{'loss': 1.027, 'grad_norm': 1.021846055984497, 'learning_rate': 2.905818257394799e-06, 'epoch': 0.93}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0293, 'grad_norm': 1.046494483947754, 'learning_rate': 2.8929804240107385e-06, 'epoch': 0.93}
{'loss': 0.7778, 'grad_norm': 1.0805435180664062, 'learning_rate': 2.880170595823395e-06, 'epoch': 0.93}
{'loss': 0.5845, 'grad_norm': 1.0008103847503662, 'learning_rate': 2.867388776527069e-06, 'epoch': 0.93}
{'loss': 0.7206, 'grad_norm': 1.0943533182144165, 'learning_rate': 2.8546349698079787e-06, 'epoch': 0.93}
{'loss': 1.1606, 'grad_norm': 1.1022846698760986, 'learning_rate': 2.8419091793442264e-06, 'epoch': 0.93}
{'loss': 1.1104, 'grad_norm': 0.9476179480552673, 'learning_rate': 2.8292114088059318e-06, 'epoch': 0.93}
{'loss': 0.9747, 'grad_norm': 1.1566568613052368, 'learning_rate': 2.8165416618549987e-06, 'epoch': 0.93}
{'loss': 0.6926, 'grad_norm': 1.1003144979476929, 'learning_rate': 2.8038999421453826e-06, 'epoch': 0.93}
{'loss': 0.7656, 'grad_norm': 0.9507908821105957, 'learning_rate': 2.7912862533228558e-06, 'epoch': 0.93}
{'loss': 0.8782, 'grad_norm': 1.2110145092010498, 'learning_rate': 2.7787005990251636e-06, 'epoch': 0.93}
{'loss': 0.7961, 'grad_norm': 1.284549593925476, 'learning_rate': 2.766142982881936e-06, 'epoch': 0.93}
{'loss': 0.7278, 'grad_norm': 1.1952391862869263, 'learning_rate': 2.7536134085147323e-06, 'epoch': 0.93}
{'loss': 0.7781, 'grad_norm': 1.2162607908248901, 'learning_rate': 2.7411118795370395e-06, 'epoch': 0.93}
{'loss': 0.8996, 'grad_norm': 1.391250729560852, 'learning_rate': 2.7286383995542065e-06, 'epoch': 0.93}
{'loss': 0.9788, 'grad_norm': 1.2795336246490479, 'learning_rate': 2.716192972163556e-06, 'epoch': 0.93}
{'loss': 1.2355, 'grad_norm': 1.6159392595291138, 'learning_rate': 2.7037756009542612e-06, 'epoch': 0.93}
{'loss': 0.7414, 'grad_norm': 1.0029752254486084, 'learning_rate': 2.691386289507458e-06, 'epoch': 0.93}
{'loss': 0.8828, 'grad_norm': 1.2239490747451782, 'learning_rate': 2.679025041396155e-06, 'epoch': 0.93}
{'loss': 0.6465, 'grad_norm': 1.004649043083191, 'learning_rate': 2.6666918601852664e-06, 'epoch': 0.93}
{'loss': 0.915, 'grad_norm': 1.2346817255020142, 'learning_rate': 2.6543867494316256e-06, 'epoch': 0.93}
{'loss': 1.2392, 'grad_norm': 1.580245018005371, 'learning_rate': 2.6421097126839712e-06, 'epoch': 0.93}
{'loss': 1.1245, 'grad_norm': 1.065621018409729, 'learning_rate': 2.629860753482949e-06, 'epoch': 0.93}
{'loss': 0.7718, 'grad_norm': 1.1432982683181763, 'learning_rate': 2.6176398753610885e-06, 'epoch': 0.93}
{'loss': 1.1519, 'grad_norm': 1.5114504098892212, 'learning_rate': 2.6054470818428377e-06, 'epoch': 0.93}
{'loss': 1.0963, 'grad_norm': 0.9914531111717224, 'learning_rate': 2.5932823764445392e-06, 'epoch': 0.93}
{'loss': 0.8864, 'grad_norm': 1.2240909337997437, 'learning_rate': 2.581145762674442e-06, 'epoch': 0.93}
{'loss': 1.1108, 'grad_norm': 0.9829847812652588, 'learning_rate': 2.5690372440326572e-06, 'epoch': 0.93}
{'loss': 0.6387, 'grad_norm': 1.125314474105835, 'learning_rate': 2.5569568240112697e-06, 'epoch': 0.94}
{'loss': 1.1608, 'grad_norm': 0.9630159139633179, 'learning_rate': 2.54490450609417e-06, 'epoch': 0.94}
{'loss': 1.0064, 'grad_norm': 1.136518120765686, 'learning_rate': 2.532880293757223e-06, 'epoch': 0.94}
{'loss': 1.0974, 'grad_norm': 1.3056485652923584, 'learning_rate': 2.5208841904681313e-06, 'epoch': 0.94}
{'loss': 1.0655, 'grad_norm': 1.2092952728271484, 'learning_rate': 2.5089161996865175e-06, 'epoch': 0.94}
{'loss': 1.0393, 'grad_norm': 1.1476045846939087, 'learning_rate': 2.496976324863898e-06, 'epoch': 0.94}
{'loss': 0.9396, 'grad_norm': 1.0911842584609985, 'learning_rate': 2.4850645694436736e-06, 'epoch': 0.94}
{'loss': 0.8678, 'grad_norm': 1.0682955980300903, 'learning_rate': 2.4731809368611413e-06, 'epoch': 0.94}
{'loss': 0.5812, 'grad_norm': 1.650314450263977, 'learning_rate': 2.461325430543482e-06, 'epoch': 0.94}
{'loss': 0.7832, 'grad_norm': 0.9554818868637085, 'learning_rate': 2.44949805390976e-06, 'epoch': 0.94}
{'loss': 0.91, 'grad_norm': 1.0604832172393799, 'learning_rate': 2.437698810370925e-06, 'epoch': 0.94}
{'loss': 0.7271, 'grad_norm': 1.0373889207839966, 'learning_rate': 2.4259277033298555e-06, 'epoch': 0.94}
{'loss': 0.723, 'grad_norm': 1.4220681190490723, 'learning_rate': 2.4141847361812462e-06, 'epoch': 0.94}
{'loss': 0.7741, 'grad_norm': 1.1089729070663452, 'learning_rate': 2.4024699123117445e-06, 'epoch': 0.94}
{'loss': 0.8651, 'grad_norm': 1.439492106437683, 'learning_rate': 2.390783235099814e-06, 'epoch': 0.94}
{'loss': 1.1618, 'grad_norm': 1.0512244701385498, 'learning_rate': 2.3791247079158584e-06, 'epoch': 0.94}
{'loss': 0.9675, 'grad_norm': 1.2304126024246216, 'learning_rate': 2.3674943341221445e-06, 'epoch': 0.94}
{'loss': 0.9026, 'grad_norm': 1.534591794013977, 'learning_rate': 2.3558921170727888e-06, 'epoch': 0.94}
{'loss': 1.0521, 'grad_norm': 1.465338110923767, 'learning_rate': 2.344318060113859e-06, 'epoch': 0.94}
{'loss': 0.9465, 'grad_norm': 1.0048363208770752, 'learning_rate': 2.332772166583208e-06, 'epoch': 0.94}
{'loss': 0.4826, 'grad_norm': 1.2522779703140259, 'learning_rate': 2.3212544398106496e-06, 'epoch': 0.94}
{'loss': 0.8726, 'grad_norm': 1.2971149682998657, 'learning_rate': 2.3097648831178376e-06, 'epoch': 0.94}
{'loss': 0.8147, 'grad_norm': 1.058375358581543, 'learning_rate': 2.2983034998182997e-06, 'epoch': 0.94}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6362, 'grad_norm': 1.0028822422027588, 'learning_rate': 2.2868702932174357e-06, 'epoch': 0.94}
{'loss': 0.9476, 'grad_norm': 1.1033860445022583, 'learning_rate': 2.275465266612542e-06, 'epoch': 0.94}
{'loss': 0.9754, 'grad_norm': 1.0235692262649536, 'learning_rate': 2.2640884232927427e-06, 'epoch': 0.94}
{'loss': 0.9217, 'grad_norm': 1.3512471914291382, 'learning_rate': 2.2527397665391027e-06, 'epoch': 0.94}
{'loss': 0.7887, 'grad_norm': 1.171826720237732, 'learning_rate': 2.241419299624514e-06, 'epoch': 0.94}
{'loss': 1.3027, 'grad_norm': 1.2107954025268555, 'learning_rate': 2.230127025813722e-06, 'epoch': 0.94}
{'loss': 1.06, 'grad_norm': 1.104066014289856, 'learning_rate': 2.218862948363387e-06, 'epoch': 0.94}
{'loss': 0.684, 'grad_norm': 1.363139033317566, 'learning_rate': 2.207627070521989e-06, 'epoch': 0.94}
{'loss': 0.7112, 'grad_norm': 1.0631202459335327, 'learning_rate': 2.196419395529936e-06, 'epoch': 0.94}
{'loss': 0.8892, 'grad_norm': 1.0410470962524414, 'learning_rate': 2.1852399266194314e-06, 'epoch': 0.94}
{'loss': 0.8269, 'grad_norm': 1.3585257530212402, 'learning_rate': 2.174088667014618e-06, 'epoch': 0.94}
{'loss': 0.8317, 'grad_norm': 0.9976444840431213, 'learning_rate': 2.162965619931423e-06, 'epoch': 0.94}
{'loss': 0.8682, 'grad_norm': 1.131901502609253, 'learning_rate': 2.1518707885777146e-06, 'epoch': 0.94}
{'loss': 0.8266, 'grad_norm': 1.0414533615112305, 'learning_rate': 2.1408041761531772e-06, 'epoch': 0.94}
{'loss': 0.7122, 'grad_norm': 1.1228301525115967, 'learning_rate': 2.129765785849358e-06, 'epoch': 0.94}
{'loss': 0.9732, 'grad_norm': 1.3208668231964111, 'learning_rate': 2.1187556208496883e-06, 'epoch': 0.94}
{'loss': 0.8634, 'grad_norm': 1.147779107093811, 'learning_rate': 2.1077736843294393e-06, 'epoch': 0.94}
{'loss': 0.8434, 'grad_norm': 1.1599674224853516, 'learning_rate': 2.096819979455755e-06, 'epoch': 0.94}
{'loss': 1.0107, 'grad_norm': 1.046252727508545, 'learning_rate': 2.0858945093876316e-06, 'epoch': 0.94}
{'loss': 0.6788, 'grad_norm': 1.288701057434082, 'learning_rate': 2.074997277275914e-06, 'epoch': 0.94}
{'loss': 0.7805, 'grad_norm': 1.578063726425171, 'learning_rate': 2.0641282862633114e-06, 'epoch': 0.94}
{'loss': 0.8592, 'grad_norm': 1.2637007236480713, 'learning_rate': 2.053287539484405e-06, 'epoch': 0.94}
{'loss': 0.6892, 'grad_norm': 1.1787744760513306, 'learning_rate': 2.0424750400655947e-06, 'epoch': 0.94}
{'loss': 0.637, 'grad_norm': 0.9072685241699219, 'learning_rate': 2.031690791125163e-06, 'epoch': 0.94}
{'loss': 0.6871, 'grad_norm': 1.1352533102035522, 'learning_rate': 2.0209347957732328e-06, 'epoch': 0.94}
{'loss': 0.7643, 'grad_norm': 1.2682865858078003, 'learning_rate': 2.0102070571117795e-06, 'epoch': 0.94}
{'loss': 0.948, 'grad_norm': 1.015658974647522, 'learning_rate': 1.9995075782346385e-06, 'epoch': 0.94}
{'loss': 1.0094, 'grad_norm': 1.1549030542373657, 'learning_rate': 1.9888363622274642e-06, 'epoch': 0.94}
{'loss': 1.0531, 'grad_norm': 1.1402043104171753, 'learning_rate': 1.978193412167828e-06, 'epoch': 0.94}
{'loss': 0.885, 'grad_norm': 1.1228805780410767, 'learning_rate': 1.9675787311250636e-06, 'epoch': 0.94}
{'loss': 0.7545, 'grad_norm': 1.1174200773239136, 'learning_rate': 1.9569923221604225e-06, 'epoch': 0.94}
{'loss': 1.0256, 'grad_norm': 1.3186867237091064, 'learning_rate': 1.946434188326951e-06, 'epoch': 0.94}
{'loss': 0.7553, 'grad_norm': 0.8574104309082031, 'learning_rate': 1.9359043326695804e-06, 'epoch': 0.94}
{'loss': 1.0185, 'grad_norm': 1.2121626138687134, 'learning_rate': 1.925402758225059e-06, 'epoch': 0.94}
{'loss': 0.4881, 'grad_norm': 1.0269392728805542, 'learning_rate': 1.9149294680220087e-06, 'epoch': 0.94}
{'loss': 0.8764, 'grad_norm': 1.137550711631775, 'learning_rate': 1.904484465080847e-06, 'epoch': 0.94}
{'loss': 0.9012, 'grad_norm': 1.1469628810882568, 'learning_rate': 1.894067752413886e-06, 'epoch': 0.94}
{'loss': 0.7828, 'grad_norm': 1.1779906749725342, 'learning_rate': 1.8836793330252456e-06, 'epoch': 0.94}
{'loss': 0.8677, 'grad_norm': 1.1310710906982422, 'learning_rate': 1.8733192099108955e-06, 'epoch': 0.94}
{'loss': 0.7525, 'grad_norm': 1.0998350381851196, 'learning_rate': 1.8629873860586566e-06, 'epoch': 0.94}
{'loss': 0.9999, 'grad_norm': 1.1437582969665527, 'learning_rate': 1.852683864448157e-06, 'epoch': 0.94}
{'loss': 0.9875, 'grad_norm': 0.9430282115936279, 'learning_rate': 1.8424086480509196e-06, 'epoch': 0.94}
{'loss': 0.8874, 'grad_norm': 1.2966464757919312, 'learning_rate': 1.8321617398302183e-06, 'epoch': 0.95}
{'loss': 1.0185, 'grad_norm': 1.5042608976364136, 'learning_rate': 1.821943142741256e-06, 'epoch': 0.95}
{'loss': 1.1627, 'grad_norm': 1.1239532232284546, 'learning_rate': 1.8117528597309862e-06, 'epoch': 0.95}
{'loss': 1.1194, 'grad_norm': 0.9377619624137878, 'learning_rate': 1.8015908937382586e-06, 'epoch': 0.95}
{'loss': 1.0652, 'grad_norm': 1.113690733909607, 'learning_rate': 1.7914572476937508e-06, 'epoch': 0.95}
{'loss': 0.8676, 'grad_norm': 1.5842198133468628, 'learning_rate': 1.781351924519925e-06, 'epoch': 0.95}
{'loss': 0.9219, 'grad_norm': 1.1843279600143433, 'learning_rate': 1.771274927131139e-06, 'epoch': 0.95}
{'loss': 1.1681, 'grad_norm': 1.034010410308838, 'learning_rate': 1.7612262584335237e-06, 'epoch': 0.95}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9431, 'grad_norm': 1.1886473894119263, 'learning_rate': 1.751205921325083e-06, 'epoch': 0.95}
{'loss': 0.8875, 'grad_norm': 1.160043478012085, 'learning_rate': 1.7412139186956278e-06, 'epoch': 0.95}
{'loss': 0.5902, 'grad_norm': 0.9920651912689209, 'learning_rate': 1.7312502534268082e-06, 'epoch': 0.95}
{'loss': 1.22, 'grad_norm': 1.0225536823272705, 'learning_rate': 1.7213149283920927e-06, 'epoch': 0.95}
{'loss': 0.8522, 'grad_norm': 1.055935025215149, 'learning_rate': 1.7114079464567888e-06, 'epoch': 0.95}
{'loss': 0.6175, 'grad_norm': 1.2513915300369263, 'learning_rate': 1.7015293104780005e-06, 'epoch': 0.95}
{'loss': 1.0151, 'grad_norm': 1.1470966339111328, 'learning_rate': 1.691679023304704e-06, 'epoch': 0.95}
{'loss': 1.0138, 'grad_norm': 1.2308019399642944, 'learning_rate': 1.6818570877776718e-06, 'epoch': 0.95}
{'loss': 1.1039, 'grad_norm': 1.270408034324646, 'learning_rate': 1.6720635067294932e-06, 'epoch': 0.95}
{'loss': 0.9893, 'grad_norm': 0.9934328198432922, 'learning_rate': 1.6622982829845867e-06, 'epoch': 0.95}
{'loss': 0.7868, 'grad_norm': 1.0595592260360718, 'learning_rate': 1.6525614193591998e-06, 'epoch': 0.95}
{'loss': 1.1346, 'grad_norm': 0.9429527521133423, 'learning_rate': 1.6428529186614195e-06, 'epoch': 0.95}
{'loss': 0.9533, 'grad_norm': 1.2204374074935913, 'learning_rate': 1.6331727836910837e-06, 'epoch': 0.95}
{'loss': 0.6849, 'grad_norm': 0.9989741444587708, 'learning_rate': 1.6235210172399372e-06, 'epoch': 0.95}
{'loss': 0.9584, 'grad_norm': 1.1049957275390625, 'learning_rate': 1.613897622091487e-06, 'epoch': 0.95}
{'loss': 0.6801, 'grad_norm': 1.0369080305099487, 'learning_rate': 1.6043026010210793e-06, 'epoch': 0.95}
{'loss': 0.8346, 'grad_norm': 1.1374263763427734, 'learning_rate': 1.5947359567958674e-06, 'epoch': 0.95}
{'loss': 0.974, 'grad_norm': 1.0282825231552124, 'learning_rate': 1.5851976921748225e-06, 'epoch': 0.95}
{'loss': 0.7519, 'grad_norm': 0.9787928462028503, 'learning_rate': 1.5756878099087435e-06, 'epoch': 0.95}
{'loss': 0.839, 'grad_norm': 1.1522470712661743, 'learning_rate': 1.566206312740226e-06, 'epoch': 0.95}
{'loss': 1.1042, 'grad_norm': 1.0367100238800049, 'learning_rate': 1.5567532034036936e-06, 'epoch': 0.95}
{'loss': 0.9783, 'grad_norm': 1.1105453968048096, 'learning_rate': 1.5473284846253767e-06, 'epoch': 0.95}
{'loss': 0.8429, 'grad_norm': 1.153093934059143, 'learning_rate': 1.537932159123312e-06, 'epoch': 0.95}
{'loss': 1.169, 'grad_norm': 1.0986037254333496, 'learning_rate': 1.5285642296073654e-06, 'epoch': 0.95}
{'loss': 0.9323, 'grad_norm': 1.2651731967926025, 'learning_rate': 1.5192246987791981e-06, 'epoch': 0.95}
{'loss': 1.0276, 'grad_norm': 0.9993103742599487, 'learning_rate': 1.5099135693322774e-06, 'epoch': 0.95}
{'loss': 0.628, 'grad_norm': 0.845618486404419, 'learning_rate': 1.5006308439519001e-06, 'epoch': 0.95}
{'loss': 1.0756, 'grad_norm': 1.2117756605148315, 'learning_rate': 1.491376525315158e-06, 'epoch': 0.95}
{'loss': 0.7676, 'grad_norm': 1.0565071105957031, 'learning_rate': 1.4821506160909493e-06, 'epoch': 0.95}
{'loss': 0.8796, 'grad_norm': 1.511393666267395, 'learning_rate': 1.4729531189399903e-06, 'epoch': 0.95}
{'loss': 0.9816, 'grad_norm': 1.7497042417526245, 'learning_rate': 1.4637840365147703e-06, 'epoch': 0.95}
{'loss': 0.7321, 'grad_norm': 1.0493853092193604, 'learning_rate': 1.4546433714596296e-06, 'epoch': 0.95}
{'loss': 1.1387, 'grad_norm': 4.056273460388184, 'learning_rate': 1.4455311264106818e-06, 'epoch': 0.95}
{'loss': 1.1351, 'grad_norm': 1.0403136014938354, 'learning_rate': 1.4364473039958692e-06, 'epoch': 0.95}
{'loss': 1.0221, 'grad_norm': 1.0657145977020264, 'learning_rate': 1.4273919068349184e-06, 'epoch': 0.95}
{'loss': 0.9935, 'grad_norm': 1.200851321220398, 'learning_rate': 1.4183649375393404e-06, 'epoch': 0.95}
{'loss': 0.6201, 'grad_norm': 1.4374281167984009, 'learning_rate': 1.409366398712497e-06, 'epoch': 0.95}
{'loss': 0.7736, 'grad_norm': 1.0303798913955688, 'learning_rate': 1.400396292949513e-06, 'epoch': 0.95}
{'loss': 0.7917, 'grad_norm': 1.0338115692138672, 'learning_rate': 1.3914546228373182e-06, 'epoch': 0.95}
{'loss': 0.9368, 'grad_norm': 1.0310344696044922, 'learning_rate': 1.3825413909546503e-06, 'epoch': 0.95}
{'loss': 1.0099, 'grad_norm': 1.0118257999420166, 'learning_rate': 1.373656599872053e-06, 'epoch': 0.95}
{'loss': 1.0186, 'grad_norm': 1.5235131978988647, 'learning_rate': 1.3648002521518323e-06, 'epoch': 0.95}
{'loss': 0.715, 'grad_norm': 0.9271767735481262, 'learning_rate': 1.3559723503481558e-06, 'epoch': 0.95}
{'loss': 1.1548, 'grad_norm': 1.0673861503601074, 'learning_rate': 1.3471728970068987e-06, 'epoch': 0.95}
{'loss': 0.8241, 'grad_norm': 1.8106207847595215, 'learning_rate': 1.3384018946658306e-06, 'epoch': 0.95}
{'loss': 1.0493, 'grad_norm': 1.2474952936172485, 'learning_rate': 1.3296593458544281e-06, 'epoch': 0.95}
{'loss': 0.8667, 'grad_norm': 1.0696427822113037, 'learning_rate': 1.3209452530940192e-06, 'epoch': 0.95}
{'loss': 1.1632, 'grad_norm': 1.4563796520233154, 'learning_rate': 1.3122596188977043e-06, 'epoch': 0.95}
{'loss': 0.9496, 'grad_norm': 1.583925724029541, 'learning_rate': 1.3036024457703798e-06, 'epoch': 0.95}
{'loss': 1.2258, 'grad_norm': 1.0851737260818481, 'learning_rate': 1.2949737362087156e-06, 'epoch': 0.95}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1561, 'grad_norm': 1.1814481019973755, 'learning_rate': 1.2863734927012095e-06, 'epoch': 0.95}
{'loss': 0.9477, 'grad_norm': 1.095624327659607, 'learning_rate': 1.277801717728122e-06, 'epoch': 0.95}
{'loss': 0.8788, 'grad_norm': 1.2650424242019653, 'learning_rate': 1.2692584137615204e-06, 'epoch': 0.95}
{'loss': 0.9688, 'grad_norm': 1.4380890130996704, 'learning_rate': 1.2607435832652448e-06, 'epoch': 0.95}
{'loss': 0.9386, 'grad_norm': 1.1183981895446777, 'learning_rate': 1.2522572286949197e-06, 'epoch': 0.95}
{'loss': 0.8976, 'grad_norm': 1.0961798429489136, 'learning_rate': 1.2437993524979984e-06, 'epoch': 0.95}
{'loss': 1.1141, 'grad_norm': 1.0251368284225464, 'learning_rate': 1.2353699571136634e-06, 'epoch': 0.95}
{'loss': 0.9365, 'grad_norm': 0.9831815958023071, 'learning_rate': 1.2269690449729253e-06, 'epoch': 0.96}
{'loss': 0.9025, 'grad_norm': 1.0142911672592163, 'learning_rate': 1.2185966184985685e-06, 'epoch': 0.96}
{'loss': 0.9076, 'grad_norm': 1.0166990756988525, 'learning_rate': 1.2102526801051506e-06, 'epoch': 0.96}
{'loss': 0.8691, 'grad_norm': 1.2329598665237427, 'learning_rate': 1.2019372321990352e-06, 'epoch': 0.96}
{'loss': 0.9959, 'grad_norm': 1.3029325008392334, 'learning_rate': 1.1936502771783486e-06, 'epoch': 0.96}
{'loss': 0.668, 'grad_norm': 0.9745898246765137, 'learning_rate': 1.1853918174330125e-06, 'epoch': 0.96}
{'loss': 0.9126, 'grad_norm': 0.9491377472877502, 'learning_rate': 1.1771618553447216e-06, 'epoch': 0.96}
{'loss': 0.8007, 'grad_norm': 1.0901933908462524, 'learning_rate': 1.1689603932869665e-06, 'epoch': 0.96}
{'loss': 0.8692, 'grad_norm': 1.3488422632217407, 'learning_rate': 1.1607874336249992e-06, 'epoch': 0.96}
{'loss': 0.7834, 'grad_norm': 1.256331443786621, 'learning_rate': 1.1526429787158676e-06, 'epoch': 0.96}
{'loss': 0.9123, 'grad_norm': 1.1483498811721802, 'learning_rate': 1.1445270309083933e-06, 'epoch': 0.96}
{'loss': 0.9542, 'grad_norm': 0.9923380017280579, 'learning_rate': 1.1364395925431814e-06, 'epoch': 0.96}
{'loss': 0.9298, 'grad_norm': 1.3258689641952515, 'learning_rate': 1.1283806659525996e-06, 'epoch': 0.96}
{'loss': 1.1759, 'grad_norm': 1.0290414094924927, 'learning_rate': 1.1203502534608113e-06, 'epoch': 0.96}
{'loss': 0.8896, 'grad_norm': 1.107075572013855, 'learning_rate': 1.1123483573837412e-06, 'epoch': 0.96}
{'loss': 1.1385, 'grad_norm': 1.0817654132843018, 'learning_rate': 1.1043749800291102e-06, 'epoch': 0.96}
{'loss': 0.9184, 'grad_norm': 0.9490193724632263, 'learning_rate': 1.0964301236963904e-06, 'epoch': 0.96}
{'loss': 0.8313, 'grad_norm': 1.0414001941680908, 'learning_rate': 1.0885137906768372e-06, 'epoch': 0.96}
{'loss': 0.9464, 'grad_norm': 1.044185996055603, 'learning_rate': 1.0806259832535026e-06, 'epoch': 0.96}
{'loss': 1.2921, 'grad_norm': 1.0995442867279053, 'learning_rate': 1.0727667037011668e-06, 'epoch': 0.96}
{'loss': 0.8474, 'grad_norm': 1.1129839420318604, 'learning_rate': 1.064935954286428e-06, 'epoch': 0.96}
{'loss': 0.9911, 'grad_norm': 2.041635036468506, 'learning_rate': 1.0571337372676238e-06, 'epoch': 0.96}
{'loss': 0.7515, 'grad_norm': 1.0223937034606934, 'learning_rate': 1.0493600548948878e-06, 'epoch': 0.96}
{'loss': 0.6813, 'grad_norm': 1.1576848030090332, 'learning_rate': 1.0416149094101046e-06, 'epoch': 0.96}
{'loss': 0.8663, 'grad_norm': 1.496031403541565, 'learning_rate': 1.0338983030469428e-06, 'epoch': 0.96}
{'loss': 0.9191, 'grad_norm': 1.1935251951217651, 'learning_rate': 1.0262102380308226e-06, 'epoch': 0.96}
{'loss': 0.9985, 'grad_norm': 1.178710699081421, 'learning_rate': 1.0185507165789587e-06, 'epoch': 0.96}
{'loss': 0.9077, 'grad_norm': 1.6294951438903809, 'learning_rate': 1.0109197409003068e-06, 'epoch': 0.96}
{'loss': 0.8422, 'grad_norm': 1.269045352935791, 'learning_rate': 1.0033173131956176e-06, 'epoch': 0.96}
{'loss': 0.9905, 'grad_norm': 1.4274399280548096, 'learning_rate': 9.957434356573814e-07, 'epoch': 0.96}
{'loss': 0.8944, 'grad_norm': 0.9963205456733704, 'learning_rate': 9.881981104698846e-07, 'epoch': 0.96}
{'loss': 0.7293, 'grad_norm': 1.0462905168533325, 'learning_rate': 9.80681339809142e-07, 'epoch': 0.96}
{'loss': 1.101, 'grad_norm': 1.0081613063812256, 'learning_rate': 9.731931258429638e-07, 'epoch': 0.96}
{'loss': 1.0213, 'grad_norm': 1.128326177597046, 'learning_rate': 9.65733470730934e-07, 'epoch': 0.96}
{'loss': 0.745, 'grad_norm': 1.6074318885803223, 'learning_rate': 9.583023766243426e-07, 'epoch': 0.96}
{'loss': 0.8514, 'grad_norm': 1.0055736303329468, 'learning_rate': 9.508998456663088e-07, 'epoch': 0.96}
{'loss': 0.8154, 'grad_norm': 1.2043572664260864, 'learning_rate': 9.435258799916801e-07, 'epoch': 0.96}
{'loss': 0.8993, 'grad_norm': 1.0391337871551514, 'learning_rate': 9.36180481727067e-07, 'epoch': 0.96}
{'loss': 1.0858, 'grad_norm': 1.150729775428772, 'learning_rate': 9.288636529908635e-07, 'epoch': 0.96}
{'loss': 0.9588, 'grad_norm': 1.1270660161972046, 'learning_rate': 9.215753958931816e-07, 'epoch': 0.96}
{'loss': 0.6369, 'grad_norm': 1.176900029182434, 'learning_rate': 9.143157125359514e-07, 'epoch': 0.96}
{'loss': 0.9857, 'grad_norm': 1.1470603942871094, 'learning_rate': 9.07084605012798e-07, 'epoch': 0.96}
{'loss': 1.0364, 'grad_norm': 1.080008625984192, 'learning_rate': 8.998820754091531e-07, 'epoch': 0.96}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0569, 'grad_norm': 1.3342145681381226, 'learning_rate': 8.927081258021997e-07, 'epoch': 0.96}
{'loss': 1.0318, 'grad_norm': 1.1041630506515503, 'learning_rate': 8.855627582608606e-07, 'epoch': 0.96}
{'loss': 0.8994, 'grad_norm': 1.0489965677261353, 'learning_rate': 8.784459748458318e-07, 'epoch': 0.96}
{'loss': 0.8046, 'grad_norm': 1.2110284566879272, 'learning_rate': 8.713577776095494e-07, 'epoch': 0.96}
{'loss': 1.1458, 'grad_norm': 0.964522659778595, 'learning_rate': 8.642981685962226e-07, 'epoch': 0.96}
{'loss': 0.9517, 'grad_norm': 1.0537325143814087, 'learning_rate': 8.572671498418006e-07, 'epoch': 0.96}
{'loss': 1.1598, 'grad_norm': 1.3885157108306885, 'learning_rate': 8.502647233740169e-07, 'epoch': 0.96}
{'loss': 1.1671, 'grad_norm': 1.3115520477294922, 'learning_rate': 8.432908912123116e-07, 'epoch': 0.96}
{'loss': 0.8917, 'grad_norm': 1.701457142829895, 'learning_rate': 8.363456553679206e-07, 'epoch': 0.96}
{'loss': 1.041, 'grad_norm': 1.3170652389526367, 'learning_rate': 8.294290178437969e-07, 'epoch': 0.96}
{'loss': 0.7872, 'grad_norm': 1.3756746053695679, 'learning_rate': 8.225409806347006e-07, 'epoch': 0.96}
{'loss': 1.2056, 'grad_norm': 0.9524025321006775, 'learning_rate': 8.15681545727065e-07, 'epoch': 0.96}
{'loss': 0.8669, 'grad_norm': 1.3553617000579834, 'learning_rate': 8.08850715099152e-07, 'epoch': 0.96}
{'loss': 0.6326, 'grad_norm': 0.9701265692710876, 'learning_rate': 8.020484907209302e-07, 'epoch': 0.96}
{'loss': 1.078, 'grad_norm': 1.1668702363967896, 'learning_rate': 7.952748745541305e-07, 'epoch': 0.96}
{'loss': 0.9755, 'grad_norm': 0.9955810904502869, 'learning_rate': 7.885298685522235e-07, 'epoch': 0.96}
{'loss': 0.775, 'grad_norm': 0.9845119714736938, 'learning_rate': 7.818134746604311e-07, 'epoch': 0.96}
{'loss': 0.9497, 'grad_norm': 1.0772202014923096, 'learning_rate': 7.751256948157481e-07, 'epoch': 0.96}
{'loss': 0.8295, 'grad_norm': 0.8750725984573364, 'learning_rate': 7.684665309468875e-07, 'epoch': 0.96}
{'loss': 0.9852, 'grad_norm': 1.0055547952651978, 'learning_rate': 7.618359849743128e-07, 'epoch': 0.96}
{'loss': 0.8686, 'grad_norm': 1.0520238876342773, 'learning_rate': 7.552340588102613e-07, 'epoch': 0.96}
{'loss': 1.3968, 'grad_norm': 1.215619683265686, 'learning_rate': 7.486607543586766e-07, 'epoch': 0.96}
{'loss': 0.7778, 'grad_norm': 0.9731306433677673, 'learning_rate': 7.421160735152755e-07, 'epoch': 0.97}
{'loss': 1.0676, 'grad_norm': 0.9805837869644165, 'learning_rate': 7.35600018167526e-07, 'epoch': 0.97}
{'loss': 1.0276, 'grad_norm': 1.2194567918777466, 'learning_rate': 7.291125901946027e-07, 'epoch': 0.97}
{'loss': 1.0161, 'grad_norm': 1.1438262462615967, 'learning_rate': 7.226537914674647e-07, 'epoch': 0.97}
{'loss': 1.0114, 'grad_norm': 1.0621227025985718, 'learning_rate': 7.162236238487885e-07, 'epoch': 0.97}
{'loss': 0.7657, 'grad_norm': 1.08320152759552, 'learning_rate': 7.098220891930129e-07, 'epoch': 0.97}
{'loss': 0.8478, 'grad_norm': 1.0871696472167969, 'learning_rate': 7.034491893463058e-07, 'epoch': 0.97}
{'loss': 0.933, 'grad_norm': 1.2141627073287964, 'learning_rate': 6.971049261465745e-07, 'epoch': 0.97}
{'loss': 1.0579, 'grad_norm': 1.0865895748138428, 'learning_rate': 6.90789301423489e-07, 'epoch': 0.97}
{'loss': 0.8583, 'grad_norm': 1.2802844047546387, 'learning_rate': 6.845023169984366e-07, 'epoch': 0.97}
{'loss': 0.945, 'grad_norm': 1.028846263885498, 'learning_rate': 6.782439746845448e-07, 'epoch': 0.97}
{'loss': 1.2907, 'grad_norm': 1.2971043586730957, 'learning_rate': 6.720142762867032e-07, 'epoch': 0.97}
{'loss': 0.8952, 'grad_norm': 1.2723649740219116, 'learning_rate': 6.658132236015191e-07, 'epoch': 0.97}
{'loss': 0.8402, 'grad_norm': 1.2377122640609741, 'learning_rate': 6.596408184173509e-07, 'epoch': 0.97}
{'loss': 0.8627, 'grad_norm': 1.3782094717025757, 'learning_rate': 6.534970625142856e-07, 'epoch': 0.97}
{'loss': 0.8288, 'grad_norm': 1.0040998458862305, 'learning_rate': 6.473819576641504e-07, 'epoch': 0.97}
{'loss': 1.0094, 'grad_norm': 1.4494094848632812, 'learning_rate': 6.412955056305236e-07, 'epoch': 0.97}
{'loss': 1.1241, 'grad_norm': 1.1655510663986206, 'learning_rate': 6.352377081687011e-07, 'epoch': 0.97}
{'loss': 0.7278, 'grad_norm': 1.0645883083343506, 'learning_rate': 6.292085670257187e-07, 'epoch': 0.97}
{'loss': 0.9641, 'grad_norm': 1.0540422201156616, 'learning_rate': 6.232080839403631e-07, 'epoch': 0.97}
{'loss': 1.1711, 'grad_norm': 1.1288654804229736, 'learning_rate': 6.172362606431281e-07, 'epoch': 0.97}
{'loss': 0.862, 'grad_norm': 1.1863871812820435, 'learning_rate': 6.112930988562804e-07, 'epoch': 0.97}
{'loss': 0.6615, 'grad_norm': 1.0551066398620605, 'learning_rate': 6.053786002937822e-07, 'epoch': 0.97}
{'loss': 0.8762, 'grad_norm': 1.236120581626892, 'learning_rate': 5.99492766661347e-07, 'epoch': 0.97}
{'loss': 0.7395, 'grad_norm': 1.127816081047058, 'learning_rate': 5.93635599656428e-07, 'epoch': 0.97}
{'loss': 0.9291, 'grad_norm': 1.3805955648422241, 'learning_rate': 5.878071009682074e-07, 'epoch': 0.97}
{'loss': 1.1083, 'grad_norm': 1.0627716779708862, 'learning_rate': 5.820072722775849e-07, 'epoch': 0.97}
{'loss': 0.8318, 'grad_norm': 1.1647027730941772, 'learning_rate': 5.762361152572115e-07, 'epoch': 0.97}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0309, 'grad_norm': 0.9845011234283447, 'learning_rate': 5.704936315714449e-07, 'epoch': 0.97}
{'loss': 0.7561, 'grad_norm': 0.9143524169921875, 'learning_rate': 5.647798228764156e-07, 'epoch': 0.97}
{'loss': 1.1599, 'grad_norm': 1.2077465057373047, 'learning_rate': 5.590946908199501e-07, 'epoch': 0.97}
{'loss': 1.0516, 'grad_norm': 1.0649813413619995, 'learning_rate': 5.534382370416036e-07, 'epoch': 0.97}
{'loss': 0.7287, 'grad_norm': 1.2784202098846436, 'learning_rate': 5.478104631726711e-07, 'epoch': 0.97}
{'loss': 0.8569, 'grad_norm': 1.1656512022018433, 'learning_rate': 5.422113708361764e-07, 'epoch': 0.97}
{'loss': 0.6861, 'grad_norm': 0.9874894618988037, 'learning_rate': 5.366409616468837e-07, 'epoch': 0.97}
{'loss': 0.9117, 'grad_norm': 1.1991993188858032, 'learning_rate': 5.310992372112522e-07, 'epoch': 0.97}
{'loss': 0.8823, 'grad_norm': 1.2429789304733276, 'learning_rate': 5.255861991275035e-07, 'epoch': 0.97}
{'loss': 1.1674, 'grad_norm': 1.1370140314102173, 'learning_rate': 5.201018489855658e-07, 'epoch': 0.97}
{'loss': 1.0704, 'grad_norm': 1.32692289352417, 'learning_rate': 5.146461883671072e-07, 'epoch': 0.97}
{'loss': 0.9802, 'grad_norm': 1.0674282312393188, 'learning_rate': 5.092192188455025e-07, 'epoch': 0.97}
{'loss': 0.9949, 'grad_norm': 1.0858237743377686, 'learning_rate': 5.038209419858664e-07, 'epoch': 0.97}
{'loss': 0.8437, 'grad_norm': 1.1030305624008179, 'learning_rate': 4.984513593450424e-07, 'epoch': 0.97}
{'loss': 0.8574, 'grad_norm': 1.2015694379806519, 'learning_rate': 4.931104724715696e-07, 'epoch': 0.97}
{'loss': 0.8136, 'grad_norm': 1.1222448348999023, 'learning_rate': 4.877982829057714e-07, 'epoch': 0.97}
{'loss': 1.1557, 'grad_norm': 0.9030912518501282, 'learning_rate': 4.825147921796225e-07, 'epoch': 0.97}
{'loss': 1.1485, 'grad_norm': 1.0428251028060913, 'learning_rate': 4.772600018168816e-07, 'epoch': 0.97}
{'loss': 0.8047, 'grad_norm': 1.2052185535430908, 'learning_rate': 4.720339133329921e-07, 'epoch': 0.97}
{'loss': 1.0412, 'grad_norm': 1.3000152111053467, 'learning_rate': 4.668365282351372e-07, 'epoch': 0.97}
{'loss': 0.9113, 'grad_norm': 1.1114692687988281, 'learning_rate': 4.6166784802221805e-07, 'epoch': 0.97}
{'loss': 1.0779, 'grad_norm': 1.2224047183990479, 'learning_rate': 4.5652787418485334e-07, 'epoch': 0.97}
{'loss': 1.0612, 'grad_norm': 1.125948190689087, 'learning_rate': 4.514166082053795e-07, 'epoch': 0.97}
{'loss': 1.0909, 'grad_norm': 1.1066211462020874, 'learning_rate': 4.46334051557884e-07, 'epoch': 0.97}
{'loss': 1.0141, 'grad_norm': 1.1325191259384155, 'learning_rate': 4.412802057081278e-07, 'epoch': 0.97}
{'loss': 0.8739, 'grad_norm': 1.3796484470367432, 'learning_rate': 4.362550721136338e-07, 'epoch': 0.97}
{'loss': 1.1273, 'grad_norm': 1.0904184579849243, 'learning_rate': 4.312586522236206e-07, 'epoch': 0.97}
{'loss': 1.0426, 'grad_norm': 1.07816743850708, 'learning_rate': 4.262909474790133e-07, 'epoch': 0.97}
{'loss': 0.8862, 'grad_norm': 0.9650077223777771, 'learning_rate': 4.2135195931249926e-07, 'epoch': 0.97}
{'loss': 0.638, 'grad_norm': 0.9686037302017212, 'learning_rate': 4.164416891484502e-07, 'epoch': 0.97}
{'loss': 1.5647, 'grad_norm': 1.0877448320388794, 'learning_rate': 4.115601384029666e-07, 'epoch': 0.97}
{'loss': 0.8897, 'grad_norm': 1.0648704767227173, 'learning_rate': 4.0670730848385576e-07, 'epoch': 0.97}
{'loss': 0.7309, 'grad_norm': 1.3237234354019165, 'learning_rate': 4.0188320079065365e-07, 'epoch': 0.97}
{'loss': 0.7829, 'grad_norm': 1.6842998266220093, 'learning_rate': 3.9708781671462527e-07, 'epoch': 0.97}
{'loss': 0.7452, 'grad_norm': 1.0896925926208496, 'learning_rate': 3.923211576387087e-07, 'epoch': 0.97}
{'loss': 0.7702, 'grad_norm': 1.0382806062698364, 'learning_rate': 3.8758322493760436e-07, 'epoch': 0.97}
{'loss': 1.0753, 'grad_norm': 1.115488886833191, 'learning_rate': 3.828740199777081e-07, 'epoch': 0.97}
{'loss': 0.8794, 'grad_norm': 1.1452983617782593, 'learning_rate': 3.781935441171336e-07, 'epoch': 0.98}
{'loss': 1.1265, 'grad_norm': 2.1476192474365234, 'learning_rate': 3.7354179870568994e-07, 'epoch': 0.98}
{'loss': 0.9734, 'grad_norm': 1.3290976285934448, 'learning_rate': 3.689187850849374e-07, 'epoch': 0.98}
{'loss': 0.9451, 'grad_norm': 1.2648061513900757, 'learning_rate': 3.643245045881205e-07, 'epoch': 0.98}
{'loss': 0.9086, 'grad_norm': 1.302337408065796, 'learning_rate': 3.597589585402128e-07, 'epoch': 0.98}
{'loss': 1.1973, 'grad_norm': 0.9358084797859192, 'learning_rate': 3.552221482579055e-07, 'epoch': 0.98}
{'loss': 0.9601, 'grad_norm': 1.2448046207427979, 'learning_rate': 3.50714075049563e-07, 'epoch': 0.98}
{'loss': 0.9038, 'grad_norm': 1.1048429012298584, 'learning_rate': 3.462347402153232e-07, 'epoch': 0.98}
{'loss': 1.0441, 'grad_norm': 1.165272831916809, 'learning_rate': 3.4178414504698606e-07, 'epoch': 0.98}
{'loss': 0.8019, 'grad_norm': 1.2453274726867676, 'learning_rate': 3.373622908280916e-07, 'epoch': 0.98}
{'loss': 0.826, 'grad_norm': 0.9554000496864319, 'learning_rate': 3.329691788338751e-07, 'epoch': 0.98}
{'loss': 0.7885, 'grad_norm': 0.9393418431282043, 'learning_rate': 3.2860481033129e-07, 'epoch': 0.98}
{'loss': 0.9458, 'grad_norm': 1.0887880325317383, 'learning_rate': 3.2426918657900704e-07, 'epoch': 0.98}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.025, 'grad_norm': 1.3268500566482544, 'learning_rate': 3.1996230882738175e-07, 'epoch': 0.98}
{'loss': 1.0117, 'grad_norm': 1.0787456035614014, 'learning_rate': 3.156841783185205e-07, 'epoch': 0.98}
{'loss': 0.987, 'grad_norm': 1.0124839544296265, 'learning_rate': 3.114347962861919e-07, 'epoch': 0.98}
{'loss': 1.474, 'grad_norm': 1.0063079595565796, 'learning_rate': 3.072141639559045e-07, 'epoch': 0.98}
{'loss': 0.7494, 'grad_norm': 1.0788931846618652, 'learning_rate': 3.0302228254488474e-07, 'epoch': 0.98}
{'loss': 0.9672, 'grad_norm': 1.2048633098602295, 'learning_rate': 2.988591532620322e-07, 'epoch': 0.98}
{'loss': 0.9244, 'grad_norm': 1.109129548072815, 'learning_rate': 2.947247773079753e-07, 'epoch': 0.98}
{'loss': 0.8253, 'grad_norm': 1.2131763696670532, 'learning_rate': 2.9061915587506036e-07, 'epoch': 0.98}
{'loss': 0.9089, 'grad_norm': 1.0876277685165405, 'learning_rate': 2.8654229014730694e-07, 'epoch': 0.98}
{'loss': 0.8633, 'grad_norm': 1.2149715423583984, 'learning_rate': 2.8249418130049665e-07, 'epoch': 0.98}
{'loss': 1.384, 'grad_norm': 1.2314425706863403, 'learning_rate': 2.784748305020513e-07, 'epoch': 0.98}
{'loss': 1.0426, 'grad_norm': 1.1160800457000732, 'learning_rate': 2.744842389111546e-07, 'epoch': 0.98}
{'loss': 0.8638, 'grad_norm': 1.0472785234451294, 'learning_rate': 2.7052240767865276e-07, 'epoch': 0.98}
{'loss': 1.049, 'grad_norm': 1.1991477012634277, 'learning_rate': 2.665893379471429e-07, 'epoch': 0.98}
{'loss': 0.8416, 'grad_norm': 1.2568848133087158, 'learning_rate': 2.6268503085089547e-07, 'epoch': 0.98}
{'loss': 0.6318, 'grad_norm': 0.9393089413642883, 'learning_rate': 2.5880948751587643e-07, 'epoch': 0.98}
{'loss': 1.0802, 'grad_norm': 1.3481436967849731, 'learning_rate': 2.549627090598028e-07, 'epoch': 0.98}
{'loss': 0.7943, 'grad_norm': 1.208491563796997, 'learning_rate': 2.5114469659203167e-07, 'epoch': 0.98}
{'loss': 0.7372, 'grad_norm': 1.0808062553405762, 'learning_rate': 2.473554512136933e-07, 'epoch': 0.98}
{'loss': 0.7833, 'grad_norm': 1.1309267282485962, 'learning_rate': 2.4359497401758024e-07, 'epoch': 0.98}
{'loss': 0.8591, 'grad_norm': 0.9879080653190613, 'learning_rate': 2.3986326608818054e-07, 'epoch': 0.98}
{'loss': 0.942, 'grad_norm': 1.2565456628799438, 'learning_rate': 2.361603285017111e-07, 'epoch': 0.98}
{'loss': 1.0033, 'grad_norm': 1.0982543230056763, 'learning_rate': 2.3248616232608433e-07, 'epoch': 0.98}
{'loss': 0.9012, 'grad_norm': 1.0453859567642212, 'learning_rate': 2.288407686208971e-07, 'epoch': 0.98}
{'loss': 1.1531, 'grad_norm': 1.2438225746154785, 'learning_rate': 2.2522414843748618e-07, 'epoch': 0.98}
{'loss': 0.8481, 'grad_norm': 0.9848310351371765, 'learning_rate': 2.2163630281885062e-07, 'epoch': 0.98}
{'loss': 0.7645, 'grad_norm': 1.514384150505066, 'learning_rate': 2.1807723279971826e-07, 'epoch': 0.98}
{'loss': 0.8403, 'grad_norm': 1.3921810388565063, 'learning_rate': 2.145469394065014e-07, 'epoch': 0.98}
{'loss': 0.6796, 'grad_norm': 1.0328344106674194, 'learning_rate': 2.1104542365730783e-07, 'epoch': 0.98}
{'loss': 0.7585, 'grad_norm': 1.1289862394332886, 'learning_rate': 2.0757268656198537e-07, 'epoch': 0.98}
{'loss': 0.8295, 'grad_norm': 1.1177393198013306, 'learning_rate': 2.0412872912203284e-07, 'epoch': 0.98}
{'loss': 1.0535, 'grad_norm': 1.0200848579406738, 'learning_rate': 2.00713552330678e-07, 'epoch': 0.98}
{'loss': 1.0088, 'grad_norm': 1.1702747344970703, 'learning_rate': 1.973271571728441e-07, 'epoch': 0.98}
{'loss': 1.1953, 'grad_norm': 1.0745216608047485, 'learning_rate': 1.9396954462514994e-07, 'epoch': 0.98}
{'loss': 1.2523, 'grad_norm': 1.1240897178649902, 'learning_rate': 1.906407156559098e-07, 'epoch': 0.98}
{'loss': 0.8857, 'grad_norm': 1.1819061040878296, 'learning_rate': 1.8734067122514465e-07, 'epoch': 0.98}
{'loss': 1.165, 'grad_norm': 1.2260066270828247, 'learning_rate': 1.8406941228457098e-07, 'epoch': 0.98}
{'loss': 1.2162, 'grad_norm': 1.0892857313156128, 'learning_rate': 1.808269397776119e-07, 'epoch': 0.98}
{'loss': 1.022, 'grad_norm': 1.1703674793243408, 'learning_rate': 1.7761325463937494e-07, 'epoch': 0.98}
{'loss': 1.0074, 'grad_norm': 1.3073588609695435, 'learning_rate': 1.744283577966632e-07, 'epoch': 0.98}
{'loss': 0.5863, 'grad_norm': 1.142970323562622, 'learning_rate': 1.7127225016799753e-07, 'epoch': 0.98}
{'loss': 0.6786, 'grad_norm': 1.1234335899353027, 'learning_rate': 1.68144932663572e-07, 'epoch': 0.98}
{'loss': 0.8011, 'grad_norm': 1.2083803415298462, 'learning_rate': 1.6504640618530963e-07, 'epoch': 0.98}
{'loss': 0.8194, 'grad_norm': 1.221237301826477, 'learning_rate': 1.6197667162679564e-07, 'epoch': 0.98}
{'loss': 0.8163, 'grad_norm': 1.0045377016067505, 'learning_rate': 1.5893572987333293e-07, 'epoch': 0.98}
{'loss': 0.8819, 'grad_norm': 1.0443344116210938, 'learning_rate': 1.5592358180189782e-07, 'epoch': 0.98}
{'loss': 0.9, 'grad_norm': 0.8969213962554932, 'learning_rate': 1.5294022828120647e-07, 'epoch': 0.98}
{'loss': 0.8642, 'grad_norm': 1.0187489986419678, 'learning_rate': 1.4998567017162624e-07, 'epoch': 0.98}
{'loss': 1.087, 'grad_norm': 1.066584587097168, 'learning_rate': 1.470599083252311e-07, 'epoch': 0.98}
{'loss': 1.1141, 'grad_norm': 1.6517410278320312, 'learning_rate': 1.4416294358582384e-07, 'epoch': 0.98}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8383, 'grad_norm': 1.4118573665618896, 'learning_rate': 1.412947767888473e-07, 'epoch': 0.98}
{'loss': 0.9174, 'grad_norm': 1.3962352275848389, 'learning_rate': 1.3845540876147312e-07, 'epoch': 0.98}
{'loss': 0.7145, 'grad_norm': 1.1287163496017456, 'learning_rate': 1.3564484032257963e-07, 'epoch': 0.99}
{'loss': 0.9708, 'grad_norm': 1.220845103263855, 'learning_rate': 1.3286307228269623e-07, 'epoch': 0.99}
{'loss': 0.9829, 'grad_norm': 1.0294402837753296, 'learning_rate': 1.301101054440923e-07, 'epoch': 0.99}
{'loss': 0.8977, 'grad_norm': 1.0783733129501343, 'learning_rate': 1.273859406006883e-07, 'epoch': 0.99}
{'loss': 1.1809, 'grad_norm': 1.1780463457107544, 'learning_rate': 1.246905785381447e-07, 'epoch': 0.99}
{'loss': 1.1675, 'grad_norm': 1.181054711341858, 'learning_rate': 1.22024020033773e-07, 'epoch': 0.99}
{'loss': 1.0926, 'grad_norm': 1.0106302499771118, 'learning_rate': 1.193862658566025e-07, 'epoch': 0.99}
{'loss': 0.6178, 'grad_norm': 0.8459458351135254, 'learning_rate': 1.1677731676733584e-07, 'epoch': 0.99}
{'loss': 0.6563, 'grad_norm': 1.339693546295166, 'learning_rate': 1.1419717351841552e-07, 'epoch': 0.99}
{'loss': 0.8128, 'grad_norm': 0.9376049041748047, 'learning_rate': 1.1164583685390195e-07, 'epoch': 0.99}
{'loss': 0.8983, 'grad_norm': 1.3616033792495728, 'learning_rate': 1.0912330750961763e-07, 'epoch': 0.99}
{'loss': 0.8784, 'grad_norm': 1.3514643907546997, 'learning_rate': 1.066295862130362e-07, 'epoch': 0.99}
{'loss': 0.5907, 'grad_norm': 1.0876550674438477, 'learning_rate': 1.0416467368332683e-07, 'epoch': 0.99}
{'loss': 1.0301, 'grad_norm': 1.050321102142334, 'learning_rate': 1.0172857063137641e-07, 'epoch': 0.99}
{'loss': 0.8071, 'grad_norm': 1.331164836883545, 'learning_rate': 9.93212777597341e-08, 'epoch': 0.99}
{'loss': 0.8139, 'grad_norm': 1.1703808307647705, 'learning_rate': 9.694279576265564e-08, 'epoch': 0.99}
{'loss': 0.9367, 'grad_norm': 1.0357838869094849, 'learning_rate': 9.459312532608122e-08, 'epoch': 0.99}
{'loss': 0.8018, 'grad_norm': 1.0010457038879395, 'learning_rate': 9.227226712763549e-08, 'epoch': 0.99}
{'loss': 0.9576, 'grad_norm': 1.2798579931259155, 'learning_rate': 8.998022183666077e-08, 'epoch': 0.99}
{'loss': 0.9495, 'grad_norm': 0.9813137650489807, 'learning_rate': 8.771699011416168e-08, 'epoch': 0.99}
{'loss': 0.772, 'grad_norm': 1.1559979915618896, 'learning_rate': 8.548257261284941e-08, 'epoch': 0.99}
{'loss': 0.8724, 'grad_norm': 1.2192869186401367, 'learning_rate': 8.327696997710855e-08, 'epoch': 0.99}
{'loss': 0.6999, 'grad_norm': 1.470641851425171, 'learning_rate': 8.110018284304133e-08, 'epoch': 0.99}
{'loss': 0.9556, 'grad_norm': 1.084809422492981, 'learning_rate': 7.895221183840118e-08, 'epoch': 0.99}
{'loss': 0.8174, 'grad_norm': 0.9667638540267944, 'learning_rate': 7.683305758265924e-08, 'epoch': 0.99}
{'loss': 0.8784, 'grad_norm': 1.328122854232788, 'learning_rate': 7.474272068698218e-08, 'epoch': 0.99}
{'loss': 0.9699, 'grad_norm': 1.2736237049102783, 'learning_rate': 7.268120175421001e-08, 'epoch': 0.99}
{'loss': 1.0966, 'grad_norm': 2.2535552978515625, 'learning_rate': 7.064850137885604e-08, 'epoch': 0.99}
{'loss': 1.0778, 'grad_norm': 1.189590573310852, 'learning_rate': 6.864462014716245e-08, 'epoch': 0.99}
{'loss': 0.7321, 'grad_norm': 1.148496150970459, 'learning_rate': 6.666955863703361e-08, 'epoch': 0.99}
{'loss': 0.769, 'grad_norm': 1.748997449874878, 'learning_rate': 6.472331741805837e-08, 'epoch': 0.99}
{'loss': 0.7273, 'grad_norm': 1.0878841876983643, 'learning_rate': 6.280589705153217e-08, 'epoch': 0.99}
{'loss': 0.7987, 'grad_norm': 1.1437791585922241, 'learning_rate': 6.09172980904238e-08, 'epoch': 0.99}
{'loss': 0.8219, 'grad_norm': 1.191927194595337, 'learning_rate': 5.905752107940865e-08, 'epoch': 0.99}
{'loss': 0.9405, 'grad_norm': 1.1249457597732544, 'learning_rate': 5.722656655482439e-08, 'epoch': 0.99}
{'loss': 0.9828, 'grad_norm': 1.2251185178756714, 'learning_rate': 5.542443504471528e-08, 'epoch': 0.99}
{'loss': 1.0024, 'grad_norm': 1.1257078647613525, 'learning_rate': 5.3651127068798934e-08, 'epoch': 0.99}
{'loss': 0.9322, 'grad_norm': 1.2135933637619019, 'learning_rate': 5.190664313851068e-08, 'epoch': 0.99}
{'loss': 0.7428, 'grad_norm': 1.1366270780563354, 'learning_rate': 5.019098375692588e-08, 'epoch': 0.99}
{'loss': 0.8689, 'grad_norm': 0.861165463924408, 'learning_rate': 4.850414941883763e-08, 'epoch': 0.99}
{'loss': 1.0532, 'grad_norm': 1.1084290742874146, 'learning_rate': 4.684614061073456e-08, 'epoch': 0.99}
{'loss': 1.1796, 'grad_norm': 1.1768420934677124, 'learning_rate': 4.521695781076751e-08, 'epoch': 0.99}
{'loss': 0.9507, 'grad_norm': 1.1727519035339355, 'learning_rate': 4.361660148879398e-08, 'epoch': 0.99}
{'loss': 1.0937, 'grad_norm': 1.1422309875488281, 'learning_rate': 4.2045072106333684e-08, 'epoch': 0.99}
{'loss': 0.9947, 'grad_norm': 1.1068748235702515, 'learning_rate': 4.0502370116624055e-08, 'epoch': 0.99}
{'loss': 0.8348, 'grad_norm': 1.1458207368850708, 'learning_rate': 3.898849596456478e-08, 'epoch': 0.99}
{'loss': 0.9747, 'grad_norm': 1.098097562789917, 'learning_rate': 3.750345008675105e-08, 'epoch': 0.99}
{'loss': 0.9213, 'grad_norm': 1.5025495290756226, 'learning_rate': 3.60472329114625e-08, 'epoch': 0.99}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0525, 'grad_norm': 1.0496200323104858, 'learning_rate': 3.461984485866321e-08, 'epoch': 0.99}
{'loss': 0.9666, 'grad_norm': 1.071669101715088, 'learning_rate': 3.3221286340001654e-08, 'epoch': 0.99}
{'loss': 0.9201, 'grad_norm': 1.020847201347351, 'learning_rate': 3.1851557758832975e-08, 'epoch': 0.99}
{'loss': 0.8542, 'grad_norm': 1.1052509546279907, 'learning_rate': 3.0510659510163406e-08, 'epoch': 0.99}
{'loss': 1.0655, 'grad_norm': 1.1631697416305542, 'learning_rate': 2.9198591980705848e-08, 'epoch': 0.99}
{'loss': 0.9432, 'grad_norm': 0.9889925718307495, 'learning_rate': 2.7915355548857603e-08, 'epoch': 0.99}
{'loss': 0.6665, 'grad_norm': 1.1383936405181885, 'learning_rate': 2.6660950584689314e-08, 'epoch': 0.99}
{'loss': 0.8728, 'grad_norm': 1.1410914659500122, 'learning_rate': 2.543537744997826e-08, 'epoch': 0.99}
{'loss': 1.0434, 'grad_norm': 1.0810256004333496, 'learning_rate': 2.4238636498163935e-08, 'epoch': 0.99}
{'loss': 0.7244, 'grad_norm': 0.8860263228416443, 'learning_rate': 2.3070728074381376e-08, 'epoch': 0.99}
{'loss': 0.7001, 'grad_norm': 1.0164153575897217, 'learning_rate': 2.193165251545004e-08, 'epoch': 0.99}
{'loss': 0.7911, 'grad_norm': 1.1476573944091797, 'learning_rate': 2.0821410149884922e-08, 'epoch': 0.99}
{'loss': 0.575, 'grad_norm': 0.9296233654022217, 'learning_rate': 1.974000129785214e-08, 'epoch': 0.99}
{'loss': 0.8339, 'grad_norm': 1.3251205682754517, 'learning_rate': 1.8687426271246645e-08, 'epoch': 0.99}
{'loss': 1.0722, 'grad_norm': 1.2752768993377686, 'learning_rate': 1.7663685373614514e-08, 'epoch': 0.99}
{'loss': 1.128, 'grad_norm': 1.3143839836120605, 'learning_rate': 1.666877890020846e-08, 'epoch': 0.99}
{'loss': 1.0871, 'grad_norm': 1.0266081094741821, 'learning_rate': 1.570270713794342e-08, 'epoch': 0.99}
{'loss': 1.1454, 'grad_norm': 1.244694471359253, 'learning_rate': 1.4765470365429857e-08, 'epoch': 1.0}
{'loss': 0.8891, 'grad_norm': 1.0145459175109863, 'learning_rate': 1.3857068852962674e-08, 'epoch': 1.0}
{'loss': 1.0872, 'grad_norm': 0.986576497554779, 'learning_rate': 1.2977502862532297e-08, 'epoch': 1.0}
{'loss': 0.7817, 'grad_norm': 1.0530568361282349, 'learning_rate': 1.2126772647780282e-08, 'epoch': 1.0}
{'loss': 0.9101, 'grad_norm': 1.2180155515670776, 'learning_rate': 1.1304878454077017e-08, 'epoch': 1.0}
{'loss': 0.7234, 'grad_norm': 0.8748694658279419, 'learning_rate': 1.0511820518432913e-08, 'epoch': 1.0}
{'loss': 0.912, 'grad_norm': 1.009824275970459, 'learning_rate': 9.747599069576119e-09, 'epoch': 1.0}
{'loss': 0.9688, 'grad_norm': 1.1330561637878418, 'learning_rate': 9.012214327897006e-09, 'epoch': 1.0}
{'loss': 0.7467, 'grad_norm': 1.0434792041778564, 'learning_rate': 8.305666505481479e-09, 'epoch': 1.0}
{'loss': 0.6682, 'grad_norm': 1.1895869970321655, 'learning_rate': 7.627955806088772e-09, 'epoch': 1.0}
{'loss': 1.1203, 'grad_norm': 1.227810025215149, 'learning_rate': 6.9790824251736444e-09, 'epoch': 1.0}
{'loss': 0.5103, 'grad_norm': 0.932111918926239, 'learning_rate': 6.359046549864189e-09, 'epoch': 1.0}
{'loss': 0.8668, 'grad_norm': 1.1699074506759644, 'learning_rate': 5.767848358972927e-09, 'epoch': 1.0}
{'loss': 1.1275, 'grad_norm': 0.923089325428009, 'learning_rate': 5.205488022996807e-09, 'epoch': 1.0}
{'loss': 1.2587, 'grad_norm': 1.1871006488800049, 'learning_rate': 4.6719657041283114e-09, 'epoch': 1.0}
{'loss': 1.2783, 'grad_norm': 1.114850401878357, 'learning_rate': 4.167281556233249e-09, 'epoch': 1.0}
{'loss': 0.941, 'grad_norm': 1.0186055898666382, 'learning_rate': 3.6914357248507556e-09, 'epoch': 1.0}
{'loss': 0.7331, 'grad_norm': 1.1610146760940552, 'learning_rate': 3.244428347204398e-09, 'epoch': 1.0}
{'loss': 1.1232, 'grad_norm': 0.974727988243103, 'learning_rate': 2.8262595522354775e-09, 'epoch': 1.0}
{'loss': 0.7225, 'grad_norm': 0.9890177845954895, 'learning_rate': 2.4369294605253166e-09, 'epoch': 1.0}
{'loss': 1.1377, 'grad_norm': 0.9458373785018921, 'learning_rate': 2.0764381843507708e-09, 'epoch': 1.0}
{'loss': 0.8826, 'grad_norm': 1.2878363132476807, 'learning_rate': 1.7447858276842255e-09, 'epoch': 1.0}
{'loss': 0.8321, 'grad_norm': 1.0862528085708618, 'learning_rate': 1.4419724861602924e-09, 'epoch': 1.0}
{'loss': 0.7966, 'grad_norm': 1.2837707996368408, 'learning_rate': 1.167998247131319e-09, 'epoch': 1.0}
{'loss': 1.1131, 'grad_norm': 1.1105834245681763, 'learning_rate': 9.228631895896733e-10, 'epoch': 1.0}
{'loss': 0.9447, 'grad_norm': 1.202182412147522, 'learning_rate': 7.065673842454601e-10, 'epoch': 1.0}
{'loss': 1.0195, 'grad_norm': 1.2224708795547485, 'learning_rate': 5.191108934710087e-10, 'epoch': 1.0}
{'loss': 1.1032, 'grad_norm': 1.0878344774246216, 'learning_rate': 3.604937713230783e-10, 'epoch': 1.0}
{'loss': 0.8732, 'grad_norm': 1.2456035614013672, 'learning_rate': 2.3071606354285735e-10, 'epoch': 1.0}
{'loss': 1.0211, 'grad_norm': 1.308740258216858, 'learning_rate': 1.297778075781686e-10, 'epoch': 1.0}
{'loss': 1.0455, 'grad_norm': 0.9536089897155762, 'learning_rate': 5.767903251685525e-11, 'epoch': 1.0}
{'loss': 0.821, 'grad_norm': 1.0860595703125, 'learning_rate': 1.4419759164496782e-11, 'epoch': 1.0}
{'loss': 1.1662, 'grad_norm': 0.9406575560569763, 'learning_rate': 0.0, 'epoch': 1.0}
