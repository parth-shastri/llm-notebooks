/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 3.6079, 'grad_norm': 111.8030014038086, 'learning_rate': 3.076923076923077e-07, 'epoch': 0.0}
{'loss': 3.2496, 'grad_norm': 95.51612854003906, 'learning_rate': 6.153846153846154e-07, 'epoch': 0.0}
{'loss': 3.2556, 'grad_norm': 55.39114761352539, 'learning_rate': 9.230769230769232e-07, 'epoch': 0.0}
{'loss': 4.1383, 'grad_norm': 99.63709259033203, 'learning_rate': 1.2307692307692308e-06, 'epoch': 0.0}
{'loss': 3.3144, 'grad_norm': 120.47177124023438, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.0}
{'loss': 4.4969, 'grad_norm': 89.80597686767578, 'learning_rate': 1.8461538461538465e-06, 'epoch': 0.0}
{'loss': 2.4469, 'grad_norm': 72.35269927978516, 'learning_rate': 2.1538461538461538e-06, 'epoch': 0.0}
{'loss': 4.1897, 'grad_norm': 70.2944564819336, 'learning_rate': 2.4615384615384615e-06, 'epoch': 0.0}
{'loss': 3.1332, 'grad_norm': 43.622100830078125, 'learning_rate': 2.7692307692307693e-06, 'epoch': 0.0}
{'loss': 4.0497, 'grad_norm': 90.15370178222656, 'learning_rate': 3.0769230769230774e-06, 'epoch': 0.0}
{'loss': 3.9403, 'grad_norm': 81.7453842163086, 'learning_rate': 3.3846153846153848e-06, 'epoch': 0.0}
{'loss': 3.699, 'grad_norm': 102.64741516113281, 'learning_rate': 3.692307692307693e-06, 'epoch': 0.0}
{'loss': 3.9966, 'grad_norm': 50.91511535644531, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}
{'loss': 3.4789, 'grad_norm': 50.0421142578125, 'learning_rate': 4.3076923076923076e-06, 'epoch': 0.0}
{'loss': 3.7844, 'grad_norm': 53.56943130493164, 'learning_rate': 4.615384615384616e-06, 'epoch': 0.0}
{'loss': 3.9975, 'grad_norm': 53.21821594238281, 'learning_rate': 4.923076923076923e-06, 'epoch': 0.0}
{'loss': 4.0653, 'grad_norm': 101.2596435546875, 'learning_rate': 5.230769230769231e-06, 'epoch': 0.0}
{'loss': 3.4069, 'grad_norm': 48.83671188354492, 'learning_rate': 5.5384615384615385e-06, 'epoch': 0.0}
{'loss': 3.7014, 'grad_norm': 69.72761535644531, 'learning_rate': 5.846153846153846e-06, 'epoch': 0.0}
{'loss': 3.5719, 'grad_norm': 106.39552307128906, 'learning_rate': 6.153846153846155e-06, 'epoch': 0.0}
{'loss': 3.8553, 'grad_norm': 84.00667572021484, 'learning_rate': 6.461538461538462e-06, 'epoch': 0.0}
{'loss': 3.3915, 'grad_norm': 82.28372192382812, 'learning_rate': 6.7692307692307695e-06, 'epoch': 0.0}
{'loss': 2.6705, 'grad_norm': 27.22332000732422, 'learning_rate': 7.076923076923076e-06, 'epoch': 0.0}
{'loss': 3.7717, 'grad_norm': 74.0267333984375, 'learning_rate': 7.384615384615386e-06, 'epoch': 0.0}
{'loss': 4.0475, 'grad_norm': 44.4031982421875, 'learning_rate': 7.692307692307694e-06, 'epoch': 0.0}
{'loss': 3.4409, 'grad_norm': 48.5109748840332, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}
{'loss': 3.6534, 'grad_norm': 33.59290313720703, 'learning_rate': 8.307692307692307e-06, 'epoch': 0.0}
{'loss': 2.8376, 'grad_norm': 33.82050704956055, 'learning_rate': 8.615384615384615e-06, 'epoch': 0.0}
{'loss': 3.5646, 'grad_norm': 35.54010772705078, 'learning_rate': 8.923076923076923e-06, 'epoch': 0.0}
{'loss': 3.1963, 'grad_norm': 39.84110641479492, 'learning_rate': 9.230769230769232e-06, 'epoch': 0.0}
{'loss': 3.5189, 'grad_norm': 29.567293167114258, 'learning_rate': 9.538461538461538e-06, 'epoch': 0.0}
{'loss': 3.2407, 'grad_norm': 145.2592315673828, 'learning_rate': 9.846153846153846e-06, 'epoch': 0.0}
{'loss': 2.7787, 'grad_norm': 30.354389190673828, 'learning_rate': 1.0153846153846154e-05, 'epoch': 0.01}
{'loss': 3.5294, 'grad_norm': 27.614961624145508, 'learning_rate': 1.0461538461538462e-05, 'epoch': 0.01}
{'loss': 3.1521, 'grad_norm': 28.234233856201172, 'learning_rate': 1.0769230769230771e-05, 'epoch': 0.01}
{'loss': 3.0373, 'grad_norm': 21.652088165283203, 'learning_rate': 1.1076923076923077e-05, 'epoch': 0.01}
{'loss': 2.9744, 'grad_norm': 26.28086280822754, 'learning_rate': 1.1384615384615385e-05, 'epoch': 0.01}
{'loss': 2.8678, 'grad_norm': 138.29165649414062, 'learning_rate': 1.1692307692307693e-05, 'epoch': 0.01}
{'loss': 3.1648, 'grad_norm': 28.029794692993164, 'learning_rate': 1.2e-05, 'epoch': 0.01}
{'loss': 3.2742, 'grad_norm': 39.15504455566406, 'learning_rate': 1.230769230769231e-05, 'epoch': 0.01}
{'loss': 3.2844, 'grad_norm': 28.802968978881836, 'learning_rate': 1.2615384615384616e-05, 'epoch': 0.01}
{'loss': 3.0675, 'grad_norm': 24.776430130004883, 'learning_rate': 1.2923076923076924e-05, 'epoch': 0.01}
{'loss': 2.9482, 'grad_norm': 28.96681022644043, 'learning_rate': 1.3230769230769233e-05, 'epoch': 0.01}
{'loss': 2.7146, 'grad_norm': 24.201671600341797, 'learning_rate': 1.3538461538461539e-05, 'epoch': 0.01}
{'loss': 2.695, 'grad_norm': 22.7836971282959, 'learning_rate': 1.3846153846153847e-05, 'epoch': 0.01}
{'loss': 3.1194, 'grad_norm': 32.324588775634766, 'learning_rate': 1.4153846153846153e-05, 'epoch': 0.01}
{'loss': 2.5119, 'grad_norm': 22.432846069335938, 'learning_rate': 1.4461538461538462e-05, 'epoch': 0.01}
{'loss': 2.6093, 'grad_norm': 18.34882926940918, 'learning_rate': 1.4769230769230772e-05, 'epoch': 0.01}
{'loss': 2.819, 'grad_norm': 33.60871505737305, 'learning_rate': 1.5076923076923078e-05, 'epoch': 0.01}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 3.4129, 'grad_norm': 34.01448059082031, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.01}
{'loss': 2.5697, 'grad_norm': 25.413795471191406, 'learning_rate': 1.5692307692307693e-05, 'epoch': 0.01}
{'loss': 2.4242, 'grad_norm': 21.08018684387207, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.01}
{'loss': 3.2097, 'grad_norm': 34.25944900512695, 'learning_rate': 1.630769230769231e-05, 'epoch': 0.01}
{'loss': 2.7038, 'grad_norm': 22.43405532836914, 'learning_rate': 1.6615384615384615e-05, 'epoch': 0.01}
{'loss': 3.2359, 'grad_norm': 98.51358032226562, 'learning_rate': 1.6923076923076924e-05, 'epoch': 0.01}
{'loss': 3.0094, 'grad_norm': 26.36878776550293, 'learning_rate': 1.723076923076923e-05, 'epoch': 0.01}
{'loss': 2.181, 'grad_norm': 19.445791244506836, 'learning_rate': 1.753846153846154e-05, 'epoch': 0.01}
{'loss': 2.0364, 'grad_norm': 14.83959674835205, 'learning_rate': 1.7846153846153846e-05, 'epoch': 0.01}
{'loss': 3.1157, 'grad_norm': 38.42436218261719, 'learning_rate': 1.8153846153846155e-05, 'epoch': 0.01}
{'loss': 2.5562, 'grad_norm': 21.918676376342773, 'learning_rate': 1.8461538461538465e-05, 'epoch': 0.01}
{'loss': 2.607, 'grad_norm': 22.80889129638672, 'learning_rate': 1.876923076923077e-05, 'epoch': 0.01}
{'loss': 2.4301, 'grad_norm': 27.333608627319336, 'learning_rate': 1.9076923076923077e-05, 'epoch': 0.01}
{'loss': 2.8447, 'grad_norm': 27.400279998779297, 'learning_rate': 1.9384615384615383e-05, 'epoch': 0.01}
{'loss': 2.4595, 'grad_norm': 40.03514099121094, 'learning_rate': 1.9692307692307692e-05, 'epoch': 0.01}
{'loss': 1.9344, 'grad_norm': 20.712793350219727, 'learning_rate': 2e-05, 'epoch': 0.01}
{'loss': 2.6442, 'grad_norm': 22.725488662719727, 'learning_rate': 2.0307692307692308e-05, 'epoch': 0.01}
{'loss': 2.7713, 'grad_norm': 27.362913131713867, 'learning_rate': 2.0615384615384617e-05, 'epoch': 0.01}
{'loss': 3.0565, 'grad_norm': 34.72251892089844, 'learning_rate': 2.0923076923076923e-05, 'epoch': 0.01}
{'loss': 2.7585, 'grad_norm': 24.652053833007812, 'learning_rate': 2.1230769230769233e-05, 'epoch': 0.01}
{'loss': 2.7947, 'grad_norm': 38.382049560546875, 'learning_rate': 2.1538461538461542e-05, 'epoch': 0.01}
{'loss': 2.7801, 'grad_norm': 24.304500579833984, 'learning_rate': 2.1846153846153848e-05, 'epoch': 0.01}
{'loss': 2.1448, 'grad_norm': 20.210718154907227, 'learning_rate': 2.2153846153846154e-05, 'epoch': 0.01}
{'loss': 1.7064, 'grad_norm': 16.279464721679688, 'learning_rate': 2.246153846153846e-05, 'epoch': 0.01}
{'loss': 2.1501, 'grad_norm': 26.80422592163086, 'learning_rate': 2.276923076923077e-05, 'epoch': 0.01}
{'loss': 2.2317, 'grad_norm': 20.3630313873291, 'learning_rate': 2.307692307692308e-05, 'epoch': 0.01}
{'loss': 2.1326, 'grad_norm': 22.225133895874023, 'learning_rate': 2.3384615384615385e-05, 'epoch': 0.01}
{'loss': 2.4882, 'grad_norm': 36.48281478881836, 'learning_rate': 2.3692307692307695e-05, 'epoch': 0.01}
{'loss': 2.2868, 'grad_norm': 16.185802459716797, 'learning_rate': 2.4e-05, 'epoch': 0.01}
{'loss': 1.9293, 'grad_norm': 16.10304069519043, 'learning_rate': 2.430769230769231e-05, 'epoch': 0.01}
{'loss': 1.9995, 'grad_norm': 27.2982120513916, 'learning_rate': 2.461538461538462e-05, 'epoch': 0.01}
{'loss': 1.7599, 'grad_norm': 16.236757278442383, 'learning_rate': 2.4923076923076926e-05, 'epoch': 0.01}
{'loss': 1.8751, 'grad_norm': 22.606407165527344, 'learning_rate': 2.523076923076923e-05, 'epoch': 0.01}
{'loss': 2.1443, 'grad_norm': 16.848102569580078, 'learning_rate': 2.5538461538461538e-05, 'epoch': 0.01}
{'loss': 2.214, 'grad_norm': 18.012914657592773, 'learning_rate': 2.5846153846153847e-05, 'epoch': 0.01}
{'loss': 1.7563, 'grad_norm': 22.684709548950195, 'learning_rate': 2.6153846153846157e-05, 'epoch': 0.01}
{'loss': 1.7861, 'grad_norm': 22.312807083129883, 'learning_rate': 2.6461538461538466e-05, 'epoch': 0.01}
{'loss': 2.0599, 'grad_norm': 15.343188285827637, 'learning_rate': 2.676923076923077e-05, 'epoch': 0.01}
{'loss': 2.0486, 'grad_norm': 26.142810821533203, 'learning_rate': 2.7076923076923078e-05, 'epoch': 0.01}
{'loss': 1.9031, 'grad_norm': 25.36900520324707, 'learning_rate': 2.7384615384615387e-05, 'epoch': 0.01}
{'loss': 1.8577, 'grad_norm': 16.020158767700195, 'learning_rate': 2.7692307692307694e-05, 'epoch': 0.01}
{'loss': 1.4754, 'grad_norm': 15.483027458190918, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.01}
{'loss': 1.8545, 'grad_norm': 12.701605796813965, 'learning_rate': 2.8307692307692306e-05, 'epoch': 0.01}
{'loss': 1.85, 'grad_norm': 18.14093017578125, 'learning_rate': 2.8615384615384615e-05, 'epoch': 0.01}
{'loss': 1.6242, 'grad_norm': 16.253074645996094, 'learning_rate': 2.8923076923076925e-05, 'epoch': 0.01}
{'loss': 1.4013, 'grad_norm': 10.765795707702637, 'learning_rate': 2.9230769230769234e-05, 'epoch': 0.01}
{'loss': 1.4896, 'grad_norm': 21.379833221435547, 'learning_rate': 2.9538461538461543e-05, 'epoch': 0.01}
{'loss': 1.7504, 'grad_norm': 14.692676544189453, 'learning_rate': 2.9846153846153846e-05, 'epoch': 0.01}
{'loss': 2.0757, 'grad_norm': 22.424983978271484, 'learning_rate': 3.0153846153846155e-05, 'epoch': 0.02}
{'loss': 1.5408, 'grad_norm': 16.900707244873047, 'learning_rate': 3.0461538461538465e-05, 'epoch': 0.02}
{'loss': 1.66, 'grad_norm': 17.423585891723633, 'learning_rate': 3.0769230769230774e-05, 'epoch': 0.02}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.6573, 'grad_norm': 10.054043769836426, 'learning_rate': 3.107692307692308e-05, 'epoch': 0.02}
{'loss': 1.5288, 'grad_norm': 18.014530181884766, 'learning_rate': 3.1384615384615386e-05, 'epoch': 0.02}
{'loss': 1.5813, 'grad_norm': 16.952274322509766, 'learning_rate': 3.1692307692307696e-05, 'epoch': 0.02}
{'loss': 1.5427, 'grad_norm': 32.357147216796875, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.02}
{'loss': 1.3573, 'grad_norm': 7.657746315002441, 'learning_rate': 3.230769230769231e-05, 'epoch': 0.02}
{'loss': 1.5845, 'grad_norm': 15.853084564208984, 'learning_rate': 3.261538461538462e-05, 'epoch': 0.02}
{'loss': 1.5334, 'grad_norm': 13.347513198852539, 'learning_rate': 3.292307692307692e-05, 'epoch': 0.02}
{'loss': 1.2516, 'grad_norm': 19.85894012451172, 'learning_rate': 3.323076923076923e-05, 'epoch': 0.02}
{'loss': 1.1388, 'grad_norm': 5.524433135986328, 'learning_rate': 3.353846153846154e-05, 'epoch': 0.02}
{'loss': 1.2655, 'grad_norm': 82.21395874023438, 'learning_rate': 3.384615384615385e-05, 'epoch': 0.02}
{'loss': 1.2996, 'grad_norm': 5.679154872894287, 'learning_rate': 3.415384615384615e-05, 'epoch': 0.02}
{'loss': 1.5296, 'grad_norm': 16.218353271484375, 'learning_rate': 3.446153846153846e-05, 'epoch': 0.02}
{'loss': 1.2772, 'grad_norm': 6.747337818145752, 'learning_rate': 3.476923076923077e-05, 'epoch': 0.02}
{'loss': 1.1118, 'grad_norm': 12.561746597290039, 'learning_rate': 3.507692307692308e-05, 'epoch': 0.02}
{'loss': 1.4204, 'grad_norm': 12.054697036743164, 'learning_rate': 3.538461538461539e-05, 'epoch': 0.02}
{'loss': 1.383, 'grad_norm': 14.150876998901367, 'learning_rate': 3.569230769230769e-05, 'epoch': 0.02}
{'loss': 1.1736, 'grad_norm': 11.393294334411621, 'learning_rate': 3.6e-05, 'epoch': 0.02}
{'loss': 1.1852, 'grad_norm': 6.622124671936035, 'learning_rate': 3.630769230769231e-05, 'epoch': 0.02}
{'loss': 1.238, 'grad_norm': 9.608885765075684, 'learning_rate': 3.661538461538462e-05, 'epoch': 0.02}
{'loss': 1.1892, 'grad_norm': 12.957818031311035, 'learning_rate': 3.692307692307693e-05, 'epoch': 0.02}
{'loss': 1.2306, 'grad_norm': 5.880715370178223, 'learning_rate': 3.723076923076923e-05, 'epoch': 0.02}
{'loss': 1.1266, 'grad_norm': 7.66666316986084, 'learning_rate': 3.753846153846154e-05, 'epoch': 0.02}
{'loss': 1.0249, 'grad_norm': 13.506464004516602, 'learning_rate': 3.784615384615385e-05, 'epoch': 0.02}
{'loss': 1.2202, 'grad_norm': 4.783708095550537, 'learning_rate': 3.8153846153846153e-05, 'epoch': 0.02}
{'loss': 1.091, 'grad_norm': 6.199297904968262, 'learning_rate': 3.846153846153846e-05, 'epoch': 0.02}
{'loss': 1.2252, 'grad_norm': 10.760802268981934, 'learning_rate': 3.8769230769230766e-05, 'epoch': 0.02}
{'loss': 1.2158, 'grad_norm': 5.049561977386475, 'learning_rate': 3.9076923076923075e-05, 'epoch': 0.02}
{'loss': 1.1737, 'grad_norm': 8.125734329223633, 'learning_rate': 3.9384615384615384e-05, 'epoch': 0.02}
{'loss': 0.9657, 'grad_norm': 4.599031925201416, 'learning_rate': 3.9692307692307694e-05, 'epoch': 0.02}
{'loss': 1.0736, 'grad_norm': 3.6586358547210693, 'learning_rate': 4e-05, 'epoch': 0.02}
{'loss': 1.0741, 'grad_norm': 8.131081581115723, 'learning_rate': 4.0307692307692306e-05, 'epoch': 0.02}
{'loss': 1.1441, 'grad_norm': 4.766554832458496, 'learning_rate': 4.0615384615384615e-05, 'epoch': 0.02}
{'loss': 1.0008, 'grad_norm': 14.427112579345703, 'learning_rate': 4.0923076923076925e-05, 'epoch': 0.02}
{'loss': 1.188, 'grad_norm': 9.152301788330078, 'learning_rate': 4.1230769230769234e-05, 'epoch': 0.02}
{'loss': 0.8664, 'grad_norm': 44.271541595458984, 'learning_rate': 4.1538461538461544e-05, 'epoch': 0.02}
{'loss': 0.9618, 'grad_norm': 6.972227096557617, 'learning_rate': 4.1846153846153846e-05, 'epoch': 0.02}
{'loss': 1.2024, 'grad_norm': 4.958037376403809, 'learning_rate': 4.2153846153846156e-05, 'epoch': 0.02}
{'loss': 1.1526, 'grad_norm': 7.293884754180908, 'learning_rate': 4.2461538461538465e-05, 'epoch': 0.02}
{'loss': 1.0816, 'grad_norm': 8.554080963134766, 'learning_rate': 4.2769230769230775e-05, 'epoch': 0.02}
{'loss': 1.1497, 'grad_norm': 16.845651626586914, 'learning_rate': 4.3076923076923084e-05, 'epoch': 0.02}
{'loss': 1.0839, 'grad_norm': 5.869943141937256, 'learning_rate': 4.338461538461539e-05, 'epoch': 0.02}
{'loss': 0.8286, 'grad_norm': 4.358297824859619, 'learning_rate': 4.3692307692307696e-05, 'epoch': 0.02}
{'loss': 0.8663, 'grad_norm': 6.87145471572876, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.02}
{'loss': 1.2754, 'grad_norm': 5.008349895477295, 'learning_rate': 4.430769230769231e-05, 'epoch': 0.02}
{'loss': 1.1368, 'grad_norm': 2.9847142696380615, 'learning_rate': 4.461538461538462e-05, 'epoch': 0.02}
{'loss': 0.9591, 'grad_norm': 2.6204416751861572, 'learning_rate': 4.492307692307692e-05, 'epoch': 0.02}
{'loss': 1.0661, 'grad_norm': 6.888524055480957, 'learning_rate': 4.523076923076923e-05, 'epoch': 0.02}
{'loss': 1.0571, 'grad_norm': 3.761507034301758, 'learning_rate': 4.553846153846154e-05, 'epoch': 0.02}
{'loss': 0.8769, 'grad_norm': 15.961567878723145, 'learning_rate': 4.584615384615385e-05, 'epoch': 0.02}
{'loss': 1.2644, 'grad_norm': 19.967687606811523, 'learning_rate': 4.615384615384616e-05, 'epoch': 0.02}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9976, 'grad_norm': 3.965949058532715, 'learning_rate': 4.646153846153846e-05, 'epoch': 0.02}
{'loss': 1.038, 'grad_norm': 7.2306718826293945, 'learning_rate': 4.676923076923077e-05, 'epoch': 0.02}
{'loss': 1.249, 'grad_norm': 37.607872009277344, 'learning_rate': 4.707692307692308e-05, 'epoch': 0.02}
{'loss': 1.2236, 'grad_norm': 5.648831844329834, 'learning_rate': 4.738461538461539e-05, 'epoch': 0.02}
{'loss': 0.9314, 'grad_norm': 4.902263641357422, 'learning_rate': 4.76923076923077e-05, 'epoch': 0.02}
{'loss': 1.0386, 'grad_norm': 9.533854484558105, 'learning_rate': 4.8e-05, 'epoch': 0.02}
{'loss': 0.87, 'grad_norm': 5.261300563812256, 'learning_rate': 4.830769230769231e-05, 'epoch': 0.02}
{'loss': 0.816, 'grad_norm': 4.16536808013916, 'learning_rate': 4.861538461538462e-05, 'epoch': 0.02}
{'loss': 0.9004, 'grad_norm': 3.9864587783813477, 'learning_rate': 4.892307692307693e-05, 'epoch': 0.02}
{'loss': 0.9218, 'grad_norm': 7.769700527191162, 'learning_rate': 4.923076923076924e-05, 'epoch': 0.02}
{'loss': 1.3445, 'grad_norm': 9.29693603515625, 'learning_rate': 4.953846153846154e-05, 'epoch': 0.02}
{'loss': 0.9896, 'grad_norm': 38.636680603027344, 'learning_rate': 4.984615384615385e-05, 'epoch': 0.02}
{'loss': 0.964, 'grad_norm': 5.748257160186768, 'learning_rate': 5.0153846153846154e-05, 'epoch': 0.03}
{'loss': 1.2979, 'grad_norm': 3.8605875968933105, 'learning_rate': 5.046153846153846e-05, 'epoch': 0.03}
{'loss': 0.7122, 'grad_norm': 6.59590482711792, 'learning_rate': 5.0769230769230766e-05, 'epoch': 0.03}
{'loss': 1.3148, 'grad_norm': 4.512307167053223, 'learning_rate': 5.1076923076923075e-05, 'epoch': 0.03}
{'loss': 0.9457, 'grad_norm': 7.7489447593688965, 'learning_rate': 5.1384615384615385e-05, 'epoch': 0.03}
{'loss': 0.8373, 'grad_norm': 5.576143264770508, 'learning_rate': 5.1692307692307694e-05, 'epoch': 0.03}
{'loss': 1.1055, 'grad_norm': 6.593087196350098, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.03}
{'loss': 1.3015, 'grad_norm': 9.131176948547363, 'learning_rate': 5.230769230769231e-05, 'epoch': 0.03}
{'loss': 1.1486, 'grad_norm': 2.165017604827881, 'learning_rate': 5.261538461538462e-05, 'epoch': 0.03}
{'loss': 1.262, 'grad_norm': 10.860014915466309, 'learning_rate': 5.292307692307693e-05, 'epoch': 0.03}
{'loss': 1.0339, 'grad_norm': 2.5964267253875732, 'learning_rate': 5.323076923076923e-05, 'epoch': 0.03}
{'loss': 1.1038, 'grad_norm': 12.010851860046387, 'learning_rate': 5.353846153846154e-05, 'epoch': 0.03}
{'loss': 0.7421, 'grad_norm': 24.896921157836914, 'learning_rate': 5.384615384615385e-05, 'epoch': 0.03}
{'loss': 1.1802, 'grad_norm': 13.376309394836426, 'learning_rate': 5.4153846153846156e-05, 'epoch': 0.03}
{'loss': 1.0704, 'grad_norm': 8.092555046081543, 'learning_rate': 5.4461538461538466e-05, 'epoch': 0.03}
{'loss': 1.1866, 'grad_norm': 5.51077127456665, 'learning_rate': 5.4769230769230775e-05, 'epoch': 0.03}
{'loss': 0.8944, 'grad_norm': 7.989601135253906, 'learning_rate': 5.5076923076923084e-05, 'epoch': 0.03}
{'loss': 1.0057, 'grad_norm': 5.842649459838867, 'learning_rate': 5.538461538461539e-05, 'epoch': 0.03}
{'loss': 1.1778, 'grad_norm': 5.055967330932617, 'learning_rate': 5.5692307692307696e-05, 'epoch': 0.03}
{'loss': 1.0581, 'grad_norm': 4.849943161010742, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.03}
{'loss': 1.1236, 'grad_norm': 7.9717841148376465, 'learning_rate': 5.630769230769231e-05, 'epoch': 0.03}
{'loss': 1.0713, 'grad_norm': 5.357387065887451, 'learning_rate': 5.661538461538461e-05, 'epoch': 0.03}
{'loss': 1.3238, 'grad_norm': 5.201524257659912, 'learning_rate': 5.692307692307692e-05, 'epoch': 0.03}
{'loss': 1.1015, 'grad_norm': 4.77038049697876, 'learning_rate': 5.723076923076923e-05, 'epoch': 0.03}
{'loss': 0.7391, 'grad_norm': 4.238638401031494, 'learning_rate': 5.753846153846154e-05, 'epoch': 0.03}
{'loss': 0.9622, 'grad_norm': 4.224969863891602, 'learning_rate': 5.784615384615385e-05, 'epoch': 0.03}
{'loss': 1.1509, 'grad_norm': 3.4271490573883057, 'learning_rate': 5.815384615384616e-05, 'epoch': 0.03}
{'loss': 0.927, 'grad_norm': 4.027668476104736, 'learning_rate': 5.846153846153847e-05, 'epoch': 0.03}
{'loss': 1.1954, 'grad_norm': 2.553755760192871, 'learning_rate': 5.876923076923078e-05, 'epoch': 0.03}
{'loss': 0.9616, 'grad_norm': 4.409134864807129, 'learning_rate': 5.907692307692309e-05, 'epoch': 0.03}
{'loss': 0.98, 'grad_norm': 16.4100284576416, 'learning_rate': 5.938461538461538e-05, 'epoch': 0.03}
{'loss': 0.8797, 'grad_norm': 4.119593143463135, 'learning_rate': 5.969230769230769e-05, 'epoch': 0.03}
{'loss': 1.0959, 'grad_norm': 3.5372467041015625, 'learning_rate': 6e-05, 'epoch': 0.03}
{'loss': 1.126, 'grad_norm': 2.359630584716797, 'learning_rate': 6.030769230769231e-05, 'epoch': 0.03}
{'loss': 1.023, 'grad_norm': 4.41295862197876, 'learning_rate': 6.061538461538462e-05, 'epoch': 0.03}
{'loss': 1.0985, 'grad_norm': 4.612451553344727, 'learning_rate': 6.092307692307693e-05, 'epoch': 0.03}
{'loss': 0.9129, 'grad_norm': 3.611419439315796, 'learning_rate': 6.123076923076923e-05, 'epoch': 0.03}
{'loss': 0.9498, 'grad_norm': 2.1832220554351807, 'learning_rate': 6.153846153846155e-05, 'epoch': 0.03}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.166, 'grad_norm': 7.16668701171875, 'learning_rate': 6.184615384615385e-05, 'epoch': 0.03}
{'loss': 1.1807, 'grad_norm': 4.401615142822266, 'learning_rate': 6.215384615384615e-05, 'epoch': 0.03}
{'loss': 1.1205, 'grad_norm': 6.235953330993652, 'learning_rate': 6.246153846153846e-05, 'epoch': 0.03}
{'loss': 1.0433, 'grad_norm': 3.5329370498657227, 'learning_rate': 6.276923076923077e-05, 'epoch': 0.03}
{'loss': 0.8245, 'grad_norm': 4.060758590698242, 'learning_rate': 6.307692307692308e-05, 'epoch': 0.03}
{'loss': 0.8895, 'grad_norm': 3.335479736328125, 'learning_rate': 6.338461538461539e-05, 'epoch': 0.03}
{'loss': 1.0223, 'grad_norm': 2.547222852706909, 'learning_rate': 6.36923076923077e-05, 'epoch': 0.03}
{'loss': 0.8089, 'grad_norm': 2.5622434616088867, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.03}
{'loss': 0.977, 'grad_norm': 4.675917625427246, 'learning_rate': 6.430769230769231e-05, 'epoch': 0.03}
{'loss': 0.9139, 'grad_norm': 4.207716464996338, 'learning_rate': 6.461538461538462e-05, 'epoch': 0.03}
{'loss': 1.0531, 'grad_norm': 2.208704710006714, 'learning_rate': 6.492307692307693e-05, 'epoch': 0.03}
{'loss': 1.1101, 'grad_norm': 2.497438669204712, 'learning_rate': 6.523076923076923e-05, 'epoch': 0.03}
{'loss': 0.7574, 'grad_norm': 3.1385996341705322, 'learning_rate': 6.553846153846154e-05, 'epoch': 0.03}
{'loss': 0.8862, 'grad_norm': 3.8255648612976074, 'learning_rate': 6.584615384615384e-05, 'epoch': 0.03}
{'loss': 1.019, 'grad_norm': 2.9044933319091797, 'learning_rate': 6.615384615384616e-05, 'epoch': 0.03}
{'loss': 0.8398, 'grad_norm': 4.116861820220947, 'learning_rate': 6.646153846153846e-05, 'epoch': 0.03}
{'loss': 0.8291, 'grad_norm': 3.8520705699920654, 'learning_rate': 6.676923076923078e-05, 'epoch': 0.03}
{'loss': 0.9998, 'grad_norm': 3.5362138748168945, 'learning_rate': 6.707692307692308e-05, 'epoch': 0.03}
{'loss': 0.8088, 'grad_norm': 5.2653303146362305, 'learning_rate': 6.73846153846154e-05, 'epoch': 0.03}
{'loss': 0.7684, 'grad_norm': 2.7362425327301025, 'learning_rate': 6.76923076923077e-05, 'epoch': 0.03}
{'loss': 1.0582, 'grad_norm': 3.151599168777466, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.03}
{'loss': 0.8284, 'grad_norm': 4.307023048400879, 'learning_rate': 6.83076923076923e-05, 'epoch': 0.03}
{'loss': 0.9489, 'grad_norm': 5.871078968048096, 'learning_rate': 6.861538461538462e-05, 'epoch': 0.03}
{'loss': 1.0155, 'grad_norm': 3.0834696292877197, 'learning_rate': 6.892307692307692e-05, 'epoch': 0.03}
{'loss': 0.9103, 'grad_norm': 6.202025890350342, 'learning_rate': 6.923076923076924e-05, 'epoch': 0.03}
{'loss': 1.1342, 'grad_norm': 7.437889099121094, 'learning_rate': 6.953846153846154e-05, 'epoch': 0.03}
{'loss': 0.856, 'grad_norm': 13.689546585083008, 'learning_rate': 6.984615384615386e-05, 'epoch': 0.03}
{'loss': 1.3149, 'grad_norm': 7.526970863342285, 'learning_rate': 7.015384615384616e-05, 'epoch': 0.04}
{'loss': 0.5488, 'grad_norm': 4.7448954582214355, 'learning_rate': 7.046153846153846e-05, 'epoch': 0.04}
{'loss': 1.0507, 'grad_norm': 2.9688398838043213, 'learning_rate': 7.076923076923078e-05, 'epoch': 0.04}
{'loss': 0.7528, 'grad_norm': 2.692781686782837, 'learning_rate': 7.107692307692308e-05, 'epoch': 0.04}
{'loss': 0.9712, 'grad_norm': 4.134861469268799, 'learning_rate': 7.138461538461538e-05, 'epoch': 0.04}
{'loss': 1.0192, 'grad_norm': 3.431917905807495, 'learning_rate': 7.169230769230769e-05, 'epoch': 0.04}
{'loss': 0.9807, 'grad_norm': 2.669315814971924, 'learning_rate': 7.2e-05, 'epoch': 0.04}
{'loss': 1.0843, 'grad_norm': 4.890309810638428, 'learning_rate': 7.23076923076923e-05, 'epoch': 0.04}
{'loss': 1.1537, 'grad_norm': 2.558382511138916, 'learning_rate': 7.261538461538462e-05, 'epoch': 0.04}
{'loss': 0.9127, 'grad_norm': 4.353058338165283, 'learning_rate': 7.292307692307692e-05, 'epoch': 0.04}
{'loss': 1.0368, 'grad_norm': 2.409595012664795, 'learning_rate': 7.323076923076924e-05, 'epoch': 0.04}
{'loss': 0.906, 'grad_norm': 4.644099235534668, 'learning_rate': 7.353846153846154e-05, 'epoch': 0.04}
{'loss': 1.021, 'grad_norm': 4.983823776245117, 'learning_rate': 7.384615384615386e-05, 'epoch': 0.04}
{'loss': 0.8039, 'grad_norm': 3.6496949195861816, 'learning_rate': 7.415384615384616e-05, 'epoch': 0.04}
{'loss': 0.9893, 'grad_norm': 2.4187145233154297, 'learning_rate': 7.446153846153846e-05, 'epoch': 0.04}
{'loss': 0.9323, 'grad_norm': 23.02865982055664, 'learning_rate': 7.476923076923077e-05, 'epoch': 0.04}
{'loss': 1.0111, 'grad_norm': 2.975127696990967, 'learning_rate': 7.507692307692308e-05, 'epoch': 0.04}
{'loss': 0.8715, 'grad_norm': 5.441477298736572, 'learning_rate': 7.538461538461539e-05, 'epoch': 0.04}
{'loss': 0.8833, 'grad_norm': 3.837294578552246, 'learning_rate': 7.56923076923077e-05, 'epoch': 0.04}
{'loss': 1.0588, 'grad_norm': 4.3114333152771, 'learning_rate': 7.6e-05, 'epoch': 0.04}
{'loss': 0.8154, 'grad_norm': 3.000493288040161, 'learning_rate': 7.630769230769231e-05, 'epoch': 0.04}
{'loss': 1.0967, 'grad_norm': 2.3720483779907227, 'learning_rate': 7.661538461538462e-05, 'epoch': 0.04}
{'loss': 1.0696, 'grad_norm': 3.966280221939087, 'learning_rate': 7.692307692307693e-05, 'epoch': 0.04}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0915, 'grad_norm': 3.384061098098755, 'learning_rate': 7.723076923076924e-05, 'epoch': 0.04}
{'loss': 0.9211, 'grad_norm': 4.461874008178711, 'learning_rate': 7.753846153846153e-05, 'epoch': 0.04}
{'loss': 0.9997, 'grad_norm': 2.993863582611084, 'learning_rate': 7.784615384615385e-05, 'epoch': 0.04}
{'loss': 0.9703, 'grad_norm': 3.8392417430877686, 'learning_rate': 7.815384615384615e-05, 'epoch': 0.04}
{'loss': 0.8647, 'grad_norm': 4.474936008453369, 'learning_rate': 7.846153846153847e-05, 'epoch': 0.04}
{'loss': 0.8536, 'grad_norm': 2.399817943572998, 'learning_rate': 7.876923076923077e-05, 'epoch': 0.04}
{'loss': 1.0193, 'grad_norm': 3.661052942276001, 'learning_rate': 7.907692307692309e-05, 'epoch': 0.04}
{'loss': 0.78, 'grad_norm': 2.276395559310913, 'learning_rate': 7.938461538461539e-05, 'epoch': 0.04}
{'loss': 0.9206, 'grad_norm': 4.244130611419678, 'learning_rate': 7.96923076923077e-05, 'epoch': 0.04}
{'loss': 0.8606, 'grad_norm': 4.083436965942383, 'learning_rate': 8e-05, 'epoch': 0.04}
{'loss': 0.8669, 'grad_norm': 2.7788171768188477, 'learning_rate': 8.030769230769231e-05, 'epoch': 0.04}
{'loss': 0.7745, 'grad_norm': 3.8169541358947754, 'learning_rate': 8.061538461538461e-05, 'epoch': 0.04}
{'loss': 1.0647, 'grad_norm': 3.7274506092071533, 'learning_rate': 8.092307692307693e-05, 'epoch': 0.04}
{'loss': 1.2483, 'grad_norm': 5.594935894012451, 'learning_rate': 8.123076923076923e-05, 'epoch': 0.04}
{'loss': 0.9956, 'grad_norm': 4.87260103225708, 'learning_rate': 8.153846153846155e-05, 'epoch': 0.04}
{'loss': 1.0052, 'grad_norm': 4.839268684387207, 'learning_rate': 8.184615384615385e-05, 'epoch': 0.04}
{'loss': 1.1907, 'grad_norm': 4.08429479598999, 'learning_rate': 8.215384615384615e-05, 'epoch': 0.04}
{'loss': 1.1586, 'grad_norm': 2.716367244720459, 'learning_rate': 8.246153846153847e-05, 'epoch': 0.04}
{'loss': 1.1315, 'grad_norm': 3.2443947792053223, 'learning_rate': 8.276923076923077e-05, 'epoch': 0.04}
{'loss': 1.031, 'grad_norm': 5.9865241050720215, 'learning_rate': 8.307692307692309e-05, 'epoch': 0.04}
{'loss': 0.979, 'grad_norm': 5.273483753204346, 'learning_rate': 8.338461538461538e-05, 'epoch': 0.04}
{'loss': 1.1669, 'grad_norm': 2.986828088760376, 'learning_rate': 8.369230769230769e-05, 'epoch': 0.04}
{'loss': 1.2308, 'grad_norm': 3.085561752319336, 'learning_rate': 8.4e-05, 'epoch': 0.04}
{'loss': 0.8551, 'grad_norm': 2.694766044616699, 'learning_rate': 8.430769230769231e-05, 'epoch': 0.04}
{'loss': 0.9913, 'grad_norm': 2.854174852371216, 'learning_rate': 8.461538461538461e-05, 'epoch': 0.04}
{'loss': 1.2096, 'grad_norm': 10.244943618774414, 'learning_rate': 8.492307692307693e-05, 'epoch': 0.04}
{'loss': 0.8319, 'grad_norm': 2.796538829803467, 'learning_rate': 8.523076923076923e-05, 'epoch': 0.04}
{'loss': 0.6696, 'grad_norm': 3.024529457092285, 'learning_rate': 8.553846153846155e-05, 'epoch': 0.04}
{'loss': 0.8692, 'grad_norm': 2.4805703163146973, 'learning_rate': 8.584615384615385e-05, 'epoch': 0.04}
{'loss': 0.7627, 'grad_norm': 4.6471476554870605, 'learning_rate': 8.615384615384617e-05, 'epoch': 0.04}
{'loss': 1.0614, 'grad_norm': 5.139119625091553, 'learning_rate': 8.646153846153846e-05, 'epoch': 0.04}
{'loss': 0.7412, 'grad_norm': 2.17573618888855, 'learning_rate': 8.676923076923077e-05, 'epoch': 0.04}
{'loss': 0.9033, 'grad_norm': 2.761418104171753, 'learning_rate': 8.707692307692308e-05, 'epoch': 0.04}
{'loss': 0.876, 'grad_norm': 3.4958078861236572, 'learning_rate': 8.738461538461539e-05, 'epoch': 0.04}
{'loss': 0.9114, 'grad_norm': 3.019584894180298, 'learning_rate': 8.76923076923077e-05, 'epoch': 0.04}
{'loss': 1.0502, 'grad_norm': 14.641648292541504, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.04}
{'loss': 0.9018, 'grad_norm': 2.4435174465179443, 'learning_rate': 8.830769230769231e-05, 'epoch': 0.04}
{'loss': 0.9243, 'grad_norm': 2.4399116039276123, 'learning_rate': 8.861538461538462e-05, 'epoch': 0.04}
{'loss': 1.1602, 'grad_norm': 2.652834892272949, 'learning_rate': 8.892307692307693e-05, 'epoch': 0.04}
{'loss': 0.9463, 'grad_norm': 4.294661521911621, 'learning_rate': 8.923076923076924e-05, 'epoch': 0.04}
{'loss': 1.055, 'grad_norm': 2.5633952617645264, 'learning_rate': 8.953846153846154e-05, 'epoch': 0.04}
{'loss': 1.001, 'grad_norm': 6.189992427825928, 'learning_rate': 8.984615384615384e-05, 'epoch': 0.04}
{'loss': 0.8806, 'grad_norm': 1.9866671562194824, 'learning_rate': 9.015384615384616e-05, 'epoch': 0.05}
{'loss': 0.938, 'grad_norm': 3.340815782546997, 'learning_rate': 9.046153846153846e-05, 'epoch': 0.05}
{'loss': 0.9251, 'grad_norm': 2.773143768310547, 'learning_rate': 9.076923076923078e-05, 'epoch': 0.05}
{'loss': 1.254, 'grad_norm': 4.931754112243652, 'learning_rate': 9.107692307692308e-05, 'epoch': 0.05}
{'loss': 0.9894, 'grad_norm': 3.6064748764038086, 'learning_rate': 9.13846153846154e-05, 'epoch': 0.05}
{'loss': 1.0271, 'grad_norm': 3.3968253135681152, 'learning_rate': 9.16923076923077e-05, 'epoch': 0.05}
{'loss': 0.9809, 'grad_norm': 2.3138864040374756, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.05}
{'loss': 1.0155, 'grad_norm': 3.529294490814209, 'learning_rate': 9.230769230769232e-05, 'epoch': 0.05}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1078, 'grad_norm': 2.623657464981079, 'learning_rate': 9.261538461538462e-05, 'epoch': 0.05}
{'loss': 1.3029, 'grad_norm': 2.8236632347106934, 'learning_rate': 9.292307692307692e-05, 'epoch': 0.05}
{'loss': 0.9161, 'grad_norm': 2.758169412612915, 'learning_rate': 9.323076923076924e-05, 'epoch': 0.05}
{'loss': 0.77, 'grad_norm': 2.1217164993286133, 'learning_rate': 9.353846153846154e-05, 'epoch': 0.05}
{'loss': 0.8494, 'grad_norm': 2.3685925006866455, 'learning_rate': 9.384615384615386e-05, 'epoch': 0.05}
{'loss': 1.0339, 'grad_norm': 2.2424447536468506, 'learning_rate': 9.415384615384616e-05, 'epoch': 0.05}
{'loss': 0.8342, 'grad_norm': 2.185704469680786, 'learning_rate': 9.446153846153846e-05, 'epoch': 0.05}
{'loss': 1.0889, 'grad_norm': 3.3306097984313965, 'learning_rate': 9.476923076923078e-05, 'epoch': 0.05}
{'loss': 0.8493, 'grad_norm': 3.1684465408325195, 'learning_rate': 9.507692307692308e-05, 'epoch': 0.05}
{'loss': 0.9648, 'grad_norm': 17.989957809448242, 'learning_rate': 9.53846153846154e-05, 'epoch': 0.05}
{'loss': 1.1867, 'grad_norm': 4.231597900390625, 'learning_rate': 9.569230769230769e-05, 'epoch': 0.05}
{'loss': 1.1028, 'grad_norm': 4.445326805114746, 'learning_rate': 9.6e-05, 'epoch': 0.05}
{'loss': 0.7491, 'grad_norm': 3.387634038925171, 'learning_rate': 9.63076923076923e-05, 'epoch': 0.05}
{'loss': 0.9937, 'grad_norm': 2.614628314971924, 'learning_rate': 9.661538461538462e-05, 'epoch': 0.05}
{'loss': 0.9422, 'grad_norm': 2.5901989936828613, 'learning_rate': 9.692307692307692e-05, 'epoch': 0.05}
{'loss': 0.9748, 'grad_norm': 2.8910715579986572, 'learning_rate': 9.723076923076924e-05, 'epoch': 0.05}
{'loss': 0.9208, 'grad_norm': 2.6278109550476074, 'learning_rate': 9.753846153846154e-05, 'epoch': 0.05}
{'loss': 1.0992, 'grad_norm': 2.6093175411224365, 'learning_rate': 9.784615384615386e-05, 'epoch': 0.05}
{'loss': 0.9195, 'grad_norm': 1.9688888788223267, 'learning_rate': 9.815384615384616e-05, 'epoch': 0.05}
{'loss': 1.1148, 'grad_norm': 4.028426170349121, 'learning_rate': 9.846153846153848e-05, 'epoch': 0.05}
{'loss': 0.8751, 'grad_norm': 3.1531410217285156, 'learning_rate': 9.876923076923077e-05, 'epoch': 0.05}
{'loss': 0.9947, 'grad_norm': 1.9914761781692505, 'learning_rate': 9.907692307692308e-05, 'epoch': 0.05}
{'loss': 1.1177, 'grad_norm': 3.59000825881958, 'learning_rate': 9.938461538461539e-05, 'epoch': 0.05}
{'loss': 0.9484, 'grad_norm': 2.2880361080169678, 'learning_rate': 9.96923076923077e-05, 'epoch': 0.05}
{'loss': 0.9417, 'grad_norm': 1.9785493612289429, 'learning_rate': 0.0001, 'epoch': 0.05}
{'loss': 0.9278, 'grad_norm': 2.8874828815460205, 'learning_rate': 0.00010030769230769231, 'epoch': 0.05}
{'loss': 1.02, 'grad_norm': 3.003632068634033, 'learning_rate': 0.00010061538461538462, 'epoch': 0.05}
{'loss': 1.2255, 'grad_norm': 4.623058795928955, 'learning_rate': 0.00010092307692307693, 'epoch': 0.05}
{'loss': 0.898, 'grad_norm': 2.5973916053771973, 'learning_rate': 0.00010123076923076924, 'epoch': 0.05}
{'loss': 0.9489, 'grad_norm': 4.914905548095703, 'learning_rate': 0.00010153846153846153, 'epoch': 0.05}
{'loss': 1.1662, 'grad_norm': 3.70491886138916, 'learning_rate': 0.00010184615384615386, 'epoch': 0.05}
{'loss': 0.977, 'grad_norm': 3.980668306350708, 'learning_rate': 0.00010215384615384615, 'epoch': 0.05}
{'loss': 0.941, 'grad_norm': 2.891669750213623, 'learning_rate': 0.00010246153846153848, 'epoch': 0.05}
{'loss': 0.8351, 'grad_norm': 2.1725571155548096, 'learning_rate': 0.00010276923076923077, 'epoch': 0.05}
{'loss': 0.8265, 'grad_norm': 3.149728298187256, 'learning_rate': 0.00010307692307692307, 'epoch': 0.05}
{'loss': 0.8183, 'grad_norm': 2.851210117340088, 'learning_rate': 0.00010338461538461539, 'epoch': 0.05}
{'loss': 0.9509, 'grad_norm': 1.7345739603042603, 'learning_rate': 0.00010369230769230769, 'epoch': 0.05}
{'loss': 1.1431, 'grad_norm': 2.04231333732605, 'learning_rate': 0.00010400000000000001, 'epoch': 0.05}
{'loss': 0.8843, 'grad_norm': 3.3350956439971924, 'learning_rate': 0.00010430769230769231, 'epoch': 0.05}
{'loss': 1.3592, 'grad_norm': 1.9499489068984985, 'learning_rate': 0.00010461538461538463, 'epoch': 0.05}
{'loss': 0.9621, 'grad_norm': 3.1784610748291016, 'learning_rate': 0.00010492307692307693, 'epoch': 0.05}
{'loss': 0.8333, 'grad_norm': 1.7452961206436157, 'learning_rate': 0.00010523076923076924, 'epoch': 0.05}
{'loss': 0.7646, 'grad_norm': 1.9599707126617432, 'learning_rate': 0.00010553846153846155, 'epoch': 0.05}
{'loss': 1.1716, 'grad_norm': 1.5283477306365967, 'learning_rate': 0.00010584615384615386, 'epoch': 0.05}
{'loss': 0.9196, 'grad_norm': 2.6717746257781982, 'learning_rate': 0.00010615384615384615, 'epoch': 0.05}
{'loss': 1.1018, 'grad_norm': 1.74651038646698, 'learning_rate': 0.00010646153846153846, 'epoch': 0.05}
{'loss': 0.9661, 'grad_norm': 3.1484007835388184, 'learning_rate': 0.00010676923076923077, 'epoch': 0.05}
{'loss': 0.8091, 'grad_norm': 2.2716822624206543, 'learning_rate': 0.00010707692307692307, 'epoch': 0.05}
{'loss': 0.9868, 'grad_norm': 2.510479211807251, 'learning_rate': 0.00010738461538461539, 'epoch': 0.05}
{'loss': 0.8871, 'grad_norm': 1.9792729616165161, 'learning_rate': 0.0001076923076923077, 'epoch': 0.05}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.959, 'grad_norm': 2.941561698913574, 'learning_rate': 0.00010800000000000001, 'epoch': 0.05}
{'loss': 0.9232, 'grad_norm': 1.6333509683609009, 'learning_rate': 0.00010830769230769231, 'epoch': 0.05}
{'loss': 1.0347, 'grad_norm': 3.552564859390259, 'learning_rate': 0.00010861538461538463, 'epoch': 0.05}
{'loss': 1.1976, 'grad_norm': 2.525207281112671, 'learning_rate': 0.00010892307692307693, 'epoch': 0.05}
{'loss': 1.0134, 'grad_norm': 1.8441975116729736, 'learning_rate': 0.00010923076923076922, 'epoch': 0.05}
{'loss': 0.9265, 'grad_norm': 2.7887046337127686, 'learning_rate': 0.00010953846153846155, 'epoch': 0.05}
{'loss': 1.0343, 'grad_norm': 2.163344621658325, 'learning_rate': 0.00010984615384615384, 'epoch': 0.05}
{'loss': 1.1581, 'grad_norm': 3.3604586124420166, 'learning_rate': 0.00011015384615384617, 'epoch': 0.06}
{'loss': 1.2078, 'grad_norm': 5.187289237976074, 'learning_rate': 0.00011046153846153846, 'epoch': 0.06}
{'loss': 1.1114, 'grad_norm': 2.320666790008545, 'learning_rate': 0.00011076923076923077, 'epoch': 0.06}
{'loss': 0.742, 'grad_norm': 2.440786600112915, 'learning_rate': 0.00011107692307692308, 'epoch': 0.06}
{'loss': 0.7011, 'grad_norm': 2.087374210357666, 'learning_rate': 0.00011138461538461539, 'epoch': 0.06}
{'loss': 1.1072, 'grad_norm': 2.7353458404541016, 'learning_rate': 0.0001116923076923077, 'epoch': 0.06}
{'loss': 1.0956, 'grad_norm': 2.1377313137054443, 'learning_rate': 0.00011200000000000001, 'epoch': 0.06}
{'loss': 1.125, 'grad_norm': 1.8571876287460327, 'learning_rate': 0.00011230769230769231, 'epoch': 0.06}
{'loss': 1.1104, 'grad_norm': 2.3176558017730713, 'learning_rate': 0.00011261538461538462, 'epoch': 0.06}
{'loss': 0.8499, 'grad_norm': 2.1722307205200195, 'learning_rate': 0.00011292307692307693, 'epoch': 0.06}
{'loss': 0.8814, 'grad_norm': 2.162160634994507, 'learning_rate': 0.00011323076923076922, 'epoch': 0.06}
{'loss': 1.0429, 'grad_norm': 3.0251786708831787, 'learning_rate': 0.00011353846153846155, 'epoch': 0.06}
{'loss': 0.7753, 'grad_norm': 2.1302928924560547, 'learning_rate': 0.00011384615384615384, 'epoch': 0.06}
{'loss': 1.2145, 'grad_norm': 2.620180130004883, 'learning_rate': 0.00011415384615384617, 'epoch': 0.06}
{'loss': 1.2292, 'grad_norm': 2.7294998168945312, 'learning_rate': 0.00011446153846153846, 'epoch': 0.06}
{'loss': 0.714, 'grad_norm': 2.6086034774780273, 'learning_rate': 0.00011476923076923079, 'epoch': 0.06}
{'loss': 1.1322, 'grad_norm': 1.9549663066864014, 'learning_rate': 0.00011507692307692308, 'epoch': 0.06}
{'loss': 0.9676, 'grad_norm': 4.142623424530029, 'learning_rate': 0.00011538461538461538, 'epoch': 0.06}
{'loss': 0.8875, 'grad_norm': 2.7749576568603516, 'learning_rate': 0.0001156923076923077, 'epoch': 0.06}
{'loss': 0.8139, 'grad_norm': 1.7958029508590698, 'learning_rate': 0.000116, 'epoch': 0.06}
{'loss': 0.8626, 'grad_norm': 2.1248104572296143, 'learning_rate': 0.00011630769230769232, 'epoch': 0.06}
{'loss': 1.3601, 'grad_norm': 1.9059597253799438, 'learning_rate': 0.00011661538461538462, 'epoch': 0.06}
{'loss': 1.0461, 'grad_norm': 1.8040883541107178, 'learning_rate': 0.00011692307692307694, 'epoch': 0.06}
{'loss': 0.8976, 'grad_norm': 2.5462660789489746, 'learning_rate': 0.00011723076923076924, 'epoch': 0.06}
{'loss': 0.9496, 'grad_norm': 2.3899426460266113, 'learning_rate': 0.00011753846153846155, 'epoch': 0.06}
{'loss': 0.9606, 'grad_norm': 1.9744353294372559, 'learning_rate': 0.00011784615384615386, 'epoch': 0.06}
{'loss': 0.8864, 'grad_norm': 2.631395101547241, 'learning_rate': 0.00011815384615384617, 'epoch': 0.06}
{'loss': 0.7537, 'grad_norm': 2.6832082271575928, 'learning_rate': 0.00011846153846153846, 'epoch': 0.06}
{'loss': 0.8862, 'grad_norm': 2.3448173999786377, 'learning_rate': 0.00011876923076923077, 'epoch': 0.06}
{'loss': 0.9362, 'grad_norm': 2.4676361083984375, 'learning_rate': 0.00011907692307692308, 'epoch': 0.06}
{'loss': 0.7898, 'grad_norm': 2.026521921157837, 'learning_rate': 0.00011938461538461538, 'epoch': 0.06}
{'loss': 0.9275, 'grad_norm': 1.8006917238235474, 'learning_rate': 0.0001196923076923077, 'epoch': 0.06}
{'loss': 0.8797, 'grad_norm': 2.362494945526123, 'learning_rate': 0.00012, 'epoch': 0.06}
{'loss': 0.9237, 'grad_norm': 2.2202446460723877, 'learning_rate': 0.00012030769230769232, 'epoch': 0.06}
{'loss': 0.9129, 'grad_norm': 1.5733884572982788, 'learning_rate': 0.00012061538461538462, 'epoch': 0.06}
{'loss': 1.0785, 'grad_norm': 1.5342739820480347, 'learning_rate': 0.00012092307692307694, 'epoch': 0.06}
{'loss': 1.2615, 'grad_norm': 1.6122157573699951, 'learning_rate': 0.00012123076923076924, 'epoch': 0.06}
{'loss': 0.8621, 'grad_norm': 3.0627589225769043, 'learning_rate': 0.00012153846153846153, 'epoch': 0.06}
{'loss': 1.0505, 'grad_norm': 1.849493384361267, 'learning_rate': 0.00012184615384615386, 'epoch': 0.06}
{'loss': 0.7949, 'grad_norm': 1.6078780889511108, 'learning_rate': 0.00012215384615384616, 'epoch': 0.06}
{'loss': 0.893, 'grad_norm': 1.9425320625305176, 'learning_rate': 0.00012246153846153846, 'epoch': 0.06}
{'loss': 1.0876, 'grad_norm': 2.5029382705688477, 'learning_rate': 0.00012276923076923077, 'epoch': 0.06}
{'loss': 0.9751, 'grad_norm': 2.332223653793335, 'learning_rate': 0.0001230769230769231, 'epoch': 0.06}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0231, 'grad_norm': 1.9030228853225708, 'learning_rate': 0.0001233846153846154, 'epoch': 0.06}
{'loss': 0.9674, 'grad_norm': 2.8309075832366943, 'learning_rate': 0.0001236923076923077, 'epoch': 0.06}
{'loss': 0.8101, 'grad_norm': 2.243685483932495, 'learning_rate': 0.000124, 'epoch': 0.06}
{'loss': 1.1284, 'grad_norm': 2.093799114227295, 'learning_rate': 0.0001243076923076923, 'epoch': 0.06}
{'loss': 0.8856, 'grad_norm': 2.4842255115509033, 'learning_rate': 0.0001246153846153846, 'epoch': 0.06}
{'loss': 0.8186, 'grad_norm': 3.1717584133148193, 'learning_rate': 0.0001249230769230769, 'epoch': 0.06}
{'loss': 0.8255, 'grad_norm': 2.8509268760681152, 'learning_rate': 0.00012523076923076924, 'epoch': 0.06}
{'loss': 1.0518, 'grad_norm': 2.714862823486328, 'learning_rate': 0.00012553846153846155, 'epoch': 0.06}
{'loss': 0.8719, 'grad_norm': 1.7986760139465332, 'learning_rate': 0.00012584615384615385, 'epoch': 0.06}
{'loss': 1.1482, 'grad_norm': 2.8826262950897217, 'learning_rate': 0.00012615384615384615, 'epoch': 0.06}
{'loss': 0.821, 'grad_norm': 2.007422685623169, 'learning_rate': 0.00012646153846153848, 'epoch': 0.06}
{'loss': 0.8248, 'grad_norm': 1.788085699081421, 'learning_rate': 0.00012676923076923078, 'epoch': 0.06}
{'loss': 1.3374, 'grad_norm': 1.637777328491211, 'learning_rate': 0.00012707692307692309, 'epoch': 0.06}
{'loss': 0.6674, 'grad_norm': 1.391581416130066, 'learning_rate': 0.0001273846153846154, 'epoch': 0.06}
{'loss': 1.0138, 'grad_norm': 2.1849446296691895, 'learning_rate': 0.0001276923076923077, 'epoch': 0.06}
{'loss': 1.4436, 'grad_norm': 3.501757860183716, 'learning_rate': 0.00012800000000000002, 'epoch': 0.06}
{'loss': 0.9218, 'grad_norm': 2.132204294204712, 'learning_rate': 0.0001283076923076923, 'epoch': 0.06}
{'loss': 0.8253, 'grad_norm': 2.47345232963562, 'learning_rate': 0.00012861538461538463, 'epoch': 0.06}
{'loss': 0.8111, 'grad_norm': 1.5278496742248535, 'learning_rate': 0.00012892307692307693, 'epoch': 0.06}
{'loss': 1.0319, 'grad_norm': 1.8599804639816284, 'learning_rate': 0.00012923076923076923, 'epoch': 0.06}
{'loss': 1.0486, 'grad_norm': 1.8921599388122559, 'learning_rate': 0.00012953846153846153, 'epoch': 0.06}
{'loss': 0.8955, 'grad_norm': 1.8963172435760498, 'learning_rate': 0.00012984615384615386, 'epoch': 0.06}
{'loss': 0.9875, 'grad_norm': 1.6550005674362183, 'learning_rate': 0.00013015384615384617, 'epoch': 0.07}
{'loss': 0.8697, 'grad_norm': 1.5927860736846924, 'learning_rate': 0.00013046153846153847, 'epoch': 0.07}
{'loss': 0.9229, 'grad_norm': 1.8850077390670776, 'learning_rate': 0.00013076923076923077, 'epoch': 0.07}
{'loss': 0.8704, 'grad_norm': 2.23060941696167, 'learning_rate': 0.00013107692307692308, 'epoch': 0.07}
{'loss': 0.7809, 'grad_norm': 2.8574156761169434, 'learning_rate': 0.0001313846153846154, 'epoch': 0.07}
{'loss': 0.9549, 'grad_norm': 2.3356387615203857, 'learning_rate': 0.00013169230769230768, 'epoch': 0.07}
{'loss': 1.4177, 'grad_norm': 2.11188006401062, 'learning_rate': 0.000132, 'epoch': 0.07}
{'loss': 1.1307, 'grad_norm': 1.8097448348999023, 'learning_rate': 0.0001323076923076923, 'epoch': 0.07}
{'loss': 1.0599, 'grad_norm': 1.9943588972091675, 'learning_rate': 0.00013261538461538464, 'epoch': 0.07}
{'loss': 0.9836, 'grad_norm': 2.638692617416382, 'learning_rate': 0.00013292307692307692, 'epoch': 0.07}
{'loss': 0.8325, 'grad_norm': 2.35774564743042, 'learning_rate': 0.00013323076923076925, 'epoch': 0.07}
{'loss': 1.0502, 'grad_norm': 1.7274510860443115, 'learning_rate': 0.00013353846153846155, 'epoch': 0.07}
{'loss': 1.0338, 'grad_norm': 2.1319167613983154, 'learning_rate': 0.00013384615384615385, 'epoch': 0.07}
{'loss': 0.9011, 'grad_norm': 3.9858486652374268, 'learning_rate': 0.00013415384615384616, 'epoch': 0.07}
{'loss': 1.0442, 'grad_norm': 2.8329195976257324, 'learning_rate': 0.00013446153846153846, 'epoch': 0.07}
{'loss': 0.9639, 'grad_norm': 1.567704439163208, 'learning_rate': 0.0001347692307692308, 'epoch': 0.07}
{'loss': 0.9276, 'grad_norm': 1.7135426998138428, 'learning_rate': 0.0001350769230769231, 'epoch': 0.07}
{'loss': 1.2346, 'grad_norm': 1.778052806854248, 'learning_rate': 0.0001353846153846154, 'epoch': 0.07}
{'loss': 1.1197, 'grad_norm': 1.909363031387329, 'learning_rate': 0.0001356923076923077, 'epoch': 0.07}
{'loss': 1.5285, 'grad_norm': 3.545564651489258, 'learning_rate': 0.00013600000000000003, 'epoch': 0.07}
{'loss': 0.9441, 'grad_norm': 1.8329861164093018, 'learning_rate': 0.0001363076923076923, 'epoch': 0.07}
{'loss': 0.9934, 'grad_norm': 1.8672007322311401, 'learning_rate': 0.0001366153846153846, 'epoch': 0.07}
{'loss': 1.3855, 'grad_norm': 2.8157169818878174, 'learning_rate': 0.00013692307692307693, 'epoch': 0.07}
{'loss': 0.8937, 'grad_norm': 1.9562231302261353, 'learning_rate': 0.00013723076923076924, 'epoch': 0.07}
{'loss': 0.7787, 'grad_norm': 1.3429609537124634, 'learning_rate': 0.00013753846153846154, 'epoch': 0.07}
{'loss': 0.7518, 'grad_norm': 1.7551790475845337, 'learning_rate': 0.00013784615384615384, 'epoch': 0.07}
{'loss': 1.1825, 'grad_norm': 1.627596378326416, 'learning_rate': 0.00013815384615384617, 'epoch': 0.07}
{'loss': 0.9728, 'grad_norm': 1.7941373586654663, 'learning_rate': 0.00013846153846153847, 'epoch': 0.07}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1638, 'grad_norm': 1.4386218786239624, 'learning_rate': 0.00013876923076923078, 'epoch': 0.07}
{'loss': 0.7844, 'grad_norm': 3.151674270629883, 'learning_rate': 0.00013907692307692308, 'epoch': 0.07}
{'loss': 1.005, 'grad_norm': 2.021761894226074, 'learning_rate': 0.0001393846153846154, 'epoch': 0.07}
{'loss': 0.8603, 'grad_norm': 2.1695940494537354, 'learning_rate': 0.0001396923076923077, 'epoch': 0.07}
{'loss': 0.9838, 'grad_norm': 1.4658088684082031, 'learning_rate': 0.00014, 'epoch': 0.07}
{'loss': 1.1011, 'grad_norm': 1.5397586822509766, 'learning_rate': 0.00014030769230769232, 'epoch': 0.07}
{'loss': 0.8131, 'grad_norm': 1.6390575170516968, 'learning_rate': 0.00014061538461538462, 'epoch': 0.07}
{'loss': 0.859, 'grad_norm': 1.3540493249893188, 'learning_rate': 0.00014092307692307692, 'epoch': 0.07}
{'loss': 0.723, 'grad_norm': 1.188504934310913, 'learning_rate': 0.00014123076923076923, 'epoch': 0.07}
{'loss': 0.8913, 'grad_norm': 1.8766722679138184, 'learning_rate': 0.00014153846153846156, 'epoch': 0.07}
{'loss': 0.7967, 'grad_norm': 2.1667022705078125, 'learning_rate': 0.00014184615384615386, 'epoch': 0.07}
{'loss': 0.7377, 'grad_norm': 1.778506875038147, 'learning_rate': 0.00014215384615384616, 'epoch': 0.07}
{'loss': 1.0084, 'grad_norm': 2.11873459815979, 'learning_rate': 0.00014246153846153846, 'epoch': 0.07}
{'loss': 1.0401, 'grad_norm': 1.701642632484436, 'learning_rate': 0.00014276923076923077, 'epoch': 0.07}
{'loss': 1.1469, 'grad_norm': 1.635451316833496, 'learning_rate': 0.0001430769230769231, 'epoch': 0.07}
{'loss': 0.9351, 'grad_norm': 2.2548165321350098, 'learning_rate': 0.00014338461538461537, 'epoch': 0.07}
{'loss': 0.8651, 'grad_norm': 1.4244487285614014, 'learning_rate': 0.0001436923076923077, 'epoch': 0.07}
{'loss': 0.9445, 'grad_norm': 1.684125304222107, 'learning_rate': 0.000144, 'epoch': 0.07}
{'loss': 1.0914, 'grad_norm': 1.8377128839492798, 'learning_rate': 0.00014430769230769233, 'epoch': 0.07}
{'loss': 0.6996, 'grad_norm': 2.044626474380493, 'learning_rate': 0.0001446153846153846, 'epoch': 0.07}
{'loss': 0.9126, 'grad_norm': 2.0103940963745117, 'learning_rate': 0.00014492307692307694, 'epoch': 0.07}
{'loss': 0.9953, 'grad_norm': 1.7943342924118042, 'learning_rate': 0.00014523076923076924, 'epoch': 0.07}
{'loss': 1.0222, 'grad_norm': 1.678149700164795, 'learning_rate': 0.00014553846153846154, 'epoch': 0.07}
{'loss': 0.8364, 'grad_norm': 1.8297353982925415, 'learning_rate': 0.00014584615384615385, 'epoch': 0.07}
{'loss': 0.9084, 'grad_norm': 1.2803125381469727, 'learning_rate': 0.00014615384615384615, 'epoch': 0.07}
{'loss': 0.9036, 'grad_norm': 1.8500255346298218, 'learning_rate': 0.00014646153846153848, 'epoch': 0.07}
{'loss': 0.8895, 'grad_norm': 1.4540176391601562, 'learning_rate': 0.00014676923076923078, 'epoch': 0.07}
{'loss': 0.9465, 'grad_norm': 2.3861756324768066, 'learning_rate': 0.00014707692307692308, 'epoch': 0.07}
{'loss': 1.2794, 'grad_norm': 2.437485456466675, 'learning_rate': 0.0001473846153846154, 'epoch': 0.07}
{'loss': 0.9537, 'grad_norm': 2.405600070953369, 'learning_rate': 0.00014769230769230772, 'epoch': 0.07}
{'loss': 1.0012, 'grad_norm': 1.3839023113250732, 'learning_rate': 0.000148, 'epoch': 0.07}
{'loss': 1.2279, 'grad_norm': 1.9480583667755127, 'learning_rate': 0.00014830769230769232, 'epoch': 0.07}
{'loss': 1.0139, 'grad_norm': 2.180877208709717, 'learning_rate': 0.00014861538461538462, 'epoch': 0.07}
{'loss': 0.6932, 'grad_norm': 1.9089120626449585, 'learning_rate': 0.00014892307692307693, 'epoch': 0.07}
{'loss': 0.9931, 'grad_norm': 3.241981029510498, 'learning_rate': 0.00014923076923076923, 'epoch': 0.07}
{'loss': 1.028, 'grad_norm': 1.3839991092681885, 'learning_rate': 0.00014953846153846153, 'epoch': 0.07}
{'loss': 1.0478, 'grad_norm': 1.8641607761383057, 'learning_rate': 0.00014984615384615386, 'epoch': 0.07}
{'loss': 1.0641, 'grad_norm': 1.5833820104599, 'learning_rate': 0.00015015384615384617, 'epoch': 0.08}
{'loss': 0.7502, 'grad_norm': 3.593587636947632, 'learning_rate': 0.00015046153846153847, 'epoch': 0.08}
{'loss': 1.0496, 'grad_norm': 1.4111591577529907, 'learning_rate': 0.00015076923076923077, 'epoch': 0.08}
{'loss': 0.8182, 'grad_norm': 2.0657224655151367, 'learning_rate': 0.0001510769230769231, 'epoch': 0.08}
{'loss': 1.1732, 'grad_norm': 1.3544230461120605, 'learning_rate': 0.0001513846153846154, 'epoch': 0.08}
{'loss': 0.9725, 'grad_norm': 2.226255178451538, 'learning_rate': 0.00015169230769230768, 'epoch': 0.08}
{'loss': 1.0285, 'grad_norm': 1.517592430114746, 'learning_rate': 0.000152, 'epoch': 0.08}
{'loss': 0.8887, 'grad_norm': 1.4205973148345947, 'learning_rate': 0.0001523076923076923, 'epoch': 0.08}
{'loss': 0.866, 'grad_norm': 1.589130163192749, 'learning_rate': 0.00015261538461538461, 'epoch': 0.08}
{'loss': 1.1417, 'grad_norm': 2.782914638519287, 'learning_rate': 0.00015292307692307692, 'epoch': 0.08}
{'loss': 1.0629, 'grad_norm': 1.3821048736572266, 'learning_rate': 0.00015323076923076925, 'epoch': 0.08}
{'loss': 1.1519, 'grad_norm': 1.7796554565429688, 'learning_rate': 0.00015353846153846155, 'epoch': 0.08}
{'loss': 0.9153, 'grad_norm': 1.8473328351974487, 'learning_rate': 0.00015384615384615385, 'epoch': 0.08}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.5243, 'grad_norm': 1.5531738996505737, 'learning_rate': 0.00015415384615384615, 'epoch': 0.08}
{'loss': 1.1359, 'grad_norm': 1.7838259935379028, 'learning_rate': 0.00015446153846153848, 'epoch': 0.08}
{'loss': 1.0874, 'grad_norm': 2.1258203983306885, 'learning_rate': 0.0001547692307692308, 'epoch': 0.08}
{'loss': 1.0143, 'grad_norm': 1.594096302986145, 'learning_rate': 0.00015507692307692306, 'epoch': 0.08}
{'loss': 0.9005, 'grad_norm': 2.573239326477051, 'learning_rate': 0.0001553846153846154, 'epoch': 0.08}
{'loss': 1.5259, 'grad_norm': 1.2212105989456177, 'learning_rate': 0.0001556923076923077, 'epoch': 0.08}
{'loss': 1.0856, 'grad_norm': 1.775555968284607, 'learning_rate': 0.00015600000000000002, 'epoch': 0.08}
{'loss': 1.0129, 'grad_norm': 1.6761175394058228, 'learning_rate': 0.0001563076923076923, 'epoch': 0.08}
{'loss': 0.9021, 'grad_norm': 1.6997262239456177, 'learning_rate': 0.00015661538461538463, 'epoch': 0.08}
{'loss': 0.9434, 'grad_norm': 1.5149427652359009, 'learning_rate': 0.00015692307692307693, 'epoch': 0.08}
{'loss': 1.1166, 'grad_norm': 1.6656920909881592, 'learning_rate': 0.00015723076923076923, 'epoch': 0.08}
{'loss': 0.6927, 'grad_norm': 1.413483738899231, 'learning_rate': 0.00015753846153846154, 'epoch': 0.08}
{'loss': 0.7032, 'grad_norm': 1.8959373235702515, 'learning_rate': 0.00015784615384615384, 'epoch': 0.08}
{'loss': 1.1236, 'grad_norm': 1.7543445825576782, 'learning_rate': 0.00015815384615384617, 'epoch': 0.08}
{'loss': 0.8886, 'grad_norm': 3.2104721069335938, 'learning_rate': 0.00015846153846153847, 'epoch': 0.08}
{'loss': 1.0508, 'grad_norm': 1.5169973373413086, 'learning_rate': 0.00015876923076923078, 'epoch': 0.08}
{'loss': 1.068, 'grad_norm': 1.4115476608276367, 'learning_rate': 0.00015907692307692308, 'epoch': 0.08}
{'loss': 0.9211, 'grad_norm': 1.3235830068588257, 'learning_rate': 0.0001593846153846154, 'epoch': 0.08}
{'loss': 1.0946, 'grad_norm': 1.4746203422546387, 'learning_rate': 0.00015969230769230768, 'epoch': 0.08}
{'loss': 0.7987, 'grad_norm': 1.4598273038864136, 'learning_rate': 0.00016, 'epoch': 0.08}
{'loss': 1.1983, 'grad_norm': 1.7119500637054443, 'learning_rate': 0.00016030769230769232, 'epoch': 0.08}
{'loss': 1.2444, 'grad_norm': 16.047895431518555, 'learning_rate': 0.00016061538461538462, 'epoch': 0.08}
{'loss': 0.8425, 'grad_norm': 1.7809747457504272, 'learning_rate': 0.00016092307692307692, 'epoch': 0.08}
{'loss': 0.8608, 'grad_norm': 1.997578501701355, 'learning_rate': 0.00016123076923076922, 'epoch': 0.08}
{'loss': 1.0498, 'grad_norm': 1.681480884552002, 'learning_rate': 0.00016153846153846155, 'epoch': 0.08}
{'loss': 0.9422, 'grad_norm': 3.591407060623169, 'learning_rate': 0.00016184615384615386, 'epoch': 0.08}
{'loss': 0.9985, 'grad_norm': 1.7677892446517944, 'learning_rate': 0.00016215384615384616, 'epoch': 0.08}
{'loss': 0.711, 'grad_norm': 2.066756248474121, 'learning_rate': 0.00016246153846153846, 'epoch': 0.08}
{'loss': 0.9305, 'grad_norm': 1.5934957265853882, 'learning_rate': 0.0001627692307692308, 'epoch': 0.08}
{'loss': 1.0543, 'grad_norm': 1.8018187284469604, 'learning_rate': 0.0001630769230769231, 'epoch': 0.08}
{'loss': 0.9648, 'grad_norm': 1.9762952327728271, 'learning_rate': 0.0001633846153846154, 'epoch': 0.08}
{'loss': 0.969, 'grad_norm': 2.7024905681610107, 'learning_rate': 0.0001636923076923077, 'epoch': 0.08}
{'loss': 0.822, 'grad_norm': 1.890751838684082, 'learning_rate': 0.000164, 'epoch': 0.08}
{'loss': 0.9635, 'grad_norm': 2.769631862640381, 'learning_rate': 0.0001643076923076923, 'epoch': 0.08}
{'loss': 1.1938, 'grad_norm': 2.2382588386535645, 'learning_rate': 0.0001646153846153846, 'epoch': 0.08}
{'loss': 1.0476, 'grad_norm': 2.044889211654663, 'learning_rate': 0.00016492307692307694, 'epoch': 0.08}
{'loss': 0.9117, 'grad_norm': 2.2339141368865967, 'learning_rate': 0.00016523076923076924, 'epoch': 0.08}
{'loss': 0.9107, 'grad_norm': 5.037364959716797, 'learning_rate': 0.00016553846153846154, 'epoch': 0.08}
{'loss': 0.873, 'grad_norm': 1.4988144636154175, 'learning_rate': 0.00016584615384615384, 'epoch': 0.08}
{'loss': 1.1129, 'grad_norm': 2.020885705947876, 'learning_rate': 0.00016615384615384617, 'epoch': 0.08}
{'loss': 1.0113, 'grad_norm': 2.04166579246521, 'learning_rate': 0.00016646153846153848, 'epoch': 0.08}
{'loss': 1.0538, 'grad_norm': 1.6319721937179565, 'learning_rate': 0.00016676923076923075, 'epoch': 0.08}
{'loss': 1.2633, 'grad_norm': 2.3531696796417236, 'learning_rate': 0.00016707692307692308, 'epoch': 0.08}
{'loss': 0.7509, 'grad_norm': 2.1579482555389404, 'learning_rate': 0.00016738461538461539, 'epoch': 0.08}
{'loss': 0.9856, 'grad_norm': 1.4824728965759277, 'learning_rate': 0.00016769230769230772, 'epoch': 0.08}
{'loss': 1.0126, 'grad_norm': 2.115023136138916, 'learning_rate': 0.000168, 'epoch': 0.08}
{'loss': 0.9508, 'grad_norm': 3.5852177143096924, 'learning_rate': 0.00016830769230769232, 'epoch': 0.08}
{'loss': 0.7905, 'grad_norm': 6.995704650878906, 'learning_rate': 0.00016861538461538462, 'epoch': 0.08}
{'loss': 0.9127, 'grad_norm': 1.3084378242492676, 'learning_rate': 0.00016892307692307695, 'epoch': 0.08}
{'loss': 0.8514, 'grad_norm': 1.5423799753189087, 'learning_rate': 0.00016923076923076923, 'epoch': 0.08}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1243, 'grad_norm': 1.5074963569641113, 'learning_rate': 0.00016953846153846156, 'epoch': 0.08}
{'loss': 1.0358, 'grad_norm': 2.0800840854644775, 'learning_rate': 0.00016984615384615386, 'epoch': 0.08}
{'loss': 1.075, 'grad_norm': 2.071366548538208, 'learning_rate': 0.00017015384615384616, 'epoch': 0.09}
{'loss': 0.9895, 'grad_norm': 2.4056851863861084, 'learning_rate': 0.00017046153846153847, 'epoch': 0.09}
{'loss': 0.8584, 'grad_norm': 1.9084409475326538, 'learning_rate': 0.00017076923076923077, 'epoch': 0.09}
{'loss': 0.8737, 'grad_norm': 1.8260705471038818, 'learning_rate': 0.0001710769230769231, 'epoch': 0.09}
{'loss': 1.0586, 'grad_norm': 1.7226916551589966, 'learning_rate': 0.0001713846153846154, 'epoch': 0.09}
{'loss': 1.0667, 'grad_norm': 1.449908971786499, 'learning_rate': 0.0001716923076923077, 'epoch': 0.09}
{'loss': 0.9016, 'grad_norm': 1.719163179397583, 'learning_rate': 0.000172, 'epoch': 0.09}
{'loss': 0.735, 'grad_norm': 1.2045642137527466, 'learning_rate': 0.00017230769230769234, 'epoch': 0.09}
{'loss': 1.2275, 'grad_norm': 1.6729819774627686, 'learning_rate': 0.0001726153846153846, 'epoch': 0.09}
{'loss': 1.0418, 'grad_norm': 2.2009100914001465, 'learning_rate': 0.00017292307692307691, 'epoch': 0.09}
{'loss': 1.0064, 'grad_norm': 1.752455234527588, 'learning_rate': 0.00017323076923076924, 'epoch': 0.09}
{'loss': 0.9721, 'grad_norm': 1.3306761980056763, 'learning_rate': 0.00017353846153846155, 'epoch': 0.09}
{'loss': 0.9083, 'grad_norm': 2.122016668319702, 'learning_rate': 0.00017384615384615385, 'epoch': 0.09}
{'loss': 1.1462, 'grad_norm': 2.532494306564331, 'learning_rate': 0.00017415384615384615, 'epoch': 0.09}
{'loss': 0.9504, 'grad_norm': 2.6080734729766846, 'learning_rate': 0.00017446153846153848, 'epoch': 0.09}
{'loss': 0.7779, 'grad_norm': 2.4996964931488037, 'learning_rate': 0.00017476923076923078, 'epoch': 0.09}
{'loss': 1.1819, 'grad_norm': 1.9499977827072144, 'learning_rate': 0.0001750769230769231, 'epoch': 0.09}
{'loss': 0.9248, 'grad_norm': 1.547340750694275, 'learning_rate': 0.0001753846153846154, 'epoch': 0.09}
{'loss': 1.0871, 'grad_norm': 1.3819186687469482, 'learning_rate': 0.00017569230769230772, 'epoch': 0.09}
{'loss': 0.9631, 'grad_norm': 1.7681572437286377, 'learning_rate': 0.00017600000000000002, 'epoch': 0.09}
{'loss': 0.9434, 'grad_norm': 1.8547499179840088, 'learning_rate': 0.0001763076923076923, 'epoch': 0.09}
{'loss': 0.9148, 'grad_norm': 1.3939298391342163, 'learning_rate': 0.00017661538461538463, 'epoch': 0.09}
{'loss': 0.7921, 'grad_norm': 1.9853746891021729, 'learning_rate': 0.00017692307692307693, 'epoch': 0.09}
{'loss': 0.9262, 'grad_norm': 1.87468683719635, 'learning_rate': 0.00017723076923076923, 'epoch': 0.09}
{'loss': 0.7181, 'grad_norm': 1.8010532855987549, 'learning_rate': 0.00017753846153846154, 'epoch': 0.09}
{'loss': 0.9164, 'grad_norm': 1.747843623161316, 'learning_rate': 0.00017784615384615387, 'epoch': 0.09}
{'loss': 0.9277, 'grad_norm': 1.6410984992980957, 'learning_rate': 0.00017815384615384617, 'epoch': 0.09}
{'loss': 0.8099, 'grad_norm': 1.5989429950714111, 'learning_rate': 0.00017846153846153847, 'epoch': 0.09}
{'loss': 1.1534, 'grad_norm': 1.556931495666504, 'learning_rate': 0.00017876923076923077, 'epoch': 0.09}
{'loss': 0.8955, 'grad_norm': 1.4013428688049316, 'learning_rate': 0.00017907692307692308, 'epoch': 0.09}
{'loss': 1.0576, 'grad_norm': 1.3526265621185303, 'learning_rate': 0.0001793846153846154, 'epoch': 0.09}
{'loss': 1.008, 'grad_norm': 2.1669676303863525, 'learning_rate': 0.00017969230769230768, 'epoch': 0.09}
{'loss': 0.8308, 'grad_norm': 2.5247018337249756, 'learning_rate': 0.00018, 'epoch': 0.09}
{'loss': 0.7584, 'grad_norm': 1.68805730342865, 'learning_rate': 0.00018030769230769231, 'epoch': 0.09}
{'loss': 0.8946, 'grad_norm': 2.133887767791748, 'learning_rate': 0.00018061538461538464, 'epoch': 0.09}
{'loss': 1.3617, 'grad_norm': 3.015516757965088, 'learning_rate': 0.00018092307692307692, 'epoch': 0.09}
{'loss': 0.5922, 'grad_norm': 2.8261735439300537, 'learning_rate': 0.00018123076923076925, 'epoch': 0.09}
{'loss': 1.3046, 'grad_norm': 1.741973876953125, 'learning_rate': 0.00018153846153846155, 'epoch': 0.09}
{'loss': 1.0484, 'grad_norm': 2.0399365425109863, 'learning_rate': 0.00018184615384615385, 'epoch': 0.09}
{'loss': 0.8476, 'grad_norm': 2.7136316299438477, 'learning_rate': 0.00018215384615384616, 'epoch': 0.09}
{'loss': 1.2502, 'grad_norm': 2.45623779296875, 'learning_rate': 0.00018246153846153846, 'epoch': 0.09}
{'loss': 1.0255, 'grad_norm': 2.3535380363464355, 'learning_rate': 0.0001827692307692308, 'epoch': 0.09}
{'loss': 1.0089, 'grad_norm': 2.295504570007324, 'learning_rate': 0.0001830769230769231, 'epoch': 0.09}
{'loss': 1.1452, 'grad_norm': 1.399267554283142, 'learning_rate': 0.0001833846153846154, 'epoch': 0.09}
{'loss': 1.316, 'grad_norm': 1.5551351308822632, 'learning_rate': 0.0001836923076923077, 'epoch': 0.09}
{'loss': 0.948, 'grad_norm': 3.91402006149292, 'learning_rate': 0.00018400000000000003, 'epoch': 0.09}
{'loss': 1.2539, 'grad_norm': 4.176878929138184, 'learning_rate': 0.0001843076923076923, 'epoch': 0.09}
{'loss': 0.9988, 'grad_norm': 2.1650149822235107, 'learning_rate': 0.00018461538461538463, 'epoch': 0.09}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1981, 'grad_norm': 1.495493769645691, 'learning_rate': 0.00018492307692307694, 'epoch': 0.09}
{'loss': 0.9981, 'grad_norm': 1.9050099849700928, 'learning_rate': 0.00018523076923076924, 'epoch': 0.09}
{'loss': 1.0914, 'grad_norm': 1.640710711479187, 'learning_rate': 0.00018553846153846154, 'epoch': 0.09}
{'loss': 1.2275, 'grad_norm': 2.344308614730835, 'learning_rate': 0.00018584615384615384, 'epoch': 0.09}
{'loss': 0.8852, 'grad_norm': 1.4173927307128906, 'learning_rate': 0.00018615384615384617, 'epoch': 0.09}
{'loss': 1.4301, 'grad_norm': 2.5368149280548096, 'learning_rate': 0.00018646153846153848, 'epoch': 0.09}
{'loss': 1.1852, 'grad_norm': 1.3859506845474243, 'learning_rate': 0.00018676923076923078, 'epoch': 0.09}
{'loss': 1.1157, 'grad_norm': 1.9954450130462646, 'learning_rate': 0.00018707692307692308, 'epoch': 0.09}
{'loss': 1.0978, 'grad_norm': 1.4866161346435547, 'learning_rate': 0.0001873846153846154, 'epoch': 0.09}
{'loss': 1.2536, 'grad_norm': 1.424911618232727, 'learning_rate': 0.0001876923076923077, 'epoch': 0.09}
{'loss': 0.9858, 'grad_norm': 1.4902303218841553, 'learning_rate': 0.000188, 'epoch': 0.09}
{'loss': 1.271, 'grad_norm': 1.3429265022277832, 'learning_rate': 0.00018830769230769232, 'epoch': 0.09}
{'loss': 1.1566, 'grad_norm': 2.3045814037323, 'learning_rate': 0.00018861538461538462, 'epoch': 0.09}
{'loss': 0.8529, 'grad_norm': 1.471767544746399, 'learning_rate': 0.00018892307692307692, 'epoch': 0.09}
{'loss': 1.2079, 'grad_norm': 1.9533535242080688, 'learning_rate': 0.00018923076923076923, 'epoch': 0.09}
{'loss': 1.0296, 'grad_norm': 2.432539463043213, 'learning_rate': 0.00018953846153846156, 'epoch': 0.09}
{'loss': 1.1582, 'grad_norm': 1.4934314489364624, 'learning_rate': 0.00018984615384615386, 'epoch': 0.09}
{'loss': 0.7342, 'grad_norm': 1.4483466148376465, 'learning_rate': 0.00019015384615384616, 'epoch': 0.1}
{'loss': 0.9836, 'grad_norm': 1.3922239542007446, 'learning_rate': 0.00019046153846153846, 'epoch': 0.1}
{'loss': 0.9133, 'grad_norm': 1.788011908531189, 'learning_rate': 0.0001907692307692308, 'epoch': 0.1}
{'loss': 1.173, 'grad_norm': 2.621326208114624, 'learning_rate': 0.0001910769230769231, 'epoch': 0.1}
{'loss': 0.7641, 'grad_norm': 1.8791604042053223, 'learning_rate': 0.00019138461538461537, 'epoch': 0.1}
{'loss': 0.8613, 'grad_norm': 2.5323009490966797, 'learning_rate': 0.0001916923076923077, 'epoch': 0.1}
{'loss': 0.9024, 'grad_norm': 2.471419334411621, 'learning_rate': 0.000192, 'epoch': 0.1}
{'loss': 1.1045, 'grad_norm': 2.002250909805298, 'learning_rate': 0.00019230769230769233, 'epoch': 0.1}
{'loss': 1.1119, 'grad_norm': 1.337036371231079, 'learning_rate': 0.0001926153846153846, 'epoch': 0.1}
{'loss': 1.2031, 'grad_norm': 1.6959425210952759, 'learning_rate': 0.00019292307692307694, 'epoch': 0.1}
{'loss': 0.8681, 'grad_norm': 2.4508471488952637, 'learning_rate': 0.00019323076923076924, 'epoch': 0.1}
{'loss': 1.0542, 'grad_norm': 1.8728325366973877, 'learning_rate': 0.00019353846153846155, 'epoch': 0.1}
{'loss': 0.6629, 'grad_norm': 1.6085925102233887, 'learning_rate': 0.00019384615384615385, 'epoch': 0.1}
{'loss': 1.11, 'grad_norm': 2.9200761318206787, 'learning_rate': 0.00019415384615384615, 'epoch': 0.1}
{'loss': 0.9284, 'grad_norm': 2.4328536987304688, 'learning_rate': 0.00019446153846153848, 'epoch': 0.1}
{'loss': 1.1173, 'grad_norm': 1.326578974723816, 'learning_rate': 0.00019476923076923078, 'epoch': 0.1}
{'loss': 1.1422, 'grad_norm': 1.4053785800933838, 'learning_rate': 0.00019507692307692309, 'epoch': 0.1}
{'loss': 0.9758, 'grad_norm': 2.1578338146209717, 'learning_rate': 0.0001953846153846154, 'epoch': 0.1}
{'loss': 0.9528, 'grad_norm': 2.6475048065185547, 'learning_rate': 0.00019569230769230772, 'epoch': 0.1}
{'loss': 0.9249, 'grad_norm': 1.4593197107315063, 'learning_rate': 0.000196, 'epoch': 0.1}
{'loss': 1.0438, 'grad_norm': 1.4461559057235718, 'learning_rate': 0.00019630769230769232, 'epoch': 0.1}
{'loss': 0.6755, 'grad_norm': 2.046834707260132, 'learning_rate': 0.00019661538461538463, 'epoch': 0.1}
{'loss': 0.8299, 'grad_norm': 1.2591608762741089, 'learning_rate': 0.00019692307692307696, 'epoch': 0.1}
{'loss': 1.0509, 'grad_norm': 2.274320363998413, 'learning_rate': 0.00019723076923076923, 'epoch': 0.1}
{'loss': 0.9946, 'grad_norm': 1.5887030363082886, 'learning_rate': 0.00019753846153846153, 'epoch': 0.1}
{'loss': 1.2516, 'grad_norm': 1.8511861562728882, 'learning_rate': 0.00019784615384615386, 'epoch': 0.1}
{'loss': 1.2115, 'grad_norm': 8.714466094970703, 'learning_rate': 0.00019815384615384617, 'epoch': 0.1}
{'loss': 1.165, 'grad_norm': 1.7436306476593018, 'learning_rate': 0.00019846153846153847, 'epoch': 0.1}
{'loss': 0.9918, 'grad_norm': 1.5578614473342896, 'learning_rate': 0.00019876923076923077, 'epoch': 0.1}
{'loss': 0.9994, 'grad_norm': 1.6014569997787476, 'learning_rate': 0.0001990769230769231, 'epoch': 0.1}
{'loss': 0.7552, 'grad_norm': 1.3040601015090942, 'learning_rate': 0.0001993846153846154, 'epoch': 0.1}
{'loss': 0.9538, 'grad_norm': 1.7307230234146118, 'learning_rate': 0.0001996923076923077, 'epoch': 0.1}
{'loss': 1.0689, 'grad_norm': 2.0041346549987793, 'learning_rate': 0.0002, 'epoch': 0.1}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0178, 'grad_norm': 1.6103663444519043, 'learning_rate': 0.00019999998558024084, 'epoch': 0.1}
{'loss': 1.1359, 'grad_norm': 1.4001939296722412, 'learning_rate': 0.00019999994232096748, 'epoch': 0.1}
{'loss': 0.9858, 'grad_norm': 1.1285191774368286, 'learning_rate': 0.00019999987022219245, 'epoch': 0.1}
{'loss': 0.9761, 'grad_norm': 2.149423122406006, 'learning_rate': 0.00019999976928393646, 'epoch': 0.1}
{'loss': 0.973, 'grad_norm': 1.2871110439300537, 'learning_rate': 0.00019999963950622867, 'epoch': 0.1}
{'loss': 0.8337, 'grad_norm': 1.727840781211853, 'learning_rate': 0.00019999948088910655, 'epoch': 0.1}
{'loss': 1.2726, 'grad_norm': 1.6845299005508423, 'learning_rate': 0.00019999929343261576, 'epoch': 0.1}
{'loss': 0.8445, 'grad_norm': 1.9865672588348389, 'learning_rate': 0.00019999907713681041, 'epoch': 0.1}
{'loss': 1.1243, 'grad_norm': 6.339419841766357, 'learning_rate': 0.00019999883200175287, 'epoch': 0.1}
{'loss': 1.2257, 'grad_norm': 1.6784332990646362, 'learning_rate': 0.00019999855802751385, 'epoch': 0.1}
{'loss': 0.91, 'grad_norm': 1.4737482070922852, 'learning_rate': 0.00019999825521417234, 'epoch': 0.1}
{'loss': 1.122, 'grad_norm': 1.3220982551574707, 'learning_rate': 0.00019999792356181566, 'epoch': 0.1}
{'loss': 0.9394, 'grad_norm': 1.4282431602478027, 'learning_rate': 0.00019999756307053948, 'epoch': 0.1}
{'loss': 0.9021, 'grad_norm': 1.5661991834640503, 'learning_rate': 0.00019999717374044777, 'epoch': 0.1}
{'loss': 1.102, 'grad_norm': 1.4951177835464478, 'learning_rate': 0.0001999967555716528, 'epoch': 0.1}
{'loss': 0.6093, 'grad_norm': 1.3707339763641357, 'learning_rate': 0.00019999630856427515, 'epoch': 0.1}
{'loss': 0.8839, 'grad_norm': 1.2555787563323975, 'learning_rate': 0.00019999583271844377, 'epoch': 0.1}
{'loss': 1.0559, 'grad_norm': 1.4629114866256714, 'learning_rate': 0.0001999953280342959, 'epoch': 0.1}
{'loss': 1.0225, 'grad_norm': 2.1465160846710205, 'learning_rate': 0.000199994794511977, 'epoch': 0.1}
{'loss': 1.1774, 'grad_norm': 1.4082114696502686, 'learning_rate': 0.00019999423215164103, 'epoch': 0.1}
{'loss': 1.1041, 'grad_norm': 1.3095386028289795, 'learning_rate': 0.00019999364095345015, 'epoch': 0.1}
{'loss': 0.8585, 'grad_norm': 1.4620413780212402, 'learning_rate': 0.00019999302091757484, 'epoch': 0.1}
{'loss': 0.9821, 'grad_norm': 1.21446692943573, 'learning_rate': 0.00019999237204419392, 'epoch': 0.1}
{'loss': 0.8977, 'grad_norm': 1.327086329460144, 'learning_rate': 0.0001999916943334945, 'epoch': 0.1}
{'loss': 1.1748, 'grad_norm': 1.3560298681259155, 'learning_rate': 0.00019999098778567212, 'epoch': 0.1}
{'loss': 1.1598, 'grad_norm': 1.4747260808944702, 'learning_rate': 0.00019999025240093044, 'epoch': 0.1}
{'loss': 0.9862, 'grad_norm': 1.6366559267044067, 'learning_rate': 0.00019998948817948157, 'epoch': 0.1}
{'loss': 1.125, 'grad_norm': 1.613845944404602, 'learning_rate': 0.00019998869512154595, 'epoch': 0.1}
{'loss': 0.8421, 'grad_norm': 1.2490150928497314, 'learning_rate': 0.0001999878732273522, 'epoch': 0.1}
{'loss': 1.1419, 'grad_norm': 1.262442946434021, 'learning_rate': 0.00019998702249713748, 'epoch': 0.1}
{'loss': 1.0523, 'grad_norm': 1.2167028188705444, 'learning_rate': 0.00019998614293114707, 'epoch': 0.1}
{'loss': 1.1706, 'grad_norm': 2.2248382568359375, 'learning_rate': 0.00019998523452963458, 'epoch': 0.1}
{'loss': 1.1066, 'grad_norm': 2.1296920776367188, 'learning_rate': 0.00019998429729286204, 'epoch': 0.11}
{'loss': 1.1805, 'grad_norm': 1.427815556526184, 'learning_rate': 0.0001999833312210998, 'epoch': 0.11}
{'loss': 0.9221, 'grad_norm': 1.512859582901001, 'learning_rate': 0.0001999823363146264, 'epoch': 0.11}
{'loss': 1.0727, 'grad_norm': 1.6327215433120728, 'learning_rate': 0.00019998131257372876, 'epoch': 0.11}
{'loss': 0.8083, 'grad_norm': 1.0754525661468506, 'learning_rate': 0.00019998025999870217, 'epoch': 0.11}
{'loss': 0.8632, 'grad_norm': 1.4424952268600464, 'learning_rate': 0.0001999791785898501, 'epoch': 0.11}
{'loss': 0.9702, 'grad_norm': 2.865715980529785, 'learning_rate': 0.00019997806834748456, 'epoch': 0.11}
{'loss': 1.2607, 'grad_norm': 1.4233003854751587, 'learning_rate': 0.00019997692927192562, 'epoch': 0.11}
{'loss': 0.6895, 'grad_norm': 1.4828588962554932, 'learning_rate': 0.00019997576136350184, 'epoch': 0.11}
{'loss': 1.034, 'grad_norm': 1.1511772871017456, 'learning_rate': 0.00019997456462255003, 'epoch': 0.11}
{'loss': 1.3911, 'grad_norm': 1.1570395231246948, 'learning_rate': 0.0001999733390494153, 'epoch': 0.11}
{'loss': 0.7672, 'grad_norm': 1.524087905883789, 'learning_rate': 0.00019997208464445115, 'epoch': 0.11}
{'loss': 0.9973, 'grad_norm': 1.7594575881958008, 'learning_rate': 0.00019997080140801932, 'epoch': 0.11}
{'loss': 0.7084, 'grad_norm': 1.3096846342086792, 'learning_rate': 0.00019996948934048985, 'epoch': 0.11}
{'loss': 0.9051, 'grad_norm': 1.3833390474319458, 'learning_rate': 0.00019996814844224119, 'epoch': 0.11}
{'loss': 0.9805, 'grad_norm': 1.5707415342330933, 'learning_rate': 0.00019996677871366, 'epoch': 0.11}
{'loss': 0.9491, 'grad_norm': 1.9406397342681885, 'learning_rate': 0.00019996538015514133, 'epoch': 0.11}
{'loss': 0.8873, 'grad_norm': 1.4673235416412354, 'learning_rate': 0.00019996395276708856, 'epoch': 0.11}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9761, 'grad_norm': 2.015160322189331, 'learning_rate': 0.00019996249654991326, 'epoch': 0.11}
{'loss': 0.8372, 'grad_norm': 1.6150745153427124, 'learning_rate': 0.00019996101150403543, 'epoch': 0.11}
{'loss': 1.1502, 'grad_norm': 1.5967401266098022, 'learning_rate': 0.0001999594976298834, 'epoch': 0.11}
{'loss': 1.1519, 'grad_norm': 1.2956539392471313, 'learning_rate': 0.0001999579549278937, 'epoch': 0.11}
{'loss': 0.8731, 'grad_norm': 1.3074264526367188, 'learning_rate': 0.0001999563833985112, 'epoch': 0.11}
{'loss': 0.9898, 'grad_norm': 1.956032156944275, 'learning_rate': 0.00019995478304218924, 'epoch': 0.11}
{'loss': 0.8219, 'grad_norm': 1.3984196186065674, 'learning_rate': 0.0001999531538593893, 'epoch': 0.11}
{'loss': 0.9807, 'grad_norm': 1.7397218942642212, 'learning_rate': 0.00019995149585058116, 'epoch': 0.11}
{'loss': 0.9567, 'grad_norm': 1.1544595956802368, 'learning_rate': 0.0001999498090162431, 'epoch': 0.11}
{'loss': 0.8722, 'grad_norm': 1.2832114696502686, 'learning_rate': 0.0001999480933568615, 'epoch': 0.11}
{'loss': 0.8467, 'grad_norm': 1.4939969778060913, 'learning_rate': 0.00019994634887293122, 'epoch': 0.11}
{'loss': 0.6994, 'grad_norm': 1.0956569910049438, 'learning_rate': 0.00019994457556495528, 'epoch': 0.11}
{'loss': 1.1965, 'grad_norm': 1.6631532907485962, 'learning_rate': 0.00019994277343344518, 'epoch': 0.11}
{'loss': 0.9052, 'grad_norm': 1.3503623008728027, 'learning_rate': 0.0001999409424789206, 'epoch': 0.11}
{'loss': 1.0821, 'grad_norm': 1.4037967920303345, 'learning_rate': 0.0001999390827019096, 'epoch': 0.11}
{'loss': 0.7767, 'grad_norm': 2.2295825481414795, 'learning_rate': 0.00019993719410294847, 'epoch': 0.11}
{'loss': 0.8453, 'grad_norm': 1.6107604503631592, 'learning_rate': 0.00019993527668258195, 'epoch': 0.11}
{'loss': 0.8339, 'grad_norm': 1.497167706489563, 'learning_rate': 0.00019993333044136298, 'epoch': 0.11}
{'loss': 0.8307, 'grad_norm': 2.448382616043091, 'learning_rate': 0.00019993135537985283, 'epoch': 0.11}
{'loss': 1.0025, 'grad_norm': 1.2238744497299194, 'learning_rate': 0.00019992935149862115, 'epoch': 0.11}
{'loss': 0.9891, 'grad_norm': 1.3615084886550903, 'learning_rate': 0.0001999273187982458, 'epoch': 0.11}
{'loss': 0.5477, 'grad_norm': 1.62692391872406, 'learning_rate': 0.00019992525727931303, 'epoch': 0.11}
{'loss': 0.811, 'grad_norm': 1.4386924505233765, 'learning_rate': 0.00019992316694241735, 'epoch': 0.11}
{'loss': 1.1033, 'grad_norm': 1.3277060985565186, 'learning_rate': 0.0001999210477881616, 'epoch': 0.11}
{'loss': 0.6932, 'grad_norm': 1.2987706661224365, 'learning_rate': 0.00019991889981715698, 'epoch': 0.11}
{'loss': 1.1534, 'grad_norm': 1.5737862586975098, 'learning_rate': 0.0001999167230300229, 'epoch': 0.11}
{'loss': 0.9807, 'grad_norm': 1.8300566673278809, 'learning_rate': 0.00019991451742738716, 'epoch': 0.11}
{'loss': 0.7494, 'grad_norm': 1.5809156894683838, 'learning_rate': 0.00019991228300988585, 'epoch': 0.11}
{'loss': 1.0998, 'grad_norm': 1.5320707559585571, 'learning_rate': 0.00019991001977816335, 'epoch': 0.11}
{'loss': 0.8578, 'grad_norm': 1.3325837850570679, 'learning_rate': 0.0001999077277328724, 'epoch': 0.11}
{'loss': 1.137, 'grad_norm': 1.724345326423645, 'learning_rate': 0.0001999054068746739, 'epoch': 0.11}
{'loss': 1.2105, 'grad_norm': 1.422271966934204, 'learning_rate': 0.00019990305720423733, 'epoch': 0.11}
{'loss': 0.9815, 'grad_norm': 1.3592979907989502, 'learning_rate': 0.00019990067872224028, 'epoch': 0.11}
{'loss': 0.9086, 'grad_norm': 1.8166183233261108, 'learning_rate': 0.00019989827142936862, 'epoch': 0.11}
{'loss': 0.929, 'grad_norm': 1.221759557723999, 'learning_rate': 0.00019989583532631667, 'epoch': 0.11}
{'loss': 0.8742, 'grad_norm': 2.027599334716797, 'learning_rate': 0.00019989337041378697, 'epoch': 0.11}
{'loss': 1.245, 'grad_norm': 1.5079258680343628, 'learning_rate': 0.00019989087669249037, 'epoch': 0.11}
{'loss': 1.2088, 'grad_norm': 1.2768882513046265, 'learning_rate': 0.0001998883541631461, 'epoch': 0.11}
{'loss': 1.1994, 'grad_norm': 1.7529722452163696, 'learning_rate': 0.0001998858028264816, 'epoch': 0.11}
{'loss': 1.1732, 'grad_norm': 1.6640427112579346, 'learning_rate': 0.00019988322268323268, 'epoch': 0.11}
{'loss': 1.1909, 'grad_norm': 1.26960027217865, 'learning_rate': 0.0001998806137341434, 'epoch': 0.11}
{'loss': 1.2124, 'grad_norm': 1.3234875202178955, 'learning_rate': 0.00019987797597996622, 'epoch': 0.11}
{'loss': 0.8497, 'grad_norm': 1.446577548980713, 'learning_rate': 0.00019987530942146186, 'epoch': 0.11}
{'loss': 1.0114, 'grad_norm': 1.2764804363250732, 'learning_rate': 0.00019987261405939933, 'epoch': 0.11}
{'loss': 1.1174, 'grad_norm': 1.252191185951233, 'learning_rate': 0.00019986988989455592, 'epoch': 0.11}
{'loss': 0.9144, 'grad_norm': 1.5123002529144287, 'learning_rate': 0.00019986713692771732, 'epoch': 0.11}
{'loss': 1.0102, 'grad_norm': 1.2742245197296143, 'learning_rate': 0.00019986435515967742, 'epoch': 0.11}
{'loss': 1.1251, 'grad_norm': 1.9253827333450317, 'learning_rate': 0.00019986154459123855, 'epoch': 0.12}
{'loss': 0.8978, 'grad_norm': 1.5793628692626953, 'learning_rate': 0.00019985870522321118, 'epoch': 0.12}
{'loss': 0.9446, 'grad_norm': 2.1281628608703613, 'learning_rate': 0.00019985583705641418, 'epoch': 0.12}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0344, 'grad_norm': 1.2112950086593628, 'learning_rate': 0.0001998529400916748, 'epoch': 0.12}
{'loss': 1.2485, 'grad_norm': 1.0904353857040405, 'learning_rate': 0.0001998500143298284, 'epoch': 0.12}
{'loss': 0.9708, 'grad_norm': 1.3626220226287842, 'learning_rate': 0.0001998470597717188, 'epoch': 0.12}
{'loss': 0.9177, 'grad_norm': 2.4336941242218018, 'learning_rate': 0.00019984407641819812, 'epoch': 0.12}
{'loss': 1.0228, 'grad_norm': 1.9085954427719116, 'learning_rate': 0.00019984106427012668, 'epoch': 0.12}
{'loss': 0.9872, 'grad_norm': 1.1907049417495728, 'learning_rate': 0.0001998380233283732, 'epoch': 0.12}
{'loss': 0.9552, 'grad_norm': 1.4648693799972534, 'learning_rate': 0.0001998349535938147, 'epoch': 0.12}
{'loss': 1.08, 'grad_norm': 1.6157475709915161, 'learning_rate': 0.0001998318550673364, 'epoch': 0.12}
{'loss': 0.9757, 'grad_norm': 1.674003005027771, 'learning_rate': 0.000199828727749832, 'epoch': 0.12}
{'loss': 1.3208, 'grad_norm': 1.3850282430648804, 'learning_rate': 0.00019982557164220335, 'epoch': 0.12}
{'loss': 1.1953, 'grad_norm': 1.5525712966918945, 'learning_rate': 0.00019982238674536062, 'epoch': 0.12}
{'loss': 0.961, 'grad_norm': 1.408664584159851, 'learning_rate': 0.0001998191730602224, 'epoch': 0.12}
{'loss': 0.9997, 'grad_norm': 1.4781039953231812, 'learning_rate': 0.00019981593058771544, 'epoch': 0.12}
{'loss': 0.8568, 'grad_norm': 1.5791348218917847, 'learning_rate': 0.00019981265932877488, 'epoch': 0.12}
{'loss': 1.0272, 'grad_norm': 1.8441169261932373, 'learning_rate': 0.0001998093592843441, 'epoch': 0.12}
{'loss': 1.2318, 'grad_norm': 1.334337592124939, 'learning_rate': 0.00019980603045537487, 'epoch': 0.12}
{'loss': 1.0193, 'grad_norm': 1.7582963705062866, 'learning_rate': 0.00019980267284282717, 'epoch': 0.12}
{'loss': 1.2092, 'grad_norm': 2.848147392272949, 'learning_rate': 0.00019979928644766934, 'epoch': 0.12}
{'loss': 1.1457, 'grad_norm': 1.3950613737106323, 'learning_rate': 0.000199795871270878, 'epoch': 0.12}
{'loss': 1.0997, 'grad_norm': 1.3093838691711426, 'learning_rate': 0.00019979242731343804, 'epoch': 0.12}
{'loss': 1.0029, 'grad_norm': 1.2749091386795044, 'learning_rate': 0.0001997889545763427, 'epoch': 0.12}
{'loss': 1.0978, 'grad_norm': 1.6511143445968628, 'learning_rate': 0.0001997854530605935, 'epoch': 0.12}
{'loss': 1.124, 'grad_norm': 1.9727180004119873, 'learning_rate': 0.0001997819227672003, 'epoch': 0.12}
{'loss': 0.7188, 'grad_norm': 1.8369834423065186, 'learning_rate': 0.00019977836369718116, 'epoch': 0.12}
{'loss': 0.8821, 'grad_norm': 1.569379448890686, 'learning_rate': 0.00019977477585156252, 'epoch': 0.12}
{'loss': 0.9319, 'grad_norm': 1.8561272621154785, 'learning_rate': 0.00019977115923137912, 'epoch': 0.12}
{'loss': 1.2293, 'grad_norm': 1.3013406991958618, 'learning_rate': 0.0001997675138376739, 'epoch': 0.12}
{'loss': 1.0212, 'grad_norm': 1.5083949565887451, 'learning_rate': 0.0001997638396714983, 'epoch': 0.12}
{'loss': 0.9308, 'grad_norm': 1.8361146450042725, 'learning_rate': 0.00019976013673391182, 'epoch': 0.12}
{'loss': 0.7694, 'grad_norm': 1.6938326358795166, 'learning_rate': 0.00019975640502598244, 'epoch': 0.12}
{'loss': 1.336, 'grad_norm': 1.222116231918335, 'learning_rate': 0.00019975264454878634, 'epoch': 0.12}
{'loss': 0.919, 'grad_norm': 1.3564248085021973, 'learning_rate': 0.00019974885530340796, 'epoch': 0.12}
{'loss': 1.0703, 'grad_norm': 1.19813871383667, 'learning_rate': 0.00019974503729094023, 'epoch': 0.12}
{'loss': 1.012, 'grad_norm': 1.6549227237701416, 'learning_rate': 0.00019974119051248415, 'epoch': 0.12}
{'loss': 0.9781, 'grad_norm': 2.3209304809570312, 'learning_rate': 0.00019973731496914914, 'epoch': 0.12}
{'loss': 1.1154, 'grad_norm': 0.9995704889297485, 'learning_rate': 0.00019973341066205287, 'epoch': 0.12}
{'loss': 0.9609, 'grad_norm': 1.6702455282211304, 'learning_rate': 0.00019972947759232135, 'epoch': 0.12}
{'loss': 0.9555, 'grad_norm': 1.2511563301086426, 'learning_rate': 0.00019972551576108886, 'epoch': 0.12}
{'loss': 1.0611, 'grad_norm': 1.500274658203125, 'learning_rate': 0.00019972152516949796, 'epoch': 0.12}
{'loss': 1.1105, 'grad_norm': 1.8524006605148315, 'learning_rate': 0.00019971750581869953, 'epoch': 0.12}
{'loss': 1.0189, 'grad_norm': 1.3128972053527832, 'learning_rate': 0.00019971345770985268, 'epoch': 0.12}
{'loss': 1.0846, 'grad_norm': 1.1141024827957153, 'learning_rate': 0.00019970938084412496, 'epoch': 0.12}
{'loss': 0.7076, 'grad_norm': 1.313515305519104, 'learning_rate': 0.00019970527522269205, 'epoch': 0.12}
{'loss': 1.0436, 'grad_norm': 1.3149449825286865, 'learning_rate': 0.00019970114084673796, 'epoch': 0.12}
{'loss': 1.1531, 'grad_norm': 1.272456169128418, 'learning_rate': 0.00019969697771745511, 'epoch': 0.12}
{'loss': 1.0786, 'grad_norm': 1.7700011730194092, 'learning_rate': 0.0001996927858360441, 'epoch': 0.12}
{'loss': 0.9116, 'grad_norm': 1.3642504215240479, 'learning_rate': 0.0001996885652037138, 'epoch': 0.12}
{'loss': 0.8865, 'grad_norm': 1.4874955415725708, 'learning_rate': 0.0001996843158216815, 'epoch': 0.12}
{'loss': 1.0938, 'grad_norm': 1.191689133644104, 'learning_rate': 0.00019968003769117262, 'epoch': 0.12}
{'loss': 1.0571, 'grad_norm': 1.815022349357605, 'learning_rate': 0.00019967573081342103, 'epoch': 0.12}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0366, 'grad_norm': 1.3872870206832886, 'learning_rate': 0.0001996713951896687, 'epoch': 0.12}
{'loss': 0.8439, 'grad_norm': 1.6419185400009155, 'learning_rate': 0.00019966703082116615, 'epoch': 0.12}
{'loss': 1.0238, 'grad_norm': 1.823647379875183, 'learning_rate': 0.00019966263770917193, 'epoch': 0.12}
{'loss': 1.2012, 'grad_norm': 1.3322056531906128, 'learning_rate': 0.00019965821585495302, 'epoch': 0.12}
{'loss': 1.0377, 'grad_norm': 1.480913758277893, 'learning_rate': 0.0001996537652597847, 'epoch': 0.12}
{'loss': 0.9149, 'grad_norm': 8.720141410827637, 'learning_rate': 0.00019964928592495045, 'epoch': 0.12}
{'loss': 1.0971, 'grad_norm': 1.225233793258667, 'learning_rate': 0.00019964477785174213, 'epoch': 0.12}
{'loss': 1.0777, 'grad_norm': 2.2535476684570312, 'learning_rate': 0.0001996402410414598, 'epoch': 0.12}
{'loss': 0.6867, 'grad_norm': 1.619566798210144, 'learning_rate': 0.0001996356754954119, 'epoch': 0.12}
{'loss': 1.0343, 'grad_norm': 1.1801937818527222, 'learning_rate': 0.00019963108121491508, 'epoch': 0.12}
{'loss': 1.0997, 'grad_norm': 1.6333603858947754, 'learning_rate': 0.00019962645820129432, 'epoch': 0.12}
{'loss': 1.3772, 'grad_norm': 1.4605133533477783, 'learning_rate': 0.0001996218064558829, 'epoch': 0.12}
{'loss': 1.0836, 'grad_norm': 0.9753067493438721, 'learning_rate': 0.0001996171259800223, 'epoch': 0.13}
{'loss': 0.9847, 'grad_norm': 1.241119623184204, 'learning_rate': 0.0001996124167750624, 'epoch': 0.13}
{'loss': 0.9605, 'grad_norm': 1.6751911640167236, 'learning_rate': 0.0001996076788423613, 'epoch': 0.13}
{'loss': 0.8652, 'grad_norm': 1.5223941802978516, 'learning_rate': 0.00019960291218328537, 'epoch': 0.13}
{'loss': 1.2612, 'grad_norm': 1.9900058507919312, 'learning_rate': 0.00019959811679920938, 'epoch': 0.13}
{'loss': 1.0229, 'grad_norm': 1.9804083108901978, 'learning_rate': 0.00019959329269151615, 'epoch': 0.13}
{'loss': 1.0328, 'grad_norm': 1.3018252849578857, 'learning_rate': 0.00019958843986159704, 'epoch': 0.13}
{'loss': 0.8774, 'grad_norm': 1.7421337366104126, 'learning_rate': 0.00019958355831085155, 'epoch': 0.13}
{'loss': 0.6548, 'grad_norm': 1.2158921957015991, 'learning_rate': 0.0001995786480406875, 'epoch': 0.13}
{'loss': 0.961, 'grad_norm': 1.4404048919677734, 'learning_rate': 0.000199573709052521, 'epoch': 0.13}
{'loss': 0.9442, 'grad_norm': 1.2937564849853516, 'learning_rate': 0.0001995687413477764, 'epoch': 0.13}
{'loss': 1.0723, 'grad_norm': 1.4732553958892822, 'learning_rate': 0.0001995637449278864, 'epoch': 0.13}
{'loss': 0.8747, 'grad_norm': 1.4935510158538818, 'learning_rate': 0.0001995587197942919, 'epoch': 0.13}
{'loss': 1.0279, 'grad_norm': 1.1953474283218384, 'learning_rate': 0.00019955366594844213, 'epoch': 0.13}
{'loss': 1.0298, 'grad_norm': 1.4760998487472534, 'learning_rate': 0.00019954858339179464, 'epoch': 0.13}
{'loss': 0.9525, 'grad_norm': 1.729148030281067, 'learning_rate': 0.00019954347212581514, 'epoch': 0.13}
{'loss': 1.1178, 'grad_norm': 2.0701961517333984, 'learning_rate': 0.0001995383321519778, 'epoch': 0.13}
{'loss': 1.0961, 'grad_norm': 1.9190012216567993, 'learning_rate': 0.00019953316347176488, 'epoch': 0.13}
{'loss': 1.2661, 'grad_norm': 1.283487319946289, 'learning_rate': 0.000199527966086667, 'epoch': 0.13}
{'loss': 1.0633, 'grad_norm': 1.5547994375228882, 'learning_rate': 0.0001995227399981831, 'epoch': 0.13}
{'loss': 0.9891, 'grad_norm': 1.6201808452606201, 'learning_rate': 0.0001995174852078204, 'epoch': 0.13}
{'loss': 0.7428, 'grad_norm': 1.7596538066864014, 'learning_rate': 0.00019951220171709425, 'epoch': 0.13}
{'loss': 1.0782, 'grad_norm': 1.4051631689071655, 'learning_rate': 0.00019950688952752844, 'epoch': 0.13}
{'loss': 0.8692, 'grad_norm': 1.3875752687454224, 'learning_rate': 0.00019950154864065497, 'epoch': 0.13}
{'loss': 0.8774, 'grad_norm': 1.250746250152588, 'learning_rate': 0.00019949617905801415, 'epoch': 0.13}
{'loss': 1.0101, 'grad_norm': 1.52135169506073, 'learning_rate': 0.0001994907807811545, 'epoch': 0.13}
{'loss': 1.1051, 'grad_norm': 1.2593421936035156, 'learning_rate': 0.00019948535381163288, 'epoch': 0.13}
{'loss': 0.966, 'grad_norm': 1.548313856124878, 'learning_rate': 0.00019947989815101444, 'epoch': 0.13}
{'loss': 1.0489, 'grad_norm': 1.3968364000320435, 'learning_rate': 0.0001994744138008725, 'epoch': 0.13}
{'loss': 0.7948, 'grad_norm': 1.7362470626831055, 'learning_rate': 0.00019946890076278875, 'epoch': 0.13}
{'loss': 0.9344, 'grad_norm': 2.0385825634002686, 'learning_rate': 0.00019946335903835315, 'epoch': 0.13}
{'loss': 0.8272, 'grad_norm': 1.3929373025894165, 'learning_rate': 0.00019945778862916384, 'epoch': 0.13}
{'loss': 0.8929, 'grad_norm': 2.000000238418579, 'learning_rate': 0.00019945218953682734, 'epoch': 0.13}
{'loss': 0.8801, 'grad_norm': 1.8816487789154053, 'learning_rate': 0.0001994465617629584, 'epoch': 0.13}
{'loss': 0.9917, 'grad_norm': 1.2827662229537964, 'learning_rate': 0.00019944090530918005, 'epoch': 0.13}
{'loss': 0.809, 'grad_norm': 1.6240506172180176, 'learning_rate': 0.00019943522017712358, 'epoch': 0.13}
{'loss': 0.9057, 'grad_norm': 1.9323060512542725, 'learning_rate': 0.00019942950636842857, 'epoch': 0.13}
{'loss': 0.8354, 'grad_norm': 1.8393259048461914, 'learning_rate': 0.0001994237638847428, 'epoch': 0.13}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9445, 'grad_norm': 1.3852636814117432, 'learning_rate': 0.00019941799272772245, 'epoch': 0.13}
{'loss': 1.0536, 'grad_norm': 1.331411361694336, 'learning_rate': 0.00019941219289903178, 'epoch': 0.13}
{'loss': 0.7533, 'grad_norm': 1.5989820957183838, 'learning_rate': 0.00019940636440034357, 'epoch': 0.13}
{'loss': 0.8978, 'grad_norm': 2.465461015701294, 'learning_rate': 0.00019940050723333866, 'epoch': 0.13}
{'loss': 1.3095, 'grad_norm': 1.280496597290039, 'learning_rate': 0.00019939462139970622, 'epoch': 0.13}
{'loss': 1.2049, 'grad_norm': 1.4628137350082397, 'learning_rate': 0.00019938870690114374, 'epoch': 0.13}
{'loss': 1.1192, 'grad_norm': 1.2407950162887573, 'learning_rate': 0.00019938276373935687, 'epoch': 0.13}
{'loss': 1.1284, 'grad_norm': 1.3208825588226318, 'learning_rate': 0.00019937679191605963, 'epoch': 0.13}
{'loss': 0.8637, 'grad_norm': 1.5748149156570435, 'learning_rate': 0.0001993707914329743, 'epoch': 0.13}
{'loss': 1.046, 'grad_norm': 1.8686500787734985, 'learning_rate': 0.00019936476229183133, 'epoch': 0.13}
{'loss': 0.912, 'grad_norm': 1.3401031494140625, 'learning_rate': 0.00019935870449436948, 'epoch': 0.13}
{'loss': 0.9166, 'grad_norm': 1.0182231664657593, 'learning_rate': 0.00019935261804233587, 'epoch': 0.13}
{'loss': 0.9611, 'grad_norm': 1.5770081281661987, 'learning_rate': 0.00019934650293748572, 'epoch': 0.13}
{'loss': 0.7995, 'grad_norm': 1.2891114950180054, 'learning_rate': 0.00019934035918158264, 'epoch': 0.13}
{'loss': 0.8468, 'grad_norm': 1.4252703189849854, 'learning_rate': 0.00019933418677639848, 'epoch': 0.13}
{'loss': 1.0099, 'grad_norm': 1.6592764854431152, 'learning_rate': 0.0001993279857237133, 'epoch': 0.13}
{'loss': 1.5111, 'grad_norm': 2.0748937129974365, 'learning_rate': 0.00019932175602531546, 'epoch': 0.13}
{'loss': 1.0579, 'grad_norm': 1.4171772003173828, 'learning_rate': 0.00019931549768300157, 'epoch': 0.13}
{'loss': 1.1174, 'grad_norm': 1.2083078622817993, 'learning_rate': 0.0001993092106985765, 'epoch': 0.13}
{'loss': 1.1162, 'grad_norm': 1.2142245769500732, 'learning_rate': 0.00019930289507385344, 'epoch': 0.13}
{'loss': 1.171, 'grad_norm': 1.1761054992675781, 'learning_rate': 0.0001992965508106537, 'epoch': 0.13}
{'loss': 0.9458, 'grad_norm': 1.2742670774459839, 'learning_rate': 0.000199290177910807, 'epoch': 0.13}
{'loss': 1.1562, 'grad_norm': 1.6697202920913696, 'learning_rate': 0.00019928377637615122, 'epoch': 0.13}
{'loss': 0.87, 'grad_norm': 1.3334922790527344, 'learning_rate': 0.00019927734620853253, 'epoch': 0.13}
{'loss': 1.1379, 'grad_norm': 1.321995496749878, 'learning_rate': 0.0001992708874098054, 'epoch': 0.13}
{'loss': 0.8589, 'grad_norm': 1.6266170740127563, 'learning_rate': 0.00019926439998183248, 'epoch': 0.13}
{'loss': 0.7127, 'grad_norm': 1.5471534729003906, 'learning_rate': 0.00019925788392648473, 'epoch': 0.13}
{'loss': 0.8134, 'grad_norm': 1.308386206626892, 'learning_rate': 0.00019925133924564135, 'epoch': 0.14}
{'loss': 1.1618, 'grad_norm': 1.9882433414459229, 'learning_rate': 0.00019924476594118974, 'epoch': 0.14}
{'loss': 1.2926, 'grad_norm': 1.905382513999939, 'learning_rate': 0.00019923816401502567, 'epoch': 0.14}
{'loss': 0.9902, 'grad_norm': 1.5696394443511963, 'learning_rate': 0.00019923153346905313, 'epoch': 0.14}
{'loss': 0.9805, 'grad_norm': 1.9137699604034424, 'learning_rate': 0.00019922487430518426, 'epoch': 0.14}
{'loss': 0.8952, 'grad_norm': 1.192258596420288, 'learning_rate': 0.00019921818652533956, 'epoch': 0.14}
{'loss': 1.1593, 'grad_norm': 1.6665290594100952, 'learning_rate': 0.0001992114701314478, 'epoch': 0.14}
{'loss': 0.8202, 'grad_norm': 1.477079153060913, 'learning_rate': 0.00019920472512544588, 'epoch': 0.14}
{'loss': 1.027, 'grad_norm': 1.6140358448028564, 'learning_rate': 0.00019919795150927908, 'epoch': 0.14}
{'loss': 0.9136, 'grad_norm': 1.7337210178375244, 'learning_rate': 0.00019919114928490086, 'epoch': 0.14}
{'loss': 0.8382, 'grad_norm': 2.028134346008301, 'learning_rate': 0.00019918431845427294, 'epoch': 0.14}
{'loss': 1.0185, 'grad_norm': 2.1881346702575684, 'learning_rate': 0.0001991774590193653, 'epoch': 0.14}
{'loss': 0.819, 'grad_norm': 1.5525273084640503, 'learning_rate': 0.0001991705709821562, 'epoch': 0.14}
{'loss': 1.0727, 'grad_norm': 1.8543593883514404, 'learning_rate': 0.0001991636543446321, 'epoch': 0.14}
{'loss': 1.0163, 'grad_norm': 4.831684112548828, 'learning_rate': 0.0001991567091087877, 'epoch': 0.14}
{'loss': 0.9132, 'grad_norm': 1.202690839767456, 'learning_rate': 0.000199149735276626, 'epoch': 0.14}
{'loss': 0.9854, 'grad_norm': 1.6258318424224854, 'learning_rate': 0.00019914273285015822, 'epoch': 0.14}
{'loss': 0.8685, 'grad_norm': 1.2277206182479858, 'learning_rate': 0.00019913570183140378, 'epoch': 0.14}
{'loss': 0.9318, 'grad_norm': 2.0632500648498535, 'learning_rate': 0.00019912864222239044, 'epoch': 0.14}
{'loss': 0.8138, 'grad_norm': 1.6470392942428589, 'learning_rate': 0.00019912155402515417, 'epoch': 0.14}
{'loss': 1.1674, 'grad_norm': 1.1655170917510986, 'learning_rate': 0.00019911443724173914, 'epoch': 0.14}
{'loss': 0.9214, 'grad_norm': 1.3346405029296875, 'learning_rate': 0.00019910729187419781, 'epoch': 0.14}
{'loss': 0.8254, 'grad_norm': 2.064495325088501, 'learning_rate': 0.00019910011792459087, 'epoch': 0.14}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8228, 'grad_norm': 1.4696406126022339, 'learning_rate': 0.0001990929153949872, 'epoch': 0.14}
{'loss': 1.2802, 'grad_norm': 1.884728193283081, 'learning_rate': 0.0001990856842874641, 'epoch': 0.14}
{'loss': 1.1028, 'grad_norm': 1.2482644319534302, 'learning_rate': 0.0001990784246041068, 'epoch': 0.14}
{'loss': 0.9155, 'grad_norm': 1.5651174783706665, 'learning_rate': 0.00019907113634700917, 'epoch': 0.14}
{'loss': 1.1353, 'grad_norm': 1.0273628234863281, 'learning_rate': 0.00019906381951827293, 'epoch': 0.14}
{'loss': 0.9367, 'grad_norm': 1.1529102325439453, 'learning_rate': 0.0001990564741200083, 'epoch': 0.14}
{'loss': 0.8664, 'grad_norm': 1.3341628313064575, 'learning_rate': 0.00019904910015433372, 'epoch': 0.14}
{'loss': 1.0735, 'grad_norm': 1.317061424255371, 'learning_rate': 0.0001990416976233757, 'epoch': 0.14}
{'loss': 1.2254, 'grad_norm': 1.402801513671875, 'learning_rate': 0.0001990342665292691, 'epoch': 0.14}
{'loss': 1.0747, 'grad_norm': 2.000593900680542, 'learning_rate': 0.00019902680687415705, 'epoch': 0.14}
{'loss': 1.0867, 'grad_norm': 1.1327370405197144, 'learning_rate': 0.00019901931866019089, 'epoch': 0.14}
{'loss': 0.9412, 'grad_norm': 1.2958333492279053, 'learning_rate': 0.00019901180188953012, 'epoch': 0.14}
{'loss': 1.0565, 'grad_norm': 1.5354126691818237, 'learning_rate': 0.00019900425656434263, 'epoch': 0.14}
{'loss': 0.9966, 'grad_norm': 1.3910197019577026, 'learning_rate': 0.0001989966826868044, 'epoch': 0.14}
{'loss': 0.8999, 'grad_norm': 1.1397570371627808, 'learning_rate': 0.00019898908025909972, 'epoch': 0.14}
{'loss': 1.1223, 'grad_norm': 1.5484638214111328, 'learning_rate': 0.00019898144928342105, 'epoch': 0.14}
{'loss': 0.9978, 'grad_norm': 1.0576499700546265, 'learning_rate': 0.0001989737897619692, 'epoch': 0.14}
{'loss': 0.8485, 'grad_norm': 1.7744604349136353, 'learning_rate': 0.00019896610169695307, 'epoch': 0.14}
{'loss': 1.0153, 'grad_norm': 1.0199592113494873, 'learning_rate': 0.0001989583850905899, 'epoch': 0.14}
{'loss': 1.1743, 'grad_norm': 1.2058262825012207, 'learning_rate': 0.0001989506399451051, 'epoch': 0.14}
{'loss': 1.0244, 'grad_norm': 1.6932640075683594, 'learning_rate': 0.00019894286626273238, 'epoch': 0.14}
{'loss': 1.0379, 'grad_norm': 1.2691280841827393, 'learning_rate': 0.00019893506404571358, 'epoch': 0.14}
{'loss': 0.7714, 'grad_norm': 1.383039116859436, 'learning_rate': 0.00019892723329629887, 'epoch': 0.14}
{'loss': 0.646, 'grad_norm': 2.2109339237213135, 'learning_rate': 0.00019891937401674649, 'epoch': 0.14}
{'loss': 0.6847, 'grad_norm': 1.3990792036056519, 'learning_rate': 0.00019891148620932318, 'epoch': 0.14}
{'loss': 1.2632, 'grad_norm': 1.3735381364822388, 'learning_rate': 0.00019890356987630362, 'epoch': 0.14}
{'loss': 0.8629, 'grad_norm': 1.1843723058700562, 'learning_rate': 0.0001988956250199709, 'epoch': 0.14}
{'loss': 0.9034, 'grad_norm': 1.1852091550827026, 'learning_rate': 0.00019888765164261627, 'epoch': 0.14}
{'loss': 0.8335, 'grad_norm': 1.2805254459381104, 'learning_rate': 0.00019887964974653918, 'epoch': 0.14}
{'loss': 1.4155, 'grad_norm': 1.4063440561294556, 'learning_rate': 0.0001988716193340474, 'epoch': 0.14}
{'loss': 0.8507, 'grad_norm': 1.6863913536071777, 'learning_rate': 0.00019886356040745682, 'epoch': 0.14}
{'loss': 1.0406, 'grad_norm': 1.4516935348510742, 'learning_rate': 0.0001988554729690916, 'epoch': 0.14}
{'loss': 0.9752, 'grad_norm': 1.8797690868377686, 'learning_rate': 0.00019884735702128413, 'epoch': 0.14}
{'loss': 0.8767, 'grad_norm': 1.2640775442123413, 'learning_rate': 0.000198839212566375, 'epoch': 0.14}
{'loss': 1.2419, 'grad_norm': 1.5248184204101562, 'learning_rate': 0.00019883103960671305, 'epoch': 0.14}
{'loss': 0.6339, 'grad_norm': 1.3027628660202026, 'learning_rate': 0.0001988228381446553, 'epoch': 0.14}
{'loss': 0.7023, 'grad_norm': 1.5818591117858887, 'learning_rate': 0.000198814608182567, 'epoch': 0.14}
{'loss': 0.9331, 'grad_norm': 1.5109754800796509, 'learning_rate': 0.00019880634972282166, 'epoch': 0.14}
{'loss': 1.1665, 'grad_norm': 1.6481010913848877, 'learning_rate': 0.00019879806276780097, 'epoch': 0.14}
{'loss': 0.9497, 'grad_norm': 1.644275426864624, 'learning_rate': 0.00019878974731989485, 'epoch': 0.14}
{'loss': 1.0138, 'grad_norm': 1.2955316305160522, 'learning_rate': 0.00019878140338150144, 'epoch': 0.14}
{'loss': 0.9455, 'grad_norm': 1.3068443536758423, 'learning_rate': 0.00019877303095502708, 'epoch': 0.14}
{'loss': 1.4207, 'grad_norm': 1.3733335733413696, 'learning_rate': 0.00019876463004288633, 'epoch': 0.15}
{'loss': 1.3351, 'grad_norm': 1.3200652599334717, 'learning_rate': 0.00019875620064750202, 'epoch': 0.15}
{'loss': 1.3069, 'grad_norm': 3.233260154724121, 'learning_rate': 0.0001987477427713051, 'epoch': 0.15}
{'loss': 0.7894, 'grad_norm': 1.3940966129302979, 'learning_rate': 0.0001987392564167348, 'epoch': 0.15}
{'loss': 0.979, 'grad_norm': 1.4307414293289185, 'learning_rate': 0.0001987307415862385, 'epoch': 0.15}
{'loss': 1.2236, 'grad_norm': 1.4686949253082275, 'learning_rate': 0.00019872219828227187, 'epoch': 0.15}
{'loss': 1.2574, 'grad_norm': 1.046170711517334, 'learning_rate': 0.0001987136265072988, 'epoch': 0.15}
{'loss': 0.8399, 'grad_norm': 1.3629019260406494, 'learning_rate': 0.00019870502626379127, 'epoch': 0.15}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1707, 'grad_norm': 1.5557996034622192, 'learning_rate': 0.00019869639755422963, 'epoch': 0.15}
{'loss': 0.8313, 'grad_norm': 2.1604080200195312, 'learning_rate': 0.0001986877403811023, 'epoch': 0.15}
{'loss': 0.8903, 'grad_norm': 1.647642731666565, 'learning_rate': 0.000198679054746906, 'epoch': 0.15}
{'loss': 1.1003, 'grad_norm': 1.3183882236480713, 'learning_rate': 0.0001986703406541456, 'epoch': 0.15}
{'loss': 0.9541, 'grad_norm': 1.4295666217803955, 'learning_rate': 0.00019866159810533418, 'epoch': 0.15}
{'loss': 0.9135, 'grad_norm': 1.7263000011444092, 'learning_rate': 0.0001986528271029931, 'epoch': 0.15}
{'loss': 0.9268, 'grad_norm': 1.4079182147979736, 'learning_rate': 0.00019864402764965187, 'epoch': 0.15}
{'loss': 1.2215, 'grad_norm': 1.2504363059997559, 'learning_rate': 0.00019863519974784816, 'epoch': 0.15}
{'loss': 1.2837, 'grad_norm': 1.1856077909469604, 'learning_rate': 0.00019862634340012795, 'epoch': 0.15}
{'loss': 1.0292, 'grad_norm': 1.6971731185913086, 'learning_rate': 0.00019861745860904538, 'epoch': 0.15}
{'loss': 0.8435, 'grad_norm': 1.9751149415969849, 'learning_rate': 0.0001986085453771627, 'epoch': 0.15}
{'loss': 1.0215, 'grad_norm': 1.141425371170044, 'learning_rate': 0.0001985996037070505, 'epoch': 0.15}
{'loss': 0.9534, 'grad_norm': 1.393720030784607, 'learning_rate': 0.00019859063360128752, 'epoch': 0.15}
{'loss': 1.1154, 'grad_norm': 1.2378112077713013, 'learning_rate': 0.00019858163506246066, 'epoch': 0.15}
{'loss': 0.9628, 'grad_norm': 1.3839155435562134, 'learning_rate': 0.0001985726080931651, 'epoch': 0.15}
{'loss': 1.0817, 'grad_norm': 1.9300249814987183, 'learning_rate': 0.00019856355269600413, 'epoch': 0.15}
{'loss': 0.9765, 'grad_norm': 1.4652647972106934, 'learning_rate': 0.00019855446887358932, 'epoch': 0.15}
{'loss': 0.7605, 'grad_norm': 1.4141929149627686, 'learning_rate': 0.00019854535662854037, 'epoch': 0.15}
{'loss': 0.8672, 'grad_norm': 1.2558143138885498, 'learning_rate': 0.00019853621596348524, 'epoch': 0.15}
{'loss': 1.0311, 'grad_norm': 1.2552015781402588, 'learning_rate': 0.00019852704688106003, 'epoch': 0.15}
{'loss': 0.9915, 'grad_norm': 1.3080470561981201, 'learning_rate': 0.00019851784938390904, 'epoch': 0.15}
{'loss': 0.8715, 'grad_norm': 1.507529616355896, 'learning_rate': 0.00019850862347468488, 'epoch': 0.15}
{'loss': 1.2306, 'grad_norm': 1.4502923488616943, 'learning_rate': 0.0001984993691560481, 'epoch': 0.15}
{'loss': 0.7366, 'grad_norm': 1.4302014112472534, 'learning_rate': 0.00019849008643066772, 'epoch': 0.15}
{'loss': 0.72, 'grad_norm': 1.0735143423080444, 'learning_rate': 0.00019848077530122083, 'epoch': 0.15}
{'loss': 1.0778, 'grad_norm': 1.2013288736343384, 'learning_rate': 0.00019847143577039265, 'epoch': 0.15}
{'loss': 0.868, 'grad_norm': 1.1948561668395996, 'learning_rate': 0.0001984620678408767, 'epoch': 0.15}
{'loss': 1.2457, 'grad_norm': 1.154301404953003, 'learning_rate': 0.00019845267151537464, 'epoch': 0.15}
{'loss': 1.3783, 'grad_norm': 1.1032078266143799, 'learning_rate': 0.0001984432467965963, 'epoch': 0.15}
{'loss': 0.9717, 'grad_norm': 1.4651063680648804, 'learning_rate': 0.00019843379368725977, 'epoch': 0.15}
{'loss': 0.8411, 'grad_norm': 1.1787223815917969, 'learning_rate': 0.00019842431219009127, 'epoch': 0.15}
{'loss': 1.0566, 'grad_norm': 1.4182156324386597, 'learning_rate': 0.00019841480230782519, 'epoch': 0.15}
{'loss': 0.9259, 'grad_norm': 1.0868110656738281, 'learning_rate': 0.00019840526404320415, 'epoch': 0.15}
{'loss': 0.9869, 'grad_norm': 2.0623791217803955, 'learning_rate': 0.00019839569739897895, 'epoch': 0.15}
{'loss': 1.1869, 'grad_norm': 2.6057558059692383, 'learning_rate': 0.0001983861023779085, 'epoch': 0.15}
{'loss': 0.9788, 'grad_norm': 1.2547568082809448, 'learning_rate': 0.00019837647898276006, 'epoch': 0.15}
{'loss': 1.1271, 'grad_norm': 1.5983428955078125, 'learning_rate': 0.00019836682721630894, 'epoch': 0.15}
{'loss': 0.9812, 'grad_norm': 2.301755905151367, 'learning_rate': 0.00019835714708133862, 'epoch': 0.15}
{'loss': 1.1745, 'grad_norm': 1.6950695514678955, 'learning_rate': 0.0001983474385806408, 'epoch': 0.15}
{'loss': 1.1637, 'grad_norm': 1.2180155515670776, 'learning_rate': 0.00019833770171701542, 'epoch': 0.15}
{'loss': 1.064, 'grad_norm': 1.4011824131011963, 'learning_rate': 0.0001983279364932705, 'epoch': 0.15}
{'loss': 0.8463, 'grad_norm': 1.2518012523651123, 'learning_rate': 0.00019831814291222232, 'epoch': 0.15}
{'loss': 0.9554, 'grad_norm': 1.3560371398925781, 'learning_rate': 0.0001983083209766953, 'epoch': 0.15}
{'loss': 1.1748, 'grad_norm': 1.2829838991165161, 'learning_rate': 0.00019829847068952202, 'epoch': 0.15}
{'loss': 0.9911, 'grad_norm': 1.1330848932266235, 'learning_rate': 0.00019828859205354323, 'epoch': 0.15}
{'loss': 1.1655, 'grad_norm': 1.4751514196395874, 'learning_rate': 0.00019827868507160792, 'epoch': 0.15}
{'loss': 0.7508, 'grad_norm': 1.423686146736145, 'learning_rate': 0.0001982687497465732, 'epoch': 0.15}
{'loss': 0.9508, 'grad_norm': 1.1035137176513672, 'learning_rate': 0.00019825878608130436, 'epoch': 0.15}
{'loss': 0.8189, 'grad_norm': 1.1077089309692383, 'learning_rate': 0.00019824879407867493, 'epoch': 0.15}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9319, 'grad_norm': 1.5809534788131714, 'learning_rate': 0.00019823877374156647, 'epoch': 0.15}
{'loss': 0.9287, 'grad_norm': 1.1893328428268433, 'learning_rate': 0.0001982287250728689, 'epoch': 0.15}
{'loss': 1.1067, 'grad_norm': 1.1541208028793335, 'learning_rate': 0.00019821864807548009, 'epoch': 0.15}
{'loss': 0.7606, 'grad_norm': 1.3221327066421509, 'learning_rate': 0.00019820854275230627, 'epoch': 0.15}
{'loss': 1.0958, 'grad_norm': 1.18315851688385, 'learning_rate': 0.00019819840910626174, 'epoch': 0.15}
{'loss': 0.8119, 'grad_norm': 1.2846381664276123, 'learning_rate': 0.00019818824714026905, 'epoch': 0.15}
{'loss': 1.0526, 'grad_norm': 1.3176361322402954, 'learning_rate': 0.00019817805685725875, 'epoch': 0.15}
{'loss': 1.0604, 'grad_norm': 1.5857908725738525, 'learning_rate': 0.0001981678382601698, 'epoch': 0.15}
{'loss': 1.1455, 'grad_norm': 1.3614850044250488, 'learning_rate': 0.00019815759135194912, 'epoch': 0.16}
{'loss': 0.7837, 'grad_norm': 1.49601411819458, 'learning_rate': 0.00019814731613555184, 'epoch': 0.16}
{'loss': 0.8815, 'grad_norm': 1.4093496799468994, 'learning_rate': 0.00019813701261394136, 'epoch': 0.16}
{'loss': 0.8978, 'grad_norm': 1.2098674774169922, 'learning_rate': 0.00019812668079008912, 'epoch': 0.16}
{'loss': 1.1885, 'grad_norm': 1.338593602180481, 'learning_rate': 0.00019811632066697477, 'epoch': 0.16}
{'loss': 1.2978, 'grad_norm': 1.1606258153915405, 'learning_rate': 0.00019810593224758614, 'epoch': 0.16}
{'loss': 1.1417, 'grad_norm': 1.5268168449401855, 'learning_rate': 0.00019809551553491916, 'epoch': 0.16}
{'loss': 1.2152, 'grad_norm': 1.403198480606079, 'learning_rate': 0.00019808507053197802, 'epoch': 0.16}
{'loss': 1.0172, 'grad_norm': 1.1827532052993774, 'learning_rate': 0.00019807459724177496, 'epoch': 0.16}
{'loss': 0.83, 'grad_norm': 1.2620062828063965, 'learning_rate': 0.00019806409566733044, 'epoch': 0.16}
{'loss': 0.9013, 'grad_norm': 1.5791351795196533, 'learning_rate': 0.00019805356581167308, 'epoch': 0.16}
{'loss': 0.8811, 'grad_norm': 1.678418755531311, 'learning_rate': 0.00019804300767783958, 'epoch': 0.16}
{'loss': 1.1588, 'grad_norm': 1.4669843912124634, 'learning_rate': 0.00019803242126887493, 'epoch': 0.16}
{'loss': 1.0662, 'grad_norm': 1.3394964933395386, 'learning_rate': 0.0001980218065878322, 'epoch': 0.16}
{'loss': 1.5089, 'grad_norm': 1.1805479526519775, 'learning_rate': 0.00019801116363777252, 'epoch': 0.16}
{'loss': 1.1678, 'grad_norm': 1.2366489171981812, 'learning_rate': 0.00019800049242176538, 'epoch': 0.16}
{'loss': 1.1225, 'grad_norm': 1.4682003259658813, 'learning_rate': 0.00019798979294288822, 'epoch': 0.16}
{'loss': 1.2087, 'grad_norm': 1.3716768026351929, 'learning_rate': 0.00019797906520422677, 'epoch': 0.16}
{'loss': 1.1618, 'grad_norm': 1.06330144405365, 'learning_rate': 0.00019796830920887484, 'epoch': 0.16}
{'loss': 0.9903, 'grad_norm': 1.478050947189331, 'learning_rate': 0.0001979575249599344, 'epoch': 0.16}
{'loss': 0.8661, 'grad_norm': 1.4851332902908325, 'learning_rate': 0.0001979467124605156, 'epoch': 0.16}
{'loss': 0.9473, 'grad_norm': 1.0689228773117065, 'learning_rate': 0.0001979358717137367, 'epoch': 0.16}
{'loss': 1.0964, 'grad_norm': 1.4803701639175415, 'learning_rate': 0.0001979250027227241, 'epoch': 0.16}
{'loss': 0.9342, 'grad_norm': 1.401171088218689, 'learning_rate': 0.00019791410549061238, 'epoch': 0.16}
{'loss': 0.9375, 'grad_norm': 1.4528666734695435, 'learning_rate': 0.00019790318002054424, 'epoch': 0.16}
{'loss': 1.1182, 'grad_norm': 1.2923665046691895, 'learning_rate': 0.00019789222631567057, 'epoch': 0.16}
{'loss': 0.9811, 'grad_norm': 1.2644410133361816, 'learning_rate': 0.0001978812443791503, 'epoch': 0.16}
{'loss': 0.9818, 'grad_norm': 1.2893576622009277, 'learning_rate': 0.00019787023421415065, 'epoch': 0.16}
{'loss': 0.995, 'grad_norm': 1.344772458076477, 'learning_rate': 0.00019785919582384685, 'epoch': 0.16}
{'loss': 0.9207, 'grad_norm': 1.393376350402832, 'learning_rate': 0.0001978481292114223, 'epoch': 0.16}
{'loss': 1.3417, 'grad_norm': 1.0992072820663452, 'learning_rate': 0.00019783703438006859, 'epoch': 0.16}
{'loss': 1.2732, 'grad_norm': 1.4313098192214966, 'learning_rate': 0.0001978259113329854, 'epoch': 0.16}
{'loss': 0.9811, 'grad_norm': 1.2566173076629639, 'learning_rate': 0.00019781476007338058, 'epoch': 0.16}
{'loss': 1.0509, 'grad_norm': 1.1441378593444824, 'learning_rate': 0.00019780358060447005, 'epoch': 0.16}
{'loss': 0.8277, 'grad_norm': 2.0255048274993896, 'learning_rate': 0.00019779237292947804, 'epoch': 0.16}
{'loss': 1.1751, 'grad_norm': 1.1124773025512695, 'learning_rate': 0.00019778113705163662, 'epoch': 0.16}
{'loss': 0.9955, 'grad_norm': 1.824549674987793, 'learning_rate': 0.00019776987297418628, 'epoch': 0.16}
{'loss': 1.2962, 'grad_norm': 1.3396563529968262, 'learning_rate': 0.0001977585807003755, 'epoch': 0.16}
{'loss': 1.0645, 'grad_norm': 1.2923660278320312, 'learning_rate': 0.0001977472602334609, 'epoch': 0.16}
{'loss': 1.1952, 'grad_norm': 1.1306906938552856, 'learning_rate': 0.00019773591157670728, 'epoch': 0.16}
{'loss': 1.0845, 'grad_norm': 1.3015507459640503, 'learning_rate': 0.00019772453473338749, 'epoch': 0.16}
{'loss': 1.0646, 'grad_norm': 1.676203966140747, 'learning_rate': 0.00019771312970678258, 'epoch': 0.16}
{'loss': 1.164, 'grad_norm': 1.0577585697174072, 'learning_rate': 0.00019770169650018172, 'epoch': 0.16}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9076, 'grad_norm': 1.4479666948318481, 'learning_rate': 0.00019769023511688217, 'epoch': 0.16}
{'loss': 0.7032, 'grad_norm': 1.4507086277008057, 'learning_rate': 0.00019767874556018934, 'epoch': 0.16}
{'loss': 1.0319, 'grad_norm': 1.757204532623291, 'learning_rate': 0.0001976672278334168, 'epoch': 0.16}
{'loss': 1.0227, 'grad_norm': 1.1100183725357056, 'learning_rate': 0.00019765568193988618, 'epoch': 0.16}
{'loss': 0.7943, 'grad_norm': 1.408842921257019, 'learning_rate': 0.00019764410788292722, 'epoch': 0.16}
{'loss': 0.9853, 'grad_norm': 1.1121455430984497, 'learning_rate': 0.00019763250566587787, 'epoch': 0.16}
{'loss': 1.0649, 'grad_norm': 1.2960703372955322, 'learning_rate': 0.00019762087529208414, 'epoch': 0.16}
{'loss': 0.6381, 'grad_norm': 1.4458471536636353, 'learning_rate': 0.00019760921676490018, 'epoch': 0.16}
{'loss': 0.9948, 'grad_norm': 1.3366745710372925, 'learning_rate': 0.00019759753008768827, 'epoch': 0.16}
{'loss': 0.9646, 'grad_norm': 1.4206182956695557, 'learning_rate': 0.00019758581526381876, 'epoch': 0.16}
{'loss': 1.1397, 'grad_norm': 1.29909086227417, 'learning_rate': 0.00019757407229667015, 'epoch': 0.16}
{'loss': 0.8802, 'grad_norm': 1.0611168146133423, 'learning_rate': 0.0001975623011896291, 'epoch': 0.16}
{'loss': 1.0613, 'grad_norm': 1.0000965595245361, 'learning_rate': 0.00019755050194609025, 'epoch': 0.16}
{'loss': 0.9197, 'grad_norm': 1.2904894351959229, 'learning_rate': 0.0001975386745694565, 'epoch': 0.16}
{'loss': 1.1689, 'grad_norm': 1.4092702865600586, 'learning_rate': 0.00019752681906313888, 'epoch': 0.16}
{'loss': 1.0561, 'grad_norm': 1.3890995979309082, 'learning_rate': 0.00019751493543055632, 'epoch': 0.16}
{'loss': 0.9376, 'grad_norm': 1.3635897636413574, 'learning_rate': 0.00019750302367513612, 'epoch': 0.16}
{'loss': 1.2212, 'grad_norm': 1.2418440580368042, 'learning_rate': 0.0001974910838003135, 'epoch': 0.16}
{'loss': 1.0152, 'grad_norm': 1.2214171886444092, 'learning_rate': 0.00019747911580953188, 'epoch': 0.16}
{'loss': 0.9016, 'grad_norm': 1.2981072664260864, 'learning_rate': 0.0001974671197062428, 'epoch': 0.16}
{'loss': 0.9375, 'grad_norm': 1.1045759916305542, 'learning_rate': 0.00019745509549390583, 'epoch': 0.16}
{'loss': 1.1471, 'grad_norm': 1.0030766725540161, 'learning_rate': 0.00019744304317598874, 'epoch': 0.16}
{'loss': 0.8746, 'grad_norm': 1.1232720613479614, 'learning_rate': 0.00019743096275596735, 'epoch': 0.17}
{'loss': 1.2546, 'grad_norm': 1.5072096586227417, 'learning_rate': 0.00019741885423732558, 'epoch': 0.17}
{'loss': 1.3774, 'grad_norm': 1.18204927444458, 'learning_rate': 0.00019740671762355548, 'epoch': 0.17}
{'loss': 1.074, 'grad_norm': 1.2626625299453735, 'learning_rate': 0.00019739455291815716, 'epoch': 0.17}
{'loss': 0.9514, 'grad_norm': 1.447407841682434, 'learning_rate': 0.00019738236012463892, 'epoch': 0.17}
{'loss': 1.1108, 'grad_norm': 1.2470060586929321, 'learning_rate': 0.00019737013924651706, 'epoch': 0.17}
{'loss': 0.8428, 'grad_norm': 1.1978715658187866, 'learning_rate': 0.00019735789028731604, 'epoch': 0.17}
{'loss': 1.1045, 'grad_norm': 1.2401384115219116, 'learning_rate': 0.00019734561325056838, 'epoch': 0.17}
{'loss': 1.0121, 'grad_norm': 1.341397762298584, 'learning_rate': 0.00019733330813981477, 'epoch': 0.17}
{'loss': 1.0392, 'grad_norm': 1.1873505115509033, 'learning_rate': 0.00019732097495860386, 'epoch': 0.17}
{'loss': 1.2423, 'grad_norm': 1.2679485082626343, 'learning_rate': 0.00019730861371049257, 'epoch': 0.17}
{'loss': 1.1159, 'grad_norm': 1.2596012353897095, 'learning_rate': 0.00019729622439904575, 'epoch': 0.17}
{'loss': 1.0706, 'grad_norm': 1.1156139373779297, 'learning_rate': 0.00019728380702783643, 'epoch': 0.17}
{'loss': 1.2593, 'grad_norm': 1.2583836317062378, 'learning_rate': 0.0001972713616004458, 'epoch': 0.17}
{'loss': 0.9036, 'grad_norm': 1.2025892734527588, 'learning_rate': 0.00019725888812046298, 'epoch': 0.17}
{'loss': 1.0521, 'grad_norm': 1.8243309259414673, 'learning_rate': 0.00019724638659148527, 'epoch': 0.17}
{'loss': 1.014, 'grad_norm': 1.3197877407073975, 'learning_rate': 0.00019723385701711808, 'epoch': 0.17}
{'loss': 0.7586, 'grad_norm': 1.1711310148239136, 'learning_rate': 0.00019722129940097488, 'epoch': 0.17}
{'loss': 1.1787, 'grad_norm': 1.0715184211730957, 'learning_rate': 0.00019720871374667714, 'epoch': 0.17}
{'loss': 0.8975, 'grad_norm': 1.1798357963562012, 'learning_rate': 0.00019719610005785465, 'epoch': 0.17}
{'loss': 1.3466, 'grad_norm': 1.5581146478652954, 'learning_rate': 0.00019718345833814502, 'epoch': 0.17}
{'loss': 0.9024, 'grad_norm': 1.7541348934173584, 'learning_rate': 0.0001971707885911941, 'epoch': 0.17}
{'loss': 1.2149, 'grad_norm': 1.4057092666625977, 'learning_rate': 0.00019715809082065578, 'epoch': 0.17}
{'loss': 0.7695, 'grad_norm': 1.319937825202942, 'learning_rate': 0.00019714536503019202, 'epoch': 0.17}
{'loss': 1.0585, 'grad_norm': 1.5289398431777954, 'learning_rate': 0.00019713261122347294, 'epoch': 0.17}
{'loss': 1.0559, 'grad_norm': 1.1567186117172241, 'learning_rate': 0.00019711982940417663, 'epoch': 0.17}
{'loss': 1.0407, 'grad_norm': 1.2445851564407349, 'learning_rate': 0.00019710701957598926, 'epoch': 0.17}
{'loss': 0.939, 'grad_norm': 1.1236196756362915, 'learning_rate': 0.0001970941817426052, 'epoch': 0.17}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.899, 'grad_norm': 1.3214234113693237, 'learning_rate': 0.0001970813159077268, 'epoch': 0.17}
{'loss': 1.1842, 'grad_norm': 1.0838738679885864, 'learning_rate': 0.0001970684220750645, 'epoch': 0.17}
{'loss': 0.7613, 'grad_norm': 1.3287245035171509, 'learning_rate': 0.00019705550024833678, 'epoch': 0.17}
{'loss': 1.0141, 'grad_norm': 1.4539260864257812, 'learning_rate': 0.00019704255043127028, 'epoch': 0.17}
{'loss': 1.0421, 'grad_norm': 1.5468249320983887, 'learning_rate': 0.00019702957262759965, 'epoch': 0.17}
{'loss': 0.9445, 'grad_norm': 2.0116255283355713, 'learning_rate': 0.00019701656684106763, 'epoch': 0.17}
{'loss': 0.762, 'grad_norm': 1.5996816158294678, 'learning_rate': 0.00019700353307542504, 'epoch': 0.17}
{'loss': 1.1559, 'grad_norm': 1.0898879766464233, 'learning_rate': 0.0001969904713344307, 'epoch': 0.17}
{'loss': 0.9683, 'grad_norm': 1.0412225723266602, 'learning_rate': 0.00019697738162185161, 'epoch': 0.17}
{'loss': 0.6377, 'grad_norm': 1.4551708698272705, 'learning_rate': 0.00019696426394146278, 'epoch': 0.17}
{'loss': 0.906, 'grad_norm': 1.7987815141677856, 'learning_rate': 0.00019695111829704724, 'epoch': 0.17}
{'loss': 1.1293, 'grad_norm': 1.7954636812210083, 'learning_rate': 0.0001969379446923962, 'epoch': 0.17}
{'loss': 0.9185, 'grad_norm': 0.9943289756774902, 'learning_rate': 0.00019692474313130878, 'epoch': 0.17}
{'loss': 1.2608, 'grad_norm': 1.8019869327545166, 'learning_rate': 0.0001969115136175923, 'epoch': 0.17}
{'loss': 1.0051, 'grad_norm': 1.3024154901504517, 'learning_rate': 0.00019689825615506207, 'epoch': 0.17}
{'loss': 1.0524, 'grad_norm': 4.064797878265381, 'learning_rate': 0.0001968849707475415, 'epoch': 0.17}
{'loss': 1.3319, 'grad_norm': 1.4289829730987549, 'learning_rate': 0.00019687165739886198, 'epoch': 0.17}
{'loss': 0.9486, 'grad_norm': 2.8616878986358643, 'learning_rate': 0.0001968583161128631, 'epoch': 0.17}
{'loss': 1.0102, 'grad_norm': 1.3792734146118164, 'learning_rate': 0.0001968449468933924, 'epoch': 0.17}
{'loss': 1.3421, 'grad_norm': 1.3989311456680298, 'learning_rate': 0.00019683154974430543, 'epoch': 0.17}
{'loss': 0.8876, 'grad_norm': 1.342255711555481, 'learning_rate': 0.00019681812466946594, 'epoch': 0.17}
{'loss': 1.2384, 'grad_norm': 1.5155459642410278, 'learning_rate': 0.00019680467167274563, 'epoch': 0.17}
{'loss': 1.0435, 'grad_norm': 1.6999156475067139, 'learning_rate': 0.0001967911907580243, 'epoch': 0.17}
{'loss': 1.2488, 'grad_norm': 1.110810399055481, 'learning_rate': 0.00019677768192918971, 'epoch': 0.17}
{'loss': 1.1326, 'grad_norm': 1.4922653436660767, 'learning_rate': 0.00019676414519013781, 'epoch': 0.17}
{'loss': 1.0173, 'grad_norm': 1.559982180595398, 'learning_rate': 0.00019675058054477253, 'epoch': 0.17}
{'loss': 0.9712, 'grad_norm': 1.0563892126083374, 'learning_rate': 0.0001967369879970058, 'epoch': 0.17}
{'loss': 1.0128, 'grad_norm': 1.1569461822509766, 'learning_rate': 0.0001967233675507577, 'epoch': 0.17}
{'loss': 1.0847, 'grad_norm': 1.3677195310592651, 'learning_rate': 0.00019670971920995628, 'epoch': 0.17}
{'loss': 1.0784, 'grad_norm': 1.577858805656433, 'learning_rate': 0.00019669604297853764, 'epoch': 0.17}
{'loss': 0.905, 'grad_norm': 1.1767853498458862, 'learning_rate': 0.00019668233886044597, 'epoch': 0.17}
{'loss': 1.478, 'grad_norm': 1.1874918937683105, 'learning_rate': 0.00019666860685963342, 'epoch': 0.17}
{'loss': 0.9624, 'grad_norm': 1.1638976335525513, 'learning_rate': 0.00019665484698006027, 'epoch': 0.17}
{'loss': 1.0557, 'grad_norm': 1.6535024642944336, 'learning_rate': 0.00019664105922569483, 'epoch': 0.17}
{'loss': 1.2014, 'grad_norm': 1.5175644159317017, 'learning_rate': 0.00019662724360051335, 'epoch': 0.17}
{'loss': 0.8837, 'grad_norm': 1.2132610082626343, 'learning_rate': 0.00019661340010850026, 'epoch': 0.17}
{'loss': 1.1201, 'grad_norm': 1.2590100765228271, 'learning_rate': 0.0001965995287536479, 'epoch': 0.17}
{'loss': 0.9843, 'grad_norm': 1.3053556680679321, 'learning_rate': 0.00019658562953995677, 'epoch': 0.18}
{'loss': 1.0747, 'grad_norm': 0.9997606873512268, 'learning_rate': 0.00019657170247143525, 'epoch': 0.18}
{'loss': 0.8235, 'grad_norm': 1.2100709676742554, 'learning_rate': 0.0001965577475520999, 'epoch': 0.18}
{'loss': 0.9697, 'grad_norm': 1.6297770738601685, 'learning_rate': 0.00019654376478597525, 'epoch': 0.18}
{'loss': 1.1463, 'grad_norm': 2.35806941986084, 'learning_rate': 0.00019652975417709385, 'epoch': 0.18}
{'loss': 1.2601, 'grad_norm': 1.2685825824737549, 'learning_rate': 0.00019651571572949626, 'epoch': 0.18}
{'loss': 1.1853, 'grad_norm': 1.767335295677185, 'learning_rate': 0.00019650164944723115, 'epoch': 0.18}
{'loss': 1.0416, 'grad_norm': 1.2748397588729858, 'learning_rate': 0.00019648755533435518, 'epoch': 0.18}
{'loss': 0.9708, 'grad_norm': 1.24242103099823, 'learning_rate': 0.00019647343339493294, 'epoch': 0.18}
{'loss': 0.6778, 'grad_norm': 1.1970289945602417, 'learning_rate': 0.0001964592836330372, 'epoch': 0.18}
{'loss': 1.0918, 'grad_norm': 0.9858988523483276, 'learning_rate': 0.00019644510605274868, 'epoch': 0.18}
{'loss': 1.0285, 'grad_norm': 1.224533200263977, 'learning_rate': 0.00019643090065815612, 'epoch': 0.18}
{'loss': 1.0157, 'grad_norm': 1.3238332271575928, 'learning_rate': 0.00019641666745335624, 'epoch': 0.18}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1343, 'grad_norm': 1.5981899499893188, 'learning_rate': 0.0001964024064424539, 'epoch': 0.18}
{'loss': 1.0613, 'grad_norm': 1.1225959062576294, 'learning_rate': 0.00019638811762956186, 'epoch': 0.18}
{'loss': 1.2245, 'grad_norm': 1.9399018287658691, 'learning_rate': 0.00019637380101880097, 'epoch': 0.18}
{'loss': 0.8454, 'grad_norm': 1.1459544897079468, 'learning_rate': 0.00019635945661430006, 'epoch': 0.18}
{'loss': 0.9732, 'grad_norm': 1.3028310537338257, 'learning_rate': 0.00019634508442019596, 'epoch': 0.18}
{'loss': 1.1533, 'grad_norm': 4.838940620422363, 'learning_rate': 0.0001963306844406336, 'epoch': 0.18}
{'loss': 0.9553, 'grad_norm': 1.0552728176116943, 'learning_rate': 0.00019631625667976583, 'epoch': 0.18}
{'loss': 0.7608, 'grad_norm': 1.3032753467559814, 'learning_rate': 0.00019630180114175354, 'epoch': 0.18}
{'loss': 0.878, 'grad_norm': 1.409012794494629, 'learning_rate': 0.00019628731783076563, 'epoch': 0.18}
{'loss': 0.9277, 'grad_norm': 1.3172308206558228, 'learning_rate': 0.00019627280675097908, 'epoch': 0.18}
{'loss': 1.1001, 'grad_norm': 1.1757986545562744, 'learning_rate': 0.00019625826790657875, 'epoch': 0.18}
{'loss': 1.0552, 'grad_norm': 1.490966796875, 'learning_rate': 0.0001962437013017576, 'epoch': 0.18}
{'loss': 1.2007, 'grad_norm': 0.9842265844345093, 'learning_rate': 0.00019622910694071656, 'epoch': 0.18}
{'loss': 1.0701, 'grad_norm': 1.334212064743042, 'learning_rate': 0.00019621448482766454, 'epoch': 0.18}
{'loss': 1.0659, 'grad_norm': 1.4737383127212524, 'learning_rate': 0.00019619983496681854, 'epoch': 0.18}
{'loss': 1.1247, 'grad_norm': 1.2930346727371216, 'learning_rate': 0.00019618515736240352, 'epoch': 0.18}
{'loss': 0.7184, 'grad_norm': 1.2717585563659668, 'learning_rate': 0.0001961704520186524, 'epoch': 0.18}
{'loss': 0.9534, 'grad_norm': 1.2147904634475708, 'learning_rate': 0.00019615571893980612, 'epoch': 0.18}
{'loss': 1.0708, 'grad_norm': 1.1802021265029907, 'learning_rate': 0.00019614095813011364, 'epoch': 0.18}
{'loss': 0.9163, 'grad_norm': 1.1789933443069458, 'learning_rate': 0.0001961261695938319, 'epoch': 0.18}
{'loss': 1.0622, 'grad_norm': 1.1784355640411377, 'learning_rate': 0.00019611135333522586, 'epoch': 0.18}
{'loss': 0.9903, 'grad_norm': 1.116777777671814, 'learning_rate': 0.00019609650935856844, 'epoch': 0.18}
{'loss': 1.217, 'grad_norm': 1.1328449249267578, 'learning_rate': 0.0001960816376681406, 'epoch': 0.18}
{'loss': 0.9236, 'grad_norm': 1.7881053686141968, 'learning_rate': 0.00019606673826823122, 'epoch': 0.18}
{'loss': 1.0142, 'grad_norm': 1.3251851797103882, 'learning_rate': 0.00019605181116313724, 'epoch': 0.18}
{'loss': 1.2189, 'grad_norm': 1.1134475469589233, 'learning_rate': 0.00019603685635716356, 'epoch': 0.18}
{'loss': 0.8488, 'grad_norm': 1.5804381370544434, 'learning_rate': 0.0001960218738546231, 'epoch': 0.18}
{'loss': 0.9639, 'grad_norm': 1.33755362033844, 'learning_rate': 0.00019600686365983672, 'epoch': 0.18}
{'loss': 0.8179, 'grad_norm': 1.4064881801605225, 'learning_rate': 0.00019599182577713327, 'epoch': 0.18}
{'loss': 0.7584, 'grad_norm': 1.2659200429916382, 'learning_rate': 0.00019597676021084964, 'epoch': 0.18}
{'loss': 1.063, 'grad_norm': 1.77727210521698, 'learning_rate': 0.0001959616669653306, 'epoch': 0.18}
{'loss': 1.1162, 'grad_norm': 1.626752495765686, 'learning_rate': 0.00019594654604492906, 'epoch': 0.18}
{'loss': 1.4785, 'grad_norm': 1.242380976676941, 'learning_rate': 0.00019593139745400576, 'epoch': 0.18}
{'loss': 0.9179, 'grad_norm': 1.2540313005447388, 'learning_rate': 0.0001959162211969295, 'epoch': 0.18}
{'loss': 1.1132, 'grad_norm': 1.1863490343093872, 'learning_rate': 0.00019590101727807703, 'epoch': 0.18}
{'loss': 0.9988, 'grad_norm': 1.1419909000396729, 'learning_rate': 0.0001958857857018331, 'epoch': 0.18}
{'loss': 1.0595, 'grad_norm': 1.3598791360855103, 'learning_rate': 0.00019587052647259044, 'epoch': 0.18}
{'loss': 1.2517, 'grad_norm': 1.1827775239944458, 'learning_rate': 0.0001958552395947497, 'epoch': 0.18}
{'loss': 0.8234, 'grad_norm': 1.2444275617599487, 'learning_rate': 0.00019583992507271955, 'epoch': 0.18}
{'loss': 0.8713, 'grad_norm': 1.3317553997039795, 'learning_rate': 0.00019582458291091663, 'epoch': 0.18}
{'loss': 1.0678, 'grad_norm': 1.0782415866851807, 'learning_rate': 0.00019580921311376556, 'epoch': 0.18}
{'loss': 0.9269, 'grad_norm': 1.206400990486145, 'learning_rate': 0.00019579381568569888, 'epoch': 0.18}
{'loss': 1.0562, 'grad_norm': 1.2067031860351562, 'learning_rate': 0.00019577839063115716, 'epoch': 0.18}
{'loss': 1.1689, 'grad_norm': 1.1519074440002441, 'learning_rate': 0.00019576293795458893, 'epoch': 0.18}
{'loss': 1.1004, 'grad_norm': 1.1642253398895264, 'learning_rate': 0.00019574745766045063, 'epoch': 0.18}
{'loss': 1.0573, 'grad_norm': 1.676946759223938, 'learning_rate': 0.00019573194975320673, 'epoch': 0.18}
{'loss': 0.998, 'grad_norm': 0.9942699074745178, 'learning_rate': 0.00019571641423732963, 'epoch': 0.18}
{'loss': 0.9694, 'grad_norm': 1.4514671564102173, 'learning_rate': 0.00019570085111729964, 'epoch': 0.18}
{'loss': 1.1102, 'grad_norm': 1.005658745765686, 'learning_rate': 0.0001956852603976052, 'epoch': 0.18}
{'loss': 0.8121, 'grad_norm': 1.2617413997650146, 'learning_rate': 0.00019566964208274254, 'epoch': 0.18}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9601, 'grad_norm': 1.2514915466308594, 'learning_rate': 0.00019565399617721586, 'epoch': 0.18}
{'loss': 0.8389, 'grad_norm': 1.4128715991973877, 'learning_rate': 0.00019563832268553743, 'epoch': 0.18}
{'loss': 0.7865, 'grad_norm': 1.4074211120605469, 'learning_rate': 0.0001956226216122274, 'epoch': 0.19}
{'loss': 0.8932, 'grad_norm': 1.6718299388885498, 'learning_rate': 0.00019560689296181386, 'epoch': 0.19}
{'loss': 1.0629, 'grad_norm': 1.973826289176941, 'learning_rate': 0.0001955911367388329, 'epoch': 0.19}
{'loss': 0.989, 'grad_norm': 1.3027849197387695, 'learning_rate': 0.00019557535294782854, 'epoch': 0.19}
{'loss': 0.8463, 'grad_norm': 2.0750749111175537, 'learning_rate': 0.0001955595415933527, 'epoch': 0.19}
{'loss': 0.8552, 'grad_norm': 1.1583386659622192, 'learning_rate': 0.00019554370267996538, 'epoch': 0.19}
{'loss': 1.2391, 'grad_norm': 1.2175685167312622, 'learning_rate': 0.00019552783621223436, 'epoch': 0.19}
{'loss': 1.1056, 'grad_norm': 1.2986233234405518, 'learning_rate': 0.00019551194219473553, 'epoch': 0.19}
{'loss': 1.1978, 'grad_norm': 1.5483909845352173, 'learning_rate': 0.0001954960206320526, 'epoch': 0.19}
{'loss': 1.0051, 'grad_norm': 1.0524179935455322, 'learning_rate': 0.0001954800715287773, 'epoch': 0.19}
{'loss': 1.3419, 'grad_norm': 1.1724395751953125, 'learning_rate': 0.00019546409488950924, 'epoch': 0.19}
{'loss': 0.8753, 'grad_norm': 1.529647946357727, 'learning_rate': 0.00019544809071885604, 'epoch': 0.19}
{'loss': 0.7972, 'grad_norm': 1.341101050376892, 'learning_rate': 0.0001954320590214332, 'epoch': 0.19}
{'loss': 0.9361, 'grad_norm': 1.1774615049362183, 'learning_rate': 0.0001954159998018642, 'epoch': 0.19}
{'loss': 0.7771, 'grad_norm': 1.1321274042129517, 'learning_rate': 0.00019539991306478046, 'epoch': 0.19}
{'loss': 0.8502, 'grad_norm': 1.3318736553192139, 'learning_rate': 0.00019538379881482127, 'epoch': 0.19}
{'loss': 1.3041, 'grad_norm': 1.1830928325653076, 'learning_rate': 0.00019536765705663393, 'epoch': 0.19}
{'loss': 0.9359, 'grad_norm': 1.9205784797668457, 'learning_rate': 0.00019535148779487363, 'epoch': 0.19}
{'loss': 0.9885, 'grad_norm': 1.2396960258483887, 'learning_rate': 0.00019533529103420353, 'epoch': 0.19}
{'loss': 0.9028, 'grad_norm': 1.4520671367645264, 'learning_rate': 0.0001953190667792947, 'epoch': 0.19}
{'loss': 0.95, 'grad_norm': 2.3758201599121094, 'learning_rate': 0.00019530281503482612, 'epoch': 0.19}
{'loss': 1.026, 'grad_norm': 1.4463729858398438, 'learning_rate': 0.00019528653580548472, 'epoch': 0.19}
{'loss': 0.884, 'grad_norm': 1.3925211429595947, 'learning_rate': 0.00019527022909596536, 'epoch': 0.19}
{'loss': 0.9538, 'grad_norm': 0.9650259017944336, 'learning_rate': 0.0001952538949109708, 'epoch': 0.19}
{'loss': 0.9227, 'grad_norm': 1.522398829460144, 'learning_rate': 0.00019523753325521176, 'epoch': 0.19}
{'loss': 1.4453, 'grad_norm': 1.4844070672988892, 'learning_rate': 0.00019522114413340682, 'epoch': 0.19}
{'loss': 0.8758, 'grad_norm': 1.968732476234436, 'learning_rate': 0.00019520472755028256, 'epoch': 0.19}
{'loss': 0.9328, 'grad_norm': 1.3463068008422852, 'learning_rate': 0.00019518828351057345, 'epoch': 0.19}
{'loss': 1.1195, 'grad_norm': 1.0652917623519897, 'learning_rate': 0.00019517181201902183, 'epoch': 0.19}
{'loss': 1.1124, 'grad_norm': 1.3309916257858276, 'learning_rate': 0.00019515531308037808, 'epoch': 0.19}
{'loss': 0.9221, 'grad_norm': 1.1630538702011108, 'learning_rate': 0.00019513878669940034, 'epoch': 0.19}
{'loss': 0.8564, 'grad_norm': 1.4000290632247925, 'learning_rate': 0.00019512223288085476, 'epoch': 0.19}
{'loss': 1.2348, 'grad_norm': 1.5089080333709717, 'learning_rate': 0.00019510565162951537, 'epoch': 0.19}
{'loss': 1.11, 'grad_norm': 1.2531448602676392, 'learning_rate': 0.00019508904295016414, 'epoch': 0.19}
{'loss': 0.8784, 'grad_norm': 1.3229373693466187, 'learning_rate': 0.00019507240684759093, 'epoch': 0.19}
{'loss': 1.0612, 'grad_norm': 1.0813626050949097, 'learning_rate': 0.0001950557433265935, 'epoch': 0.19}
{'loss': 1.1716, 'grad_norm': 1.934210181236267, 'learning_rate': 0.00019503905239197757, 'epoch': 0.19}
{'loss': 1.3982, 'grad_norm': 1.10759699344635, 'learning_rate': 0.0001950223340485567, 'epoch': 0.19}
{'loss': 1.1072, 'grad_norm': 1.2225178480148315, 'learning_rate': 0.00019500558830115233, 'epoch': 0.19}
{'loss': 1.212, 'grad_norm': 1.3359733819961548, 'learning_rate': 0.00019498881515459394, 'epoch': 0.19}
{'loss': 1.2823, 'grad_norm': 0.9857993721961975, 'learning_rate': 0.00019497201461371878, 'epoch': 0.19}
{'loss': 1.0008, 'grad_norm': 1.0563238859176636, 'learning_rate': 0.00019495518668337201, 'epoch': 0.19}
{'loss': 0.997, 'grad_norm': 1.293912649154663, 'learning_rate': 0.00019493833136840683, 'epoch': 0.19}
{'loss': 1.1906, 'grad_norm': 1.6599606275558472, 'learning_rate': 0.0001949214486736841, 'epoch': 0.19}
{'loss': 0.9032, 'grad_norm': 1.3480662107467651, 'learning_rate': 0.00019490453860407278, 'epoch': 0.19}
{'loss': 1.1929, 'grad_norm': 1.3338574171066284, 'learning_rate': 0.00019488760116444966, 'epoch': 0.19}
{'loss': 0.967, 'grad_norm': 1.461532473564148, 'learning_rate': 0.0001948706363596994, 'epoch': 0.19}
{'loss': 0.9788, 'grad_norm': 1.3135920763015747, 'learning_rate': 0.00019485364419471454, 'epoch': 0.19}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.02, 'grad_norm': 1.4193123579025269, 'learning_rate': 0.00019483662467439561, 'epoch': 0.19}
{'loss': 0.8848, 'grad_norm': 1.4365692138671875, 'learning_rate': 0.0001948195778036509, 'epoch': 0.19}
{'loss': 1.0755, 'grad_norm': 1.5190198421478271, 'learning_rate': 0.00019480250358739663, 'epoch': 0.19}
{'loss': 0.7986, 'grad_norm': 1.1624345779418945, 'learning_rate': 0.00019478540203055698, 'epoch': 0.19}
{'loss': 1.0952, 'grad_norm': 1.3168880939483643, 'learning_rate': 0.00019476827313806393, 'epoch': 0.19}
{'loss': 0.971, 'grad_norm': 1.0477923154830933, 'learning_rate': 0.00019475111691485736, 'epoch': 0.19}
{'loss': 0.7376, 'grad_norm': 1.4620860815048218, 'learning_rate': 0.00019473393336588506, 'epoch': 0.19}
{'loss': 0.9087, 'grad_norm': 1.0210572481155396, 'learning_rate': 0.00019471672249610266, 'epoch': 0.19}
{'loss': 0.9711, 'grad_norm': 1.2689489126205444, 'learning_rate': 0.0001946994843104737, 'epoch': 0.19}
{'loss': 1.2313, 'grad_norm': 1.1412577629089355, 'learning_rate': 0.0001946822188139696, 'epoch': 0.19}
{'loss': 1.1059, 'grad_norm': 1.4980714321136475, 'learning_rate': 0.00019466492601156966, 'epoch': 0.19}
{'loss': 1.0557, 'grad_norm': 1.726240634918213, 'learning_rate': 0.00019464760590826098, 'epoch': 0.19}
{'loss': 0.6972, 'grad_norm': 1.2102656364440918, 'learning_rate': 0.00019463025850903866, 'epoch': 0.19}
{'loss': 0.7815, 'grad_norm': 1.5085601806640625, 'learning_rate': 0.00019461288381890558, 'epoch': 0.19}
{'loss': 1.2515, 'grad_norm': 1.5951341390609741, 'learning_rate': 0.00019459548184287253, 'epoch': 0.19}
{'loss': 1.0881, 'grad_norm': 1.4781297445297241, 'learning_rate': 0.00019457805258595815, 'epoch': 0.19}
{'loss': 1.0346, 'grad_norm': 1.0953208208084106, 'learning_rate': 0.00019456059605318894, 'epoch': 0.19}
{'loss': 0.9123, 'grad_norm': 1.3824710845947266, 'learning_rate': 0.00019454311224959927, 'epoch': 0.2}
{'loss': 0.7381, 'grad_norm': 1.3965977430343628, 'learning_rate': 0.0001945256011802314, 'epoch': 0.2}
{'loss': 1.113, 'grad_norm': 1.3432137966156006, 'learning_rate': 0.00019450806285013547, 'epoch': 0.2}
{'loss': 0.9519, 'grad_norm': 1.0450551509857178, 'learning_rate': 0.0001944904972643694, 'epoch': 0.2}
{'loss': 0.6442, 'grad_norm': 1.3271478414535522, 'learning_rate': 0.00019447290442799905, 'epoch': 0.2}
{'loss': 1.1022, 'grad_norm': 1.3526922464370728, 'learning_rate': 0.0001944552843460981, 'epoch': 0.2}
{'loss': 0.788, 'grad_norm': 1.1325371265411377, 'learning_rate': 0.00019443763702374812, 'epoch': 0.2}
{'loss': 0.7474, 'grad_norm': 1.2655500173568726, 'learning_rate': 0.00019441996246603846, 'epoch': 0.2}
{'loss': 1.2685, 'grad_norm': 3.1571407318115234, 'learning_rate': 0.00019440226067806644, 'epoch': 0.2}
{'loss': 0.889, 'grad_norm': 1.2255504131317139, 'learning_rate': 0.00019438453166493712, 'epoch': 0.2}
{'loss': 0.9694, 'grad_norm': 1.572306513786316, 'learning_rate': 0.00019436677543176347, 'epoch': 0.2}
{'loss': 1.0085, 'grad_norm': 1.2022262811660767, 'learning_rate': 0.00019434899198366633, 'epoch': 0.2}
{'loss': 1.0693, 'grad_norm': 1.1946861743927002, 'learning_rate': 0.0001943311813257743, 'epoch': 0.2}
{'loss': 0.9422, 'grad_norm': 1.2968802452087402, 'learning_rate': 0.00019431334346322397, 'epoch': 0.2}
{'loss': 1.0417, 'grad_norm': 1.242856502532959, 'learning_rate': 0.00019429547840115968, 'epoch': 0.2}
{'loss': 0.9734, 'grad_norm': 1.2792637348175049, 'learning_rate': 0.00019427758614473355, 'epoch': 0.2}
{'loss': 1.0547, 'grad_norm': 1.3017704486846924, 'learning_rate': 0.0001942596666991057, 'epoch': 0.2}
{'loss': 1.1827, 'grad_norm': 1.0838110446929932, 'learning_rate': 0.00019424172006944398, 'epoch': 0.2}
{'loss': 0.8604, 'grad_norm': 1.6653927564620972, 'learning_rate': 0.0001942237462609241, 'epoch': 0.2}
{'loss': 1.0735, 'grad_norm': 1.6748791933059692, 'learning_rate': 0.00019420574527872968, 'epoch': 0.2}
{'loss': 1.0508, 'grad_norm': 1.1127710342407227, 'learning_rate': 0.00019418771712805203, 'epoch': 0.2}
{'loss': 0.7686, 'grad_norm': 1.175180196762085, 'learning_rate': 0.00019416966181409046, 'epoch': 0.2}
{'loss': 1.0683, 'grad_norm': 1.349395513534546, 'learning_rate': 0.000194151579342052, 'epoch': 0.2}
{'loss': 1.3536, 'grad_norm': 1.8481751680374146, 'learning_rate': 0.00019413346971715152, 'epoch': 0.2}
{'loss': 1.1446, 'grad_norm': 1.5706709623336792, 'learning_rate': 0.0001941153329446118, 'epoch': 0.2}
{'loss': 1.1453, 'grad_norm': 1.0969535112380981, 'learning_rate': 0.00019409716902966335, 'epoch': 0.2}
{'loss': 1.4589, 'grad_norm': 1.5022969245910645, 'learning_rate': 0.0001940789779775446, 'epoch': 0.2}
{'loss': 1.3598, 'grad_norm': 0.9716764092445374, 'learning_rate': 0.00019406075979350174, 'epoch': 0.2}
{'loss': 0.7885, 'grad_norm': 1.2231686115264893, 'learning_rate': 0.0001940425144827888, 'epoch': 0.2}
{'loss': 1.2164, 'grad_norm': 1.1419814825057983, 'learning_rate': 0.00019402424205066765, 'epoch': 0.2}
{'loss': 1.0457, 'grad_norm': 1.017904281616211, 'learning_rate': 0.00019400594250240798, 'epoch': 0.2}
{'loss': 0.7944, 'grad_norm': 1.2110564708709717, 'learning_rate': 0.00019398761584328727, 'epoch': 0.2}
{'loss': 1.2357, 'grad_norm': 1.3045934438705444, 'learning_rate': 0.00019396926207859084, 'epoch': 0.2}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0168, 'grad_norm': 1.0419598817825317, 'learning_rate': 0.00019395088121361187, 'epoch': 0.2}
{'loss': 0.9408, 'grad_norm': 1.219273328781128, 'learning_rate': 0.00019393247325365126, 'epoch': 0.2}
{'loss': 0.7636, 'grad_norm': 1.2450463771820068, 'learning_rate': 0.00019391403820401778, 'epoch': 0.2}
{'loss': 0.8506, 'grad_norm': 1.2913705110549927, 'learning_rate': 0.00019389557607002805, 'epoch': 0.2}
{'loss': 1.061, 'grad_norm': 1.0698268413543701, 'learning_rate': 0.00019387708685700646, 'epoch': 0.2}
{'loss': 1.1133, 'grad_norm': 1.3445497751235962, 'learning_rate': 0.00019385857057028518, 'epoch': 0.2}
{'loss': 1.1239, 'grad_norm': 1.2851386070251465, 'learning_rate': 0.0001938400272152042, 'epoch': 0.2}
{'loss': 1.0796, 'grad_norm': 1.1365379095077515, 'learning_rate': 0.00019382145679711142, 'epoch': 0.2}
{'loss': 0.8942, 'grad_norm': 1.470218300819397, 'learning_rate': 0.00019380285932136233, 'epoch': 0.2}
{'loss': 1.0702, 'grad_norm': 1.5275323390960693, 'learning_rate': 0.00019378423479332046, 'epoch': 0.2}
{'loss': 0.812, 'grad_norm': 1.7951290607452393, 'learning_rate': 0.00019376558321835698, 'epoch': 0.2}
{'loss': 0.9582, 'grad_norm': 1.0280472040176392, 'learning_rate': 0.00019374690460185093, 'epoch': 0.2}
{'loss': 1.1182, 'grad_norm': 1.7025060653686523, 'learning_rate': 0.00019372819894918915, 'epoch': 0.2}
{'loss': 0.9439, 'grad_norm': 1.3221781253814697, 'learning_rate': 0.00019370946626576626, 'epoch': 0.2}
{'loss': 1.1142, 'grad_norm': 1.373955488204956, 'learning_rate': 0.00019369070655698463, 'epoch': 0.2}
{'loss': 1.2279, 'grad_norm': 1.0907948017120361, 'learning_rate': 0.0001936719198282545, 'epoch': 0.2}
{'loss': 0.9609, 'grad_norm': 1.3163150548934937, 'learning_rate': 0.0001936531060849939, 'epoch': 0.2}
{'loss': 1.2585, 'grad_norm': 1.1777029037475586, 'learning_rate': 0.00019363426533262857, 'epoch': 0.2}
{'loss': 0.8322, 'grad_norm': 1.2068051099777222, 'learning_rate': 0.0001936153975765921, 'epoch': 0.2}
{'loss': 1.3516, 'grad_norm': 1.1135352849960327, 'learning_rate': 0.0001935965028223259, 'epoch': 0.2}
{'loss': 1.0534, 'grad_norm': 1.9121474027633667, 'learning_rate': 0.00019357758107527913, 'epoch': 0.2}
{'loss': 1.1672, 'grad_norm': 1.1508512496948242, 'learning_rate': 0.00019355863234090864, 'epoch': 0.2}
{'loss': 1.0117, 'grad_norm': 1.4382133483886719, 'learning_rate': 0.00019353965662467925, 'epoch': 0.2}
{'loss': 0.8535, 'grad_norm': 1.1892266273498535, 'learning_rate': 0.00019352065393206345, 'epoch': 0.2}
{'loss': 1.1374, 'grad_norm': 1.3335559368133545, 'learning_rate': 0.0001935016242685415, 'epoch': 0.2}
{'loss': 1.3107, 'grad_norm': 1.1466667652130127, 'learning_rate': 0.00019348256763960145, 'epoch': 0.2}
{'loss': 1.0211, 'grad_norm': 1.264670968055725, 'learning_rate': 0.00019346348405073916, 'epoch': 0.2}
{'loss': 1.2466, 'grad_norm': 1.1765164136886597, 'learning_rate': 0.00019344437350745828, 'epoch': 0.2}
{'loss': 0.8885, 'grad_norm': 1.114032506942749, 'learning_rate': 0.00019342523601527015, 'epoch': 0.2}
{'loss': 1.0958, 'grad_norm': 1.3746339082717896, 'learning_rate': 0.00019340607157969394, 'epoch': 0.2}
{'loss': 0.7583, 'grad_norm': 1.3318538665771484, 'learning_rate': 0.00019338688020625658, 'epoch': 0.2}
{'loss': 1.0456, 'grad_norm': 1.1899510622024536, 'learning_rate': 0.00019336766190049278, 'epoch': 0.2}
{'loss': 1.2728, 'grad_norm': 1.1871896982192993, 'learning_rate': 0.000193348416667945, 'epoch': 0.21}
{'loss': 1.1068, 'grad_norm': 1.4903769493103027, 'learning_rate': 0.00019332914451416347, 'epoch': 0.21}
{'loss': 1.0221, 'grad_norm': 1.527241826057434, 'learning_rate': 0.0001933098454447062, 'epoch': 0.21}
{'loss': 1.2801, 'grad_norm': 1.1452354192733765, 'learning_rate': 0.00019329051946513897, 'epoch': 0.21}
{'loss': 1.2257, 'grad_norm': 1.2781842947006226, 'learning_rate': 0.00019327116658103525, 'epoch': 0.21}
{'loss': 1.0172, 'grad_norm': 1.1689088344573975, 'learning_rate': 0.0001932517867979763, 'epoch': 0.21}
{'loss': 1.2481, 'grad_norm': 1.2083829641342163, 'learning_rate': 0.00019323238012155123, 'epoch': 0.21}
{'loss': 1.2553, 'grad_norm': 1.4629751443862915, 'learning_rate': 0.0001932129465573568, 'epoch': 0.21}
{'loss': 1.078, 'grad_norm': 1.4457645416259766, 'learning_rate': 0.00019319348611099752, 'epoch': 0.21}
{'loss': 1.0755, 'grad_norm': 1.3790444135665894, 'learning_rate': 0.00019317399878808575, 'epoch': 0.21}
{'loss': 1.0126, 'grad_norm': 1.234203815460205, 'learning_rate': 0.0001931544845942415, 'epoch': 0.21}
{'loss': 1.0737, 'grad_norm': 1.244493007659912, 'learning_rate': 0.00019313494353509258, 'epoch': 0.21}
{'loss': 0.8898, 'grad_norm': 1.2957688570022583, 'learning_rate': 0.00019311537561627454, 'epoch': 0.21}
{'loss': 0.8665, 'grad_norm': 1.258927822113037, 'learning_rate': 0.0001930957808434307, 'epoch': 0.21}
{'loss': 1.2274, 'grad_norm': 1.142086148262024, 'learning_rate': 0.00019307615922221203, 'epoch': 0.21}
{'loss': 0.9109, 'grad_norm': 1.7960364818572998, 'learning_rate': 0.00019305651075827735, 'epoch': 0.21}
{'loss': 0.8099, 'grad_norm': 1.261418342590332, 'learning_rate': 0.00019303683545729322, 'epoch': 0.21}
{'loss': 1.1938, 'grad_norm': 1.1340256929397583, 'learning_rate': 0.00019301713332493386, 'epoch': 0.21}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8656, 'grad_norm': 1.2684406042099, 'learning_rate': 0.00019299740436688124, 'epoch': 0.21}
{'loss': 1.0989, 'grad_norm': 1.5123722553253174, 'learning_rate': 0.00019297764858882514, 'epoch': 0.21}
{'loss': 1.2341, 'grad_norm': 1.0612460374832153, 'learning_rate': 0.00019295786599646305, 'epoch': 0.21}
{'loss': 1.0006, 'grad_norm': 1.2160035371780396, 'learning_rate': 0.00019293805659550013, 'epoch': 0.21}
{'loss': 1.001, 'grad_norm': 2.2221460342407227, 'learning_rate': 0.00019291822039164933, 'epoch': 0.21}
{'loss': 0.6259, 'grad_norm': 1.1266388893127441, 'learning_rate': 0.00019289835739063133, 'epoch': 0.21}
{'loss': 1.2882, 'grad_norm': 1.198315143585205, 'learning_rate': 0.00019287846759817451, 'epoch': 0.21}
{'loss': 1.2378, 'grad_norm': 1.0558596849441528, 'learning_rate': 0.00019285855102001498, 'epoch': 0.21}
{'loss': 1.0112, 'grad_norm': 1.416991949081421, 'learning_rate': 0.0001928386076618966, 'epoch': 0.21}
{'loss': 1.0024, 'grad_norm': 1.5469778776168823, 'learning_rate': 0.00019281863752957095, 'epoch': 0.21}
{'loss': 1.3475, 'grad_norm': 1.7951241731643677, 'learning_rate': 0.00019279864062879732, 'epoch': 0.21}
{'loss': 0.9973, 'grad_norm': 1.2692197561264038, 'learning_rate': 0.00019277861696534267, 'epoch': 0.21}
{'loss': 1.1986, 'grad_norm': 1.3665763139724731, 'learning_rate': 0.00019275856654498179, 'epoch': 0.21}
{'loss': 1.1108, 'grad_norm': 1.1713554859161377, 'learning_rate': 0.0001927384893734971, 'epoch': 0.21}
{'loss': 1.1371, 'grad_norm': 0.9761093854904175, 'learning_rate': 0.00019271838545667876, 'epoch': 0.21}
{'loss': 1.1931, 'grad_norm': 1.5060715675354004, 'learning_rate': 0.00019269825480032463, 'epoch': 0.21}
{'loss': 0.7439, 'grad_norm': 1.2302021980285645, 'learning_rate': 0.0001926780974102403, 'epoch': 0.21}
{'loss': 0.9828, 'grad_norm': 1.237130045890808, 'learning_rate': 0.00019265791329223909, 'epoch': 0.21}
{'loss': 1.0975, 'grad_norm': 1.4778386354446411, 'learning_rate': 0.00019263770245214198, 'epoch': 0.21}
{'loss': 0.9347, 'grad_norm': 1.2495723962783813, 'learning_rate': 0.00019261746489577765, 'epoch': 0.21}
{'loss': 0.9098, 'grad_norm': 1.339635968208313, 'learning_rate': 0.00019259720062898258, 'epoch': 0.21}
{'loss': 0.7474, 'grad_norm': 1.3292332887649536, 'learning_rate': 0.00019257690965760084, 'epoch': 0.21}
{'loss': 1.189, 'grad_norm': 1.284663200378418, 'learning_rate': 0.0001925565919874843, 'epoch': 0.21}
{'loss': 0.9609, 'grad_norm': 1.3588711023330688, 'learning_rate': 0.00019253624762449238, 'epoch': 0.21}
{'loss': 0.8622, 'grad_norm': 1.2643588781356812, 'learning_rate': 0.00019251587657449236, 'epoch': 0.21}
{'loss': 0.7374, 'grad_norm': 1.5875017642974854, 'learning_rate': 0.00019249547884335916, 'epoch': 0.21}
{'loss': 1.0263, 'grad_norm': 1.0659676790237427, 'learning_rate': 0.00019247505443697538, 'epoch': 0.21}
{'loss': 1.0609, 'grad_norm': 1.1081453561782837, 'learning_rate': 0.00019245460336123134, 'epoch': 0.21}
{'loss': 1.1884, 'grad_norm': 1.252335548400879, 'learning_rate': 0.00019243412562202497, 'epoch': 0.21}
{'loss': 1.2491, 'grad_norm': 1.239081859588623, 'learning_rate': 0.000192413621225262, 'epoch': 0.21}
{'loss': 1.2754, 'grad_norm': 1.2131283283233643, 'learning_rate': 0.0001923930901768558, 'epoch': 0.21}
{'loss': 1.1816, 'grad_norm': 1.16130530834198, 'learning_rate': 0.0001923725324827274, 'epoch': 0.21}
{'loss': 0.9914, 'grad_norm': 1.1003986597061157, 'learning_rate': 0.00019235194814880554, 'epoch': 0.21}
{'loss': 1.0809, 'grad_norm': 1.8745821714401245, 'learning_rate': 0.00019233133718102668, 'epoch': 0.21}
{'loss': 0.9543, 'grad_norm': 1.2240488529205322, 'learning_rate': 0.0001923106995853349, 'epoch': 0.21}
{'loss': 1.0079, 'grad_norm': 1.6157357692718506, 'learning_rate': 0.00019229003536768197, 'epoch': 0.21}
{'loss': 1.3323, 'grad_norm': 1.8113787174224854, 'learning_rate': 0.00019226934453402738, 'epoch': 0.21}
{'loss': 0.7105, 'grad_norm': 1.588972568511963, 'learning_rate': 0.00019224862709033824, 'epoch': 0.21}
{'loss': 0.8446, 'grad_norm': 0.9182182550430298, 'learning_rate': 0.00019222788304258938, 'epoch': 0.21}
{'loss': 1.2011, 'grad_norm': 1.3341646194458008, 'learning_rate': 0.00019220711239676327, 'epoch': 0.21}
{'loss': 0.9274, 'grad_norm': 1.071091890335083, 'learning_rate': 0.00019218631515885006, 'epoch': 0.21}
{'loss': 0.8568, 'grad_norm': 1.7788965702056885, 'learning_rate': 0.0001921654913348476, 'epoch': 0.21}
{'loss': 0.6759, 'grad_norm': 1.3805372714996338, 'learning_rate': 0.00019214464093076137, 'epoch': 0.21}
{'loss': 0.9887, 'grad_norm': 1.1233789920806885, 'learning_rate': 0.00019212376395260448, 'epoch': 0.21}
{'loss': 0.7633, 'grad_norm': 1.4649633169174194, 'learning_rate': 0.00019210286040639783, 'epoch': 0.21}
{'loss': 1.184, 'grad_norm': 1.344361424446106, 'learning_rate': 0.00019208193029816983, 'epoch': 0.21}
{'loss': 0.8864, 'grad_norm': 1.5644571781158447, 'learning_rate': 0.0001920609736339567, 'epoch': 0.21}
{'loss': 1.0513, 'grad_norm': 1.222674012184143, 'learning_rate': 0.00019203999041980213, 'epoch': 0.22}
{'loss': 1.0779, 'grad_norm': 1.2470167875289917, 'learning_rate': 0.0001920189806617577, 'epoch': 0.22}
{'loss': 0.998, 'grad_norm': 1.1315593719482422, 'learning_rate': 0.00019199794436588243, 'epoch': 0.22}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1007, 'grad_norm': 1.1422172784805298, 'learning_rate': 0.00019197688153824313, 'epoch': 0.22}
{'loss': 1.2059, 'grad_norm': 1.286698818206787, 'learning_rate': 0.0001919557921849142, 'epoch': 0.22}
{'loss': 0.7969, 'grad_norm': 1.0704727172851562, 'learning_rate': 0.00019193467631197777, 'epoch': 0.22}
{'loss': 0.9237, 'grad_norm': 1.1708884239196777, 'learning_rate': 0.00019191353392552344, 'epoch': 0.22}
{'loss': 0.7125, 'grad_norm': 1.226882815361023, 'learning_rate': 0.0001918923650316487, 'epoch': 0.22}
{'loss': 1.2611, 'grad_norm': 1.0745104551315308, 'learning_rate': 0.00019187116963645842, 'epoch': 0.22}
{'loss': 0.7609, 'grad_norm': 1.2635799646377563, 'learning_rate': 0.0001918499477460654, 'epoch': 0.22}
{'loss': 1.0888, 'grad_norm': 1.2645364999771118, 'learning_rate': 0.0001918286993665898, 'epoch': 0.22}
{'loss': 1.2539, 'grad_norm': 1.2711220979690552, 'learning_rate': 0.00019180742450415964, 'epoch': 0.22}
{'loss': 1.0045, 'grad_norm': 1.1542115211486816, 'learning_rate': 0.0001917861231649104, 'epoch': 0.22}
{'loss': 1.4674, 'grad_norm': 1.0684815645217896, 'learning_rate': 0.0001917647953549854, 'epoch': 0.22}
{'loss': 0.8844, 'grad_norm': 1.09617018699646, 'learning_rate': 0.00019174344108053537, 'epoch': 0.22}
{'loss': 0.7454, 'grad_norm': 1.287771224975586, 'learning_rate': 0.0001917220603477188, 'epoch': 0.22}
{'loss': 0.9543, 'grad_norm': 0.9827642440795898, 'learning_rate': 0.00019170065316270187, 'epoch': 0.22}
{'loss': 1.1245, 'grad_norm': 1.1420639753341675, 'learning_rate': 0.00019167921953165825, 'epoch': 0.22}
{'loss': 0.9562, 'grad_norm': 1.1294323205947876, 'learning_rate': 0.00019165775946076928, 'epoch': 0.22}
{'loss': 1.136, 'grad_norm': 1.844887375831604, 'learning_rate': 0.00019163627295622397, 'epoch': 0.22}
{'loss': 0.7471, 'grad_norm': 1.672685146331787, 'learning_rate': 0.0001916147600242189, 'epoch': 0.22}
{'loss': 0.9725, 'grad_norm': 1.3028554916381836, 'learning_rate': 0.00019159322067095833, 'epoch': 0.22}
{'loss': 0.9455, 'grad_norm': 1.1204355955123901, 'learning_rate': 0.00019157165490265408, 'epoch': 0.22}
{'loss': 1.1893, 'grad_norm': 1.8684887886047363, 'learning_rate': 0.00019155006272552561, 'epoch': 0.22}
{'loss': 0.8754, 'grad_norm': 2.4733943939208984, 'learning_rate': 0.00019152844414580003, 'epoch': 0.22}
{'loss': 0.801, 'grad_norm': 1.3904438018798828, 'learning_rate': 0.000191506799169712, 'epoch': 0.22}
{'loss': 1.0778, 'grad_norm': 1.1334115266799927, 'learning_rate': 0.00019148512780350384, 'epoch': 0.22}
{'loss': 1.1832, 'grad_norm': 1.5711134672164917, 'learning_rate': 0.00019146343005342547, 'epoch': 0.22}
{'loss': 0.785, 'grad_norm': 1.4140816926956177, 'learning_rate': 0.00019144170592573444, 'epoch': 0.22}
{'loss': 1.112, 'grad_norm': 1.2844319343566895, 'learning_rate': 0.0001914199554266958, 'epoch': 0.22}
{'loss': 0.711, 'grad_norm': 1.3847533464431763, 'learning_rate': 0.0001913981785625824, 'epoch': 0.22}
{'loss': 0.9063, 'grad_norm': 1.5443651676177979, 'learning_rate': 0.0001913763753396745, 'epoch': 0.22}
{'loss': 1.1798, 'grad_norm': 1.0597889423370361, 'learning_rate': 0.0001913545457642601, 'epoch': 0.22}
{'loss': 1.1958, 'grad_norm': 1.3094711303710938, 'learning_rate': 0.00019133268984263473, 'epoch': 0.22}
{'loss': 1.1634, 'grad_norm': 0.9884665608406067, 'learning_rate': 0.00019131080758110148, 'epoch': 0.22}
{'loss': 0.9524, 'grad_norm': 1.3625057935714722, 'learning_rate': 0.00019128889898597116, 'epoch': 0.22}
{'loss': 1.1393, 'grad_norm': 1.036407232284546, 'learning_rate': 0.00019126696406356205, 'epoch': 0.22}
{'loss': 1.118, 'grad_norm': 1.4404114484786987, 'learning_rate': 0.00019124500282020014, 'epoch': 0.22}
{'loss': 1.0967, 'grad_norm': 1.1866508722305298, 'learning_rate': 0.0001912230152622189, 'epoch': 0.22}
{'loss': 1.0589, 'grad_norm': 1.0886152982711792, 'learning_rate': 0.0001912010013959594, 'epoch': 0.22}
{'loss': 1.0841, 'grad_norm': 1.2449661493301392, 'learning_rate': 0.00019117896122777044, 'epoch': 0.22}
{'loss': 1.3518, 'grad_norm': 1.1968014240264893, 'learning_rate': 0.00019115689476400816, 'epoch': 0.22}
{'loss': 0.8066, 'grad_norm': 1.1333287954330444, 'learning_rate': 0.00019113480201103658, 'epoch': 0.22}
{'loss': 1.0697, 'grad_norm': 1.0245003700256348, 'learning_rate': 0.000191112682975227, 'epoch': 0.22}
{'loss': 1.0294, 'grad_norm': 1.1374155282974243, 'learning_rate': 0.0001910905376629585, 'epoch': 0.22}
{'loss': 0.9511, 'grad_norm': 1.2987022399902344, 'learning_rate': 0.00019106836608061772, 'epoch': 0.22}
{'loss': 1.0739, 'grad_norm': 1.721836805343628, 'learning_rate': 0.0001910461682345988, 'epoch': 0.22}
{'loss': 0.7864, 'grad_norm': 1.0282493829727173, 'learning_rate': 0.00019102394413130346, 'epoch': 0.22}
{'loss': 0.9967, 'grad_norm': 1.3851313591003418, 'learning_rate': 0.0001910016937771411, 'epoch': 0.22}
{'loss': 1.1275, 'grad_norm': 1.54634690284729, 'learning_rate': 0.00019097941717852854, 'epoch': 0.22}
{'loss': 1.0197, 'grad_norm': 1.2106531858444214, 'learning_rate': 0.0001909571143418903, 'epoch': 0.22}
{'loss': 0.9264, 'grad_norm': 1.2715483903884888, 'learning_rate': 0.00019093478527365838, 'epoch': 0.22}
{'loss': 1.0979, 'grad_norm': 1.3472881317138672, 'learning_rate': 0.0001909124299802724, 'epoch': 0.22}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1467, 'grad_norm': 1.3499120473861694, 'learning_rate': 0.00019089004846817946, 'epoch': 0.22}
{'loss': 1.0041, 'grad_norm': 1.368805170059204, 'learning_rate': 0.00019086764074383435, 'epoch': 0.22}
{'loss': 0.9824, 'grad_norm': 1.333094596862793, 'learning_rate': 0.0001908452068136993, 'epoch': 0.22}
{'loss': 0.786, 'grad_norm': 1.0752086639404297, 'learning_rate': 0.00019082274668424422, 'epoch': 0.22}
{'loss': 0.8462, 'grad_norm': 1.4476836919784546, 'learning_rate': 0.00019080026036194642, 'epoch': 0.22}
{'loss': 1.2883, 'grad_norm': 1.255099892616272, 'learning_rate': 0.00019077774785329087, 'epoch': 0.22}
{'loss': 0.9516, 'grad_norm': 1.1423206329345703, 'learning_rate': 0.0001907552091647701, 'epoch': 0.22}
{'loss': 1.3217, 'grad_norm': 1.207245111465454, 'learning_rate': 0.00019073264430288415, 'epoch': 0.22}
{'loss': 0.7171, 'grad_norm': 1.4141932725906372, 'learning_rate': 0.00019071005327414055, 'epoch': 0.22}
{'loss': 0.7423, 'grad_norm': 1.7483161687850952, 'learning_rate': 0.00019068743608505455, 'epoch': 0.22}
{'loss': 1.1276, 'grad_norm': 1.381138801574707, 'learning_rate': 0.00019066479274214876, 'epoch': 0.22}
{'loss': 0.9931, 'grad_norm': 1.0227093696594238, 'learning_rate': 0.00019064212325195347, 'epoch': 0.22}
{'loss': 1.1463, 'grad_norm': 1.3848539590835571, 'learning_rate': 0.0001906194276210064, 'epoch': 0.23}
{'loss': 0.9008, 'grad_norm': 1.1739600896835327, 'learning_rate': 0.0001905967058558529, 'epoch': 0.23}
{'loss': 0.7795, 'grad_norm': 1.2227967977523804, 'learning_rate': 0.00019057395796304578, 'epoch': 0.23}
{'loss': 0.9724, 'grad_norm': 1.1157712936401367, 'learning_rate': 0.00019055118394914545, 'epoch': 0.23}
{'loss': 0.9122, 'grad_norm': 1.417648434638977, 'learning_rate': 0.0001905283838207198, 'epoch': 0.23}
{'loss': 0.8262, 'grad_norm': 1.1764436960220337, 'learning_rate': 0.00019050555758434433, 'epoch': 0.23}
{'loss': 1.1015, 'grad_norm': 1.7616069316864014, 'learning_rate': 0.00019048270524660196, 'epoch': 0.23}
{'loss': 0.9956, 'grad_norm': 1.7919138669967651, 'learning_rate': 0.00019045982681408324, 'epoch': 0.23}
{'loss': 1.0005, 'grad_norm': 1.0822958946228027, 'learning_rate': 0.00019043692229338613, 'epoch': 0.23}
{'loss': 0.9914, 'grad_norm': 0.9098637700080872, 'learning_rate': 0.00019041399169111625, 'epoch': 0.23}
{'loss': 0.9605, 'grad_norm': 1.197454571723938, 'learning_rate': 0.00019039103501388666, 'epoch': 0.23}
{'loss': 0.9106, 'grad_norm': 1.2041456699371338, 'learning_rate': 0.00019036805226831796, 'epoch': 0.23}
{'loss': 1.1268, 'grad_norm': 1.1822632551193237, 'learning_rate': 0.00019034504346103823, 'epoch': 0.23}
{'loss': 1.0608, 'grad_norm': 1.593031406402588, 'learning_rate': 0.00019032200859868314, 'epoch': 0.23}
{'loss': 1.0021, 'grad_norm': 1.279775857925415, 'learning_rate': 0.0001902989476878958, 'epoch': 0.23}
{'loss': 1.2637, 'grad_norm': 1.1512001752853394, 'learning_rate': 0.0001902758607353269, 'epoch': 0.23}
{'loss': 0.859, 'grad_norm': 1.4688398838043213, 'learning_rate': 0.00019025274774763458, 'epoch': 0.23}
{'loss': 0.9174, 'grad_norm': 1.160731315612793, 'learning_rate': 0.0001902296087314845, 'epoch': 0.23}
{'loss': 1.061, 'grad_norm': 1.4757736921310425, 'learning_rate': 0.00019020644369354987, 'epoch': 0.23}
{'loss': 1.0518, 'grad_norm': 1.2659714221954346, 'learning_rate': 0.0001901832526405114, 'epoch': 0.23}
{'loss': 0.8516, 'grad_norm': 1.1453180313110352, 'learning_rate': 0.0001901600355790572, 'epoch': 0.23}
{'loss': 0.8164, 'grad_norm': 1.6172832250595093, 'learning_rate': 0.00019013679251588303, 'epoch': 0.23}
{'loss': 1.1581, 'grad_norm': 1.032461404800415, 'learning_rate': 0.00019011352345769204, 'epoch': 0.23}
{'loss': 0.7671, 'grad_norm': 1.4378386735916138, 'learning_rate': 0.00019009022841119495, 'epoch': 0.23}
{'loss': 1.0723, 'grad_norm': 1.4411613941192627, 'learning_rate': 0.00019006690738310989, 'epoch': 0.23}
{'loss': 0.9407, 'grad_norm': 0.9782558679580688, 'learning_rate': 0.00019004356038016256, 'epoch': 0.23}
{'loss': 1.0664, 'grad_norm': 1.3239372968673706, 'learning_rate': 0.0001900201874090861, 'epoch': 0.23}
{'loss': 0.9472, 'grad_norm': 1.7368643283843994, 'learning_rate': 0.0001899967884766212, 'epoch': 0.23}
{'loss': 1.0275, 'grad_norm': 1.6404517889022827, 'learning_rate': 0.00018997336358951602, 'epoch': 0.23}
{'loss': 0.9414, 'grad_norm': 1.5181422233581543, 'learning_rate': 0.00018994991275452612, 'epoch': 0.23}
{'loss': 1.1506, 'grad_norm': 1.1511200666427612, 'learning_rate': 0.00018992643597841462, 'epoch': 0.23}
{'loss': 1.1942, 'grad_norm': 1.6226164102554321, 'learning_rate': 0.0001899029332679521, 'epoch': 0.23}
{'loss': 1.12, 'grad_norm': 1.080323576927185, 'learning_rate': 0.0001898794046299167, 'epoch': 0.23}
{'loss': 1.1689, 'grad_norm': 1.431417465209961, 'learning_rate': 0.0001898558500710939, 'epoch': 0.23}
{'loss': 1.1062, 'grad_norm': 1.270867109298706, 'learning_rate': 0.00018983226959827675, 'epoch': 0.23}
{'loss': 1.2209, 'grad_norm': 1.214011549949646, 'learning_rate': 0.0001898086632182657, 'epoch': 0.23}
{'loss': 0.9766, 'grad_norm': 1.0805342197418213, 'learning_rate': 0.00018978503093786882, 'epoch': 0.23}
{'loss': 1.0014, 'grad_norm': 1.0911394357681274, 'learning_rate': 0.0001897613727639014, 'epoch': 0.23}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.191, 'grad_norm': 1.4656343460083008, 'learning_rate': 0.00018973768870318646, 'epoch': 0.23}
{'loss': 1.0659, 'grad_norm': 1.6871960163116455, 'learning_rate': 0.0001897139787625543, 'epoch': 0.23}
{'loss': 0.7345, 'grad_norm': 1.1120620965957642, 'learning_rate': 0.00018969024294884283, 'epoch': 0.23}
{'loss': 1.1368, 'grad_norm': 1.4421088695526123, 'learning_rate': 0.00018966648126889725, 'epoch': 0.23}
{'loss': 1.3853, 'grad_norm': 1.041695475578308, 'learning_rate': 0.00018964269372957038, 'epoch': 0.23}
{'loss': 0.9989, 'grad_norm': 1.4262887239456177, 'learning_rate': 0.0001896188803377224, 'epoch': 0.23}
{'loss': 1.1429, 'grad_norm': 1.3787057399749756, 'learning_rate': 0.00018959504110022102, 'epoch': 0.23}
{'loss': 1.0979, 'grad_norm': 1.509636640548706, 'learning_rate': 0.0001895711760239413, 'epoch': 0.23}
{'loss': 1.0353, 'grad_norm': 1.30776047706604, 'learning_rate': 0.00018954728511576586, 'epoch': 0.23}
{'loss': 0.7389, 'grad_norm': 1.6400624513626099, 'learning_rate': 0.0001895233683825847, 'epoch': 0.23}
{'loss': 0.9773, 'grad_norm': 1.3451472520828247, 'learning_rate': 0.00018949942583129528, 'epoch': 0.23}
{'loss': 0.9968, 'grad_norm': 1.0469754934310913, 'learning_rate': 0.00018947545746880254, 'epoch': 0.23}
{'loss': 1.0033, 'grad_norm': 1.2765296697616577, 'learning_rate': 0.00018945146330201883, 'epoch': 0.23}
{'loss': 1.065, 'grad_norm': 1.1704998016357422, 'learning_rate': 0.00018942744333786397, 'epoch': 0.23}
{'loss': 1.1676, 'grad_norm': 1.2053951025009155, 'learning_rate': 0.0001894033975832652, 'epoch': 0.23}
{'loss': 0.8937, 'grad_norm': 1.1708977222442627, 'learning_rate': 0.00018937932604515714, 'epoch': 0.23}
{'loss': 1.0498, 'grad_norm': 1.3682029247283936, 'learning_rate': 0.000189355228730482, 'epoch': 0.23}
{'loss': 1.1808, 'grad_norm': 1.1219149827957153, 'learning_rate': 0.00018933110564618927, 'epoch': 0.23}
{'loss': 1.0193, 'grad_norm': 1.1821081638336182, 'learning_rate': 0.00018930695679923593, 'epoch': 0.23}
{'loss': 1.2157, 'grad_norm': 1.3990730047225952, 'learning_rate': 0.00018928278219658643, 'epoch': 0.23}
{'loss': 0.8836, 'grad_norm': 1.1759005784988403, 'learning_rate': 0.00018925858184521256, 'epoch': 0.23}
{'loss': 1.1395, 'grad_norm': 1.0663589239120483, 'learning_rate': 0.0001892343557520936, 'epoch': 0.23}
{'loss': 1.0208, 'grad_norm': 1.4296382665634155, 'learning_rate': 0.00018921010392421628, 'epoch': 0.23}
{'loss': 0.9981, 'grad_norm': 1.1447267532348633, 'learning_rate': 0.00018918582636857467, 'epoch': 0.23}
{'loss': 1.1601, 'grad_norm': 1.118078351020813, 'learning_rate': 0.0001891615230921703, 'epoch': 0.23}
{'loss': 0.801, 'grad_norm': 1.0415778160095215, 'learning_rate': 0.0001891371941020121, 'epoch': 0.23}
{'loss': 0.902, 'grad_norm': 1.159741997718811, 'learning_rate': 0.0001891128394051165, 'epoch': 0.23}
{'loss': 0.8878, 'grad_norm': 1.0587952136993408, 'learning_rate': 0.00018908845900850722, 'epoch': 0.24}
{'loss': 1.3128, 'grad_norm': 1.002563238143921, 'learning_rate': 0.00018906405291921547, 'epoch': 0.24}
{'loss': 1.0959, 'grad_norm': 1.4594271183013916, 'learning_rate': 0.00018903962114427984, 'epoch': 0.24}
{'loss': 1.0797, 'grad_norm': 1.37224280834198, 'learning_rate': 0.00018901516369074635, 'epoch': 0.24}
{'loss': 1.0189, 'grad_norm': 1.0058472156524658, 'learning_rate': 0.0001889906805656684, 'epoch': 0.24}
{'loss': 1.1981, 'grad_norm': 1.0581220388412476, 'learning_rate': 0.0001889661717761068, 'epoch': 0.24}
{'loss': 0.9881, 'grad_norm': 2.225121021270752, 'learning_rate': 0.00018894163732912977, 'epoch': 0.24}
{'loss': 0.9833, 'grad_norm': 1.0531002283096313, 'learning_rate': 0.00018891707723181294, 'epoch': 0.24}
{'loss': 0.8194, 'grad_norm': 1.2964651584625244, 'learning_rate': 0.00018889249149123927, 'epoch': 0.24}
{'loss': 1.0605, 'grad_norm': 1.1111634969711304, 'learning_rate': 0.00018886788011449927, 'epoch': 0.24}
{'loss': 0.9398, 'grad_norm': 1.6793855428695679, 'learning_rate': 0.00018884324310869068, 'epoch': 0.24}
{'loss': 1.0274, 'grad_norm': 1.4024678468704224, 'learning_rate': 0.0001888185804809187, 'epoch': 0.24}
{'loss': 0.9512, 'grad_norm': 1.1010065078735352, 'learning_rate': 0.00018879389223829592, 'epoch': 0.24}
{'loss': 0.9276, 'grad_norm': 1.4690967798233032, 'learning_rate': 0.00018876917838794226, 'epoch': 0.24}
{'loss': 1.1454, 'grad_norm': 1.0448544025421143, 'learning_rate': 0.00018874443893698514, 'epoch': 0.24}
{'loss': 0.8597, 'grad_norm': 1.201615810394287, 'learning_rate': 0.00018871967389255928, 'epoch': 0.24}
{'loss': 1.141, 'grad_norm': 1.1570510864257812, 'learning_rate': 0.00018869488326180679, 'epoch': 0.24}
{'loss': 0.9736, 'grad_norm': 1.0286071300506592, 'learning_rate': 0.0001886700670518772, 'epoch': 0.24}
{'loss': 1.3032, 'grad_norm': 1.678180456161499, 'learning_rate': 0.00018864522526992734, 'epoch': 0.24}
{'loss': 0.8553, 'grad_norm': 1.388262152671814, 'learning_rate': 0.00018862035792312147, 'epoch': 0.24}
{'loss': 0.9766, 'grad_norm': 1.1697380542755127, 'learning_rate': 0.00018859546501863125, 'epoch': 0.24}
{'loss': 0.921, 'grad_norm': 1.212471604347229, 'learning_rate': 0.00018857054656363565, 'epoch': 0.24}
{'loss': 0.9717, 'grad_norm': 1.1175501346588135, 'learning_rate': 0.000188545602565321, 'epoch': 0.24}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8513, 'grad_norm': 1.5811609029769897, 'learning_rate': 0.00018852063303088107, 'epoch': 0.24}
{'loss': 1.0627, 'grad_norm': 1.508521318435669, 'learning_rate': 0.00018849563796751695, 'epoch': 0.24}
{'loss': 0.8476, 'grad_norm': 1.1700143814086914, 'learning_rate': 0.0001884706173824371, 'epoch': 0.24}
{'loss': 0.906, 'grad_norm': 1.2374941110610962, 'learning_rate': 0.00018844557128285729, 'epoch': 0.24}
{'loss': 0.7156, 'grad_norm': 1.2980564832687378, 'learning_rate': 0.00018842049967600076, 'epoch': 0.24}
{'loss': 0.998, 'grad_norm': 1.3667296171188354, 'learning_rate': 0.00018839540256909799, 'epoch': 0.24}
{'loss': 0.9971, 'grad_norm': 1.2114698886871338, 'learning_rate': 0.0001883702799693869, 'epoch': 0.24}
{'loss': 0.9161, 'grad_norm': 1.111538052558899, 'learning_rate': 0.00018834513188411268, 'epoch': 0.24}
{'loss': 0.9285, 'grad_norm': 1.0822895765304565, 'learning_rate': 0.000188319958320528, 'epoch': 0.24}
{'loss': 0.9457, 'grad_norm': 2.041996479034424, 'learning_rate': 0.00018829475928589271, 'epoch': 0.24}
{'loss': 1.0707, 'grad_norm': 1.8848625421524048, 'learning_rate': 0.00018826953478747412, 'epoch': 0.24}
{'loss': 1.0264, 'grad_norm': 1.4087778329849243, 'learning_rate': 0.00018824428483254685, 'epoch': 0.24}
{'loss': 1.0203, 'grad_norm': 1.6780128479003906, 'learning_rate': 0.0001882190094283929, 'epoch': 0.24}
{'loss': 0.6435, 'grad_norm': 1.206183910369873, 'learning_rate': 0.00018819370858230152, 'epoch': 0.24}
{'loss': 1.2136, 'grad_norm': 1.256549596786499, 'learning_rate': 0.00018816838230156942, 'epoch': 0.24}
{'loss': 1.0623, 'grad_norm': 1.7103910446166992, 'learning_rate': 0.0001881430305935005, 'epoch': 0.24}
{'loss': 0.869, 'grad_norm': 1.3251574039459229, 'learning_rate': 0.0001881176534654061, 'epoch': 0.24}
{'loss': 1.0797, 'grad_norm': 1.5070602893829346, 'learning_rate': 0.00018809225092460488, 'epoch': 0.24}
{'loss': 1.2109, 'grad_norm': 1.0049326419830322, 'learning_rate': 0.0001880668229784228, 'epoch': 0.24}
{'loss': 1.0861, 'grad_norm': 1.2414144277572632, 'learning_rate': 0.00018804136963419316, 'epoch': 0.24}
{'loss': 0.7434, 'grad_norm': 1.1475080251693726, 'learning_rate': 0.00018801589089925656, 'epoch': 0.24}
{'loss': 1.4115, 'grad_norm': 1.6423842906951904, 'learning_rate': 0.00018799038678096096, 'epoch': 0.24}
{'loss': 1.0281, 'grad_norm': 1.319246530532837, 'learning_rate': 0.00018796485728666165, 'epoch': 0.24}
{'loss': 1.0376, 'grad_norm': 4.557610034942627, 'learning_rate': 0.0001879393024237212, 'epoch': 0.24}
{'loss': 1.2213, 'grad_norm': 1.3656755685806274, 'learning_rate': 0.00018791372219950948, 'epoch': 0.24}
{'loss': 1.0045, 'grad_norm': 1.1548750400543213, 'learning_rate': 0.0001878881166214037, 'epoch': 0.24}
{'loss': 1.0945, 'grad_norm': 1.2702661752700806, 'learning_rate': 0.00018786248569678846, 'epoch': 0.24}
{'loss': 1.3028, 'grad_norm': 1.1842687129974365, 'learning_rate': 0.0001878368294330555, 'epoch': 0.24}
{'loss': 1.0765, 'grad_norm': 1.149559497833252, 'learning_rate': 0.00018781114783760402, 'epoch': 0.24}
{'loss': 0.9665, 'grad_norm': 1.1140785217285156, 'learning_rate': 0.00018778544091784048, 'epoch': 0.24}
{'loss': 1.1029, 'grad_norm': 1.0408467054367065, 'learning_rate': 0.00018775970868117856, 'epoch': 0.24}
{'loss': 0.737, 'grad_norm': 1.1932603120803833, 'learning_rate': 0.00018773395113503937, 'epoch': 0.24}
{'loss': 1.0103, 'grad_norm': 1.426366925239563, 'learning_rate': 0.0001877081682868513, 'epoch': 0.24}
{'loss': 0.8523, 'grad_norm': 1.3584717512130737, 'learning_rate': 0.00018768236014404991, 'epoch': 0.24}
{'loss': 1.078, 'grad_norm': 1.300368070602417, 'learning_rate': 0.0001876565267140782, 'epoch': 0.24}
{'loss': 1.1818, 'grad_norm': 1.7969995737075806, 'learning_rate': 0.00018763066800438636, 'epoch': 0.24}
{'loss': 1.3031, 'grad_norm': 1.0879265069961548, 'learning_rate': 0.00018760478402243198, 'epoch': 0.24}
{'loss': 1.2495, 'grad_norm': 1.6809070110321045, 'learning_rate': 0.00018757887477567983, 'epoch': 0.24}
{'loss': 1.0951, 'grad_norm': 1.378016710281372, 'learning_rate': 0.00018755294027160204, 'epoch': 0.24}
{'loss': 0.954, 'grad_norm': 1.5289782285690308, 'learning_rate': 0.000187526980517678, 'epoch': 0.24}
{'loss': 1.0605, 'grad_norm': 1.4351524114608765, 'learning_rate': 0.00018750099552139433, 'epoch': 0.24}
{'loss': 1.0551, 'grad_norm': 1.5299850702285767, 'learning_rate': 0.000187474985290245, 'epoch': 0.24}
{'loss': 0.8957, 'grad_norm': 1.4610810279846191, 'learning_rate': 0.00018744894983173128, 'epoch': 0.25}
{'loss': 1.1683, 'grad_norm': 1.1503243446350098, 'learning_rate': 0.00018742288915336164, 'epoch': 0.25}
{'loss': 1.1838, 'grad_norm': 1.278205156326294, 'learning_rate': 0.0001873968032626518, 'epoch': 0.25}
{'loss': 0.9327, 'grad_norm': 1.274885654449463, 'learning_rate': 0.00018737069216712488, 'epoch': 0.25}
{'loss': 1.087, 'grad_norm': 2.1272239685058594, 'learning_rate': 0.00018734455587431117, 'epoch': 0.25}
{'loss': 1.1987, 'grad_norm': 1.6739404201507568, 'learning_rate': 0.00018731839439174825, 'epoch': 0.25}
{'loss': 0.7917, 'grad_norm': 1.5431288480758667, 'learning_rate': 0.00018729220772698097, 'epoch': 0.25}
{'loss': 1.371, 'grad_norm': 1.1351265907287598, 'learning_rate': 0.00018726599588756145, 'epoch': 0.25}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9229, 'grad_norm': 1.5260090827941895, 'learning_rate': 0.000187239758881049, 'epoch': 0.25}
{'loss': 0.976, 'grad_norm': 1.4704790115356445, 'learning_rate': 0.0001872134967150103, 'epoch': 0.25}
{'loss': 0.9787, 'grad_norm': 1.8645561933517456, 'learning_rate': 0.00018718720939701924, 'epoch': 0.25}
{'loss': 1.124, 'grad_norm': 1.209236741065979, 'learning_rate': 0.00018716089693465696, 'epoch': 0.25}
{'loss': 1.2778, 'grad_norm': 1.3314666748046875, 'learning_rate': 0.00018713455933551176, 'epoch': 0.25}
{'loss': 1.0993, 'grad_norm': 1.1245423555374146, 'learning_rate': 0.00018710819660717936, 'epoch': 0.25}
{'loss': 1.075, 'grad_norm': 1.9390333890914917, 'learning_rate': 0.00018708180875726265, 'epoch': 0.25}
{'loss': 0.8696, 'grad_norm': 1.2340558767318726, 'learning_rate': 0.0001870553957933717, 'epoch': 0.25}
{'loss': 1.0295, 'grad_norm': 1.3620728254318237, 'learning_rate': 0.00018702895772312395, 'epoch': 0.25}
{'loss': 1.3251, 'grad_norm': 1.2852896451950073, 'learning_rate': 0.00018700249455414394, 'epoch': 0.25}
{'loss': 1.2404, 'grad_norm': 1.7977184057235718, 'learning_rate': 0.00018697600629406357, 'epoch': 0.25}
{'loss': 0.9992, 'grad_norm': 1.6719777584075928, 'learning_rate': 0.0001869494929505219, 'epoch': 0.25}
{'loss': 0.7764, 'grad_norm': 1.1212635040283203, 'learning_rate': 0.0001869229545311653, 'epoch': 0.25}
{'loss': 1.0267, 'grad_norm': 1.3997009992599487, 'learning_rate': 0.00018689639104364723, 'epoch': 0.25}
{'loss': 1.2397, 'grad_norm': 0.9876320958137512, 'learning_rate': 0.00018686980249562859, 'epoch': 0.25}
{'loss': 1.184, 'grad_norm': 1.3263334035873413, 'learning_rate': 0.0001868431888947773, 'epoch': 0.25}
{'loss': 0.924, 'grad_norm': 1.3446464538574219, 'learning_rate': 0.00018681655024876864, 'epoch': 0.25}
{'loss': 0.877, 'grad_norm': 1.2144291400909424, 'learning_rate': 0.00018678988656528503, 'epoch': 0.25}
{'loss': 0.8921, 'grad_norm': 1.175261378288269, 'learning_rate': 0.00018676319785201616, 'epoch': 0.25}
{'loss': 1.0151, 'grad_norm': 1.3094128370285034, 'learning_rate': 0.00018673648411665893, 'epoch': 0.25}
{'loss': 1.0469, 'grad_norm': 1.1718624830245972, 'learning_rate': 0.00018670974536691746, 'epoch': 0.25}
{'loss': 1.2852, 'grad_norm': 1.5256128311157227, 'learning_rate': 0.00018668298161050309, 'epoch': 0.25}
{'loss': 1.1448, 'grad_norm': 1.2029080390930176, 'learning_rate': 0.00018665619285513434, 'epoch': 0.25}
{'loss': 1.069, 'grad_norm': 0.971663236618042, 'learning_rate': 0.00018662937910853694, 'epoch': 0.25}
{'loss': 1.0865, 'grad_norm': 1.2718355655670166, 'learning_rate': 0.00018660254037844388, 'epoch': 0.25}
{'loss': 0.9772, 'grad_norm': 1.1077462434768677, 'learning_rate': 0.00018657567667259528, 'epoch': 0.25}
{'loss': 0.8859, 'grad_norm': 1.030003547668457, 'learning_rate': 0.00018654878799873856, 'epoch': 0.25}
{'loss': 0.985, 'grad_norm': 1.2634249925613403, 'learning_rate': 0.00018652187436462824, 'epoch': 0.25}
{'loss': 1.3217, 'grad_norm': 1.3212404251098633, 'learning_rate': 0.00018649493577802608, 'epoch': 0.25}
{'loss': 1.3134, 'grad_norm': 1.0728293657302856, 'learning_rate': 0.0001864679722467011, 'epoch': 0.25}
{'loss': 1.1656, 'grad_norm': 1.2260470390319824, 'learning_rate': 0.00018644098377842934, 'epoch': 0.25}
{'loss': 1.1772, 'grad_norm': 1.077301263809204, 'learning_rate': 0.00018641397038099427, 'epoch': 0.25}
{'loss': 1.1517, 'grad_norm': 0.8898483514785767, 'learning_rate': 0.0001863869320621863, 'epoch': 0.25}
{'loss': 0.7644, 'grad_norm': 1.2916111946105957, 'learning_rate': 0.00018635986882980325, 'epoch': 0.25}
{'loss': 1.2832, 'grad_norm': 1.1363139152526855, 'learning_rate': 0.00018633278069165, 'epoch': 0.25}
{'loss': 0.7565, 'grad_norm': 1.014695644378662, 'learning_rate': 0.00018630566765553863, 'epoch': 0.25}
{'loss': 1.0875, 'grad_norm': 1.0846344232559204, 'learning_rate': 0.00018627852972928838, 'epoch': 0.25}
{'loss': 0.9491, 'grad_norm': 1.194569706916809, 'learning_rate': 0.00018625136692072575, 'epoch': 0.25}
{'loss': 0.754, 'grad_norm': 1.4789761304855347, 'learning_rate': 0.00018622417923768434, 'epoch': 0.25}
{'loss': 1.2835, 'grad_norm': 1.5550676584243774, 'learning_rate': 0.00018619696668800492, 'epoch': 0.25}
{'loss': 1.1296, 'grad_norm': 1.2906553745269775, 'learning_rate': 0.00018616972927953552, 'epoch': 0.25}
{'loss': 1.2124, 'grad_norm': 1.3127737045288086, 'learning_rate': 0.00018614246702013123, 'epoch': 0.25}
{'loss': 1.0721, 'grad_norm': 1.5614162683486938, 'learning_rate': 0.00018611517991765433, 'epoch': 0.25}
{'loss': 1.0405, 'grad_norm': 1.3076014518737793, 'learning_rate': 0.00018608786797997436, 'epoch': 0.25}
{'loss': 0.7612, 'grad_norm': 1.2486605644226074, 'learning_rate': 0.00018606053121496792, 'epoch': 0.25}
{'loss': 0.8452, 'grad_norm': 1.0440889596939087, 'learning_rate': 0.00018603316963051878, 'epoch': 0.25}
{'loss': 0.6663, 'grad_norm': 1.307187557220459, 'learning_rate': 0.0001860057832345179, 'epoch': 0.25}
{'loss': 1.1141, 'grad_norm': 1.3745265007019043, 'learning_rate': 0.0001859783720348634, 'epoch': 0.25}
{'loss': 1.1967, 'grad_norm': 1.5290149450302124, 'learning_rate': 0.00018595093603946053, 'epoch': 0.25}
{'loss': 0.9392, 'grad_norm': 1.3958626985549927, 'learning_rate': 0.0001859234752562217, 'epoch': 0.25}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1784, 'grad_norm': 1.0016111135482788, 'learning_rate': 0.00018589598969306645, 'epoch': 0.25}
{'loss': 0.6985, 'grad_norm': 1.0573664903640747, 'learning_rate': 0.0001858684793579215, 'epoch': 0.25}
{'loss': 1.0956, 'grad_norm': 1.2137688398361206, 'learning_rate': 0.00018584094425872072, 'epoch': 0.25}
{'loss': 1.0014, 'grad_norm': 1.193198561668396, 'learning_rate': 0.00018581338440340508, 'epoch': 0.25}
{'loss': 1.0207, 'grad_norm': 1.339587926864624, 'learning_rate': 0.00018578579979992266, 'epoch': 0.25}
{'loss': 0.7819, 'grad_norm': 1.3504325151443481, 'learning_rate': 0.0001857581904562288, 'epoch': 0.25}
{'loss': 0.9685, 'grad_norm': 1.1621277332305908, 'learning_rate': 0.0001857305563802859, 'epoch': 0.25}
{'loss': 0.8135, 'grad_norm': 1.185244083404541, 'learning_rate': 0.00018570289758006346, 'epoch': 0.26}
{'loss': 0.8083, 'grad_norm': 1.4401582479476929, 'learning_rate': 0.00018567521406353814, 'epoch': 0.26}
{'loss': 0.976, 'grad_norm': 1.3814191818237305, 'learning_rate': 0.00018564750583869373, 'epoch': 0.26}
{'loss': 0.606, 'grad_norm': 1.358444094657898, 'learning_rate': 0.0001856197729135212, 'epoch': 0.26}
{'loss': 1.1703, 'grad_norm': 1.7552026510238647, 'learning_rate': 0.00018559201529601854, 'epoch': 0.26}
{'loss': 1.164, 'grad_norm': 1.413787603378296, 'learning_rate': 0.00018556423299419096, 'epoch': 0.26}
{'loss': 0.9671, 'grad_norm': 1.4315015077590942, 'learning_rate': 0.00018553642601605068, 'epoch': 0.26}
{'loss': 0.88, 'grad_norm': 1.7190321683883667, 'learning_rate': 0.00018550859436961714, 'epoch': 0.26}
{'loss': 1.1124, 'grad_norm': 1.5031245946884155, 'learning_rate': 0.00018548073806291685, 'epoch': 0.26}
{'loss': 1.104, 'grad_norm': 1.4690163135528564, 'learning_rate': 0.00018545285710398342, 'epoch': 0.26}
{'loss': 0.9322, 'grad_norm': 1.468213677406311, 'learning_rate': 0.0001854249515008576, 'epoch': 0.26}
{'loss': 0.5672, 'grad_norm': 1.267777919769287, 'learning_rate': 0.00018539702126158723, 'epoch': 0.26}
{'loss': 0.7714, 'grad_norm': 1.2630947828292847, 'learning_rate': 0.00018536906639422725, 'epoch': 0.26}
{'loss': 1.2663, 'grad_norm': 1.4583287239074707, 'learning_rate': 0.0001853410869068397, 'epoch': 0.26}
{'loss': 0.923, 'grad_norm': 1.2700388431549072, 'learning_rate': 0.00018531308280749374, 'epoch': 0.26}
{'loss': 0.9885, 'grad_norm': 0.9762224555015564, 'learning_rate': 0.00018528505410426562, 'epoch': 0.26}
{'loss': 1.2341, 'grad_norm': 2.4024136066436768, 'learning_rate': 0.0001852570008052387, 'epoch': 0.26}
{'loss': 1.0458, 'grad_norm': 1.2748819589614868, 'learning_rate': 0.00018522892291850335, 'epoch': 0.26}
{'loss': 1.0547, 'grad_norm': 1.018755316734314, 'learning_rate': 0.0001852008204521572, 'epoch': 0.26}
{'loss': 1.1434, 'grad_norm': 1.5908266305923462, 'learning_rate': 0.00018517269341430476, 'epoch': 0.26}
{'loss': 1.0654, 'grad_norm': 1.3110015392303467, 'learning_rate': 0.0001851445418130578, 'epoch': 0.26}
{'loss': 0.9244, 'grad_norm': 1.3569159507751465, 'learning_rate': 0.00018511636565653511, 'epoch': 0.26}
{'loss': 0.4373, 'grad_norm': 1.058912754058838, 'learning_rate': 0.0001850881649528625, 'epoch': 0.26}
{'loss': 0.986, 'grad_norm': 1.3175103664398193, 'learning_rate': 0.00018505993971017302, 'epoch': 0.26}
{'loss': 0.8905, 'grad_norm': 1.1255030632019043, 'learning_rate': 0.00018503168993660657, 'epoch': 0.26}
{'loss': 0.944, 'grad_norm': 1.736032247543335, 'learning_rate': 0.0001850034156403103, 'epoch': 0.26}
{'loss': 0.9779, 'grad_norm': 1.0041632652282715, 'learning_rate': 0.0001849751168294384, 'epoch': 0.26}
{'loss': 1.0018, 'grad_norm': 1.4003751277923584, 'learning_rate': 0.0001849467935121521, 'epoch': 0.26}
{'loss': 1.0776, 'grad_norm': 1.2102024555206299, 'learning_rate': 0.0001849184456966197, 'epoch': 0.26}
{'loss': 1.0542, 'grad_norm': 2.3245694637298584, 'learning_rate': 0.0001848900733910166, 'epoch': 0.26}
{'loss': 0.8676, 'grad_norm': 1.6015088558197021, 'learning_rate': 0.0001848616766035252, 'epoch': 0.26}
{'loss': 1.1418, 'grad_norm': 0.8954825401306152, 'learning_rate': 0.00018483325534233506, 'epoch': 0.26}
{'loss': 0.8175, 'grad_norm': 1.5537314414978027, 'learning_rate': 0.0001848048096156426, 'epoch': 0.26}
{'loss': 0.9241, 'grad_norm': 1.342781901359558, 'learning_rate': 0.00018477633943165156, 'epoch': 0.26}
{'loss': 0.7808, 'grad_norm': 1.168811559677124, 'learning_rate': 0.00018474784479857257, 'epoch': 0.26}
{'loss': 0.9503, 'grad_norm': 1.509049892425537, 'learning_rate': 0.00018471932572462332, 'epoch': 0.26}
{'loss': 0.84, 'grad_norm': 1.6463866233825684, 'learning_rate': 0.0001846907822180286, 'epoch': 0.26}
{'loss': 1.1254, 'grad_norm': 1.1866693496704102, 'learning_rate': 0.0001846622142870202, 'epoch': 0.26}
{'loss': 0.9702, 'grad_norm': 0.8698257207870483, 'learning_rate': 0.00018463362193983703, 'epoch': 0.26}
{'loss': 1.1719, 'grad_norm': 1.3589266538619995, 'learning_rate': 0.00018460500518472487, 'epoch': 0.26}
{'loss': 1.4305, 'grad_norm': 1.1069533824920654, 'learning_rate': 0.00018457636402993677, 'epoch': 0.26}
{'loss': 1.1946, 'grad_norm': 1.0998517274856567, 'learning_rate': 0.00018454769848373262, 'epoch': 0.26}
{'loss': 0.6848, 'grad_norm': 1.7896761894226074, 'learning_rate': 0.0001845190085543795, 'epoch': 0.26}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1634, 'grad_norm': 1.6724828481674194, 'learning_rate': 0.00018449029425015136, 'epoch': 0.26}
{'loss': 1.0605, 'grad_norm': 1.2068344354629517, 'learning_rate': 0.00018446155557932934, 'epoch': 0.26}
{'loss': 0.9499, 'grad_norm': 2.7015514373779297, 'learning_rate': 0.00018443279255020152, 'epoch': 0.26}
{'loss': 1.0111, 'grad_norm': 1.5208708047866821, 'learning_rate': 0.00018440400517106298, 'epoch': 0.26}
{'loss': 1.1207, 'grad_norm': 1.2344917058944702, 'learning_rate': 0.0001843751934502159, 'epoch': 0.26}
{'loss': 0.9233, 'grad_norm': 1.270396113395691, 'learning_rate': 0.00018434635739596943, 'epoch': 0.26}
{'loss': 1.0136, 'grad_norm': 1.6755748987197876, 'learning_rate': 0.0001843174970166398, 'epoch': 0.26}
{'loss': 1.0281, 'grad_norm': 1.614595890045166, 'learning_rate': 0.0001842886123205501, 'epoch': 0.26}
{'loss': 0.9355, 'grad_norm': 1.166793942451477, 'learning_rate': 0.00018425970331603056, 'epoch': 0.26}
{'loss': 1.0531, 'grad_norm': 1.2145466804504395, 'learning_rate': 0.0001842307700114185, 'epoch': 0.26}
{'loss': 0.8111, 'grad_norm': 1.2546966075897217, 'learning_rate': 0.00018420181241505804, 'epoch': 0.26}
{'loss': 0.9954, 'grad_norm': 1.18753981590271, 'learning_rate': 0.00018417283053530044, 'epoch': 0.26}
{'loss': 1.0081, 'grad_norm': 1.1644880771636963, 'learning_rate': 0.00018414382438050394, 'epoch': 0.26}
{'loss': 0.9113, 'grad_norm': 1.0609359741210938, 'learning_rate': 0.0001841147939590338, 'epoch': 0.26}
{'loss': 1.0123, 'grad_norm': 1.5624947547912598, 'learning_rate': 0.00018408573927926222, 'epoch': 0.26}
{'loss': 0.9199, 'grad_norm': 1.1016111373901367, 'learning_rate': 0.00018405666034956844, 'epoch': 0.26}
{'loss': 1.3046, 'grad_norm': 1.2618248462677002, 'learning_rate': 0.00018402755717833867, 'epoch': 0.26}
{'loss': 0.9545, 'grad_norm': 1.4095698595046997, 'learning_rate': 0.00018399842977396614, 'epoch': 0.26}
{'loss': 0.7154, 'grad_norm': 1.3982875347137451, 'learning_rate': 0.00018396927814485106, 'epoch': 0.26}
{'loss': 0.9204, 'grad_norm': 1.0210926532745361, 'learning_rate': 0.0001839401022994006, 'epoch': 0.26}
{'loss': 0.7916, 'grad_norm': 1.0377533435821533, 'learning_rate': 0.00018391090224602894, 'epoch': 0.26}
{'loss': 1.0492, 'grad_norm': 1.401130199432373, 'learning_rate': 0.00018388167799315727, 'epoch': 0.26}
{'loss': 1.0749, 'grad_norm': 1.0631628036499023, 'learning_rate': 0.00018385242954921366, 'epoch': 0.27}
{'loss': 0.6952, 'grad_norm': 1.3049730062484741, 'learning_rate': 0.00018382315692263323, 'epoch': 0.27}
{'loss': 1.0389, 'grad_norm': 0.9526602625846863, 'learning_rate': 0.00018379386012185814, 'epoch': 0.27}
{'loss': 0.8038, 'grad_norm': 2.023397922515869, 'learning_rate': 0.00018376453915533734, 'epoch': 0.27}
{'loss': 0.9911, 'grad_norm': 1.0836602449417114, 'learning_rate': 0.00018373519403152696, 'epoch': 0.27}
{'loss': 0.9861, 'grad_norm': 1.2024309635162354, 'learning_rate': 0.00018370582475888992, 'epoch': 0.27}
{'loss': 0.8217, 'grad_norm': 1.2002209424972534, 'learning_rate': 0.00018367643134589617, 'epoch': 0.27}
{'loss': 0.9348, 'grad_norm': 1.310684323310852, 'learning_rate': 0.00018364701380102266, 'epoch': 0.27}
{'loss': 1.0735, 'grad_norm': 1.3737918138504028, 'learning_rate': 0.0001836175721327533, 'epoch': 0.27}
{'loss': 1.1284, 'grad_norm': 1.6303170919418335, 'learning_rate': 0.00018358810634957887, 'epoch': 0.27}
{'loss': 0.9878, 'grad_norm': 1.4871927499771118, 'learning_rate': 0.00018355861645999718, 'epoch': 0.27}
{'loss': 0.8647, 'grad_norm': 1.0220375061035156, 'learning_rate': 0.000183529102472513, 'epoch': 0.27}
{'loss': 1.0035, 'grad_norm': 1.2377084493637085, 'learning_rate': 0.00018349956439563794, 'epoch': 0.27}
{'loss': 0.9966, 'grad_norm': 1.5426278114318848, 'learning_rate': 0.0001834700022378907, 'epoch': 0.27}
{'loss': 0.8004, 'grad_norm': 1.1019980907440186, 'learning_rate': 0.0001834404160077969, 'epoch': 0.27}
{'loss': 1.0148, 'grad_norm': 1.4912501573562622, 'learning_rate': 0.00018341080571388898, 'epoch': 0.27}
{'loss': 0.9054, 'grad_norm': 2.1592414379119873, 'learning_rate': 0.00018338117136470648, 'epoch': 0.27}
{'loss': 0.9359, 'grad_norm': 1.8178569078445435, 'learning_rate': 0.00018335151296879575, 'epoch': 0.27}
{'loss': 0.7299, 'grad_norm': 1.3274176120758057, 'learning_rate': 0.00018332183053471014, 'epoch': 0.27}
{'loss': 0.8998, 'grad_norm': 1.5516202449798584, 'learning_rate': 0.00018329212407100994, 'epoch': 0.27}
{'loss': 0.9254, 'grad_norm': 1.3376033306121826, 'learning_rate': 0.00018326239358626239, 'epoch': 0.27}
{'loss': 1.1262, 'grad_norm': 1.084368109703064, 'learning_rate': 0.0001832326390890415, 'epoch': 0.27}
{'loss': 0.8539, 'grad_norm': 0.9497449398040771, 'learning_rate': 0.00018320286058792843, 'epoch': 0.27}
{'loss': 1.0813, 'grad_norm': 1.233533263206482, 'learning_rate': 0.00018317305809151114, 'epoch': 0.27}
{'loss': 0.984, 'grad_norm': 1.2543690204620361, 'learning_rate': 0.00018314323160838447, 'epoch': 0.27}
{'loss': 1.0149, 'grad_norm': 1.285609483718872, 'learning_rate': 0.0001831133811471503, 'epoch': 0.27}
{'loss': 0.869, 'grad_norm': 1.3795298337936401, 'learning_rate': 0.0001830835067164173, 'epoch': 0.27}
{'loss': 1.167, 'grad_norm': 1.1156671047210693, 'learning_rate': 0.00018305360832480117, 'epoch': 0.27}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0038, 'grad_norm': 1.3453521728515625, 'learning_rate': 0.00018302368598092442, 'epoch': 0.27}
{'loss': 0.9203, 'grad_norm': 1.1865835189819336, 'learning_rate': 0.00018299373969341655, 'epoch': 0.27}
{'loss': 0.9642, 'grad_norm': 1.0895212888717651, 'learning_rate': 0.00018296376947091387, 'epoch': 0.27}
{'loss': 0.6204, 'grad_norm': 1.030215859413147, 'learning_rate': 0.00018293377532205968, 'epoch': 0.27}
{'loss': 1.055, 'grad_norm': 1.5413727760314941, 'learning_rate': 0.00018290375725550417, 'epoch': 0.27}
{'loss': 1.4664, 'grad_norm': 0.9560033679008484, 'learning_rate': 0.00018287371527990438, 'epoch': 0.27}
{'loss': 1.085, 'grad_norm': 1.659749150276184, 'learning_rate': 0.00018284364940392424, 'epoch': 0.27}
{'loss': 1.0776, 'grad_norm': 1.0553793907165527, 'learning_rate': 0.00018281355963623467, 'epoch': 0.27}
{'loss': 1.1005, 'grad_norm': 1.2957040071487427, 'learning_rate': 0.0001827834459855134, 'epoch': 0.27}
{'loss': 1.1544, 'grad_norm': 1.13237464427948, 'learning_rate': 0.000182753308460445, 'epoch': 0.27}
{'loss': 0.9994, 'grad_norm': 1.1756410598754883, 'learning_rate': 0.00018272314706972104, 'epoch': 0.27}
{'loss': 1.0913, 'grad_norm': 1.4134900569915771, 'learning_rate': 0.0001826929618220399, 'epoch': 0.27}
{'loss': 1.1536, 'grad_norm': 1.07057523727417, 'learning_rate': 0.0001826627527261069, 'epoch': 0.27}
{'loss': 1.0383, 'grad_norm': 1.2863296270370483, 'learning_rate': 0.00018263251979063413, 'epoch': 0.27}
{'loss': 0.8905, 'grad_norm': 1.4501162767410278, 'learning_rate': 0.0001826022630243407, 'epoch': 0.27}
{'loss': 1.0857, 'grad_norm': 1.512040138244629, 'learning_rate': 0.0001825719824359524, 'epoch': 0.27}
{'loss': 0.709, 'grad_norm': 1.2786929607391357, 'learning_rate': 0.00018254167803420215, 'epoch': 0.27}
{'loss': 1.1684, 'grad_norm': 1.4383316040039062, 'learning_rate': 0.00018251134982782952, 'epoch': 0.27}
{'loss': 1.0817, 'grad_norm': 1.3767167329788208, 'learning_rate': 0.000182480997825581, 'epoch': 0.27}
{'loss': 1.0406, 'grad_norm': 1.5585131645202637, 'learning_rate': 0.00018245062203621002, 'epoch': 0.27}
{'loss': 1.0369, 'grad_norm': 1.4030324220657349, 'learning_rate': 0.00018242022246847675, 'epoch': 0.27}
{'loss': 1.1395, 'grad_norm': 1.293990969657898, 'learning_rate': 0.0001823897991311483, 'epoch': 0.27}
{'loss': 1.2985, 'grad_norm': 2.2911150455474854, 'learning_rate': 0.00018235935203299867, 'epoch': 0.27}
{'loss': 1.1266, 'grad_norm': 1.198487401008606, 'learning_rate': 0.00018232888118280854, 'epoch': 0.27}
{'loss': 0.8997, 'grad_norm': 1.2779661417007446, 'learning_rate': 0.00018229838658936564, 'epoch': 0.27}
{'loss': 1.1489, 'grad_norm': 0.9876161813735962, 'learning_rate': 0.00018226786826146447, 'epoch': 0.27}
{'loss': 0.7489, 'grad_norm': 1.5861867666244507, 'learning_rate': 0.00018223732620790633, 'epoch': 0.27}
{'loss': 1.0717, 'grad_norm': 1.3490891456604004, 'learning_rate': 0.0001822067604374994, 'epoch': 0.27}
{'loss': 0.9975, 'grad_norm': 2.104849338531494, 'learning_rate': 0.00018217617095905874, 'epoch': 0.27}
{'loss': 1.0323, 'grad_norm': 1.016554832458496, 'learning_rate': 0.00018214555778140617, 'epoch': 0.27}
{'loss': 1.0671, 'grad_norm': 1.486087441444397, 'learning_rate': 0.00018211492091337042, 'epoch': 0.27}
{'loss': 0.8783, 'grad_norm': 1.1970736980438232, 'learning_rate': 0.00018208426036378698, 'epoch': 0.27}
{'loss': 1.2285, 'grad_norm': 1.1788930892944336, 'learning_rate': 0.00018205357614149822, 'epoch': 0.27}
{'loss': 0.7146, 'grad_norm': 0.9105081558227539, 'learning_rate': 0.0001820228682553533, 'epoch': 0.27}
{'loss': 1.0512, 'grad_norm': 1.1115063428878784, 'learning_rate': 0.00018199213671420827, 'epoch': 0.27}
{'loss': 0.962, 'grad_norm': 1.0044065713882446, 'learning_rate': 0.00018196138152692593, 'epoch': 0.27}
{'loss': 1.0721, 'grad_norm': 1.4064397811889648, 'learning_rate': 0.00018193060270237595, 'epoch': 0.27}
{'loss': 1.0298, 'grad_norm': 1.190083622932434, 'learning_rate': 0.00018189980024943477, 'epoch': 0.28}
{'loss': 1.2985, 'grad_norm': 1.0918821096420288, 'learning_rate': 0.00018186897417698562, 'epoch': 0.28}
{'loss': 0.9932, 'grad_norm': 1.289617896080017, 'learning_rate': 0.0001818381244939187, 'epoch': 0.28}
{'loss': 0.874, 'grad_norm': 1.0481992959976196, 'learning_rate': 0.00018180725120913084, 'epoch': 0.28}
{'loss': 0.8601, 'grad_norm': 1.240788221359253, 'learning_rate': 0.0001817763543315258, 'epoch': 0.28}
{'loss': 0.948, 'grad_norm': 1.211630940437317, 'learning_rate': 0.000181745433870014, 'epoch': 0.28}
{'loss': 1.1374, 'grad_norm': 1.0133496522903442, 'learning_rate': 0.00018171448983351284, 'epoch': 0.28}
{'loss': 1.0527, 'grad_norm': 1.3675438165664673, 'learning_rate': 0.0001816835222309464, 'epoch': 0.28}
{'loss': 1.1369, 'grad_norm': 1.258857011795044, 'learning_rate': 0.0001816525310712456, 'epoch': 0.28}
{'loss': 1.2665, 'grad_norm': 1.0244908332824707, 'learning_rate': 0.0001816215163633481, 'epoch': 0.28}
{'loss': 0.8464, 'grad_norm': 1.32329523563385, 'learning_rate': 0.00018159047811619843, 'epoch': 0.28}
{'loss': 1.1092, 'grad_norm': 1.55557119846344, 'learning_rate': 0.00018155941633874787, 'epoch': 0.28}
{'loss': 0.839, 'grad_norm': 1.537670612335205, 'learning_rate': 0.00018152833103995443, 'epoch': 0.28}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9138, 'grad_norm': 1.3483504056930542, 'learning_rate': 0.00018149722222878304, 'epoch': 0.28}
{'loss': 1.0994, 'grad_norm': 1.3729485273361206, 'learning_rate': 0.00018146608991420534, 'epoch': 0.28}
{'loss': 0.9834, 'grad_norm': 1.4705005884170532, 'learning_rate': 0.00018143493410519964, 'epoch': 0.28}
{'loss': 0.8455, 'grad_norm': 1.3008393049240112, 'learning_rate': 0.0001814037548107512, 'epoch': 0.28}
{'loss': 0.999, 'grad_norm': 1.1860917806625366, 'learning_rate': 0.00018137255203985197, 'epoch': 0.28}
{'loss': 0.9505, 'grad_norm': 1.414821982383728, 'learning_rate': 0.00018134132580150064, 'epoch': 0.28}
{'loss': 1.0358, 'grad_norm': 1.0264892578125, 'learning_rate': 0.00018131007610470276, 'epoch': 0.28}
{'loss': 0.788, 'grad_norm': 1.1890321969985962, 'learning_rate': 0.00018127880295847057, 'epoch': 0.28}
{'loss': 0.884, 'grad_norm': 1.038784146308899, 'learning_rate': 0.00018124750637182313, 'epoch': 0.28}
{'loss': 0.5474, 'grad_norm': 1.0349855422973633, 'learning_rate': 0.00018121618635378616, 'epoch': 0.28}
{'loss': 0.848, 'grad_norm': 1.3526499271392822, 'learning_rate': 0.0001811848429133922, 'epoch': 0.28}
{'loss': 0.8694, 'grad_norm': 1.37656831741333, 'learning_rate': 0.00018115347605968061, 'epoch': 0.28}
{'loss': 1.0448, 'grad_norm': 1.5471603870391846, 'learning_rate': 0.0001811220858016974, 'epoch': 0.28}
{'loss': 1.1082, 'grad_norm': 1.2979810237884521, 'learning_rate': 0.00018109067214849538, 'epoch': 0.28}
{'loss': 1.0155, 'grad_norm': 1.022936224937439, 'learning_rate': 0.0001810592351091341, 'epoch': 0.28}
{'loss': 0.8846, 'grad_norm': 1.0710129737854004, 'learning_rate': 0.00018102777469267985, 'epoch': 0.28}
{'loss': 0.9281, 'grad_norm': 1.0887669324874878, 'learning_rate': 0.00018099629090820562, 'epoch': 0.28}
{'loss': 1.1115, 'grad_norm': 1.4896477460861206, 'learning_rate': 0.00018096478376479125, 'epoch': 0.28}
{'loss': 1.2297, 'grad_norm': 1.5601822137832642, 'learning_rate': 0.00018093325327152323, 'epoch': 0.28}
{'loss': 0.9065, 'grad_norm': 1.6455708742141724, 'learning_rate': 0.00018090169943749476, 'epoch': 0.28}
{'loss': 1.0886, 'grad_norm': 1.072269320487976, 'learning_rate': 0.00018087012227180585, 'epoch': 0.28}
{'loss': 1.122, 'grad_norm': 1.2562553882598877, 'learning_rate': 0.0001808385217835632, 'epoch': 0.28}
{'loss': 1.0761, 'grad_norm': 1.5420997142791748, 'learning_rate': 0.00018080689798188022, 'epoch': 0.28}
{'loss': 1.5012, 'grad_norm': 1.557147741317749, 'learning_rate': 0.0001807752508758771, 'epoch': 0.28}
{'loss': 0.9312, 'grad_norm': 1.3164328336715698, 'learning_rate': 0.0001807435804746807, 'epoch': 0.28}
{'loss': 1.0635, 'grad_norm': 1.5942853689193726, 'learning_rate': 0.00018071188678742456, 'epoch': 0.28}
{'loss': 0.8679, 'grad_norm': 1.3172402381896973, 'learning_rate': 0.00018068016982324903, 'epoch': 0.28}
{'loss': 1.0941, 'grad_norm': 1.879552960395813, 'learning_rate': 0.00018064842959130117, 'epoch': 0.28}
{'loss': 1.054, 'grad_norm': 0.9880273342132568, 'learning_rate': 0.00018061666610073464, 'epoch': 0.28}
{'loss': 1.2385, 'grad_norm': 1.1200876235961914, 'learning_rate': 0.00018058487936070992, 'epoch': 0.28}
{'loss': 1.3436, 'grad_norm': 1.1716376543045044, 'learning_rate': 0.0001805530693803941, 'epoch': 0.28}
{'loss': 1.0231, 'grad_norm': 1.0793060064315796, 'learning_rate': 0.0001805212361689611, 'epoch': 0.28}
{'loss': 1.041, 'grad_norm': 1.421269178390503, 'learning_rate': 0.0001804893797355914, 'epoch': 0.28}
{'loss': 1.1095, 'grad_norm': 0.9881038665771484, 'learning_rate': 0.0001804575000894723, 'epoch': 0.28}
{'loss': 1.0108, 'grad_norm': 1.37803053855896, 'learning_rate': 0.0001804255972397977, 'epoch': 0.28}
{'loss': 1.0133, 'grad_norm': 1.1779927015304565, 'learning_rate': 0.00018039367119576825, 'epoch': 0.28}
{'loss': 0.8818, 'grad_norm': 1.2149931192398071, 'learning_rate': 0.00018036172196659117, 'epoch': 0.28}
{'loss': 0.929, 'grad_norm': 1.1761590242385864, 'learning_rate': 0.00018032974956148063, 'epoch': 0.28}
{'loss': 0.8706, 'grad_norm': 1.2184991836547852, 'learning_rate': 0.0001802977539896572, 'epoch': 0.28}
{'loss': 1.2104, 'grad_norm': 1.6979708671569824, 'learning_rate': 0.0001802657352603483, 'epoch': 0.28}
{'loss': 1.0608, 'grad_norm': 1.314462423324585, 'learning_rate': 0.00018023369338278795, 'epoch': 0.28}
{'loss': 0.6987, 'grad_norm': 1.0890740156173706, 'learning_rate': 0.00018020162836621687, 'epoch': 0.28}
{'loss': 0.893, 'grad_norm': 1.7619805335998535, 'learning_rate': 0.00018016954021988246, 'epoch': 0.28}
{'loss': 0.8648, 'grad_norm': 1.2903910875320435, 'learning_rate': 0.00018013742895303883, 'epoch': 0.28}
{'loss': 1.1742, 'grad_norm': 1.2461562156677246, 'learning_rate': 0.00018010529457494664, 'epoch': 0.28}
{'loss': 0.7793, 'grad_norm': 1.1827363967895508, 'learning_rate': 0.00018007313709487334, 'epoch': 0.28}
{'loss': 0.8681, 'grad_norm': 1.0644913911819458, 'learning_rate': 0.00018004095652209302, 'epoch': 0.28}
{'loss': 0.9581, 'grad_norm': 1.3162132501602173, 'learning_rate': 0.00018000875286588633, 'epoch': 0.28}
{'loss': 0.81, 'grad_norm': 1.7169795036315918, 'learning_rate': 0.0001799765261355407, 'epoch': 0.28}
{'loss': 0.8911, 'grad_norm': 1.3645614385604858, 'learning_rate': 0.00017994427634035015, 'epoch': 0.28}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0051, 'grad_norm': 1.1791017055511475, 'learning_rate': 0.00017991200348961532, 'epoch': 0.28}
{'loss': 1.0877, 'grad_norm': 1.483688235282898, 'learning_rate': 0.00017987970759264363, 'epoch': 0.28}
{'loss': 1.0917, 'grad_norm': 1.4553594589233398, 'learning_rate': 0.00017984738865874902, 'epoch': 0.29}
{'loss': 1.2076, 'grad_norm': 1.611822247505188, 'learning_rate': 0.00017981504669725208, 'epoch': 0.29}
{'loss': 1.005, 'grad_norm': 1.868277668952942, 'learning_rate': 0.00017978268171748015, 'epoch': 0.29}
{'loss': 0.8815, 'grad_norm': 1.299551010131836, 'learning_rate': 0.00017975029372876706, 'epoch': 0.29}
{'loss': 1.1403, 'grad_norm': 1.6026322841644287, 'learning_rate': 0.0001797178827404534, 'epoch': 0.29}
{'loss': 1.1351, 'grad_norm': 1.1066854000091553, 'learning_rate': 0.00017968544876188633, 'epoch': 0.29}
{'loss': 1.0833, 'grad_norm': 1.313019037246704, 'learning_rate': 0.00017965299180241963, 'epoch': 0.29}
{'loss': 0.7933, 'grad_norm': 1.1806546449661255, 'learning_rate': 0.0001796205118714138, 'epoch': 0.29}
{'loss': 1.3313, 'grad_norm': 1.0245392322540283, 'learning_rate': 0.0001795880089782358, 'epoch': 0.29}
{'loss': 1.2179, 'grad_norm': 1.1504756212234497, 'learning_rate': 0.00017955548313225935, 'epoch': 0.29}
{'loss': 0.9452, 'grad_norm': 1.5533283948898315, 'learning_rate': 0.0001795229343428648, 'epoch': 0.29}
{'loss': 1.0115, 'grad_norm': 1.1591235399246216, 'learning_rate': 0.00017949036261943896, 'epoch': 0.29}
{'loss': 1.0386, 'grad_norm': 1.2541123628616333, 'learning_rate': 0.00017945776797137543, 'epoch': 0.29}
{'loss': 0.9585, 'grad_norm': 1.513603687286377, 'learning_rate': 0.00017942515040807435, 'epoch': 0.29}
{'loss': 0.9502, 'grad_norm': 1.2978509664535522, 'learning_rate': 0.00017939250993894245, 'epoch': 0.29}
{'loss': 0.6901, 'grad_norm': 1.7063385248184204, 'learning_rate': 0.0001793598465733931, 'epoch': 0.29}
{'loss': 1.2538, 'grad_norm': 1.1054365634918213, 'learning_rate': 0.00017932716032084618, 'epoch': 0.29}
{'loss': 0.8502, 'grad_norm': 1.3965750932693481, 'learning_rate': 0.00017929445119072837, 'epoch': 0.29}
{'loss': 0.9589, 'grad_norm': 1.197959542274475, 'learning_rate': 0.00017926171919247275, 'epoch': 0.29}
{'loss': 1.0801, 'grad_norm': 1.6660606861114502, 'learning_rate': 0.00017922896433551907, 'epoch': 0.29}
{'loss': 0.9846, 'grad_norm': 1.3079193830490112, 'learning_rate': 0.0001791961866293137, 'epoch': 0.29}
{'loss': 1.0258, 'grad_norm': 1.311807632446289, 'learning_rate': 0.0001791633860833096, 'epoch': 0.29}
{'loss': 1.0642, 'grad_norm': 1.3565913438796997, 'learning_rate': 0.0001791305627069662, 'epoch': 0.29}
{'loss': 0.8941, 'grad_norm': 1.7985215187072754, 'learning_rate': 0.0001790977165097497, 'epoch': 0.29}
{'loss': 0.7616, 'grad_norm': 1.2232483625411987, 'learning_rate': 0.0001790648475011327, 'epoch': 0.29}
{'loss': 0.9855, 'grad_norm': 1.0423572063446045, 'learning_rate': 0.00017903195569059452, 'epoch': 0.29}
{'loss': 0.9917, 'grad_norm': 1.2360869646072388, 'learning_rate': 0.000178999041087621, 'epoch': 0.29}
{'loss': 1.1817, 'grad_norm': 1.1502901315689087, 'learning_rate': 0.0001789661037017045, 'epoch': 0.29}
{'loss': 1.1505, 'grad_norm': 1.240148663520813, 'learning_rate': 0.00017893314354234407, 'epoch': 0.29}
{'loss': 1.0462, 'grad_norm': 1.0947307348251343, 'learning_rate': 0.00017890016061904523, 'epoch': 0.29}
{'loss': 1.083, 'grad_norm': 1.0655938386917114, 'learning_rate': 0.00017886715494132006, 'epoch': 0.29}
{'loss': 1.1682, 'grad_norm': 1.0520601272583008, 'learning_rate': 0.00017883412651868735, 'epoch': 0.29}
{'loss': 0.7345, 'grad_norm': 1.2898521423339844, 'learning_rate': 0.00017880107536067218, 'epoch': 0.29}
{'loss': 0.7987, 'grad_norm': 1.4896870851516724, 'learning_rate': 0.0001787680014768065, 'epoch': 0.29}
{'loss': 0.9678, 'grad_norm': 1.297580599784851, 'learning_rate': 0.00017873490487662857, 'epoch': 0.29}
{'loss': 1.1514, 'grad_norm': 1.1635652780532837, 'learning_rate': 0.0001787017855696833, 'epoch': 0.29}
{'loss': 1.02, 'grad_norm': 1.511924386024475, 'learning_rate': 0.00017866864356552213, 'epoch': 0.29}
{'loss': 0.9064, 'grad_norm': 1.3984493017196655, 'learning_rate': 0.0001786354788737031, 'epoch': 0.29}
{'loss': 0.935, 'grad_norm': 1.1249394416809082, 'learning_rate': 0.00017860229150379068, 'epoch': 0.29}
{'loss': 0.8062, 'grad_norm': 1.3694133758544922, 'learning_rate': 0.00017856908146535603, 'epoch': 0.29}
{'loss': 1.0499, 'grad_norm': 1.0673490762710571, 'learning_rate': 0.00017853584876797668, 'epoch': 0.29}
{'loss': 0.9275, 'grad_norm': 1.3220570087432861, 'learning_rate': 0.00017850259342123685, 'epoch': 0.29}
{'loss': 1.0294, 'grad_norm': 1.1878160238265991, 'learning_rate': 0.0001784693154347272, 'epoch': 0.29}
{'loss': 0.9722, 'grad_norm': 1.4073538780212402, 'learning_rate': 0.00017843601481804493, 'epoch': 0.29}
{'loss': 0.8863, 'grad_norm': 1.0297125577926636, 'learning_rate': 0.00017840269158079378, 'epoch': 0.29}
{'loss': 0.7969, 'grad_norm': 1.15805184841156, 'learning_rate': 0.000178369345732584, 'epoch': 0.29}
{'loss': 1.0461, 'grad_norm': 1.174855351448059, 'learning_rate': 0.00017833597728303236, 'epoch': 0.29}
{'loss': 1.0362, 'grad_norm': 1.8682422637939453, 'learning_rate': 0.00017830258624176225, 'epoch': 0.29}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8889, 'grad_norm': 1.0426814556121826, 'learning_rate': 0.00017826917261840337, 'epoch': 0.29}
{'loss': 1.0148, 'grad_norm': 1.358533263206482, 'learning_rate': 0.0001782357364225921, 'epoch': 0.29}
{'loss': 1.025, 'grad_norm': 1.196641206741333, 'learning_rate': 0.0001782022776639713, 'epoch': 0.29}
{'loss': 1.2119, 'grad_norm': 1.4804425239562988, 'learning_rate': 0.00017816879635219028, 'epoch': 0.29}
{'loss': 1.0698, 'grad_norm': 1.2908490896224976, 'learning_rate': 0.0001781352924969049, 'epoch': 0.29}
{'loss': 1.1107, 'grad_norm': 1.1451040506362915, 'learning_rate': 0.0001781017661077775, 'epoch': 0.29}
{'loss': 1.2374, 'grad_norm': 1.014189600944519, 'learning_rate': 0.00017806821719447695, 'epoch': 0.29}
{'loss': 1.0778, 'grad_norm': 1.3833585977554321, 'learning_rate': 0.00017803464576667859, 'epoch': 0.29}
{'loss': 0.9835, 'grad_norm': 1.1326911449432373, 'learning_rate': 0.00017800105183406423, 'epoch': 0.29}
{'loss': 0.8154, 'grad_norm': 1.3120205402374268, 'learning_rate': 0.00017796743540632223, 'epoch': 0.29}
{'loss': 1.2111, 'grad_norm': 1.8195017576217651, 'learning_rate': 0.00017793379649314744, 'epoch': 0.29}
{'loss': 1.3037, 'grad_norm': 1.1382853984832764, 'learning_rate': 0.00017790013510424107, 'epoch': 0.29}
{'loss': 0.974, 'grad_norm': 1.0747424364089966, 'learning_rate': 0.00017786645124931094, 'epoch': 0.29}
{'loss': 0.8922, 'grad_norm': 1.2950811386108398, 'learning_rate': 0.00017783274493807135, 'epoch': 0.29}
{'loss': 0.7932, 'grad_norm': 1.4365850687026978, 'learning_rate': 0.00017779901618024301, 'epoch': 0.29}
{'loss': 0.7977, 'grad_norm': 1.3532848358154297, 'learning_rate': 0.00017776526498555312, 'epoch': 0.29}
{'loss': 1.1374, 'grad_norm': 1.3378487825393677, 'learning_rate': 0.00017773149136373538, 'epoch': 0.29}
{'loss': 0.7742, 'grad_norm': 1.168708086013794, 'learning_rate': 0.0001776976953245299, 'epoch': 0.3}
{'loss': 1.0743, 'grad_norm': 1.192629098892212, 'learning_rate': 0.0001776638768776834, 'epoch': 0.3}
{'loss': 0.8118, 'grad_norm': 1.228677749633789, 'learning_rate': 0.0001776300360329488, 'epoch': 0.3}
{'loss': 0.9679, 'grad_norm': 1.5129318237304688, 'learning_rate': 0.00017759617280008576, 'epoch': 0.3}
{'loss': 1.2187, 'grad_norm': 1.2306441068649292, 'learning_rate': 0.00017756228718886026, 'epoch': 0.3}
{'loss': 0.8335, 'grad_norm': 1.0782246589660645, 'learning_rate': 0.00017752837920904466, 'epoch': 0.3}
{'loss': 0.7945, 'grad_norm': 1.5835380554199219, 'learning_rate': 0.00017749444887041799, 'epoch': 0.3}
{'loss': 1.0413, 'grad_norm': 1.2271536588668823, 'learning_rate': 0.00017746049618276545, 'epoch': 0.3}
{'loss': 0.8885, 'grad_norm': 1.0234047174453735, 'learning_rate': 0.00017742652115587896, 'epoch': 0.3}
{'loss': 0.8983, 'grad_norm': 1.3787938356399536, 'learning_rate': 0.00017739252379955672, 'epoch': 0.3}
{'loss': 1.0904, 'grad_norm': 1.28083074092865, 'learning_rate': 0.00017735850412360331, 'epoch': 0.3}
{'loss': 1.1286, 'grad_norm': 1.3823822736740112, 'learning_rate': 0.00017732446213782996, 'epoch': 0.3}
{'loss': 1.0492, 'grad_norm': 1.513014554977417, 'learning_rate': 0.0001772903978520542, 'epoch': 0.3}
{'loss': 1.2171, 'grad_norm': 1.2576791048049927, 'learning_rate': 0.0001772563112760999, 'epoch': 0.3}
{'loss': 1.1904, 'grad_norm': 1.1216505765914917, 'learning_rate': 0.0001772222024197976, 'epoch': 0.3}
{'loss': 0.9737, 'grad_norm': 1.5864840745925903, 'learning_rate': 0.00017718807129298407, 'epoch': 0.3}
{'loss': 0.99, 'grad_norm': 1.0990955829620361, 'learning_rate': 0.00017715391790550252, 'epoch': 0.3}
{'loss': 1.1034, 'grad_norm': 1.2706772089004517, 'learning_rate': 0.00017711974226720272, 'epoch': 0.3}
{'loss': 0.8358, 'grad_norm': 1.6926989555358887, 'learning_rate': 0.00017708554438794068, 'epoch': 0.3}
{'loss': 0.8722, 'grad_norm': 1.136917233467102, 'learning_rate': 0.00017705132427757895, 'epoch': 0.3}
{'loss': 0.7714, 'grad_norm': 1.1544615030288696, 'learning_rate': 0.0001770170819459864, 'epoch': 0.3}
{'loss': 1.1291, 'grad_norm': 0.9134876728057861, 'learning_rate': 0.00017698281740303836, 'epoch': 0.3}
{'loss': 0.7799, 'grad_norm': 1.175200343132019, 'learning_rate': 0.00017694853065861662, 'epoch': 0.3}
{'loss': 0.7893, 'grad_norm': 1.045973300933838, 'learning_rate': 0.00017691422172260923, 'epoch': 0.3}
{'loss': 0.8766, 'grad_norm': 0.9360814094543457, 'learning_rate': 0.00017687989060491077, 'epoch': 0.3}
{'loss': 0.9711, 'grad_norm': 1.5134764909744263, 'learning_rate': 0.00017684553731542218, 'epoch': 0.3}
{'loss': 1.0526, 'grad_norm': 1.2272837162017822, 'learning_rate': 0.00017681116186405073, 'epoch': 0.3}
{'loss': 1.0044, 'grad_norm': 1.0562824010849, 'learning_rate': 0.00017677676426071018, 'epoch': 0.3}
{'loss': 1.073, 'grad_norm': 1.016798973083496, 'learning_rate': 0.00017674234451532065, 'epoch': 0.3}
{'loss': 1.1209, 'grad_norm': 1.1590960025787354, 'learning_rate': 0.00017670790263780856, 'epoch': 0.3}
{'loss': 0.9044, 'grad_norm': 1.2433817386627197, 'learning_rate': 0.00017667343863810681, 'epoch': 0.3}
{'loss': 0.9133, 'grad_norm': 1.3719072341918945, 'learning_rate': 0.0001766389525261547, 'epoch': 0.3}
{'loss': 0.9012, 'grad_norm': 1.079986810684204, 'learning_rate': 0.0001766044443118978, 'epoch': 0.3}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9252, 'grad_norm': 1.0140868425369263, 'learning_rate': 0.00017656991400528816, 'epoch': 0.3}
{'loss': 0.811, 'grad_norm': 1.0997689962387085, 'learning_rate': 0.0001765353616162841, 'epoch': 0.3}
{'loss': 1.3226, 'grad_norm': 1.1517424583435059, 'learning_rate': 0.0001765007871548504, 'epoch': 0.3}
{'loss': 0.8954, 'grad_norm': 1.243025302886963, 'learning_rate': 0.00017646619063095816, 'epoch': 0.3}
{'loss': 1.0818, 'grad_norm': 1.255505919456482, 'learning_rate': 0.00017643157205458483, 'epoch': 0.3}
{'loss': 1.0681, 'grad_norm': 1.0634255409240723, 'learning_rate': 0.0001763969314357143, 'epoch': 0.3}
{'loss': 0.9718, 'grad_norm': 1.2535572052001953, 'learning_rate': 0.00017636226878433667, 'epoch': 0.3}
{'loss': 0.8805, 'grad_norm': 1.3100378513336182, 'learning_rate': 0.00017632758411044856, 'epoch': 0.3}
{'loss': 1.0558, 'grad_norm': 0.937885582447052, 'learning_rate': 0.00017629287742405283, 'epoch': 0.3}
{'loss': 0.7929, 'grad_norm': 1.0397628545761108, 'learning_rate': 0.0001762581487351587, 'epoch': 0.3}
{'loss': 0.9286, 'grad_norm': 1.1733367443084717, 'learning_rate': 0.0001762233980537818, 'epoch': 0.3}
{'loss': 0.851, 'grad_norm': 1.3372175693511963, 'learning_rate': 0.00017618862538994402, 'epoch': 0.3}
{'loss': 0.9704, 'grad_norm': 1.3171744346618652, 'learning_rate': 0.0001761538307536737, 'epoch': 0.3}
{'loss': 1.2523, 'grad_norm': 1.2184003591537476, 'learning_rate': 0.00017611901415500535, 'epoch': 0.3}
{'loss': 1.2497, 'grad_norm': 3.1559972763061523, 'learning_rate': 0.00017608417560397994, 'epoch': 0.3}
{'loss': 1.2001, 'grad_norm': 1.2050904035568237, 'learning_rate': 0.00017604931511064478, 'epoch': 0.3}
{'loss': 0.8076, 'grad_norm': 1.1738680601119995, 'learning_rate': 0.00017601443268505342, 'epoch': 0.3}
{'loss': 1.2665, 'grad_norm': 1.2607929706573486, 'learning_rate': 0.00017597952833726583, 'epoch': 0.3}
{'loss': 0.8385, 'grad_norm': 1.346366047859192, 'learning_rate': 0.00017594460207734822, 'epoch': 0.3}
{'loss': 0.9937, 'grad_norm': 1.0957764387130737, 'learning_rate': 0.00017590965391537316, 'epoch': 0.3}
{'loss': 1.0047, 'grad_norm': 1.2613834142684937, 'learning_rate': 0.00017587468386141955, 'epoch': 0.3}
{'loss': 1.0225, 'grad_norm': 1.1126660108566284, 'learning_rate': 0.00017583969192557256, 'epoch': 0.3}
{'loss': 1.0445, 'grad_norm': 1.2411326169967651, 'learning_rate': 0.0001758046781179237, 'epoch': 0.3}
{'loss': 0.9387, 'grad_norm': 1.0517797470092773, 'learning_rate': 0.00017576964244857081, 'epoch': 0.3}
{'loss': 1.0472, 'grad_norm': 1.210318684577942, 'learning_rate': 0.00017573458492761801, 'epoch': 0.3}
{'loss': 0.8426, 'grad_norm': 1.3847814798355103, 'learning_rate': 0.00017569950556517566, 'epoch': 0.3}
{'loss': 0.9128, 'grad_norm': 1.230477213859558, 'learning_rate': 0.00017566440437136052, 'epoch': 0.3}
{'loss': 1.1097, 'grad_norm': 1.3649474382400513, 'learning_rate': 0.00017562928135629564, 'epoch': 0.3}
{'loss': 1.0109, 'grad_norm': 1.1692880392074585, 'learning_rate': 0.00017559413653011024, 'epoch': 0.3}
{'loss': 0.9175, 'grad_norm': 1.15702486038208, 'learning_rate': 0.00017555896990294003, 'epoch': 0.3}
{'loss': 1.0325, 'grad_norm': 1.5798722505569458, 'learning_rate': 0.00017552378148492679, 'epoch': 0.3}
{'loss': 0.8014, 'grad_norm': 0.9940038919448853, 'learning_rate': 0.00017548857128621875, 'epoch': 0.3}
{'loss': 0.8685, 'grad_norm': 1.2122725248336792, 'learning_rate': 0.00017545333931697036, 'epoch': 0.31}
{'loss': 0.9208, 'grad_norm': 1.5321979522705078, 'learning_rate': 0.00017541808558734233, 'epoch': 0.31}
{'loss': 0.9051, 'grad_norm': 1.3790825605392456, 'learning_rate': 0.0001753828101075017, 'epoch': 0.31}
{'loss': 0.9096, 'grad_norm': 1.2859679460525513, 'learning_rate': 0.0001753475128876217, 'epoch': 0.31}
{'loss': 1.0773, 'grad_norm': 1.25844407081604, 'learning_rate': 0.0001753121939378819, 'epoch': 0.31}
{'loss': 1.3112, 'grad_norm': 1.231067419052124, 'learning_rate': 0.00017527685326846815, 'epoch': 0.31}
{'loss': 1.1076, 'grad_norm': 1.0192071199417114, 'learning_rate': 0.00017524149088957245, 'epoch': 0.31}
{'loss': 0.792, 'grad_norm': 1.0331928730010986, 'learning_rate': 0.00017520610681139322, 'epoch': 0.31}
{'loss': 1.1808, 'grad_norm': 1.0798499584197998, 'learning_rate': 0.00017517070104413498, 'epoch': 0.31}
{'loss': 0.8993, 'grad_norm': 1.0733757019042969, 'learning_rate': 0.00017513527359800865, 'epoch': 0.31}
{'loss': 0.9578, 'grad_norm': 2.1574440002441406, 'learning_rate': 0.00017509982448323132, 'epoch': 0.31}
{'loss': 0.7483, 'grad_norm': 1.5468676090240479, 'learning_rate': 0.00017506435371002633, 'epoch': 0.31}
{'loss': 1.1772, 'grad_norm': 1.0290220975875854, 'learning_rate': 0.0001750288612886233, 'epoch': 0.31}
{'loss': 0.9323, 'grad_norm': 1.222542643547058, 'learning_rate': 0.00017499334722925804, 'epoch': 0.31}
{'loss': 0.8308, 'grad_norm': 0.9066481590270996, 'learning_rate': 0.00017495781154217265, 'epoch': 0.31}
{'loss': 0.9368, 'grad_norm': 1.4562926292419434, 'learning_rate': 0.0001749222542376155, 'epoch': 0.31}
{'loss': 1.116, 'grad_norm': 1.0585825443267822, 'learning_rate': 0.00017488667532584103, 'epoch': 0.31}
{'loss': 0.9404, 'grad_norm': 1.0351604223251343, 'learning_rate': 0.00017485107481711012, 'epoch': 0.31}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2831, 'grad_norm': 1.0384217500686646, 'learning_rate': 0.00017481545272168977, 'epoch': 0.31}
{'loss': 0.8758, 'grad_norm': 1.3952993154525757, 'learning_rate': 0.0001747798090498532, 'epoch': 0.31}
{'loss': 0.9457, 'grad_norm': 1.410500168800354, 'learning_rate': 0.0001747441438118799, 'epoch': 0.31}
{'loss': 1.1761, 'grad_norm': 1.1141639947891235, 'learning_rate': 0.00017470845701805552, 'epoch': 0.31}
{'loss': 1.0673, 'grad_norm': 1.0547280311584473, 'learning_rate': 0.000174672748678672, 'epoch': 0.31}
{'loss': 0.8786, 'grad_norm': 1.2159641981124878, 'learning_rate': 0.00017463701880402737, 'epoch': 0.31}
{'loss': 0.9029, 'grad_norm': 1.0757026672363281, 'learning_rate': 0.00017460126740442608, 'epoch': 0.31}
{'loss': 0.7941, 'grad_norm': 1.1053082942962646, 'learning_rate': 0.00017456549449017857, 'epoch': 0.31}
{'loss': 1.1814, 'grad_norm': 1.0962164402008057, 'learning_rate': 0.0001745297000716016, 'epoch': 0.31}
{'loss': 0.6167, 'grad_norm': 1.3500125408172607, 'learning_rate': 0.00017449388415901812, 'epoch': 0.31}
{'loss': 0.821, 'grad_norm': 1.9572432041168213, 'learning_rate': 0.00017445804676275724, 'epoch': 0.31}
{'loss': 1.0628, 'grad_norm': 1.3349339962005615, 'learning_rate': 0.00017442218789315433, 'epoch': 0.31}
{'loss': 0.9093, 'grad_norm': 1.6009196043014526, 'learning_rate': 0.00017438630756055087, 'epoch': 0.31}
{'loss': 1.4589, 'grad_norm': 1.0965558290481567, 'learning_rate': 0.00017435040577529461, 'epoch': 0.31}
{'loss': 0.866, 'grad_norm': 1.3539865016937256, 'learning_rate': 0.00017431448254773944, 'epoch': 0.31}
{'loss': 0.9931, 'grad_norm': 1.2370104789733887, 'learning_rate': 0.00017427853788824543, 'epoch': 0.31}
{'loss': 1.2678, 'grad_norm': 1.2622958421707153, 'learning_rate': 0.00017424257180717888, 'epoch': 0.31}
{'loss': 0.913, 'grad_norm': 1.0229668617248535, 'learning_rate': 0.00017420658431491223, 'epoch': 0.31}
{'loss': 0.803, 'grad_norm': 1.1876591444015503, 'learning_rate': 0.00017417057542182406, 'epoch': 0.31}
{'loss': 1.1109, 'grad_norm': 1.0257532596588135, 'learning_rate': 0.00017413454513829922, 'epoch': 0.31}
{'loss': 0.9812, 'grad_norm': 1.7144032716751099, 'learning_rate': 0.0001740984934747286, 'epoch': 0.31}
{'loss': 1.0046, 'grad_norm': 1.2851505279541016, 'learning_rate': 0.00017406242044150938, 'epoch': 0.31}
{'loss': 0.933, 'grad_norm': 1.1851686239242554, 'learning_rate': 0.00017402632604904486, 'epoch': 0.31}
{'loss': 1.0797, 'grad_norm': 1.2292803525924683, 'learning_rate': 0.00017399021030774442, 'epoch': 0.31}
{'loss': 1.0801, 'grad_norm': 1.0979055166244507, 'learning_rate': 0.00017395407322802372, 'epoch': 0.31}
{'loss': 1.181, 'grad_norm': 1.3251968622207642, 'learning_rate': 0.00017391791482030455, 'epoch': 0.31}
{'loss': 1.1182, 'grad_norm': 1.0686522722244263, 'learning_rate': 0.00017388173509501472, 'epoch': 0.31}
{'loss': 0.8773, 'grad_norm': 0.940381646156311, 'learning_rate': 0.00017384553406258842, 'epoch': 0.31}
{'loss': 1.0789, 'grad_norm': 1.284306287765503, 'learning_rate': 0.0001738093117334657, 'epoch': 0.31}
{'loss': 1.024, 'grad_norm': 1.1209032535552979, 'learning_rate': 0.00017377306811809304, 'epoch': 0.31}
{'loss': 1.2165, 'grad_norm': 0.9748145937919617, 'learning_rate': 0.00017373680322692282, 'epoch': 0.31}
{'loss': 0.6332, 'grad_norm': 1.2904497385025024, 'learning_rate': 0.00017370051707041374, 'epoch': 0.31}
{'loss': 1.2868, 'grad_norm': 1.4727081060409546, 'learning_rate': 0.00017366420965903053, 'epoch': 0.31}
{'loss': 1.1125, 'grad_norm': 1.2893246412277222, 'learning_rate': 0.00017362788100324408, 'epoch': 0.31}
{'loss': 0.9931, 'grad_norm': 0.9091047644615173, 'learning_rate': 0.00017359153111353134, 'epoch': 0.31}
{'loss': 0.6307, 'grad_norm': 1.3489391803741455, 'learning_rate': 0.00017355516000037554, 'epoch': 0.31}
{'loss': 1.1086, 'grad_norm': 1.1165460348129272, 'learning_rate': 0.00017351876767426585, 'epoch': 0.31}
{'loss': 0.8107, 'grad_norm': 1.239601492881775, 'learning_rate': 0.00017348235414569766, 'epoch': 0.31}
{'loss': 1.1176, 'grad_norm': 1.2087773084640503, 'learning_rate': 0.0001734459194251725, 'epoch': 0.31}
{'loss': 0.9365, 'grad_norm': 1.2858902215957642, 'learning_rate': 0.00017340946352319793, 'epoch': 0.31}
{'loss': 1.0776, 'grad_norm': 1.0326894521713257, 'learning_rate': 0.00017337298645028764, 'epoch': 0.31}
{'loss': 0.8755, 'grad_norm': 1.2984378337860107, 'learning_rate': 0.0001733364882169615, 'epoch': 0.31}
{'loss': 1.0632, 'grad_norm': 1.3569300174713135, 'learning_rate': 0.00017329996883374538, 'epoch': 0.31}
{'loss': 0.9251, 'grad_norm': 0.8781159520149231, 'learning_rate': 0.00017326342831117128, 'epoch': 0.31}
{'loss': 0.776, 'grad_norm': 1.2473214864730835, 'learning_rate': 0.00017322686665977737, 'epoch': 0.31}
{'loss': 1.2399, 'grad_norm': 0.9690783619880676, 'learning_rate': 0.00017319028389010778, 'epoch': 0.31}
{'loss': 1.5419, 'grad_norm': 1.2601501941680908, 'learning_rate': 0.0001731536800127129, 'epoch': 0.31}
{'loss': 0.68, 'grad_norm': 1.1797736883163452, 'learning_rate': 0.000173117055038149, 'epoch': 0.32}
{'loss': 1.1516, 'grad_norm': 1.2245373725891113, 'learning_rate': 0.00017308040897697863, 'epoch': 0.32}
{'loss': 1.0244, 'grad_norm': 1.1886886358261108, 'learning_rate': 0.00017304374183977033, 'epoch': 0.32}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0411, 'grad_norm': 1.0775591135025024, 'learning_rate': 0.00017300705363709867, 'epoch': 0.32}
{'loss': 0.7304, 'grad_norm': 1.2961004972457886, 'learning_rate': 0.00017297034437954443, 'epoch': 0.32}
{'loss': 0.6875, 'grad_norm': 1.3482446670532227, 'learning_rate': 0.00017293361407769427, 'epoch': 0.32}
{'loss': 1.1289, 'grad_norm': 1.5396044254302979, 'learning_rate': 0.00017289686274214118, 'epoch': 0.32}
{'loss': 1.0883, 'grad_norm': 1.5723546743392944, 'learning_rate': 0.00017286009038348395, 'epoch': 0.32}
{'loss': 1.1052, 'grad_norm': 1.313170313835144, 'learning_rate': 0.00017282329701232758, 'epoch': 0.32}
{'loss': 1.1228, 'grad_norm': 1.2511913776397705, 'learning_rate': 0.0001727864826392831, 'epoch': 0.32}
{'loss': 1.0287, 'grad_norm': 1.157860279083252, 'learning_rate': 0.00017274964727496768, 'epoch': 0.32}
{'loss': 1.116, 'grad_norm': 1.0151009559631348, 'learning_rate': 0.00017271279093000435, 'epoch': 0.32}
{'loss': 0.9894, 'grad_norm': 1.6974445581436157, 'learning_rate': 0.00017267591361502232, 'epoch': 0.32}
{'loss': 0.7801, 'grad_norm': 1.166517734527588, 'learning_rate': 0.0001726390153406569, 'epoch': 0.32}
{'loss': 1.116, 'grad_norm': 1.0908496379852295, 'learning_rate': 0.0001726020961175493, 'epoch': 0.32}
{'loss': 1.0063, 'grad_norm': 1.0845723152160645, 'learning_rate': 0.0001725651559563469, 'epoch': 0.32}
{'loss': 1.0222, 'grad_norm': 1.3736056089401245, 'learning_rate': 0.000172528194867703, 'epoch': 0.32}
{'loss': 1.0156, 'grad_norm': 1.1999939680099487, 'learning_rate': 0.00017249121286227705, 'epoch': 0.32}
{'loss': 1.0201, 'grad_norm': 1.3285582065582275, 'learning_rate': 0.0001724542099507345, 'epoch': 0.32}
{'loss': 1.1985, 'grad_norm': 1.0559722185134888, 'learning_rate': 0.00017241718614374678, 'epoch': 0.32}
{'loss': 1.034, 'grad_norm': 1.309802532196045, 'learning_rate': 0.00017238014145199134, 'epoch': 0.32}
{'loss': 1.0855, 'grad_norm': 1.137746810913086, 'learning_rate': 0.00017234307588615176, 'epoch': 0.32}
{'loss': 0.9023, 'grad_norm': 1.2887858152389526, 'learning_rate': 0.00017230598945691753, 'epoch': 0.32}
{'loss': 0.8439, 'grad_norm': 1.4159376621246338, 'learning_rate': 0.00017226888217498424, 'epoch': 0.32}
{'loss': 0.9181, 'grad_norm': 1.239994764328003, 'learning_rate': 0.0001722317540510534, 'epoch': 0.32}
{'loss': 0.9141, 'grad_norm': 1.313876986503601, 'learning_rate': 0.0001721946050958326, 'epoch': 0.32}
{'loss': 0.986, 'grad_norm': 1.1925839185714722, 'learning_rate': 0.00017215743532003543, 'epoch': 0.32}
{'loss': 0.9759, 'grad_norm': 1.2155574560165405, 'learning_rate': 0.00017212024473438147, 'epoch': 0.32}
{'loss': 0.987, 'grad_norm': 1.4947338104248047, 'learning_rate': 0.0001720830333495963, 'epoch': 0.32}
{'loss': 1.025, 'grad_norm': 1.376832127571106, 'learning_rate': 0.0001720458011764115, 'epoch': 0.32}
{'loss': 1.0082, 'grad_norm': 1.3737521171569824, 'learning_rate': 0.00017200854822556466, 'epoch': 0.32}
{'loss': 0.8292, 'grad_norm': 1.2537989616394043, 'learning_rate': 0.00017197127450779934, 'epoch': 0.32}
{'loss': 1.0297, 'grad_norm': 1.4426302909851074, 'learning_rate': 0.0001719339800338651, 'epoch': 0.32}
{'loss': 1.0635, 'grad_norm': 1.01982843875885, 'learning_rate': 0.00017189666481451753, 'epoch': 0.32}
{'loss': 1.0256, 'grad_norm': 1.301763892173767, 'learning_rate': 0.00017185932886051812, 'epoch': 0.32}
{'loss': 1.2808, 'grad_norm': 1.025736689567566, 'learning_rate': 0.00017182197218263437, 'epoch': 0.32}
{'loss': 1.2424, 'grad_norm': 2.0570871829986572, 'learning_rate': 0.00017178459479163976, 'epoch': 0.32}
{'loss': 1.0627, 'grad_norm': 1.3096225261688232, 'learning_rate': 0.00017174719669831384, 'epoch': 0.32}
{'loss': 1.1372, 'grad_norm': 1.504496693611145, 'learning_rate': 0.0001717097779134419, 'epoch': 0.32}
{'loss': 1.007, 'grad_norm': 0.9475060105323792, 'learning_rate': 0.00017167233844781542, 'epoch': 0.32}
{'loss': 1.0411, 'grad_norm': 1.0832929611206055, 'learning_rate': 0.00017163487831223177, 'epoch': 0.32}
{'loss': 0.9577, 'grad_norm': 1.2227944135665894, 'learning_rate': 0.0001715973975174942, 'epoch': 0.32}
{'loss': 0.9879, 'grad_norm': 1.1447772979736328, 'learning_rate': 0.00017155989607441213, 'epoch': 0.32}
{'loss': 1.0232, 'grad_norm': 1.201098084449768, 'learning_rate': 0.00017152237399380065, 'epoch': 0.32}
{'loss': 1.0612, 'grad_norm': 1.0791743993759155, 'learning_rate': 0.00017148483128648102, 'epoch': 0.32}
{'loss': 1.0154, 'grad_norm': 1.1515085697174072, 'learning_rate': 0.00017144726796328034, 'epoch': 0.32}
{'loss': 0.8487, 'grad_norm': 1.2202351093292236, 'learning_rate': 0.00017140968403503174, 'epoch': 0.32}
{'loss': 0.9937, 'grad_norm': 1.0749480724334717, 'learning_rate': 0.0001713720795125742, 'epoch': 0.32}
{'loss': 1.1736, 'grad_norm': 0.9625797271728516, 'learning_rate': 0.0001713344544067527, 'epoch': 0.32}
{'loss': 1.0607, 'grad_norm': 1.2412710189819336, 'learning_rate': 0.00017129680872841814, 'epoch': 0.32}
{'loss': 0.8306, 'grad_norm': 0.9752439856529236, 'learning_rate': 0.00017125914248842736, 'epoch': 0.32}
{'loss': 1.1857, 'grad_norm': 1.1500028371810913, 'learning_rate': 0.0001712214556976431, 'epoch': 0.32}
{'loss': 1.0522, 'grad_norm': 1.041993260383606, 'learning_rate': 0.00017118374836693406, 'epoch': 0.32}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9251, 'grad_norm': 1.1073147058486938, 'learning_rate': 0.00017114602050717487, 'epoch': 0.32}
{'loss': 1.1697, 'grad_norm': 1.5952661037445068, 'learning_rate': 0.00017110827212924603, 'epoch': 0.32}
{'loss': 1.1337, 'grad_norm': 1.9237481355667114, 'learning_rate': 0.000171070503244034, 'epoch': 0.32}
{'loss': 1.1996, 'grad_norm': 1.0279051065444946, 'learning_rate': 0.00017103271386243122, 'epoch': 0.32}
{'loss': 0.8714, 'grad_norm': 1.6985723972320557, 'learning_rate': 0.00017099490399533583, 'epoch': 0.32}
{'loss': 0.8247, 'grad_norm': 1.1884028911590576, 'learning_rate': 0.0001709570736536521, 'epoch': 0.32}
{'loss': 1.1347, 'grad_norm': 1.0035760402679443, 'learning_rate': 0.0001709192228482901, 'epoch': 0.32}
{'loss': 0.8133, 'grad_norm': 1.3217073678970337, 'learning_rate': 0.00017088135159016584, 'epoch': 0.32}
{'loss': 0.646, 'grad_norm': 1.102592945098877, 'learning_rate': 0.00017084345989020117, 'epoch': 0.32}
{'loss': 1.2357, 'grad_norm': 1.1477136611938477, 'learning_rate': 0.00017080554775932388, 'epoch': 0.32}
{'loss': 0.7672, 'grad_norm': 1.2640265226364136, 'learning_rate': 0.00017076761520846767, 'epoch': 0.32}
{'loss': 1.1431, 'grad_norm': 1.1416666507720947, 'learning_rate': 0.0001707296622485721, 'epoch': 0.32}
{'loss': 1.0509, 'grad_norm': 1.0240726470947266, 'learning_rate': 0.0001706916888905826, 'epoch': 0.33}
{'loss': 0.9642, 'grad_norm': 1.2127605676651, 'learning_rate': 0.00017065369514545053, 'epoch': 0.33}
{'loss': 0.8928, 'grad_norm': 1.1710230112075806, 'learning_rate': 0.00017061568102413307, 'epoch': 0.33}
{'loss': 1.039, 'grad_norm': 1.7348302602767944, 'learning_rate': 0.00017057764653759336, 'epoch': 0.33}
{'loss': 1.1728, 'grad_norm': 1.3592420816421509, 'learning_rate': 0.00017053959169680032, 'epoch': 0.33}
{'loss': 1.1395, 'grad_norm': 1.26509690284729, 'learning_rate': 0.0001705015165127288, 'epoch': 0.33}
{'loss': 0.7653, 'grad_norm': 1.4978190660476685, 'learning_rate': 0.00017046342099635948, 'epoch': 0.33}
{'loss': 0.9148, 'grad_norm': 1.0105369091033936, 'learning_rate': 0.00017042530515867896, 'epoch': 0.33}
{'loss': 0.8672, 'grad_norm': 1.3148492574691772, 'learning_rate': 0.00017038716901067962, 'epoch': 0.33}
{'loss': 0.8377, 'grad_norm': 1.3771092891693115, 'learning_rate': 0.00017034901256335978, 'epoch': 0.33}
{'loss': 0.8099, 'grad_norm': 1.3034285306930542, 'learning_rate': 0.00017031083582772354, 'epoch': 0.33}
{'loss': 1.1353, 'grad_norm': 1.3203301429748535, 'learning_rate': 0.00017027263881478093, 'epoch': 0.33}
{'loss': 0.9761, 'grad_norm': 1.2266485691070557, 'learning_rate': 0.00017023442153554777, 'epoch': 0.33}
{'loss': 0.8802, 'grad_norm': 1.4113904237747192, 'learning_rate': 0.00017019618400104572, 'epoch': 0.33}
{'loss': 0.992, 'grad_norm': 1.9581353664398193, 'learning_rate': 0.00017015792622230227, 'epoch': 0.33}
{'loss': 0.9735, 'grad_norm': 1.0405943393707275, 'learning_rate': 0.00017011964821035086, 'epoch': 0.33}
{'loss': 1.2733, 'grad_norm': 1.1681678295135498, 'learning_rate': 0.00017008134997623065, 'epoch': 0.33}
{'loss': 1.0479, 'grad_norm': 1.1479172706604004, 'learning_rate': 0.00017004303153098665, 'epoch': 0.33}
{'loss': 0.9262, 'grad_norm': 1.5420013666152954, 'learning_rate': 0.00017000469288566973, 'epoch': 0.33}
{'loss': 0.9685, 'grad_norm': 1.5109387636184692, 'learning_rate': 0.00016996633405133655, 'epoch': 0.33}
{'loss': 0.8434, 'grad_norm': 1.3496406078338623, 'learning_rate': 0.00016992795503904964, 'epoch': 0.33}
{'loss': 0.9051, 'grad_norm': 1.3157353401184082, 'learning_rate': 0.0001698895558598773, 'epoch': 0.33}
{'loss': 0.9826, 'grad_norm': 1.4365018606185913, 'learning_rate': 0.00016985113652489374, 'epoch': 0.33}
{'loss': 1.0648, 'grad_norm': 0.9347960948944092, 'learning_rate': 0.00016981269704517877, 'epoch': 0.33}
{'loss': 1.0376, 'grad_norm': 1.3985387086868286, 'learning_rate': 0.00016977423743181827, 'epoch': 0.33}
{'loss': 1.0367, 'grad_norm': 1.3159658908843994, 'learning_rate': 0.00016973575769590378, 'epoch': 0.33}
{'loss': 1.1011, 'grad_norm': 1.2508445978164673, 'learning_rate': 0.00016969725784853263, 'epoch': 0.33}
{'loss': 0.9065, 'grad_norm': 1.1836392879486084, 'learning_rate': 0.00016965873790080805, 'epoch': 0.33}
{'loss': 0.8559, 'grad_norm': 1.570604681968689, 'learning_rate': 0.00016962019786383894, 'epoch': 0.33}
{'loss': 0.9278, 'grad_norm': 1.2515867948532104, 'learning_rate': 0.00016958163774874013, 'epoch': 0.33}
{'loss': 0.9891, 'grad_norm': 1.194356918334961, 'learning_rate': 0.00016954305756663212, 'epoch': 0.33}
{'loss': 0.9572, 'grad_norm': 1.5351523160934448, 'learning_rate': 0.00016950445732864127, 'epoch': 0.33}
{'loss': 0.9555, 'grad_norm': 1.2194955348968506, 'learning_rate': 0.00016946583704589973, 'epoch': 0.33}
{'loss': 0.8881, 'grad_norm': 1.2814791202545166, 'learning_rate': 0.00016942719672954538, 'epoch': 0.33}
{'loss': 1.2084, 'grad_norm': 1.2879773378372192, 'learning_rate': 0.00016938853639072185, 'epoch': 0.33}
{'loss': 0.8616, 'grad_norm': 1.0672119855880737, 'learning_rate': 0.00016934985604057868, 'epoch': 0.33}
{'loss': 1.2078, 'grad_norm': 1.229782223701477, 'learning_rate': 0.00016931115569027104, 'epoch': 0.33}
{'loss': 1.1578, 'grad_norm': 1.4494725465774536, 'learning_rate': 0.00016927243535095997, 'epoch': 0.33}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0639, 'grad_norm': 1.3248671293258667, 'learning_rate': 0.00016923369503381216, 'epoch': 0.33}
{'loss': 0.9325, 'grad_norm': 1.0657179355621338, 'learning_rate': 0.0001691949347500002, 'epoch': 0.33}
{'loss': 0.652, 'grad_norm': 1.3442866802215576, 'learning_rate': 0.00016915615451070233, 'epoch': 0.33}
{'loss': 0.9662, 'grad_norm': 1.2914742231369019, 'learning_rate': 0.0001691173543271026, 'epoch': 0.33}
{'loss': 0.9482, 'grad_norm': 1.3349515199661255, 'learning_rate': 0.0001690785342103908, 'epoch': 0.33}
{'loss': 1.1239, 'grad_norm': 1.1213910579681396, 'learning_rate': 0.00016903969417176245, 'epoch': 0.33}
{'loss': 0.94, 'grad_norm': 1.004986047744751, 'learning_rate': 0.00016900083422241883, 'epoch': 0.33}
{'loss': 0.9879, 'grad_norm': 1.2358825206756592, 'learning_rate': 0.000168961954373567, 'epoch': 0.33}
{'loss': 1.3563, 'grad_norm': 1.0365705490112305, 'learning_rate': 0.00016892305463641965, 'epoch': 0.33}
{'loss': 1.2007, 'grad_norm': 1.119661808013916, 'learning_rate': 0.00016888413502219533, 'epoch': 0.33}
{'loss': 1.0672, 'grad_norm': 1.1425046920776367, 'learning_rate': 0.0001688451955421183, 'epoch': 0.33}
{'loss': 1.0365, 'grad_norm': 1.0351463556289673, 'learning_rate': 0.00016880623620741842, 'epoch': 0.33}
{'loss': 0.9878, 'grad_norm': 1.132210373878479, 'learning_rate': 0.00016876725702933144, 'epoch': 0.33}
{'loss': 1.0866, 'grad_norm': 0.9493224024772644, 'learning_rate': 0.0001687282580190988, 'epoch': 0.33}
{'loss': 0.7269, 'grad_norm': 1.2196450233459473, 'learning_rate': 0.00016868923918796753, 'epoch': 0.33}
{'loss': 1.2343, 'grad_norm': 1.0613856315612793, 'learning_rate': 0.00016865020054719057, 'epoch': 0.33}
{'loss': 0.9587, 'grad_norm': 1.1222492456436157, 'learning_rate': 0.0001686111421080264, 'epoch': 0.33}
{'loss': 0.8038, 'grad_norm': 1.2117199897766113, 'learning_rate': 0.0001685720638817393, 'epoch': 0.33}
{'loss': 0.9994, 'grad_norm': 1.425782561302185, 'learning_rate': 0.00016853296587959933, 'epoch': 0.33}
{'loss': 0.7039, 'grad_norm': 1.4124605655670166, 'learning_rate': 0.00016849384811288204, 'epoch': 0.33}
{'loss': 1.0997, 'grad_norm': 1.2388803958892822, 'learning_rate': 0.00016845471059286887, 'epoch': 0.33}
{'loss': 0.9281, 'grad_norm': 1.4838703870773315, 'learning_rate': 0.0001684155533308469, 'epoch': 0.33}
{'loss': 1.0948, 'grad_norm': 1.3858790397644043, 'learning_rate': 0.00016837637633810887, 'epoch': 0.33}
{'loss': 1.2023, 'grad_norm': 1.1367601156234741, 'learning_rate': 0.00016833717962595326, 'epoch': 0.33}
{'loss': 1.0277, 'grad_norm': 1.3801484107971191, 'learning_rate': 0.00016829796320568416, 'epoch': 0.33}
{'loss': 1.1288, 'grad_norm': 1.4714199304580688, 'learning_rate': 0.00016825872708861147, 'epoch': 0.33}
{'loss': 1.2662, 'grad_norm': 2.589423418045044, 'learning_rate': 0.00016821947128605065, 'epoch': 0.33}
{'loss': 1.2774, 'grad_norm': 1.0211399793624878, 'learning_rate': 0.00016818019580932294, 'epoch': 0.34}
{'loss': 1.122, 'grad_norm': 1.007116436958313, 'learning_rate': 0.00016814090066975509, 'epoch': 0.34}
{'loss': 1.1507, 'grad_norm': 1.0482100248336792, 'learning_rate': 0.00016810158587867973, 'epoch': 0.34}
{'loss': 1.0705, 'grad_norm': 1.0850969552993774, 'learning_rate': 0.000168062251447435, 'epoch': 0.34}
{'loss': 1.4255, 'grad_norm': 1.6263781785964966, 'learning_rate': 0.00016802289738736481, 'epoch': 0.34}
{'loss': 0.7932, 'grad_norm': 1.1530706882476807, 'learning_rate': 0.00016798352370981862, 'epoch': 0.34}
{'loss': 1.2214, 'grad_norm': 1.168082356452942, 'learning_rate': 0.00016794413042615168, 'epoch': 0.34}
{'loss': 0.7901, 'grad_norm': 1.2960790395736694, 'learning_rate': 0.00016790471754772474, 'epoch': 0.34}
{'loss': 1.062, 'grad_norm': 1.1766918897628784, 'learning_rate': 0.00016786528508590435, 'epoch': 0.34}
{'loss': 0.963, 'grad_norm': 1.444616675376892, 'learning_rate': 0.00016782583305206263, 'epoch': 0.34}
{'loss': 1.1316, 'grad_norm': 1.2581686973571777, 'learning_rate': 0.00016778636145757734, 'epoch': 0.34}
{'loss': 1.018, 'grad_norm': 1.2811788320541382, 'learning_rate': 0.00016774687031383188, 'epoch': 0.34}
{'loss': 0.7333, 'grad_norm': 1.3302910327911377, 'learning_rate': 0.0001677073596322154, 'epoch': 0.34}
{'loss': 0.9834, 'grad_norm': 1.268839955329895, 'learning_rate': 0.00016766782942412246, 'epoch': 0.34}
{'loss': 0.986, 'grad_norm': 1.114238977432251, 'learning_rate': 0.00016762827970095344, 'epoch': 0.34}
{'loss': 0.8521, 'grad_norm': 1.141237735748291, 'learning_rate': 0.00016758871047411434, 'epoch': 0.34}
{'loss': 0.777, 'grad_norm': 1.0873546600341797, 'learning_rate': 0.00016754912175501665, 'epoch': 0.34}
{'loss': 0.8564, 'grad_norm': 1.2297414541244507, 'learning_rate': 0.00016750951355507763, 'epoch': 0.34}
{'loss': 1.0699, 'grad_norm': 1.0762749910354614, 'learning_rate': 0.00016746988588572004, 'epoch': 0.34}
{'loss': 1.1514, 'grad_norm': 1.547609806060791, 'learning_rate': 0.00016743023875837233, 'epoch': 0.34}
{'loss': 1.1184, 'grad_norm': 1.0585771799087524, 'learning_rate': 0.0001673905721844686, 'epoch': 0.34}
{'loss': 0.8676, 'grad_norm': 1.3642737865447998, 'learning_rate': 0.00016735088617544837, 'epoch': 0.34}
{'loss': 0.8331, 'grad_norm': 1.0425353050231934, 'learning_rate': 0.00016731118074275704, 'epoch': 0.34}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9949, 'grad_norm': 1.3183525800704956, 'learning_rate': 0.00016727145589784533, 'epoch': 0.34}
{'loss': 1.283, 'grad_norm': 1.1109966039657593, 'learning_rate': 0.0001672317116521698, 'epoch': 0.34}
{'loss': 0.982, 'grad_norm': 1.3489924669265747, 'learning_rate': 0.00016719194801719244, 'epoch': 0.34}
{'loss': 0.9268, 'grad_norm': 1.3947560787200928, 'learning_rate': 0.00016715216500438093, 'epoch': 0.34}
{'loss': 1.194, 'grad_norm': 1.1426063776016235, 'learning_rate': 0.00016711236262520847, 'epoch': 0.34}
{'loss': 0.8943, 'grad_norm': 1.036885380744934, 'learning_rate': 0.0001670725408911539, 'epoch': 0.34}
{'loss': 1.1716, 'grad_norm': 1.210360050201416, 'learning_rate': 0.00016703269981370157, 'epoch': 0.34}
{'loss': 1.2587, 'grad_norm': 1.0264948606491089, 'learning_rate': 0.0001669928394043415, 'epoch': 0.34}
{'loss': 0.7886, 'grad_norm': 1.7105361223220825, 'learning_rate': 0.0001669529596745692, 'epoch': 0.34}
{'loss': 0.9983, 'grad_norm': 1.2828047275543213, 'learning_rate': 0.00016691306063588583, 'epoch': 0.34}
{'loss': 0.9316, 'grad_norm': 1.1366498470306396, 'learning_rate': 0.00016687314229979806, 'epoch': 0.34}
{'loss': 0.9885, 'grad_norm': 1.4371386766433716, 'learning_rate': 0.00016683320467781817, 'epoch': 0.34}
{'loss': 1.1681, 'grad_norm': 1.4123438596725464, 'learning_rate': 0.00016679324778146395, 'epoch': 0.34}
{'loss': 1.1201, 'grad_norm': 1.542830228805542, 'learning_rate': 0.0001667532716222588, 'epoch': 0.34}
{'loss': 0.8689, 'grad_norm': 1.508224368095398, 'learning_rate': 0.0001667132762117316, 'epoch': 0.34}
{'loss': 0.9748, 'grad_norm': 1.3643232583999634, 'learning_rate': 0.00016667326156141692, 'epoch': 0.34}
{'loss': 1.2136, 'grad_norm': 0.9519760608673096, 'learning_rate': 0.0001666332276828547, 'epoch': 0.34}
{'loss': 1.1842, 'grad_norm': 1.2148717641830444, 'learning_rate': 0.00016659317458759057, 'epoch': 0.34}
{'loss': 1.3026, 'grad_norm': 1.2387597560882568, 'learning_rate': 0.00016655310228717564, 'epoch': 0.34}
{'loss': 0.9905, 'grad_norm': 0.9514440298080444, 'learning_rate': 0.0001665130107931666, 'epoch': 0.34}
{'loss': 1.1155, 'grad_norm': 0.9998539686203003, 'learning_rate': 0.00016647290011712558, 'epoch': 0.34}
{'loss': 0.9258, 'grad_norm': 1.0790218114852905, 'learning_rate': 0.00016643277027062037, 'epoch': 0.34}
{'loss': 1.2009, 'grad_norm': 1.1239075660705566, 'learning_rate': 0.00016639262126522418, 'epoch': 0.34}
{'loss': 1.231, 'grad_norm': 1.068474531173706, 'learning_rate': 0.0001663524531125158, 'epoch': 0.34}
{'loss': 0.7696, 'grad_norm': 1.27198326587677, 'learning_rate': 0.00016631226582407952, 'epoch': 0.34}
{'loss': 0.9804, 'grad_norm': 1.5248149633407593, 'learning_rate': 0.0001662720594115052, 'epoch': 0.34}
{'loss': 1.0833, 'grad_norm': 0.984368085861206, 'learning_rate': 0.00016623183388638814, 'epoch': 0.34}
{'loss': 0.8954, 'grad_norm': 1.2352185249328613, 'learning_rate': 0.0001661915892603292, 'epoch': 0.34}
{'loss': 0.8803, 'grad_norm': 1.2480648756027222, 'learning_rate': 0.00016615132554493475, 'epoch': 0.34}
{'loss': 0.8692, 'grad_norm': 1.022028923034668, 'learning_rate': 0.00016611104275181664, 'epoch': 0.34}
{'loss': 0.7773, 'grad_norm': 1.0804238319396973, 'learning_rate': 0.00016607074089259222, 'epoch': 0.34}
{'loss': 0.9955, 'grad_norm': 1.2220145463943481, 'learning_rate': 0.00016603041997888436, 'epoch': 0.34}
{'loss': 0.8055, 'grad_norm': 1.1415045261383057, 'learning_rate': 0.00016599008002232143, 'epoch': 0.34}
{'loss': 0.833, 'grad_norm': 1.7780356407165527, 'learning_rate': 0.00016594972103453726, 'epoch': 0.34}
{'loss': 0.9658, 'grad_norm': 1.0815210342407227, 'learning_rate': 0.0001659093430271712, 'epoch': 0.34}
{'loss': 1.153, 'grad_norm': 1.1320093870162964, 'learning_rate': 0.00016586894601186805, 'epoch': 0.34}
{'loss': 0.9956, 'grad_norm': 1.278611421585083, 'learning_rate': 0.00016582853000027815, 'epoch': 0.34}
{'loss': 0.8799, 'grad_norm': 1.1948329210281372, 'learning_rate': 0.0001657880950040573, 'epoch': 0.34}
{'loss': 0.8417, 'grad_norm': 0.9864227175712585, 'learning_rate': 0.00016574764103486668, 'epoch': 0.34}
{'loss': 0.8814, 'grad_norm': 1.1362448930740356, 'learning_rate': 0.0001657071681043731, 'epoch': 0.34}
{'loss': 0.9617, 'grad_norm': 1.1399930715560913, 'learning_rate': 0.00016566667622424868, 'epoch': 0.34}
{'loss': 0.9829, 'grad_norm': 1.0268081426620483, 'learning_rate': 0.00016562616540617117, 'epoch': 0.34}
{'loss': 0.8819, 'grad_norm': 0.9679521918296814, 'learning_rate': 0.00016558563566182363, 'epoch': 0.35}
{'loss': 1.0239, 'grad_norm': 1.2310161590576172, 'learning_rate': 0.00016554508700289468, 'epoch': 0.35}
{'loss': 0.8962, 'grad_norm': 1.1292098760604858, 'learning_rate': 0.00016550451944107833, 'epoch': 0.35}
{'loss': 1.2203, 'grad_norm': 1.1740984916687012, 'learning_rate': 0.00016546393298807406, 'epoch': 0.35}
{'loss': 1.0636, 'grad_norm': 1.2899229526519775, 'learning_rate': 0.00016542332765558684, 'epoch': 0.35}
{'loss': 1.0137, 'grad_norm': 1.471988320350647, 'learning_rate': 0.00016538270345532708, 'epoch': 0.35}
{'loss': 0.8705, 'grad_norm': 1.2006113529205322, 'learning_rate': 0.00016534206039901057, 'epoch': 0.35}
{'loss': 0.6217, 'grad_norm': 0.945728600025177, 'learning_rate': 0.0001653013984983585, 'epoch': 0.35}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9558, 'grad_norm': 1.2315454483032227, 'learning_rate': 0.00016526071776509768, 'epoch': 0.35}
{'loss': 0.9055, 'grad_norm': 1.1854857206344604, 'learning_rate': 0.0001652200182109602, 'epoch': 0.35}
{'loss': 1.0971, 'grad_norm': 0.9874593615531921, 'learning_rate': 0.0001651792998476836, 'epoch': 0.35}
{'loss': 0.7853, 'grad_norm': 0.9421689510345459, 'learning_rate': 0.00016513856268701087, 'epoch': 0.35}
{'loss': 0.8353, 'grad_norm': 1.5091657638549805, 'learning_rate': 0.0001650978067406904, 'epoch': 0.35}
{'loss': 1.2504, 'grad_norm': 1.1925501823425293, 'learning_rate': 0.00016505703202047598, 'epoch': 0.35}
{'loss': 1.1719, 'grad_norm': 1.0045708417892456, 'learning_rate': 0.00016501623853812692, 'epoch': 0.35}
{'loss': 1.1348, 'grad_norm': 1.3695002794265747, 'learning_rate': 0.00016497542630540784, 'epoch': 0.35}
{'loss': 0.8069, 'grad_norm': 1.2356411218643188, 'learning_rate': 0.00016493459533408874, 'epoch': 0.35}
{'loss': 0.9672, 'grad_norm': 1.514804482460022, 'learning_rate': 0.0001648937456359451, 'epoch': 0.35}
{'loss': 0.8862, 'grad_norm': 1.1670905351638794, 'learning_rate': 0.0001648528772227578, 'epoch': 0.35}
{'loss': 0.837, 'grad_norm': 1.4261572360992432, 'learning_rate': 0.0001648119901063131, 'epoch': 0.35}
{'loss': 0.7922, 'grad_norm': 1.3671233654022217, 'learning_rate': 0.00016477108429840262, 'epoch': 0.35}
{'loss': 0.8359, 'grad_norm': 1.4298839569091797, 'learning_rate': 0.00016473015981082338, 'epoch': 0.35}
{'loss': 0.9618, 'grad_norm': 1.3409250974655151, 'learning_rate': 0.00016468921665537786, 'epoch': 0.35}
{'loss': 1.0688, 'grad_norm': 1.2916243076324463, 'learning_rate': 0.0001646482548438738, 'epoch': 0.35}
{'loss': 1.215, 'grad_norm': 1.283377766609192, 'learning_rate': 0.00016460727438812446, 'epoch': 0.35}
{'loss': 0.9299, 'grad_norm': 1.08756422996521, 'learning_rate': 0.0001645662752999484, 'epoch': 0.35}
{'loss': 0.8159, 'grad_norm': 1.1814045906066895, 'learning_rate': 0.00016452525759116945, 'epoch': 0.35}
{'loss': 1.1649, 'grad_norm': 1.2463738918304443, 'learning_rate': 0.00016448422127361706, 'epoch': 0.35}
{'loss': 0.9387, 'grad_norm': 1.5140364170074463, 'learning_rate': 0.00016444316635912584, 'epoch': 0.35}
{'loss': 1.0759, 'grad_norm': 1.0709973573684692, 'learning_rate': 0.00016440209285953586, 'epoch': 0.35}
{'loss': 0.7792, 'grad_norm': 1.0282760858535767, 'learning_rate': 0.00016436100078669248, 'epoch': 0.35}
{'loss': 1.1106, 'grad_norm': 1.194273591041565, 'learning_rate': 0.00016431989015244647, 'epoch': 0.35}
{'loss': 1.0016, 'grad_norm': 1.3236123323440552, 'learning_rate': 0.00016427876096865394, 'epoch': 0.35}
{'loss': 1.052, 'grad_norm': 1.3344988822937012, 'learning_rate': 0.00016423761324717634, 'epoch': 0.35}
{'loss': 1.0699, 'grad_norm': 1.0191571712493896, 'learning_rate': 0.00016419644699988052, 'epoch': 0.35}
{'loss': 1.0258, 'grad_norm': 1.0851014852523804, 'learning_rate': 0.0001641552622386386, 'epoch': 0.35}
{'loss': 0.9546, 'grad_norm': 1.0629791021347046, 'learning_rate': 0.00016411405897532802, 'epoch': 0.35}
{'loss': 0.9856, 'grad_norm': 1.3366868495941162, 'learning_rate': 0.00016407283722183165, 'epoch': 0.35}
{'loss': 1.1402, 'grad_norm': 1.6749036312103271, 'learning_rate': 0.00016403159699003767, 'epoch': 0.35}
{'loss': 1.1436, 'grad_norm': 0.9279077053070068, 'learning_rate': 0.0001639903382918395, 'epoch': 0.35}
{'loss': 0.8587, 'grad_norm': 0.8811197876930237, 'learning_rate': 0.00016394906113913603, 'epoch': 0.35}
{'loss': 1.1272, 'grad_norm': 1.0848191976547241, 'learning_rate': 0.0001639077655438313, 'epoch': 0.35}
{'loss': 1.0793, 'grad_norm': 1.020578384399414, 'learning_rate': 0.0001638664515178348, 'epoch': 0.35}
{'loss': 1.3281, 'grad_norm': 1.9655532836914062, 'learning_rate': 0.00016382511907306132, 'epoch': 0.35}
{'loss': 1.0285, 'grad_norm': 1.2101733684539795, 'learning_rate': 0.00016378376822143094, 'epoch': 0.35}
{'loss': 0.9787, 'grad_norm': 1.1642130613327026, 'learning_rate': 0.000163742398974869, 'epoch': 0.35}
{'loss': 1.2301, 'grad_norm': 0.9358528852462769, 'learning_rate': 0.0001637010113453062, 'epoch': 0.35}
{'loss': 1.1107, 'grad_norm': 0.8953216671943665, 'learning_rate': 0.00016365960534467857, 'epoch': 0.35}
{'loss': 1.2583, 'grad_norm': 0.9863710999488831, 'learning_rate': 0.0001636181809849274, 'epoch': 0.35}
{'loss': 0.9932, 'grad_norm': 0.9307188987731934, 'learning_rate': 0.00016357673827799922, 'epoch': 0.35}
{'loss': 1.1363, 'grad_norm': 1.082085371017456, 'learning_rate': 0.00016353527723584596, 'epoch': 0.35}
{'loss': 1.2257, 'grad_norm': 1.3064069747924805, 'learning_rate': 0.00016349379787042477, 'epoch': 0.35}
{'loss': 1.1351, 'grad_norm': 1.405901312828064, 'learning_rate': 0.0001634523001936981, 'epoch': 0.35}
{'loss': 1.3386, 'grad_norm': 0.9360949397087097, 'learning_rate': 0.00016341078421763368, 'epoch': 0.35}
{'loss': 0.9553, 'grad_norm': 1.2129982709884644, 'learning_rate': 0.00016336924995420454, 'epoch': 0.35}
{'loss': 0.9301, 'grad_norm': 1.4726426601409912, 'learning_rate': 0.00016332769741538892, 'epoch': 0.35}
{'loss': 0.7399, 'grad_norm': 1.1724315881729126, 'learning_rate': 0.00016328612661317034, 'epoch': 0.35}
{'loss': 0.979, 'grad_norm': 1.3409972190856934, 'learning_rate': 0.00016324453755953773, 'epoch': 0.35}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7825, 'grad_norm': 1.3122683763504028, 'learning_rate': 0.0001632029302664851, 'epoch': 0.35}
{'loss': 1.0081, 'grad_norm': 1.3502122163772583, 'learning_rate': 0.00016316130474601178, 'epoch': 0.35}
{'loss': 0.7477, 'grad_norm': 1.5211241245269775, 'learning_rate': 0.00016311966101012243, 'epoch': 0.35}
{'loss': 1.091, 'grad_norm': 1.0104773044586182, 'learning_rate': 0.00016307799907082683, 'epoch': 0.35}
{'loss': 0.871, 'grad_norm': 1.4569847583770752, 'learning_rate': 0.00016303631894014014, 'epoch': 0.35}
{'loss': 1.1939, 'grad_norm': 1.0180644989013672, 'learning_rate': 0.00016299462063008272, 'epoch': 0.35}
{'loss': 1.0819, 'grad_norm': 1.1115436553955078, 'learning_rate': 0.0001629529041526801, 'epoch': 0.35}
{'loss': 0.7177, 'grad_norm': 1.2226758003234863, 'learning_rate': 0.00016291116951996313, 'epoch': 0.36}
{'loss': 0.5469, 'grad_norm': 1.2906924486160278, 'learning_rate': 0.00016286941674396787, 'epoch': 0.36}
{'loss': 1.0949, 'grad_norm': 1.059370994567871, 'learning_rate': 0.00016282764583673567, 'epoch': 0.36}
{'loss': 0.7518, 'grad_norm': 1.2259529829025269, 'learning_rate': 0.00016278585681031303, 'epoch': 0.36}
{'loss': 1.1224, 'grad_norm': 1.180030107498169, 'learning_rate': 0.00016274404967675168, 'epoch': 0.36}
{'loss': 1.2807, 'grad_norm': 1.1325647830963135, 'learning_rate': 0.00016270222444810862, 'epoch': 0.36}
{'loss': 1.1158, 'grad_norm': 1.160499095916748, 'learning_rate': 0.00016266038113644607, 'epoch': 0.36}
{'loss': 0.8902, 'grad_norm': 1.8040876388549805, 'learning_rate': 0.00016261851975383137, 'epoch': 0.36}
{'loss': 0.9286, 'grad_norm': 1.0569396018981934, 'learning_rate': 0.00016257664031233722, 'epoch': 0.36}
{'loss': 0.875, 'grad_norm': 1.949346661567688, 'learning_rate': 0.00016253474282404139, 'epoch': 0.36}
{'loss': 1.0961, 'grad_norm': 1.1829273700714111, 'learning_rate': 0.00016249282730102692, 'epoch': 0.36}
{'loss': 0.9167, 'grad_norm': 0.9557918906211853, 'learning_rate': 0.00016245089375538206, 'epoch': 0.36}
{'loss': 1.2135, 'grad_norm': 1.1109150648117065, 'learning_rate': 0.0001624089421992003, 'epoch': 0.36}
{'loss': 1.0066, 'grad_norm': 1.226030707359314, 'learning_rate': 0.00016236697264458014, 'epoch': 0.36}
{'loss': 0.9013, 'grad_norm': 1.0168575048446655, 'learning_rate': 0.00016232498510362548, 'epoch': 0.36}
{'loss': 0.8944, 'grad_norm': 1.078552484512329, 'learning_rate': 0.00016228297958844533, 'epoch': 0.36}
{'loss': 1.0213, 'grad_norm': 0.9524631500244141, 'learning_rate': 0.00016224095611115384, 'epoch': 0.36}
{'loss': 0.7951, 'grad_norm': 1.21684730052948, 'learning_rate': 0.0001621989146838704, 'epoch': 0.36}
{'loss': 1.0239, 'grad_norm': 1.0728678703308105, 'learning_rate': 0.00016215685531871955, 'epoch': 0.36}
{'loss': 1.1412, 'grad_norm': 1.0381213426589966, 'learning_rate': 0.00016211477802783103, 'epoch': 0.36}
{'loss': 1.1427, 'grad_norm': 0.9793365597724915, 'learning_rate': 0.00016207268282333973, 'epoch': 0.36}
{'loss': 0.8513, 'grad_norm': 1.1974443197250366, 'learning_rate': 0.00016203056971738567, 'epoch': 0.36}
{'loss': 0.8296, 'grad_norm': 1.149040699005127, 'learning_rate': 0.00016198843872211404, 'epoch': 0.36}
{'loss': 0.7892, 'grad_norm': 1.2330584526062012, 'learning_rate': 0.00016194628984967526, 'epoch': 0.36}
{'loss': 0.8269, 'grad_norm': 1.046494722366333, 'learning_rate': 0.00016190412311222488, 'epoch': 0.36}
{'loss': 0.7717, 'grad_norm': 1.208796501159668, 'learning_rate': 0.00016186193852192355, 'epoch': 0.36}
{'loss': 0.9628, 'grad_norm': 1.7549293041229248, 'learning_rate': 0.00016181973609093715, 'epoch': 0.36}
{'loss': 0.8221, 'grad_norm': 1.831950306892395, 'learning_rate': 0.00016177751583143659, 'epoch': 0.36}
{'loss': 1.0665, 'grad_norm': 1.2412420511245728, 'learning_rate': 0.000161735277755598, 'epoch': 0.36}
{'loss': 0.8555, 'grad_norm': 0.992135763168335, 'learning_rate': 0.0001616930218756027, 'epoch': 0.36}
{'loss': 0.8893, 'grad_norm': 1.084380865097046, 'learning_rate': 0.00016165074820363704, 'epoch': 0.36}
{'loss': 1.1835, 'grad_norm': 1.0512335300445557, 'learning_rate': 0.00016160845675189254, 'epoch': 0.36}
{'loss': 0.8555, 'grad_norm': 1.0839625597000122, 'learning_rate': 0.0001615661475325658, 'epoch': 0.36}
{'loss': 1.0373, 'grad_norm': 1.0290483236312866, 'learning_rate': 0.00016152382055785873, 'epoch': 0.36}
{'loss': 1.1215, 'grad_norm': 0.9315683245658875, 'learning_rate': 0.00016148147583997812, 'epoch': 0.36}
{'loss': 0.9478, 'grad_norm': 1.066544532775879, 'learning_rate': 0.000161439113391136, 'epoch': 0.36}
{'loss': 0.9527, 'grad_norm': 1.147666335105896, 'learning_rate': 0.00016139673322354954, 'epoch': 0.36}
{'loss': 1.0363, 'grad_norm': 1.2243491411209106, 'learning_rate': 0.0001613543353494409, 'epoch': 0.36}
{'loss': 0.8866, 'grad_norm': 1.7088356018066406, 'learning_rate': 0.00016131191978103747, 'epoch': 0.36}
{'loss': 1.1259, 'grad_norm': 1.2013150453567505, 'learning_rate': 0.0001612694865305717, 'epoch': 0.36}
{'loss': 0.8952, 'grad_norm': 1.248759388923645, 'learning_rate': 0.00016122703561028114, 'epoch': 0.36}
{'loss': 0.8804, 'grad_norm': 1.9924265146255493, 'learning_rate': 0.0001611845670324084, 'epoch': 0.36}
{'loss': 1.0404, 'grad_norm': 0.977907657623291, 'learning_rate': 0.00016114208080920123, 'epoch': 0.36}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2934, 'grad_norm': 1.145400881767273, 'learning_rate': 0.00016109957695291244, 'epoch': 0.36}
{'loss': 0.9991, 'grad_norm': 1.0679577589035034, 'learning_rate': 0.00016105705547579997, 'epoch': 0.36}
{'loss': 1.0012, 'grad_norm': 1.0810240507125854, 'learning_rate': 0.0001610145163901268, 'epoch': 0.36}
{'loss': 0.8949, 'grad_norm': 1.254109263420105, 'learning_rate': 0.00016097195970816094, 'epoch': 0.36}
{'loss': 1.1795, 'grad_norm': 1.4664844274520874, 'learning_rate': 0.0001609293854421756, 'epoch': 0.36}
{'loss': 1.179, 'grad_norm': 1.3346939086914062, 'learning_rate': 0.00016088679360444895, 'epoch': 0.36}
{'loss': 0.8253, 'grad_norm': 1.0913251638412476, 'learning_rate': 0.00016084418420726432, 'epoch': 0.36}
{'loss': 1.2211, 'grad_norm': 11.493370056152344, 'learning_rate': 0.00016080155726291005, 'epoch': 0.36}
{'loss': 1.0369, 'grad_norm': 1.164167046546936, 'learning_rate': 0.0001607589127836795, 'epoch': 0.36}
{'loss': 0.9398, 'grad_norm': 1.7255375385284424, 'learning_rate': 0.00016071625078187114, 'epoch': 0.36}
{'loss': 1.22, 'grad_norm': 1.0107048749923706, 'learning_rate': 0.0001606735712697885, 'epoch': 0.36}
{'loss': 1.2997, 'grad_norm': 0.9862337112426758, 'learning_rate': 0.00016063087425974016, 'epoch': 0.36}
{'loss': 0.958, 'grad_norm': 1.5294597148895264, 'learning_rate': 0.00016058815976403973, 'epoch': 0.36}
{'loss': 1.0441, 'grad_norm': 1.251440405845642, 'learning_rate': 0.0001605454277950058, 'epoch': 0.36}
{'loss': 0.9946, 'grad_norm': 1.4429218769073486, 'learning_rate': 0.0001605026783649622, 'epoch': 0.36}
{'loss': 0.9092, 'grad_norm': 1.0646004676818848, 'learning_rate': 0.0001604599114862375, 'epoch': 0.36}
{'loss': 1.1424, 'grad_norm': 1.1953786611557007, 'learning_rate': 0.00016041712717116554, 'epoch': 0.36}
{'loss': 0.9987, 'grad_norm': 1.0793495178222656, 'learning_rate': 0.00016037432543208516, 'epoch': 0.36}
{'loss': 0.995, 'grad_norm': 1.740183711051941, 'learning_rate': 0.00016033150628134011, 'epoch': 0.36}
{'loss': 0.7501, 'grad_norm': 1.2099124193191528, 'learning_rate': 0.00016028866973127922, 'epoch': 0.36}
{'loss': 0.8198, 'grad_norm': 1.222029209136963, 'learning_rate': 0.0001602458157942564, 'epoch': 0.36}
{'loss': 1.2322, 'grad_norm': 1.2422887086868286, 'learning_rate': 0.00016020294448263045, 'epoch': 0.36}
{'loss': 1.1079, 'grad_norm': 1.401548147201538, 'learning_rate': 0.0001601600558087653, 'epoch': 0.37}
{'loss': 1.0233, 'grad_norm': 1.5533647537231445, 'learning_rate': 0.0001601171497850298, 'epoch': 0.37}
{'loss': 1.1275, 'grad_norm': 1.190543293952942, 'learning_rate': 0.0001600742264237979, 'epoch': 0.37}
{'loss': 1.0627, 'grad_norm': 0.9700354933738708, 'learning_rate': 0.00016003128573744845, 'epoch': 0.37}
{'loss': 0.9404, 'grad_norm': 1.0776208639144897, 'learning_rate': 0.00015998832773836532, 'epoch': 0.37}
{'loss': 0.8977, 'grad_norm': 1.2085379362106323, 'learning_rate': 0.0001599453524389374, 'epoch': 0.37}
{'loss': 0.9339, 'grad_norm': 1.1822675466537476, 'learning_rate': 0.0001599023598515586, 'epoch': 0.37}
{'loss': 0.821, 'grad_norm': 1.0573370456695557, 'learning_rate': 0.00015985934998862772, 'epoch': 0.37}
{'loss': 1.036, 'grad_norm': 1.240495204925537, 'learning_rate': 0.00015981632286254864, 'epoch': 0.37}
{'loss': 1.2583, 'grad_norm': 1.407694935798645, 'learning_rate': 0.00015977327848573015, 'epoch': 0.37}
{'loss': 0.8353, 'grad_norm': 1.1955081224441528, 'learning_rate': 0.00015973021687058604, 'epoch': 0.37}
{'loss': 0.7372, 'grad_norm': 1.2423408031463623, 'learning_rate': 0.0001596871380295351, 'epoch': 0.37}
{'loss': 0.9045, 'grad_norm': 1.6002752780914307, 'learning_rate': 0.00015964404197500102, 'epoch': 0.37}
{'loss': 1.1056, 'grad_norm': 1.431287407875061, 'learning_rate': 0.00015960092871941255, 'epoch': 0.37}
{'loss': 0.9332, 'grad_norm': 1.1235462427139282, 'learning_rate': 0.00015955779827520327, 'epoch': 0.37}
{'loss': 0.7655, 'grad_norm': 1.1396092176437378, 'learning_rate': 0.00015951465065481183, 'epoch': 0.37}
{'loss': 0.6149, 'grad_norm': 1.1589598655700684, 'learning_rate': 0.00015947148587068183, 'epoch': 0.37}
{'loss': 0.8988, 'grad_norm': 1.1175910234451294, 'learning_rate': 0.00015942830393526176, 'epoch': 0.37}
{'loss': 0.969, 'grad_norm': 1.1675249338150024, 'learning_rate': 0.00015938510486100506, 'epoch': 0.37}
{'loss': 1.0597, 'grad_norm': 1.7688573598861694, 'learning_rate': 0.00015934188866037016, 'epoch': 0.37}
{'loss': 0.9933, 'grad_norm': 1.2970542907714844, 'learning_rate': 0.00015929865534582038, 'epoch': 0.37}
{'loss': 1.0149, 'grad_norm': 1.1247491836547852, 'learning_rate': 0.00015925540492982404, 'epoch': 0.37}
{'loss': 1.0764, 'grad_norm': 1.2206811904907227, 'learning_rate': 0.0001592121374248543, 'epoch': 0.37}
{'loss': 0.8955, 'grad_norm': 1.4794121980667114, 'learning_rate': 0.00015916885284338937, 'epoch': 0.37}
{'loss': 0.9846, 'grad_norm': 1.3563307523727417, 'learning_rate': 0.00015912555119791222, 'epoch': 0.37}
{'loss': 0.8131, 'grad_norm': 1.1914373636245728, 'learning_rate': 0.00015908223250091092, 'epoch': 0.37}
{'loss': 1.003, 'grad_norm': 1.1181484460830688, 'learning_rate': 0.00015903889676487833, 'epoch': 0.37}
{'loss': 1.045, 'grad_norm': 0.994155764579773, 'learning_rate': 0.00015899554400231232, 'epoch': 0.37}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9786, 'grad_norm': 1.2587767839431763, 'learning_rate': 0.00015895217422571553, 'epoch': 0.37}
{'loss': 1.1671, 'grad_norm': 1.1921106576919556, 'learning_rate': 0.00015890878744759563, 'epoch': 0.37}
{'loss': 0.9919, 'grad_norm': 1.1279613971710205, 'learning_rate': 0.00015886538368046522, 'epoch': 0.37}
{'loss': 1.3172, 'grad_norm': 1.1325315237045288, 'learning_rate': 0.00015882196293684166, 'epoch': 0.37}
{'loss': 0.9993, 'grad_norm': 0.9413716197013855, 'learning_rate': 0.00015877852522924732, 'epoch': 0.37}
{'loss': 0.8711, 'grad_norm': 1.0245566368103027, 'learning_rate': 0.00015873507057020942, 'epoch': 0.37}
{'loss': 0.8247, 'grad_norm': 1.3197592496871948, 'learning_rate': 0.00015869159897226006, 'epoch': 0.37}
{'loss': 0.9825, 'grad_norm': 1.6961811780929565, 'learning_rate': 0.00015864811044793625, 'epoch': 0.37}
{'loss': 1.2759, 'grad_norm': 0.9673618078231812, 'learning_rate': 0.0001586046050097799, 'epoch': 0.37}
{'loss': 0.8401, 'grad_norm': 1.329690933227539, 'learning_rate': 0.00015856108267033769, 'epoch': 0.37}
{'loss': 0.8927, 'grad_norm': 1.1678593158721924, 'learning_rate': 0.00015851754344216133, 'epoch': 0.37}
{'loss': 1.3841, 'grad_norm': 1.1163095235824585, 'learning_rate': 0.00015847398733780728, 'epoch': 0.37}
{'loss': 0.5954, 'grad_norm': 1.1408755779266357, 'learning_rate': 0.00015843041436983696, 'epoch': 0.37}
{'loss': 0.8023, 'grad_norm': 1.2013332843780518, 'learning_rate': 0.00015838682455081657, 'epoch': 0.37}
{'loss': 0.9211, 'grad_norm': 1.148648738861084, 'learning_rate': 0.00015834321789331716, 'epoch': 0.37}
{'loss': 1.1703, 'grad_norm': 0.9345408082008362, 'learning_rate': 0.00015829959440991477, 'epoch': 0.37}
{'loss': 0.9459, 'grad_norm': 1.1913268566131592, 'learning_rate': 0.00015825595411319014, 'epoch': 0.37}
{'loss': 0.8791, 'grad_norm': 1.3402763605117798, 'learning_rate': 0.00015821229701572896, 'epoch': 0.37}
{'loss': 0.7658, 'grad_norm': 1.30595064163208, 'learning_rate': 0.00015816862313012167, 'epoch': 0.37}
{'loss': 1.0345, 'grad_norm': 1.0342586040496826, 'learning_rate': 0.00015812493246896366, 'epoch': 0.37}
{'loss': 0.9317, 'grad_norm': 1.0061532258987427, 'learning_rate': 0.0001580812250448551, 'epoch': 0.37}
{'loss': 0.9861, 'grad_norm': 1.025063157081604, 'learning_rate': 0.00015803750087040099, 'epoch': 0.37}
{'loss': 1.0846, 'grad_norm': 1.2197504043579102, 'learning_rate': 0.00015799375995821118, 'epoch': 0.37}
{'loss': 1.1481, 'grad_norm': 1.13002610206604, 'learning_rate': 0.00015795000232090033, 'epoch': 0.37}
{'loss': 0.7478, 'grad_norm': 1.1911022663116455, 'learning_rate': 0.0001579062279710879, 'epoch': 0.37}
{'loss': 1.0338, 'grad_norm': 1.5892516374588013, 'learning_rate': 0.00015786243692139827, 'epoch': 0.37}
{'loss': 1.2055, 'grad_norm': 1.044183373451233, 'learning_rate': 0.0001578186291844605, 'epoch': 0.37}
{'loss': 0.9627, 'grad_norm': 0.9621599912643433, 'learning_rate': 0.0001577748047729086, 'epoch': 0.37}
{'loss': 0.9242, 'grad_norm': 1.3861182928085327, 'learning_rate': 0.00015773096369938125, 'epoch': 0.37}
{'loss': 1.1491, 'grad_norm': 1.0059212446212769, 'learning_rate': 0.00015768710597652204, 'epoch': 0.37}
{'loss': 1.0589, 'grad_norm': 1.3944844007492065, 'learning_rate': 0.00015764323161697935, 'epoch': 0.37}
{'loss': 1.0785, 'grad_norm': 1.2287744283676147, 'learning_rate': 0.00015759934063340627, 'epoch': 0.37}
{'loss': 1.1079, 'grad_norm': 1.0895423889160156, 'learning_rate': 0.0001575554330384608, 'epoch': 0.37}
{'loss': 0.9477, 'grad_norm': 1.3326857089996338, 'learning_rate': 0.0001575115088448057, 'epoch': 0.37}
{'loss': 0.8334, 'grad_norm': 1.712597370147705, 'learning_rate': 0.00015746756806510838, 'epoch': 0.37}
{'loss': 1.1536, 'grad_norm': 1.2173610925674438, 'learning_rate': 0.00015742361071204128, 'epoch': 0.37}
{'loss': 1.0076, 'grad_norm': 1.5580801963806152, 'learning_rate': 0.00015737963679828142, 'epoch': 0.37}
{'loss': 0.6929, 'grad_norm': 1.5165945291519165, 'learning_rate': 0.0001573356463365107, 'epoch': 0.38}
{'loss': 1.0859, 'grad_norm': 1.118748426437378, 'learning_rate': 0.00015729163933941574, 'epoch': 0.38}
{'loss': 1.0683, 'grad_norm': 1.104012370109558, 'learning_rate': 0.00015724761581968792, 'epoch': 0.38}
{'loss': 0.9949, 'grad_norm': 0.9388037919998169, 'learning_rate': 0.00015720357579002345, 'epoch': 0.38}
{'loss': 0.8878, 'grad_norm': 1.9134578704833984, 'learning_rate': 0.00015715951926312325, 'epoch': 0.38}
{'loss': 1.0718, 'grad_norm': 1.1292492151260376, 'learning_rate': 0.000157115446251693, 'epoch': 0.38}
{'loss': 1.0742, 'grad_norm': 1.0991259813308716, 'learning_rate': 0.0001570713567684432, 'epoch': 0.38}
{'loss': 1.0676, 'grad_norm': 1.0864514112472534, 'learning_rate': 0.00015702725082608894, 'epoch': 0.38}
{'loss': 1.3407, 'grad_norm': 1.5102412700653076, 'learning_rate': 0.00015698312843735024, 'epoch': 0.38}
{'loss': 1.0842, 'grad_norm': 1.1380070447921753, 'learning_rate': 0.00015693898961495177, 'epoch': 0.38}
{'loss': 1.3471, 'grad_norm': 1.0226508378982544, 'learning_rate': 0.00015689483437162293, 'epoch': 0.38}
{'loss': 1.2364, 'grad_norm': 1.29563307762146, 'learning_rate': 0.00015685066272009792, 'epoch': 0.38}
{'loss': 1.1015, 'grad_norm': 1.7694134712219238, 'learning_rate': 0.00015680647467311557, 'epoch': 0.38}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1134, 'grad_norm': 2.3610827922821045, 'learning_rate': 0.0001567622702434196, 'epoch': 0.38}
{'loss': 0.9382, 'grad_norm': 1.6091729402542114, 'learning_rate': 0.00015671804944375825, 'epoch': 0.38}
{'loss': 0.9822, 'grad_norm': 1.0323212146759033, 'learning_rate': 0.00015667381228688461, 'epoch': 0.38}
{'loss': 0.9718, 'grad_norm': 0.9853889346122742, 'learning_rate': 0.00015662955878555652, 'epoch': 0.38}
{'loss': 0.9337, 'grad_norm': 1.4448251724243164, 'learning_rate': 0.00015658528895253642, 'epoch': 0.38}
{'loss': 0.9597, 'grad_norm': 1.4964275360107422, 'learning_rate': 0.00015654100280059155, 'epoch': 0.38}
{'loss': 1.112, 'grad_norm': 1.3811554908752441, 'learning_rate': 0.0001564967003424938, 'epoch': 0.38}
{'loss': 0.9774, 'grad_norm': 1.1511589288711548, 'learning_rate': 0.0001564523815910198, 'epoch': 0.38}
{'loss': 1.1704, 'grad_norm': 1.0907493829727173, 'learning_rate': 0.00015640804655895084, 'epoch': 0.38}
{'loss': 1.0161, 'grad_norm': 1.0719510316848755, 'learning_rate': 0.00015636369525907296, 'epoch': 0.38}
{'loss': 0.877, 'grad_norm': 1.1060758829116821, 'learning_rate': 0.00015631932770417684, 'epoch': 0.38}
{'loss': 0.7621, 'grad_norm': 1.4064680337905884, 'learning_rate': 0.0001562749439070579, 'epoch': 0.38}
{'loss': 1.0839, 'grad_norm': 1.4937951564788818, 'learning_rate': 0.00015623054388051616, 'epoch': 0.38}
{'loss': 1.0614, 'grad_norm': 1.1996804475784302, 'learning_rate': 0.00015618612763735644, 'epoch': 0.38}
{'loss': 0.9244, 'grad_norm': 1.2497243881225586, 'learning_rate': 0.0001561416951903881, 'epoch': 0.38}
{'loss': 0.8056, 'grad_norm': 1.3305388689041138, 'learning_rate': 0.0001560972465524253, 'epoch': 0.38}
{'loss': 1.1901, 'grad_norm': 1.7907845973968506, 'learning_rate': 0.0001560527817362868, 'epoch': 0.38}
{'loss': 1.1021, 'grad_norm': 1.334926962852478, 'learning_rate': 0.00015600830075479603, 'epoch': 0.38}
{'loss': 1.1382, 'grad_norm': 2.0432426929473877, 'learning_rate': 0.00015596380362078107, 'epoch': 0.38}
{'loss': 0.975, 'grad_norm': 1.120794415473938, 'learning_rate': 0.0001559192903470747, 'epoch': 0.38}
{'loss': 1.0472, 'grad_norm': 1.507521390914917, 'learning_rate': 0.00015587476094651434, 'epoch': 0.38}
{'loss': 1.1429, 'grad_norm': 1.0514494180679321, 'learning_rate': 0.00015583021543194206, 'epoch': 0.38}
{'loss': 1.3257, 'grad_norm': 1.1383956670761108, 'learning_rate': 0.00015578565381620455, 'epoch': 0.38}
{'loss': 0.8376, 'grad_norm': 1.2027801275253296, 'learning_rate': 0.00015574107611215319, 'epoch': 0.38}
{'loss': 0.8665, 'grad_norm': 0.81988126039505, 'learning_rate': 0.00015569648233264394, 'epoch': 0.38}
{'loss': 0.7828, 'grad_norm': 1.2693425416946411, 'learning_rate': 0.00015565187249053748, 'epoch': 0.38}
{'loss': 1.3411, 'grad_norm': 1.3716461658477783, 'learning_rate': 0.00015560724659869902, 'epoch': 0.38}
{'loss': 0.6632, 'grad_norm': 1.155055284500122, 'learning_rate': 0.0001555626046699985, 'epoch': 0.38}
{'loss': 0.8088, 'grad_norm': 1.1151008605957031, 'learning_rate': 0.0001555179467173104, 'epoch': 0.38}
{'loss': 0.8683, 'grad_norm': 1.2700060606002808, 'learning_rate': 0.0001554732727535139, 'epoch': 0.38}
{'loss': 0.9772, 'grad_norm': 1.156974196434021, 'learning_rate': 0.0001554285827914927, 'epoch': 0.38}
{'loss': 1.0639, 'grad_norm': 1.096773386001587, 'learning_rate': 0.00015538387684413524, 'epoch': 0.38}
{'loss': 0.804, 'grad_norm': 1.1978169679641724, 'learning_rate': 0.00015533915492433443, 'epoch': 0.38}
{'loss': 1.2481, 'grad_norm': 1.1426740884780884, 'learning_rate': 0.0001552944170449879, 'epoch': 0.38}
{'loss': 1.1965, 'grad_norm': 1.1293787956237793, 'learning_rate': 0.00015524966321899778, 'epoch': 0.38}
{'loss': 1.1056, 'grad_norm': 1.4740723371505737, 'learning_rate': 0.00015520489345927096, 'epoch': 0.38}
{'loss': 1.0058, 'grad_norm': 1.0099543333053589, 'learning_rate': 0.00015516010777871876, 'epoch': 0.38}
{'loss': 0.8271, 'grad_norm': 1.330501914024353, 'learning_rate': 0.00015511530619025714, 'epoch': 0.38}
{'loss': 0.925, 'grad_norm': 1.314534068107605, 'learning_rate': 0.00015507048870680668, 'epoch': 0.38}
{'loss': 1.0293, 'grad_norm': 1.3833707571029663, 'learning_rate': 0.00015502565534129253, 'epoch': 0.38}
{'loss': 0.9272, 'grad_norm': 1.064718246459961, 'learning_rate': 0.00015498080610664444, 'epoch': 0.38}
{'loss': 0.5947, 'grad_norm': 1.247615098953247, 'learning_rate': 0.00015493594101579668, 'epoch': 0.38}
{'loss': 0.8178, 'grad_norm': 1.4337985515594482, 'learning_rate': 0.0001548910600816881, 'epoch': 0.38}
{'loss': 1.1082, 'grad_norm': 1.563045620918274, 'learning_rate': 0.00015484616331726224, 'epoch': 0.38}
{'loss': 1.0685, 'grad_norm': 1.150045394897461, 'learning_rate': 0.00015480125073546704, 'epoch': 0.38}
{'loss': 1.0292, 'grad_norm': 1.208126187324524, 'learning_rate': 0.00015475632234925504, 'epoch': 0.38}
{'loss': 0.7099, 'grad_norm': 1.3412799835205078, 'learning_rate': 0.00015471137817158343, 'epoch': 0.38}
{'loss': 0.9879, 'grad_norm': 1.0206469297409058, 'learning_rate': 0.00015466641821541388, 'epoch': 0.38}
{'loss': 1.2659, 'grad_norm': 2.2558863162994385, 'learning_rate': 0.00015462144249371265, 'epoch': 0.38}
{'loss': 1.2926, 'grad_norm': 1.3959662914276123, 'learning_rate': 0.00015457645101945046, 'epoch': 0.38}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6642, 'grad_norm': 1.19237220287323, 'learning_rate': 0.0001545314438056027, 'epoch': 0.38}
{'loss': 0.9782, 'grad_norm': 1.1037611961364746, 'learning_rate': 0.00015448642086514917, 'epoch': 0.38}
{'loss': 0.7361, 'grad_norm': 1.139155626296997, 'learning_rate': 0.00015444138221107432, 'epoch': 0.39}
{'loss': 1.2205, 'grad_norm': 1.377007007598877, 'learning_rate': 0.00015439632785636706, 'epoch': 0.39}
{'loss': 0.9008, 'grad_norm': 1.2966653108596802, 'learning_rate': 0.00015435125781402086, 'epoch': 0.39}
{'loss': 1.2442, 'grad_norm': 1.077389121055603, 'learning_rate': 0.0001543061720970337, 'epoch': 0.39}
{'loss': 0.8343, 'grad_norm': 1.296832799911499, 'learning_rate': 0.00015426107071840806, 'epoch': 0.39}
{'loss': 1.0168, 'grad_norm': 1.779342532157898, 'learning_rate': 0.00015421595369115098, 'epoch': 0.39}
{'loss': 0.8911, 'grad_norm': 0.9805841445922852, 'learning_rate': 0.000154170821028274, 'epoch': 0.39}
{'loss': 0.9996, 'grad_norm': 1.0573232173919678, 'learning_rate': 0.00015412567274279316, 'epoch': 0.39}
{'loss': 1.0102, 'grad_norm': 1.1362777948379517, 'learning_rate': 0.00015408050884772897, 'epoch': 0.39}
{'loss': 0.9653, 'grad_norm': 1.137574315071106, 'learning_rate': 0.00015403532935610653, 'epoch': 0.39}
{'loss': 1.0238, 'grad_norm': 1.370775580406189, 'learning_rate': 0.0001539901342809554, 'epoch': 0.39}
{'loss': 1.0749, 'grad_norm': 1.5263053178787231, 'learning_rate': 0.00015394492363530958, 'epoch': 0.39}
{'loss': 0.9999, 'grad_norm': 1.238481879234314, 'learning_rate': 0.0001538996974322076, 'epoch': 0.39}
{'loss': 1.0962, 'grad_norm': 1.5022469758987427, 'learning_rate': 0.0001538544556846925, 'epoch': 0.39}
{'loss': 0.8463, 'grad_norm': 1.2837674617767334, 'learning_rate': 0.00015380919840581175, 'epoch': 0.39}
{'loss': 0.7173, 'grad_norm': 1.0604223012924194, 'learning_rate': 0.0001537639256086174, 'epoch': 0.39}
{'loss': 1.0172, 'grad_norm': 1.4786419868469238, 'learning_rate': 0.00015371863730616586, 'epoch': 0.39}
{'loss': 1.093, 'grad_norm': 1.5351825952529907, 'learning_rate': 0.00015367333351151802, 'epoch': 0.39}
{'loss': 0.763, 'grad_norm': 1.2341967821121216, 'learning_rate': 0.00015362801423773932, 'epoch': 0.39}
{'loss': 0.8208, 'grad_norm': 1.2247711420059204, 'learning_rate': 0.00015358267949789966, 'epoch': 0.39}
{'loss': 0.9347, 'grad_norm': 1.3042153120040894, 'learning_rate': 0.0001535373293050733, 'epoch': 0.39}
{'loss': 0.7111, 'grad_norm': 1.415759801864624, 'learning_rate': 0.00015349196367233904, 'epoch': 0.39}
{'loss': 1.0437, 'grad_norm': 1.227230429649353, 'learning_rate': 0.0001534465826127801, 'epoch': 0.39}
{'loss': 0.7577, 'grad_norm': 1.6075326204299927, 'learning_rate': 0.00015340118613948417, 'epoch': 0.39}
{'loss': 1.0332, 'grad_norm': 1.789927363395691, 'learning_rate': 0.0001533557742655434, 'epoch': 0.39}
{'loss': 1.1236, 'grad_norm': 1.9248088598251343, 'learning_rate': 0.00015331034700405432, 'epoch': 0.39}
{'loss': 1.2382, 'grad_norm': 1.6741299629211426, 'learning_rate': 0.0001532649043681179, 'epoch': 0.39}
{'loss': 0.9039, 'grad_norm': 1.1748861074447632, 'learning_rate': 0.00015321944637083963, 'epoch': 0.39}
{'loss': 1.1428, 'grad_norm': 1.1134082078933716, 'learning_rate': 0.00015317397302532936, 'epoch': 0.39}
{'loss': 1.157, 'grad_norm': 1.1145868301391602, 'learning_rate': 0.00015312848434470138, 'epoch': 0.39}
{'loss': 1.1873, 'grad_norm': 1.1225919723510742, 'learning_rate': 0.00015308298034207441, 'epoch': 0.39}
{'loss': 0.9761, 'grad_norm': 1.0616904497146606, 'learning_rate': 0.00015303746103057162, 'epoch': 0.39}
{'loss': 0.9555, 'grad_norm': 1.3693580627441406, 'learning_rate': 0.0001529919264233205, 'epoch': 0.39}
{'loss': 1.0457, 'grad_norm': 1.047804594039917, 'learning_rate': 0.00015294637653345307, 'epoch': 0.39}
{'loss': 1.098, 'grad_norm': 1.0287601947784424, 'learning_rate': 0.00015290081137410563, 'epoch': 0.39}
{'loss': 0.8763, 'grad_norm': 1.234571099281311, 'learning_rate': 0.00015285523095841903, 'epoch': 0.39}
{'loss': 0.7625, 'grad_norm': 1.7549570798873901, 'learning_rate': 0.0001528096352995384, 'epoch': 0.39}
{'loss': 0.9385, 'grad_norm': 1.2040907144546509, 'learning_rate': 0.0001527640244106133, 'epoch': 0.39}
{'loss': 1.2701, 'grad_norm': 1.1114096641540527, 'learning_rate': 0.00015271839830479768, 'epoch': 0.39}
{'loss': 1.0431, 'grad_norm': 1.1501058340072632, 'learning_rate': 0.00015267275699524992, 'epoch': 0.39}
{'loss': 0.7605, 'grad_norm': 1.2467092275619507, 'learning_rate': 0.0001526271004951328, 'epoch': 0.39}
{'loss': 0.9274, 'grad_norm': 1.1485215425491333, 'learning_rate': 0.00015258142881761333, 'epoch': 0.39}
{'loss': 1.2854, 'grad_norm': 1.273912787437439, 'learning_rate': 0.00015253574197586308, 'epoch': 0.39}
{'loss': 0.9172, 'grad_norm': 1.0835621356964111, 'learning_rate': 0.00015249003998305785, 'epoch': 0.39}
{'loss': 0.9493, 'grad_norm': 1.0741126537322998, 'learning_rate': 0.00015244432285237795, 'epoch': 0.39}
{'loss': 1.2083, 'grad_norm': 1.1902011632919312, 'learning_rate': 0.00015239859059700794, 'epoch': 0.39}
{'loss': 0.7019, 'grad_norm': 1.082203984260559, 'learning_rate': 0.00015235284323013675, 'epoch': 0.39}
{'loss': 1.0898, 'grad_norm': 1.1506012678146362, 'learning_rate': 0.00015230708076495775, 'epoch': 0.39}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8359, 'grad_norm': 1.3402906656265259, 'learning_rate': 0.00015226130321466865, 'epoch': 0.39}
{'loss': 1.2601, 'grad_norm': 1.266065001487732, 'learning_rate': 0.00015221551059247136, 'epoch': 0.39}
{'loss': 1.2137, 'grad_norm': 1.024189829826355, 'learning_rate': 0.00015216970291157233, 'epoch': 0.39}
{'loss': 0.9671, 'grad_norm': 1.072171926498413, 'learning_rate': 0.00015212388018518227, 'epoch': 0.39}
{'loss': 0.9794, 'grad_norm': 1.0809001922607422, 'learning_rate': 0.00015207804242651626, 'epoch': 0.39}
{'loss': 0.7468, 'grad_norm': 1.356986165046692, 'learning_rate': 0.0001520321896487936, 'epoch': 0.39}
{'loss': 0.7339, 'grad_norm': 1.0335496664047241, 'learning_rate': 0.00015198632186523808, 'epoch': 0.39}
{'loss': 1.0151, 'grad_norm': 1.2903833389282227, 'learning_rate': 0.00015194043908907775, 'epoch': 0.39}
{'loss': 0.9794, 'grad_norm': 1.0783432722091675, 'learning_rate': 0.00015189454133354493, 'epoch': 0.39}
{'loss': 1.0891, 'grad_norm': 1.418774127960205, 'learning_rate': 0.00015184862861187637, 'epoch': 0.39}
{'loss': 1.032, 'grad_norm': 1.0456387996673584, 'learning_rate': 0.00015180270093731303, 'epoch': 0.39}
{'loss': 0.7571, 'grad_norm': 1.2988736629486084, 'learning_rate': 0.00015175675832310027, 'epoch': 0.39}
{'loss': 0.8937, 'grad_norm': 1.5870437622070312, 'learning_rate': 0.0001517108007824877, 'epoch': 0.39}
{'loss': 0.8608, 'grad_norm': 1.4856493473052979, 'learning_rate': 0.00015166482832872923, 'epoch': 0.39}
{'loss': 1.1995, 'grad_norm': 1.183405876159668, 'learning_rate': 0.00015161884097508318, 'epoch': 0.39}
{'loss': 0.8796, 'grad_norm': 0.9756478667259216, 'learning_rate': 0.00015157283873481195, 'epoch': 0.39}
{'loss': 1.0844, 'grad_norm': 1.295141577720642, 'learning_rate': 0.0001515268216211825, 'epoch': 0.39}
{'loss': 1.2364, 'grad_norm': 1.0814100503921509, 'learning_rate': 0.0001514807896474658, 'epoch': 0.4}
{'loss': 0.8102, 'grad_norm': 1.4403364658355713, 'learning_rate': 0.0001514347428269374, 'epoch': 0.4}
{'loss': 0.809, 'grad_norm': 1.273666501045227, 'learning_rate': 0.0001513886811728769, 'epoch': 0.4}
{'loss': 1.0335, 'grad_norm': 1.105229139328003, 'learning_rate': 0.00015134260469856824, 'epoch': 0.4}
{'loss': 0.9802, 'grad_norm': 0.9638938903808594, 'learning_rate': 0.00015129651341729968, 'epoch': 0.4}
{'loss': 0.8088, 'grad_norm': 1.0636547803878784, 'learning_rate': 0.00015125040734236374, 'epoch': 0.4}
{'loss': 0.925, 'grad_norm': 0.9426584839820862, 'learning_rate': 0.00015120428648705717, 'epoch': 0.4}
{'loss': 0.9065, 'grad_norm': 1.1479839086532593, 'learning_rate': 0.00015115815086468102, 'epoch': 0.4}
{'loss': 0.7446, 'grad_norm': 0.9667648077011108, 'learning_rate': 0.00015111200048854056, 'epoch': 0.4}
{'loss': 1.0949, 'grad_norm': 1.2319920063018799, 'learning_rate': 0.00015106583537194532, 'epoch': 0.4}
{'loss': 1.2477, 'grad_norm': 1.2071033716201782, 'learning_rate': 0.00015101965552820915, 'epoch': 0.4}
{'loss': 0.7524, 'grad_norm': 1.2242058515548706, 'learning_rate': 0.00015097346097065007, 'epoch': 0.4}
{'loss': 1.3728, 'grad_norm': 1.34328031539917, 'learning_rate': 0.00015092725171259036, 'epoch': 0.4}
{'loss': 1.0611, 'grad_norm': 1.0899670124053955, 'learning_rate': 0.00015088102776735656, 'epoch': 0.4}
{'loss': 1.2084, 'grad_norm': 1.1866626739501953, 'learning_rate': 0.0001508347891482794, 'epoch': 0.4}
{'loss': 0.729, 'grad_norm': 1.0056447982788086, 'learning_rate': 0.00015078853586869393, 'epoch': 0.4}
{'loss': 1.1221, 'grad_norm': 1.2225960493087769, 'learning_rate': 0.00015074226794193932, 'epoch': 0.4}
{'loss': 1.2806, 'grad_norm': 0.9713446497917175, 'learning_rate': 0.00015069598538135906, 'epoch': 0.4}
{'loss': 0.9757, 'grad_norm': 1.6544610261917114, 'learning_rate': 0.0001506496882003008, 'epoch': 0.4}
{'loss': 0.9911, 'grad_norm': 1.2454227209091187, 'learning_rate': 0.00015060337641211637, 'epoch': 0.4}
{'loss': 1.2719, 'grad_norm': 1.6968876123428345, 'learning_rate': 0.00015055705003016198, 'epoch': 0.4}
{'loss': 0.9665, 'grad_norm': 1.2045505046844482, 'learning_rate': 0.00015051070906779788, 'epoch': 0.4}
{'loss': 1.0882, 'grad_norm': 1.3894760608673096, 'learning_rate': 0.00015046435353838853, 'epoch': 0.4}
{'loss': 1.0073, 'grad_norm': 1.2304943799972534, 'learning_rate': 0.0001504179834553027, 'epoch': 0.4}
{'loss': 0.9799, 'grad_norm': 1.4664506912231445, 'learning_rate': 0.00015037159883191328, 'epoch': 0.4}
{'loss': 0.7743, 'grad_norm': 1.5213158130645752, 'learning_rate': 0.00015032519968159738, 'epoch': 0.4}
{'loss': 1.0116, 'grad_norm': 1.2935595512390137, 'learning_rate': 0.00015027878601773633, 'epoch': 0.4}
{'loss': 0.9901, 'grad_norm': 1.306444764137268, 'learning_rate': 0.00015023235785371553, 'epoch': 0.4}
{'loss': 1.0178, 'grad_norm': 1.1689414978027344, 'learning_rate': 0.00015018591520292468, 'epoch': 0.4}
{'loss': 0.9972, 'grad_norm': 1.2088488340377808, 'learning_rate': 0.00015013945807875758, 'epoch': 0.4}
{'loss': 0.9626, 'grad_norm': 1.2574926614761353, 'learning_rate': 0.0001500929864946123, 'epoch': 0.4}
{'loss': 0.9085, 'grad_norm': 1.249860405921936, 'learning_rate': 0.00015004650046389098, 'epoch': 0.4}
{'loss': 0.9739, 'grad_norm': 1.3646553754806519, 'learning_rate': 0.00015000000000000001, 'epoch': 0.4}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8527, 'grad_norm': 1.1605175733566284, 'learning_rate': 0.00014995348511634985, 'epoch': 0.4}
{'loss': 1.1372, 'grad_norm': 0.9833059906959534, 'learning_rate': 0.00014990695582635518, 'epoch': 0.4}
{'loss': 1.1481, 'grad_norm': 1.5577458143234253, 'learning_rate': 0.00014986041214343486, 'epoch': 0.4}
{'loss': 0.615, 'grad_norm': 0.9348617196083069, 'learning_rate': 0.0001498138540810118, 'epoch': 0.4}
{'loss': 0.8081, 'grad_norm': 1.2662113904953003, 'learning_rate': 0.00014976728165251317, 'epoch': 0.4}
{'loss': 1.0137, 'grad_norm': 1.1614933013916016, 'learning_rate': 0.00014972069487137024, 'epoch': 0.4}
{'loss': 1.0166, 'grad_norm': 1.267613172531128, 'learning_rate': 0.00014967409375101837, 'epoch': 0.4}
{'loss': 0.8708, 'grad_norm': 1.090551495552063, 'learning_rate': 0.00014962747830489712, 'epoch': 0.4}
{'loss': 0.7096, 'grad_norm': 1.0894595384597778, 'learning_rate': 0.0001495808485464502, 'epoch': 0.4}
{'loss': 1.1119, 'grad_norm': 1.0456700325012207, 'learning_rate': 0.00014953420448912533, 'epoch': 0.4}
{'loss': 0.8409, 'grad_norm': 1.0284963846206665, 'learning_rate': 0.00014948754614637448, 'epoch': 0.4}
{'loss': 0.8098, 'grad_norm': 1.2883906364440918, 'learning_rate': 0.0001494408735316537, 'epoch': 0.4}
{'loss': 1.1422, 'grad_norm': 0.9246758818626404, 'learning_rate': 0.0001493941866584231, 'epoch': 0.4}
{'loss': 0.9728, 'grad_norm': 1.20561683177948, 'learning_rate': 0.00014934748554014702, 'epoch': 0.4}
{'loss': 0.8316, 'grad_norm': 1.2911127805709839, 'learning_rate': 0.00014930077019029375, 'epoch': 0.4}
{'loss': 0.8883, 'grad_norm': 1.5927890539169312, 'learning_rate': 0.00014925404062233585, 'epoch': 0.4}
{'loss': 1.0967, 'grad_norm': 1.4102823734283447, 'learning_rate': 0.00014920729684974986, 'epoch': 0.4}
{'loss': 0.9554, 'grad_norm': 1.067626953125, 'learning_rate': 0.0001491605388860165, 'epoch': 0.4}
{'loss': 0.8366, 'grad_norm': 1.1211057901382446, 'learning_rate': 0.00014911376674462047, 'epoch': 0.4}
{'loss': 0.9058, 'grad_norm': 1.0437450408935547, 'learning_rate': 0.00014906698043905066, 'epoch': 0.4}
{'loss': 1.1625, 'grad_norm': 1.807887077331543, 'learning_rate': 0.0001490201799828001, 'epoch': 0.4}
{'loss': 1.2658, 'grad_norm': 1.0198918581008911, 'learning_rate': 0.0001489733653893657, 'epoch': 0.4}
{'loss': 0.9222, 'grad_norm': 0.9336493015289307, 'learning_rate': 0.0001489265366722486, 'epoch': 0.4}
{'loss': 1.4615, 'grad_norm': 0.9566032886505127, 'learning_rate': 0.00014887969384495402, 'epoch': 0.4}
{'loss': 0.5847, 'grad_norm': 1.0863186120986938, 'learning_rate': 0.00014883283692099112, 'epoch': 0.4}
{'loss': 0.8605, 'grad_norm': 1.084298014640808, 'learning_rate': 0.0001487859659138733, 'epoch': 0.4}
{'loss': 1.0358, 'grad_norm': 0.9995376467704773, 'learning_rate': 0.00014873908083711784, 'epoch': 0.4}
{'loss': 1.0797, 'grad_norm': 1.1233177185058594, 'learning_rate': 0.00014869218170424626, 'epoch': 0.4}
{'loss': 0.917, 'grad_norm': 1.1121573448181152, 'learning_rate': 0.00014864526852878402, 'epoch': 0.4}
{'loss': 0.7523, 'grad_norm': 1.4850643873214722, 'learning_rate': 0.0001485983413242606, 'epoch': 0.4}
{'loss': 1.302, 'grad_norm': 1.1467993259429932, 'learning_rate': 0.00014855140010420966, 'epoch': 0.4}
{'loss': 1.1744, 'grad_norm': 1.3534923791885376, 'learning_rate': 0.00014850444488216877, 'epoch': 0.4}
{'loss': 0.9859, 'grad_norm': 1.2349399328231812, 'learning_rate': 0.00014845747567167962, 'epoch': 0.41}
{'loss': 0.9878, 'grad_norm': 1.4885470867156982, 'learning_rate': 0.00014841049248628783, 'epoch': 0.41}
{'loss': 0.9103, 'grad_norm': 1.051339864730835, 'learning_rate': 0.00014836349533954324, 'epoch': 0.41}
{'loss': 0.9188, 'grad_norm': 1.3606809377670288, 'learning_rate': 0.00014831648424499952, 'epoch': 0.41}
{'loss': 0.755, 'grad_norm': 1.1422357559204102, 'learning_rate': 0.00014826945921621448, 'epoch': 0.41}
{'loss': 1.0896, 'grad_norm': 1.1636921167373657, 'learning_rate': 0.00014822242026674989, 'epoch': 0.41}
{'loss': 1.1693, 'grad_norm': 1.0803552865982056, 'learning_rate': 0.00014817536741017152, 'epoch': 0.41}
{'loss': 1.1545, 'grad_norm': 1.4099091291427612, 'learning_rate': 0.00014812830066004927, 'epoch': 0.41}
{'loss': 0.6442, 'grad_norm': 1.4492892026901245, 'learning_rate': 0.00014808122002995692, 'epoch': 0.41}
{'loss': 0.791, 'grad_norm': 1.0816158056259155, 'learning_rate': 0.0001480341255334723, 'epoch': 0.41}
{'loss': 0.9767, 'grad_norm': 1.3327852487564087, 'learning_rate': 0.00014798701718417726, 'epoch': 0.41}
{'loss': 1.259, 'grad_norm': 1.0081182718276978, 'learning_rate': 0.00014793989499565756, 'epoch': 0.41}
{'loss': 0.9768, 'grad_norm': 1.365867018699646, 'learning_rate': 0.00014789275898150308, 'epoch': 0.41}
{'loss': 0.9751, 'grad_norm': 1.0169671773910522, 'learning_rate': 0.0001478456091553076, 'epoch': 0.41}
{'loss': 0.7231, 'grad_norm': 1.3073756694793701, 'learning_rate': 0.00014779844553066887, 'epoch': 0.41}
{'loss': 1.0827, 'grad_norm': 1.7336609363555908, 'learning_rate': 0.00014775126812118864, 'epoch': 0.41}
{'loss': 1.197, 'grad_norm': 1.126900553703308, 'learning_rate': 0.00014770407694047273, 'epoch': 0.41}
{'loss': 0.8443, 'grad_norm': 1.0322046279907227, 'learning_rate': 0.0001476568720021308, 'epoch': 0.41}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0115, 'grad_norm': 1.2264596223831177, 'learning_rate': 0.00014760965331977652, 'epoch': 0.41}
{'loss': 1.1343, 'grad_norm': 1.1291396617889404, 'learning_rate': 0.00014756242090702756, 'epoch': 0.41}
{'loss': 1.0767, 'grad_norm': 1.6873210668563843, 'learning_rate': 0.00014751517477750543, 'epoch': 0.41}
{'loss': 0.9353, 'grad_norm': 1.23250150680542, 'learning_rate': 0.00014746791494483583, 'epoch': 0.41}
{'loss': 0.8156, 'grad_norm': 1.4025661945343018, 'learning_rate': 0.00014742064142264817, 'epoch': 0.41}
{'loss': 1.1481, 'grad_norm': 1.3748042583465576, 'learning_rate': 0.00014737335422457593, 'epoch': 0.41}
{'loss': 1.2557, 'grad_norm': 1.6763403415679932, 'learning_rate': 0.00014732605336425651, 'epoch': 0.41}
{'loss': 0.9244, 'grad_norm': 1.3105601072311401, 'learning_rate': 0.00014727873885533123, 'epoch': 0.41}
{'loss': 0.8323, 'grad_norm': 1.2317402362823486, 'learning_rate': 0.0001472314107114454, 'epoch': 0.41}
{'loss': 0.8152, 'grad_norm': 1.328452229499817, 'learning_rate': 0.0001471840689462482, 'epoch': 0.41}
{'loss': 1.2439, 'grad_norm': 0.9264135360717773, 'learning_rate': 0.00014713671357339283, 'epoch': 0.41}
{'loss': 1.2197, 'grad_norm': 1.3631788492202759, 'learning_rate': 0.0001470893446065363, 'epoch': 0.41}
{'loss': 0.8827, 'grad_norm': 1.121475100517273, 'learning_rate': 0.00014704196205933955, 'epoch': 0.41}
{'loss': 1.1983, 'grad_norm': 1.3674194812774658, 'learning_rate': 0.00014699456594546756, 'epoch': 0.41}
{'loss': 1.0757, 'grad_norm': 0.9139437675476074, 'learning_rate': 0.00014694715627858908, 'epoch': 0.41}
{'loss': 1.0365, 'grad_norm': 1.1348867416381836, 'learning_rate': 0.00014689973307237687, 'epoch': 0.41}
{'loss': 0.9861, 'grad_norm': 1.0315927267074585, 'learning_rate': 0.00014685229634050755, 'epoch': 0.41}
{'loss': 1.0753, 'grad_norm': 1.1534503698349, 'learning_rate': 0.00014680484609666162, 'epoch': 0.41}
{'loss': 1.0777, 'grad_norm': 1.3433705568313599, 'learning_rate': 0.00014675738235452352, 'epoch': 0.41}
{'loss': 1.5843, 'grad_norm': 5.294741630554199, 'learning_rate': 0.00014670990512778156, 'epoch': 0.41}
{'loss': 0.795, 'grad_norm': 1.1758511066436768, 'learning_rate': 0.00014666241443012792, 'epoch': 0.41}
{'loss': 0.9554, 'grad_norm': 2.4394216537475586, 'learning_rate': 0.00014661491027525872, 'epoch': 0.41}
{'loss': 0.983, 'grad_norm': 1.2055317163467407, 'learning_rate': 0.00014656739267687396, 'epoch': 0.41}
{'loss': 1.0829, 'grad_norm': 0.9919903874397278, 'learning_rate': 0.00014651986164867744, 'epoch': 0.41}
{'loss': 1.0576, 'grad_norm': 1.2807751893997192, 'learning_rate': 0.00014647231720437686, 'epoch': 0.41}
{'loss': 1.2456, 'grad_norm': 1.0964113473892212, 'learning_rate': 0.00014642475935768386, 'epoch': 0.41}
{'loss': 1.0935, 'grad_norm': 1.7084882259368896, 'learning_rate': 0.00014637718812231384, 'epoch': 0.41}
{'loss': 0.6977, 'grad_norm': 1.3179454803466797, 'learning_rate': 0.00014632960351198618, 'epoch': 0.41}
{'loss': 0.8986, 'grad_norm': 1.1539322137832642, 'learning_rate': 0.00014628200554042402, 'epoch': 0.41}
{'loss': 1.0948, 'grad_norm': 0.952390193939209, 'learning_rate': 0.00014623439422135434, 'epoch': 0.41}
{'loss': 1.1929, 'grad_norm': 1.0340608358383179, 'learning_rate': 0.0001461867695685081, 'epoch': 0.41}
{'loss': 0.9199, 'grad_norm': 1.2499653100967407, 'learning_rate': 0.00014613913159561997, 'epoch': 0.41}
{'loss': 1.2686, 'grad_norm': 1.226976752281189, 'learning_rate': 0.00014609148031642852, 'epoch': 0.41}
{'loss': 0.8888, 'grad_norm': 1.2183293104171753, 'learning_rate': 0.00014604381574467615, 'epoch': 0.41}
{'loss': 1.2607, 'grad_norm': 1.3291115760803223, 'learning_rate': 0.0001459961378941091, 'epoch': 0.41}
{'loss': 0.8121, 'grad_norm': 1.3469161987304688, 'learning_rate': 0.00014594844677847743, 'epoch': 0.41}
{'loss': 0.8498, 'grad_norm': 1.1766629219055176, 'learning_rate': 0.000145900742411535, 'epoch': 0.41}
{'loss': 0.9048, 'grad_norm': 1.0685009956359863, 'learning_rate': 0.00014585302480703958, 'epoch': 0.41}
{'loss': 1.1035, 'grad_norm': 2.0722577571868896, 'learning_rate': 0.00014580529397875265, 'epoch': 0.41}
{'loss': 0.9321, 'grad_norm': 1.1255030632019043, 'learning_rate': 0.00014575754994043956, 'epoch': 0.41}
{'loss': 0.7559, 'grad_norm': 1.2924293279647827, 'learning_rate': 0.00014570979270586945, 'epoch': 0.41}
{'loss': 1.1101, 'grad_norm': 1.3716423511505127, 'learning_rate': 0.00014566202228881526, 'epoch': 0.41}
{'loss': 0.9513, 'grad_norm': 1.2950856685638428, 'learning_rate': 0.00014561423870305382, 'epoch': 0.41}
{'loss': 1.1882, 'grad_norm': 1.407650351524353, 'learning_rate': 0.00014556644196236565, 'epoch': 0.41}
{'loss': 0.9237, 'grad_norm': 1.2295410633087158, 'learning_rate': 0.00014551863208053507, 'epoch': 0.41}
{'loss': 1.1156, 'grad_norm': 1.0618360042572021, 'learning_rate': 0.00014547080907135026, 'epoch': 0.41}
{'loss': 0.8861, 'grad_norm': 0.9634895324707031, 'learning_rate': 0.00014542297294860312, 'epoch': 0.41}
{'loss': 0.9052, 'grad_norm': 1.3379206657409668, 'learning_rate': 0.00014537512372608937, 'epoch': 0.42}
{'loss': 1.1555, 'grad_norm': 1.3450582027435303, 'learning_rate': 0.00014532726141760848, 'epoch': 0.42}
{'loss': 1.004, 'grad_norm': 1.157804250717163, 'learning_rate': 0.00014527938603696376, 'epoch': 0.42}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2138, 'grad_norm': 1.206519365310669, 'learning_rate': 0.00014523149759796216, 'epoch': 0.42}
{'loss': 1.2282, 'grad_norm': 1.2269529104232788, 'learning_rate': 0.0001451835961144145, 'epoch': 0.42}
{'loss': 0.9968, 'grad_norm': 1.357505202293396, 'learning_rate': 0.00014513568160013537, 'epoch': 0.42}
{'loss': 1.1055, 'grad_norm': 1.5364947319030762, 'learning_rate': 0.00014508775406894307, 'epoch': 0.42}
{'loss': 0.9655, 'grad_norm': 0.8968612551689148, 'learning_rate': 0.0001450398135346597, 'epoch': 0.42}
{'loss': 1.0839, 'grad_norm': 1.0133877992630005, 'learning_rate': 0.000144991860011111, 'epoch': 0.42}
{'loss': 1.2228, 'grad_norm': 1.73014497756958, 'learning_rate': 0.00014494389351212662, 'epoch': 0.42}
{'loss': 1.1637, 'grad_norm': 1.2235890626907349, 'learning_rate': 0.00014489591405153978, 'epoch': 0.42}
{'loss': 0.8814, 'grad_norm': 0.9671534299850464, 'learning_rate': 0.0001448479216431876, 'epoch': 0.42}
{'loss': 0.7785, 'grad_norm': 1.8476588726043701, 'learning_rate': 0.00014479991630091083, 'epoch': 0.42}
{'loss': 1.0294, 'grad_norm': 1.0309009552001953, 'learning_rate': 0.00014475189803855399, 'epoch': 0.42}
{'loss': 1.0354, 'grad_norm': 1.250733494758606, 'learning_rate': 0.00014470386686996528, 'epoch': 0.42}
{'loss': 0.9318, 'grad_norm': 1.223689317703247, 'learning_rate': 0.0001446558228089967, 'epoch': 0.42}
{'loss': 1.0502, 'grad_norm': 1.0329023599624634, 'learning_rate': 0.00014460776586950393, 'epoch': 0.42}
{'loss': 0.7972, 'grad_norm': 1.0773085355758667, 'learning_rate': 0.0001445596960653463, 'epoch': 0.42}
{'loss': 0.9762, 'grad_norm': 1.3974840641021729, 'learning_rate': 0.000144511613410387, 'epoch': 0.42}
{'loss': 0.6869, 'grad_norm': 1.0466581583023071, 'learning_rate': 0.00014446351791849276, 'epoch': 0.42}
{'loss': 0.8361, 'grad_norm': 0.9404328465461731, 'learning_rate': 0.00014441540960353412, 'epoch': 0.42}
{'loss': 1.0045, 'grad_norm': 1.0984622240066528, 'learning_rate': 0.0001443672884793853, 'epoch': 0.42}
{'loss': 0.7365, 'grad_norm': 1.2193862199783325, 'learning_rate': 0.00014431915455992414, 'epoch': 0.42}
{'loss': 1.0354, 'grad_norm': 1.3348897695541382, 'learning_rate': 0.00014427100785903231, 'epoch': 0.42}
{'loss': 1.011, 'grad_norm': 1.3699901103973389, 'learning_rate': 0.00014422284839059503, 'epoch': 0.42}
{'loss': 0.8197, 'grad_norm': 1.4420629739761353, 'learning_rate': 0.00014417467616850127, 'epoch': 0.42}
{'loss': 0.9575, 'grad_norm': 1.2494416236877441, 'learning_rate': 0.0001441264912066437, 'epoch': 0.42}
{'loss': 0.7835, 'grad_norm': 0.9717352986335754, 'learning_rate': 0.00014407829351891857, 'epoch': 0.42}
{'loss': 1.022, 'grad_norm': 1.1264079809188843, 'learning_rate': 0.00014403008311922593, 'epoch': 0.42}
{'loss': 0.9682, 'grad_norm': 1.1638212203979492, 'learning_rate': 0.00014398186002146938, 'epoch': 0.42}
{'loss': 1.0764, 'grad_norm': 1.7164117097854614, 'learning_rate': 0.00014393362423955627, 'epoch': 0.42}
{'loss': 0.6436, 'grad_norm': 1.0712395906448364, 'learning_rate': 0.0001438853757873975, 'epoch': 0.42}
{'loss': 1.1947, 'grad_norm': 1.1426395177841187, 'learning_rate': 0.00014383711467890774, 'epoch': 0.42}
{'loss': 1.0762, 'grad_norm': 1.01240074634552, 'learning_rate': 0.00014378884092800526, 'epoch': 0.42}
{'loss': 0.8959, 'grad_norm': 1.071198582649231, 'learning_rate': 0.000143740554548612, 'epoch': 0.42}
{'loss': 1.0466, 'grad_norm': 1.3287607431411743, 'learning_rate': 0.00014369225555465345, 'epoch': 0.42}
{'loss': 1.1133, 'grad_norm': 1.0217598676681519, 'learning_rate': 0.00014364394396005885, 'epoch': 0.42}
{'loss': 1.4297, 'grad_norm': 1.1751102209091187, 'learning_rate': 0.00014359561977876102, 'epoch': 0.42}
{'loss': 1.026, 'grad_norm': 0.9738925099372864, 'learning_rate': 0.00014354728302469645, 'epoch': 0.42}
{'loss': 1.0477, 'grad_norm': 1.1168739795684814, 'learning_rate': 0.0001434989337118052, 'epoch': 0.42}
{'loss': 0.8163, 'grad_norm': 1.2018276453018188, 'learning_rate': 0.000143450571854031, 'epoch': 0.42}
{'loss': 0.8726, 'grad_norm': 1.126713514328003, 'learning_rate': 0.0001434021974653211, 'epoch': 0.42}
{'loss': 0.7526, 'grad_norm': 1.192618489265442, 'learning_rate': 0.00014335381055962656, 'epoch': 0.42}
{'loss': 0.9082, 'grad_norm': 1.1285725831985474, 'learning_rate': 0.00014330541115090188, 'epoch': 0.42}
{'loss': 1.1403, 'grad_norm': 1.2682586908340454, 'learning_rate': 0.00014325699925310517, 'epoch': 0.42}
{'loss': 0.8708, 'grad_norm': 1.0495918989181519, 'learning_rate': 0.00014320857488019824, 'epoch': 0.42}
{'loss': 0.8793, 'grad_norm': 1.177161455154419, 'learning_rate': 0.00014316013804614643, 'epoch': 0.42}
{'loss': 0.8088, 'grad_norm': 1.0489884614944458, 'learning_rate': 0.00014311168876491868, 'epoch': 0.42}
{'loss': 0.9533, 'grad_norm': 1.1781779527664185, 'learning_rate': 0.0001430632270504876, 'epoch': 0.42}
{'loss': 0.8728, 'grad_norm': 1.0020350217819214, 'learning_rate': 0.0001430147529168292, 'epoch': 0.42}
{'loss': 0.9613, 'grad_norm': 1.5733741521835327, 'learning_rate': 0.00014296626637792326, 'epoch': 0.42}
{'loss': 0.9648, 'grad_norm': 1.283735990524292, 'learning_rate': 0.00014291776744775305, 'epoch': 0.42}
{'loss': 1.1197, 'grad_norm': 1.3633919954299927, 'learning_rate': 0.00014286925614030542, 'epoch': 0.42}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8494, 'grad_norm': 0.8649919033050537, 'learning_rate': 0.00014282073246957083, 'epoch': 0.42}
{'loss': 0.9071, 'grad_norm': 1.2341758012771606, 'learning_rate': 0.00014277219644954323, 'epoch': 0.42}
{'loss': 0.9588, 'grad_norm': 1.1223517656326294, 'learning_rate': 0.00014272364809422017, 'epoch': 0.42}
{'loss': 0.9131, 'grad_norm': 1.0283175706863403, 'learning_rate': 0.0001426750874176028, 'epoch': 0.42}
{'loss': 1.0758, 'grad_norm': 1.1631736755371094, 'learning_rate': 0.00014262651443369576, 'epoch': 0.42}
{'loss': 1.2178, 'grad_norm': 1.0772778987884521, 'learning_rate': 0.00014257792915650728, 'epoch': 0.42}
{'loss': 0.8368, 'grad_norm': 1.2641844749450684, 'learning_rate': 0.0001425293316000491, 'epoch': 0.42}
{'loss': 0.8726, 'grad_norm': 1.1444512605667114, 'learning_rate': 0.00014248072177833654, 'epoch': 0.42}
{'loss': 0.6566, 'grad_norm': 1.3469138145446777, 'learning_rate': 0.00014243209970538846, 'epoch': 0.42}
{'loss': 0.7554, 'grad_norm': 1.187406301498413, 'learning_rate': 0.00014238346539522715, 'epoch': 0.42}
{'loss': 1.0086, 'grad_norm': 1.1131855249404907, 'learning_rate': 0.0001423348188618786, 'epoch': 0.42}
{'loss': 0.9953, 'grad_norm': 1.2058491706848145, 'learning_rate': 0.0001422861601193722, 'epoch': 0.42}
{'loss': 0.8635, 'grad_norm': 1.0936881303787231, 'learning_rate': 0.00014223748918174088, 'epoch': 0.43}
{'loss': 0.911, 'grad_norm': 1.266594648361206, 'learning_rate': 0.00014218880606302114, 'epoch': 0.43}
{'loss': 0.9015, 'grad_norm': 1.0555202960968018, 'learning_rate': 0.00014214011077725292, 'epoch': 0.43}
{'loss': 0.779, 'grad_norm': 1.5418250560760498, 'learning_rate': 0.00014209140333847973, 'epoch': 0.43}
{'loss': 1.1283, 'grad_norm': 1.0186834335327148, 'learning_rate': 0.00014204268376074856, 'epoch': 0.43}
{'loss': 1.2051, 'grad_norm': 1.1537889242172241, 'learning_rate': 0.00014199395205810987, 'epoch': 0.43}
{'loss': 1.272, 'grad_norm': 1.1037622690200806, 'learning_rate': 0.00014194520824461771, 'epoch': 0.43}
{'loss': 1.0577, 'grad_norm': 0.8257319331169128, 'learning_rate': 0.00014189645233432952, 'epoch': 0.43}
{'loss': 1.1343, 'grad_norm': 1.2345736026763916, 'learning_rate': 0.0001418476843413063, 'epoch': 0.43}
{'loss': 1.0039, 'grad_norm': 1.1928825378417969, 'learning_rate': 0.00014179890427961243, 'epoch': 0.43}
{'loss': 0.5565, 'grad_norm': 1.1505452394485474, 'learning_rate': 0.0001417501121633159, 'epoch': 0.43}
{'loss': 1.1086, 'grad_norm': 1.201615810394287, 'learning_rate': 0.00014170130800648814, 'epoch': 0.43}
{'loss': 1.0632, 'grad_norm': 1.1143697500228882, 'learning_rate': 0.00014165249182320402, 'epoch': 0.43}
{'loss': 1.018, 'grad_norm': 1.000670075416565, 'learning_rate': 0.00014160366362754183, 'epoch': 0.43}
{'loss': 1.0812, 'grad_norm': 1.5638233423233032, 'learning_rate': 0.00014155482343358345, 'epoch': 0.43}
{'loss': 0.5446, 'grad_norm': 1.2340470552444458, 'learning_rate': 0.00014150597125541417, 'epoch': 0.43}
{'loss': 0.8969, 'grad_norm': 1.4125468730926514, 'learning_rate': 0.0001414571071071227, 'epoch': 0.43}
{'loss': 0.7046, 'grad_norm': 1.2270121574401855, 'learning_rate': 0.0001414082310028012, 'epoch': 0.43}
{'loss': 1.001, 'grad_norm': 1.3757784366607666, 'learning_rate': 0.00014135934295654535, 'epoch': 0.43}
{'loss': 1.2369, 'grad_norm': 1.332961082458496, 'learning_rate': 0.0001413104429824542, 'epoch': 0.43}
{'loss': 0.9517, 'grad_norm': 1.1337484121322632, 'learning_rate': 0.00014126153109463024, 'epoch': 0.43}
{'loss': 0.7965, 'grad_norm': 1.022264838218689, 'learning_rate': 0.0001412126073071795, 'epoch': 0.43}
{'loss': 0.9013, 'grad_norm': 1.132735252380371, 'learning_rate': 0.0001411636716342113, 'epoch': 0.43}
{'loss': 0.9759, 'grad_norm': 0.9175109267234802, 'learning_rate': 0.00014111472408983843, 'epoch': 0.43}
{'loss': 1.0816, 'grad_norm': 0.9044010639190674, 'learning_rate': 0.0001410657646881772, 'epoch': 0.43}
{'loss': 1.132, 'grad_norm': 1.0567796230316162, 'learning_rate': 0.0001410167934433472, 'epoch': 0.43}
{'loss': 1.0222, 'grad_norm': 1.0808839797973633, 'learning_rate': 0.00014096781036947157, 'epoch': 0.43}
{'loss': 0.7185, 'grad_norm': 1.0039626359939575, 'learning_rate': 0.00014091881548067677, 'epoch': 0.43}
{'loss': 1.0825, 'grad_norm': 1.2879457473754883, 'learning_rate': 0.00014086980879109265, 'epoch': 0.43}
{'loss': 0.7577, 'grad_norm': 1.4172332286834717, 'learning_rate': 0.0001408207903148525, 'epoch': 0.43}
{'loss': 1.1357, 'grad_norm': 1.0547086000442505, 'learning_rate': 0.00014077176006609308, 'epoch': 0.43}
{'loss': 0.8583, 'grad_norm': 1.0156153440475464, 'learning_rate': 0.00014072271805895442, 'epoch': 0.43}
{'loss': 0.8008, 'grad_norm': 1.2855870723724365, 'learning_rate': 0.00014067366430758004, 'epoch': 0.43}
{'loss': 0.8988, 'grad_norm': 1.044897198677063, 'learning_rate': 0.00014062459882611676, 'epoch': 0.43}
{'loss': 1.121, 'grad_norm': 1.0794847011566162, 'learning_rate': 0.00014057552162871485, 'epoch': 0.43}
{'loss': 0.768, 'grad_norm': 1.5212352275848389, 'learning_rate': 0.00014052643272952794, 'epoch': 0.43}
{'loss': 0.7682, 'grad_norm': 1.4296306371688843, 'learning_rate': 0.00014047733214271304, 'epoch': 0.43}
{'loss': 1.1157, 'grad_norm': 0.972007691860199, 'learning_rate': 0.0001404282198824305, 'epoch': 0.43}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9786, 'grad_norm': 1.0105928182601929, 'learning_rate': 0.00014037909596284408, 'epoch': 0.43}
{'loss': 1.1304, 'grad_norm': 1.2655539512634277, 'learning_rate': 0.00014032996039812088, 'epoch': 0.43}
{'loss': 0.9811, 'grad_norm': 1.355920672416687, 'learning_rate': 0.00014028081320243137, 'epoch': 0.43}
{'loss': 0.8681, 'grad_norm': 1.0940055847167969, 'learning_rate': 0.0001402316543899493, 'epoch': 0.43}
{'loss': 1.004, 'grad_norm': 1.4090821743011475, 'learning_rate': 0.00014018248397485193, 'epoch': 0.43}
{'loss': 0.8437, 'grad_norm': 0.8554938435554504, 'learning_rate': 0.00014013330197131972, 'epoch': 0.43}
{'loss': 1.2359, 'grad_norm': 1.4272336959838867, 'learning_rate': 0.0001400841083935365, 'epoch': 0.43}
{'loss': 0.8973, 'grad_norm': 1.6298366785049438, 'learning_rate': 0.00014003490325568954, 'epoch': 0.43}
{'loss': 1.0126, 'grad_norm': 1.1327348947525024, 'learning_rate': 0.00013998568657196925, 'epoch': 0.43}
{'loss': 0.7038, 'grad_norm': 1.2708420753479004, 'learning_rate': 0.00013993645835656953, 'epoch': 0.43}
{'loss': 0.8642, 'grad_norm': 1.2299659252166748, 'learning_rate': 0.00013988721862368765, 'epoch': 0.43}
{'loss': 1.1439, 'grad_norm': 1.2380174398422241, 'learning_rate': 0.000139837967387524, 'epoch': 0.43}
{'loss': 1.1143, 'grad_norm': 1.0895380973815918, 'learning_rate': 0.00013978870466228247, 'epoch': 0.43}
{'loss': 0.9067, 'grad_norm': 2.0388412475585938, 'learning_rate': 0.00013973943046217014, 'epoch': 0.43}
{'loss': 0.8724, 'grad_norm': 1.3068512678146362, 'learning_rate': 0.00013969014480139747, 'epoch': 0.43}
{'loss': 0.9313, 'grad_norm': 1.4546757936477661, 'learning_rate': 0.0001396408476941782, 'epoch': 0.43}
{'loss': 1.2796, 'grad_norm': 1.2416422367095947, 'learning_rate': 0.00013959153915472943, 'epoch': 0.43}
{'loss': 0.9533, 'grad_norm': 1.2830513715744019, 'learning_rate': 0.00013954221919727146, 'epoch': 0.43}
{'loss': 1.3926, 'grad_norm': 0.9697285294532776, 'learning_rate': 0.0001394928878360279, 'epoch': 0.43}
{'loss': 0.8865, 'grad_norm': 1.2552789449691772, 'learning_rate': 0.00013944354508522574, 'epoch': 0.43}
{'loss': 1.0188, 'grad_norm': 1.2732274532318115, 'learning_rate': 0.00013939419095909512, 'epoch': 0.43}
{'loss': 0.7264, 'grad_norm': 1.9604238271713257, 'learning_rate': 0.0001393448254718696, 'epoch': 0.43}
{'loss': 0.9148, 'grad_norm': 1.1549822092056274, 'learning_rate': 0.0001392954486377859, 'epoch': 0.43}
{'loss': 0.6977, 'grad_norm': 1.1047601699829102, 'learning_rate': 0.0001392460604710841, 'epoch': 0.43}
{'loss': 0.9473, 'grad_norm': 1.1951695680618286, 'learning_rate': 0.00013919666098600753, 'epoch': 0.43}
{'loss': 1.2586, 'grad_norm': 1.059509515762329, 'learning_rate': 0.00013914725019680268, 'epoch': 0.43}
{'loss': 0.9897, 'grad_norm': 1.1195183992385864, 'learning_rate': 0.00013909782811771944, 'epoch': 0.43}
{'loss': 0.6645, 'grad_norm': 1.054813027381897, 'learning_rate': 0.0001390483947630109, 'epoch': 0.44}
{'loss': 1.5025, 'grad_norm': 1.1385374069213867, 'learning_rate': 0.0001389989501469334, 'epoch': 0.44}
{'loss': 0.9277, 'grad_norm': 1.155128836631775, 'learning_rate': 0.00013894949428374647, 'epoch': 0.44}
{'loss': 0.9029, 'grad_norm': 1.3684221506118774, 'learning_rate': 0.00013890002718771303, 'epoch': 0.44}
{'loss': 0.9191, 'grad_norm': 1.186678409576416, 'learning_rate': 0.0001388505488730991, 'epoch': 0.44}
{'loss': 0.9353, 'grad_norm': 1.2476006746292114, 'learning_rate': 0.00013880105935417403, 'epoch': 0.44}
{'loss': 1.1819, 'grad_norm': 4.326562404632568, 'learning_rate': 0.0001387515586452103, 'epoch': 0.44}
{'loss': 0.8737, 'grad_norm': 1.2402206659317017, 'learning_rate': 0.00013870204676048374, 'epoch': 0.44}
{'loss': 0.8816, 'grad_norm': 0.9856993556022644, 'learning_rate': 0.0001386525237142733, 'epoch': 0.44}
{'loss': 1.1496, 'grad_norm': 1.1844850778579712, 'learning_rate': 0.00013860298952086118, 'epoch': 0.44}
{'loss': 1.0407, 'grad_norm': 1.0998804569244385, 'learning_rate': 0.0001385534441945328, 'epoch': 0.44}
{'loss': 1.1191, 'grad_norm': 1.3525980710983276, 'learning_rate': 0.00013850388774957684, 'epoch': 0.44}
{'loss': 1.2872, 'grad_norm': 1.0752209424972534, 'learning_rate': 0.0001384543202002851, 'epoch': 0.44}
{'loss': 0.981, 'grad_norm': 1.0870885848999023, 'learning_rate': 0.00013840474156095263, 'epoch': 0.44}
{'loss': 1.1969, 'grad_norm': 1.4916331768035889, 'learning_rate': 0.00013835515184587767, 'epoch': 0.44}
{'loss': 1.1921, 'grad_norm': 1.05234694480896, 'learning_rate': 0.00013830555106936164, 'epoch': 0.44}
{'loss': 1.1669, 'grad_norm': 1.2391153573989868, 'learning_rate': 0.00013825593924570923, 'epoch': 0.44}
{'loss': 0.6937, 'grad_norm': 1.1184170246124268, 'learning_rate': 0.0001382063163892282, 'epoch': 0.44}
{'loss': 1.1301, 'grad_norm': 1.0185753107070923, 'learning_rate': 0.00013815668251422952, 'epoch': 0.44}
{'loss': 0.9945, 'grad_norm': 1.2651550769805908, 'learning_rate': 0.00013810703763502744, 'epoch': 0.44}
{'loss': 0.8534, 'grad_norm': 1.3402159214019775, 'learning_rate': 0.0001380573817659392, 'epoch': 0.44}
{'loss': 1.2375, 'grad_norm': 1.1426156759262085, 'learning_rate': 0.00013800771492128536, 'epoch': 0.44}
{'loss': 1.0296, 'grad_norm': 1.2176910638809204, 'learning_rate': 0.00013795803711538966, 'epoch': 0.44}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8665, 'grad_norm': 1.335166573524475, 'learning_rate': 0.00013790834836257884, 'epoch': 0.44}
{'loss': 1.1373, 'grad_norm': 0.9501387476921082, 'learning_rate': 0.00013785864867718294, 'epoch': 0.44}
{'loss': 1.2569, 'grad_norm': 1.9929783344268799, 'learning_rate': 0.00013780893807353512, 'epoch': 0.44}
{'loss': 1.39, 'grad_norm': 1.330438494682312, 'learning_rate': 0.0001377592165659717, 'epoch': 0.44}
{'loss': 0.7862, 'grad_norm': 1.141502022743225, 'learning_rate': 0.00013770948416883205, 'epoch': 0.44}
{'loss': 1.244, 'grad_norm': 1.2081252336502075, 'learning_rate': 0.00013765974089645884, 'epoch': 0.44}
{'loss': 1.2812, 'grad_norm': 1.367477536201477, 'learning_rate': 0.0001376099867631977, 'epoch': 0.44}
{'loss': 1.0228, 'grad_norm': 1.3246879577636719, 'learning_rate': 0.00013756022178339756, 'epoch': 0.44}
{'loss': 0.7605, 'grad_norm': 1.244739055633545, 'learning_rate': 0.00013751044597141036, 'epoch': 0.44}
{'loss': 0.8149, 'grad_norm': 1.2598142623901367, 'learning_rate': 0.00013746065934159123, 'epoch': 0.44}
{'loss': 0.8354, 'grad_norm': 1.30629563331604, 'learning_rate': 0.00013741086190829837, 'epoch': 0.44}
{'loss': 1.0717, 'grad_norm': 1.2708438634872437, 'learning_rate': 0.0001373610536858931, 'epoch': 0.44}
{'loss': 0.9561, 'grad_norm': 0.9533486366271973, 'learning_rate': 0.00013731123468873992, 'epoch': 0.44}
{'loss': 1.1773, 'grad_norm': 1.3542712926864624, 'learning_rate': 0.0001372614049312064, 'epoch': 0.44}
{'loss': 0.9108, 'grad_norm': 1.143584966659546, 'learning_rate': 0.00013721156442766312, 'epoch': 0.44}
{'loss': 1.1037, 'grad_norm': 1.365431785583496, 'learning_rate': 0.00013716171319248392, 'epoch': 0.44}
{'loss': 1.1029, 'grad_norm': 1.403562307357788, 'learning_rate': 0.0001371118512400456, 'epoch': 0.44}
{'loss': 0.9979, 'grad_norm': 0.9948317408561707, 'learning_rate': 0.0001370619785847282, 'epoch': 0.44}
{'loss': 0.9474, 'grad_norm': 1.1160387992858887, 'learning_rate': 0.00013701209524091465, 'epoch': 0.44}
{'loss': 0.7341, 'grad_norm': 1.1067386865615845, 'learning_rate': 0.00013696220122299112, 'epoch': 0.44}
{'loss': 0.6712, 'grad_norm': 1.207169771194458, 'learning_rate': 0.00013691229654534676, 'epoch': 0.44}
{'loss': 0.8847, 'grad_norm': 1.064423680305481, 'learning_rate': 0.0001368623812223739, 'epoch': 0.44}
{'loss': 1.0216, 'grad_norm': 1.5990501642227173, 'learning_rate': 0.00013681245526846783, 'epoch': 0.44}
{'loss': 1.1836, 'grad_norm': 1.2926037311553955, 'learning_rate': 0.00013676251869802693, 'epoch': 0.44}
{'loss': 0.8585, 'grad_norm': 0.9627915024757385, 'learning_rate': 0.00013671257152545277, 'epoch': 0.44}
{'loss': 1.3034, 'grad_norm': 1.2630022764205933, 'learning_rate': 0.00013666261376514977, 'epoch': 0.44}
{'loss': 1.0546, 'grad_norm': 1.3459949493408203, 'learning_rate': 0.0001366126454315256, 'epoch': 0.44}
{'loss': 1.1541, 'grad_norm': 1.1689945459365845, 'learning_rate': 0.0001365626665389908, 'epoch': 0.44}
{'loss': 1.0082, 'grad_norm': 0.8805193305015564, 'learning_rate': 0.00013651267710195907, 'epoch': 0.44}
{'loss': 0.8213, 'grad_norm': 1.2403315305709839, 'learning_rate': 0.00013646267713484717, 'epoch': 0.44}
{'loss': 1.0448, 'grad_norm': 1.192863941192627, 'learning_rate': 0.00013641266665207478, 'epoch': 0.44}
{'loss': 0.8415, 'grad_norm': 1.6724135875701904, 'learning_rate': 0.0001363626456680647, 'epoch': 0.44}
{'loss': 1.1878, 'grad_norm': 1.0559598207473755, 'learning_rate': 0.0001363126141972428, 'epoch': 0.44}
{'loss': 0.827, 'grad_norm': 1.0979528427124023, 'learning_rate': 0.00013626257225403783, 'epoch': 0.44}
{'loss': 1.1752, 'grad_norm': 1.3667668104171753, 'learning_rate': 0.0001362125198528817, 'epoch': 0.44}
{'loss': 1.1217, 'grad_norm': 1.2417552471160889, 'learning_rate': 0.00013616245700820922, 'epoch': 0.44}
{'loss': 1.2283, 'grad_norm': 1.034035563468933, 'learning_rate': 0.00013611238373445838, 'epoch': 0.44}
{'loss': 0.8189, 'grad_norm': 1.0884777307510376, 'learning_rate': 0.00013606230004607, 'epoch': 0.44}
{'loss': 1.4527, 'grad_norm': 4.0971221923828125, 'learning_rate': 0.000136012205957488, 'epoch': 0.44}
{'loss': 1.4743, 'grad_norm': 1.283015489578247, 'learning_rate': 0.00013596210148315917, 'epoch': 0.44}
{'loss': 1.0094, 'grad_norm': 0.9480211138725281, 'learning_rate': 0.00013591198663753356, 'epoch': 0.44}
{'loss': 0.9589, 'grad_norm': 0.9693893790245056, 'learning_rate': 0.000135861861435064, 'epoch': 0.44}
{'loss': 1.3318, 'grad_norm': 1.374550223350525, 'learning_rate': 0.0001358117258902063, 'epoch': 0.45}
{'loss': 1.0305, 'grad_norm': 1.2212438583374023, 'learning_rate': 0.00013576158001741932, 'epoch': 0.45}
{'loss': 0.9512, 'grad_norm': 1.3682326078414917, 'learning_rate': 0.00013571142383116496, 'epoch': 0.45}
{'loss': 0.9153, 'grad_norm': 1.4553037881851196, 'learning_rate': 0.00013566125734590793, 'epoch': 0.45}
{'loss': 0.9122, 'grad_norm': 1.172353982925415, 'learning_rate': 0.00013561108057611606, 'epoch': 0.45}
{'loss': 1.101, 'grad_norm': 1.1468431949615479, 'learning_rate': 0.0001355608935362601, 'epoch': 0.45}
{'loss': 0.8957, 'grad_norm': 1.011993646621704, 'learning_rate': 0.0001355106962408137, 'epoch': 0.45}
{'loss': 0.7291, 'grad_norm': 1.2418092489242554, 'learning_rate': 0.00013546048870425356, 'epoch': 0.45}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0914, 'grad_norm': 1.6326329708099365, 'learning_rate': 0.00013541027094105928, 'epoch': 0.45}
{'loss': 1.2548, 'grad_norm': 1.143354892730713, 'learning_rate': 0.00013536004296571343, 'epoch': 0.45}
{'loss': 0.7615, 'grad_norm': 1.5688910484313965, 'learning_rate': 0.0001353098047927015, 'epoch': 0.45}
{'loss': 1.0376, 'grad_norm': 1.3554922342300415, 'learning_rate': 0.00013525955643651193, 'epoch': 0.45}
{'loss': 0.9506, 'grad_norm': 1.2086994647979736, 'learning_rate': 0.00013520929791163607, 'epoch': 0.45}
{'loss': 1.0938, 'grad_norm': 1.0361251831054688, 'learning_rate': 0.0001351590292325683, 'epoch': 0.45}
{'loss': 0.9011, 'grad_norm': 1.3402020931243896, 'learning_rate': 0.0001351087504138059, 'epoch': 0.45}
{'loss': 0.7622, 'grad_norm': 1.0614492893218994, 'learning_rate': 0.00013505846146984895, 'epoch': 0.45}
{'loss': 1.1347, 'grad_norm': 1.1753370761871338, 'learning_rate': 0.00013500816241520058, 'epoch': 0.45}
{'loss': 0.9694, 'grad_norm': 1.0251797437667847, 'learning_rate': 0.0001349578532643668, 'epoch': 0.45}
{'loss': 1.0861, 'grad_norm': 1.075058102607727, 'learning_rate': 0.0001349075340318565, 'epoch': 0.45}
{'loss': 0.8416, 'grad_norm': 1.2292765378952026, 'learning_rate': 0.00013485720473218154, 'epoch': 0.45}
{'loss': 1.085, 'grad_norm': 1.298109769821167, 'learning_rate': 0.0001348068653798566, 'epoch': 0.45}
{'loss': 1.042, 'grad_norm': 0.9636276364326477, 'learning_rate': 0.00013475651598939936, 'epoch': 0.45}
{'loss': 1.3369, 'grad_norm': 0.9536562561988831, 'learning_rate': 0.0001347061565753303, 'epoch': 0.45}
{'loss': 0.9452, 'grad_norm': 0.9602422714233398, 'learning_rate': 0.0001346557871521729, 'epoch': 0.45}
{'loss': 0.7724, 'grad_norm': 1.3080437183380127, 'learning_rate': 0.0001346054077344533, 'epoch': 0.45}
{'loss': 0.9482, 'grad_norm': 1.3781758546829224, 'learning_rate': 0.00013455501833670088, 'epoch': 0.45}
{'loss': 1.0286, 'grad_norm': 1.1266926527023315, 'learning_rate': 0.00013450461897344758, 'epoch': 0.45}
{'loss': 1.1561, 'grad_norm': 1.0690863132476807, 'learning_rate': 0.00013445420965922837, 'epoch': 0.45}
{'loss': 1.0867, 'grad_norm': 1.0229485034942627, 'learning_rate': 0.00013440379040858104, 'epoch': 0.45}
{'loss': 1.0072, 'grad_norm': 1.257082462310791, 'learning_rate': 0.00013435336123604626, 'epoch': 0.45}
{'loss': 1.0737, 'grad_norm': 1.7792967557907104, 'learning_rate': 0.0001343029221561676, 'epoch': 0.45}
{'loss': 1.1335, 'grad_norm': 1.1902202367782593, 'learning_rate': 0.00013425247318349137, 'epoch': 0.45}
{'loss': 0.9275, 'grad_norm': 1.1336477994918823, 'learning_rate': 0.00013420201433256689, 'epoch': 0.45}
{'loss': 1.013, 'grad_norm': 1.2980473041534424, 'learning_rate': 0.0001341515456179462, 'epoch': 0.45}
{'loss': 1.0466, 'grad_norm': 1.2965103387832642, 'learning_rate': 0.00013410106705418425, 'epoch': 0.45}
{'loss': 1.1244, 'grad_norm': 1.1119219064712524, 'learning_rate': 0.00013405057865583877, 'epoch': 0.45}
{'loss': 0.8438, 'grad_norm': 1.0310173034667969, 'learning_rate': 0.00013400008043747046, 'epoch': 0.45}
{'loss': 1.0734, 'grad_norm': 1.4760034084320068, 'learning_rate': 0.00013394957241364273, 'epoch': 0.45}
{'loss': 0.8371, 'grad_norm': 1.2330280542373657, 'learning_rate': 0.00013389905459892183, 'epoch': 0.45}
{'loss': 1.102, 'grad_norm': 1.2122926712036133, 'learning_rate': 0.00013384852700787681, 'epoch': 0.45}
{'loss': 1.1911, 'grad_norm': 1.09632408618927, 'learning_rate': 0.00013379798965507968, 'epoch': 0.45}
{'loss': 0.9609, 'grad_norm': 1.244010329246521, 'learning_rate': 0.00013374744255510512, 'epoch': 0.45}
{'loss': 0.9644, 'grad_norm': 1.0925642251968384, 'learning_rate': 0.0001336968857225307, 'epoch': 0.45}
{'loss': 1.0286, 'grad_norm': 1.2960246801376343, 'learning_rate': 0.0001336463191719367, 'epoch': 0.45}
{'loss': 0.9871, 'grad_norm': 1.1924537420272827, 'learning_rate': 0.00013359574291790634, 'epoch': 0.45}
{'loss': 0.9002, 'grad_norm': 0.9148080348968506, 'learning_rate': 0.00013354515697502553, 'epoch': 0.45}
{'loss': 1.0662, 'grad_norm': 1.1796743869781494, 'learning_rate': 0.00013349456135788298, 'epoch': 0.45}
{'loss': 0.8365, 'grad_norm': 1.0931845903396606, 'learning_rate': 0.00013344395608107031, 'epoch': 0.45}
{'loss': 0.7107, 'grad_norm': 1.0313016176223755, 'learning_rate': 0.0001333933411591818, 'epoch': 0.45}
{'loss': 0.7782, 'grad_norm': 1.7517080307006836, 'learning_rate': 0.00013334271660681455, 'epoch': 0.45}
{'loss': 1.0416, 'grad_norm': 1.6694601774215698, 'learning_rate': 0.0001332920824385684, 'epoch': 0.45}
{'loss': 1.2269, 'grad_norm': 1.1672444343566895, 'learning_rate': 0.00013324143866904603, 'epoch': 0.45}
{'loss': 0.9953, 'grad_norm': 1.3620944023132324, 'learning_rate': 0.00013319078531285285, 'epoch': 0.45}
{'loss': 0.9678, 'grad_norm': 1.0528507232666016, 'learning_rate': 0.00013314012238459707, 'epoch': 0.45}
{'loss': 0.7672, 'grad_norm': 1.1316472291946411, 'learning_rate': 0.0001330894498988896, 'epoch': 0.45}
{'loss': 0.9449, 'grad_norm': 1.0575354099273682, 'learning_rate': 0.0001330387678703442, 'epoch': 0.45}
{'loss': 1.0643, 'grad_norm': 1.4004077911376953, 'learning_rate': 0.00013298807631357724, 'epoch': 0.45}
{'loss': 0.8788, 'grad_norm': 1.1747220754623413, 'learning_rate': 0.00013293737524320797, 'epoch': 0.45}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6627, 'grad_norm': 0.8795223236083984, 'learning_rate': 0.00013288666467385833, 'epoch': 0.45}
{'loss': 1.0442, 'grad_norm': 1.1823322772979736, 'learning_rate': 0.000132835944620153, 'epoch': 0.45}
{'loss': 0.9378, 'grad_norm': 1.535625696182251, 'learning_rate': 0.0001327852150967194, 'epoch': 0.45}
{'loss': 1.134, 'grad_norm': 0.9591116309165955, 'learning_rate': 0.00013273447611818767, 'epoch': 0.45}
{'loss': 1.1134, 'grad_norm': 1.3135631084442139, 'learning_rate': 0.00013268372769919073, 'epoch': 0.45}
{'loss': 1.161, 'grad_norm': 1.12391996383667, 'learning_rate': 0.00013263296985436412, 'epoch': 0.45}
{'loss': 1.0758, 'grad_norm': 1.2585574388504028, 'learning_rate': 0.00013258220259834618, 'epoch': 0.45}
{'loss': 1.2345, 'grad_norm': 0.9854950308799744, 'learning_rate': 0.00013253142594577794, 'epoch': 0.46}
{'loss': 1.0272, 'grad_norm': 1.0328789949417114, 'learning_rate': 0.00013248063991130315, 'epoch': 0.46}
{'loss': 1.0744, 'grad_norm': 1.305056095123291, 'learning_rate': 0.00013242984450956828, 'epoch': 0.46}
{'loss': 1.1666, 'grad_norm': 1.1436563730239868, 'learning_rate': 0.00013237903975522243, 'epoch': 0.46}
{'loss': 1.046, 'grad_norm': 1.0475425720214844, 'learning_rate': 0.00013232822566291747, 'epoch': 0.46}
{'loss': 0.772, 'grad_norm': 0.8632734417915344, 'learning_rate': 0.00013227740224730798, 'epoch': 0.46}
{'loss': 0.7098, 'grad_norm': 1.0355114936828613, 'learning_rate': 0.00013222656952305113, 'epoch': 0.46}
{'loss': 1.1274, 'grad_norm': 1.2828471660614014, 'learning_rate': 0.00013217572750480686, 'epoch': 0.46}
{'loss': 1.0013, 'grad_norm': 1.0101639032363892, 'learning_rate': 0.0001321248762072377, 'epoch': 0.46}
{'loss': 0.9805, 'grad_norm': 1.0978764295578003, 'learning_rate': 0.00013207401564500903, 'epoch': 0.46}
{'loss': 1.0337, 'grad_norm': 1.3539252281188965, 'learning_rate': 0.0001320231458327887, 'epoch': 0.46}
{'loss': 0.8385, 'grad_norm': 1.0861798524856567, 'learning_rate': 0.00013197226678524738, 'epoch': 0.46}
{'loss': 0.9451, 'grad_norm': 1.424925684928894, 'learning_rate': 0.0001319213785170583, 'epoch': 0.46}
{'loss': 0.9177, 'grad_norm': 0.954484224319458, 'learning_rate': 0.0001318704810428974, 'epoch': 0.46}
{'loss': 1.0624, 'grad_norm': 1.5390644073486328, 'learning_rate': 0.00013181957437744332, 'epoch': 0.46}
{'loss': 0.7299, 'grad_norm': 1.2625024318695068, 'learning_rate': 0.00013176865853537723, 'epoch': 0.46}
{'loss': 1.1102, 'grad_norm': 1.4007689952850342, 'learning_rate': 0.00013171773353138304, 'epoch': 0.46}
{'loss': 1.2896, 'grad_norm': 1.1672251224517822, 'learning_rate': 0.00013166679938014726, 'epoch': 0.46}
{'loss': 0.8107, 'grad_norm': 1.089363932609558, 'learning_rate': 0.00013161585609635908, 'epoch': 0.46}
{'loss': 1.2629, 'grad_norm': 0.9723061323165894, 'learning_rate': 0.00013156490369471027, 'epoch': 0.46}
{'loss': 1.1654, 'grad_norm': 1.1927367448806763, 'learning_rate': 0.00013151394218989526, 'epoch': 0.46}
{'loss': 0.987, 'grad_norm': 1.283411979675293, 'learning_rate': 0.00013146297159661113, 'epoch': 0.46}
{'loss': 0.9542, 'grad_norm': 1.247140645980835, 'learning_rate': 0.00013141199192955751, 'epoch': 0.46}
{'loss': 1.1483, 'grad_norm': 1.5299345254898071, 'learning_rate': 0.00013136100320343673, 'epoch': 0.46}
{'loss': 0.7985, 'grad_norm': 1.0816417932510376, 'learning_rate': 0.0001313100054329537, 'epoch': 0.46}
{'loss': 0.8873, 'grad_norm': 1.1506028175354004, 'learning_rate': 0.00013125899863281588, 'epoch': 0.46}
{'loss': 0.818, 'grad_norm': 1.2205878496170044, 'learning_rate': 0.00013120798281773347, 'epoch': 0.46}
{'loss': 1.5372, 'grad_norm': 1.1658103466033936, 'learning_rate': 0.00013115695800241904, 'epoch': 0.46}
{'loss': 0.7868, 'grad_norm': 1.0211122035980225, 'learning_rate': 0.00013110592420158807, 'epoch': 0.46}
{'loss': 1.0471, 'grad_norm': 1.3819293975830078, 'learning_rate': 0.00013105488142995837, 'epoch': 0.46}
{'loss': 1.1467, 'grad_norm': 0.9834332466125488, 'learning_rate': 0.00013100382970225046, 'epoch': 0.46}
{'loss': 0.942, 'grad_norm': 0.9860702753067017, 'learning_rate': 0.00013095276903318735, 'epoch': 0.46}
{'loss': 0.9518, 'grad_norm': 1.213042140007019, 'learning_rate': 0.00013090169943749476, 'epoch': 0.46}
{'loss': 0.9377, 'grad_norm': 1.322321891784668, 'learning_rate': 0.00013085062092990088, 'epoch': 0.46}
{'loss': 1.3619, 'grad_norm': 1.1229212284088135, 'learning_rate': 0.00013079953352513655, 'epoch': 0.46}
{'loss': 1.2675, 'grad_norm': 1.0469434261322021, 'learning_rate': 0.00013074843723793507, 'epoch': 0.46}
{'loss': 0.9614, 'grad_norm': 1.3231556415557861, 'learning_rate': 0.00013069733208303242, 'epoch': 0.46}
{'loss': 0.9737, 'grad_norm': 1.0765239000320435, 'learning_rate': 0.00013064621807516702, 'epoch': 0.46}
{'loss': 1.1307, 'grad_norm': 1.0113649368286133, 'learning_rate': 0.00013059509522907997, 'epoch': 0.46}
{'loss': 1.0106, 'grad_norm': 1.1976256370544434, 'learning_rate': 0.0001305439635595148, 'epoch': 0.46}
{'loss': 1.039, 'grad_norm': 2.4237143993377686, 'learning_rate': 0.0001304928230812177, 'epoch': 0.46}
{'loss': 0.8658, 'grad_norm': 1.0834693908691406, 'learning_rate': 0.00013044167380893727, 'epoch': 0.46}
{'loss': 1.0472, 'grad_norm': 1.4261963367462158, 'learning_rate': 0.0001303905157574247, 'epoch': 0.46}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9564, 'grad_norm': 1.0749108791351318, 'learning_rate': 0.0001303393489414338, 'epoch': 0.46}
{'loss': 1.0632, 'grad_norm': 1.2199480533599854, 'learning_rate': 0.00013028817337572076, 'epoch': 0.46}
{'loss': 1.1508, 'grad_norm': 1.0834827423095703, 'learning_rate': 0.00013023698907504446, 'epoch': 0.46}
{'loss': 1.1419, 'grad_norm': 1.3015825748443604, 'learning_rate': 0.0001301857960541661, 'epoch': 0.46}
{'loss': 1.0044, 'grad_norm': 1.6034390926361084, 'learning_rate': 0.00013013459432784961, 'epoch': 0.46}
{'loss': 0.8972, 'grad_norm': 0.8568868041038513, 'learning_rate': 0.00013008338391086124, 'epoch': 0.46}
{'loss': 1.0059, 'grad_norm': 1.102287769317627, 'learning_rate': 0.00013003216481796982, 'epoch': 0.46}
{'loss': 0.9369, 'grad_norm': 1.3262920379638672, 'learning_rate': 0.00012998093706394675, 'epoch': 0.46}
{'loss': 1.2822, 'grad_norm': 1.0940654277801514, 'learning_rate': 0.00012992970066356583, 'epoch': 0.46}
{'loss': 1.2404, 'grad_norm': 1.1861447095870972, 'learning_rate': 0.0001298784556316034, 'epoch': 0.46}
{'loss': 1.1446, 'grad_norm': 1.1806787252426147, 'learning_rate': 0.00012982720198283825, 'epoch': 0.46}
{'loss': 0.775, 'grad_norm': 1.2161166667938232, 'learning_rate': 0.00012977593973205175, 'epoch': 0.46}
{'loss': 0.834, 'grad_norm': 1.2264611721038818, 'learning_rate': 0.00012972466889402763, 'epoch': 0.46}
{'loss': 1.1074, 'grad_norm': 1.1403398513793945, 'learning_rate': 0.00012967338948355217, 'epoch': 0.46}
{'loss': 1.0217, 'grad_norm': 1.1624451875686646, 'learning_rate': 0.00012962210151541413, 'epoch': 0.46}
{'loss': 0.9041, 'grad_norm': 1.2381577491760254, 'learning_rate': 0.00012957080500440468, 'epoch': 0.46}
{'loss': 0.7882, 'grad_norm': 1.2180920839309692, 'learning_rate': 0.0001295194999653175, 'epoch': 0.46}
{'loss': 1.1258, 'grad_norm': 1.1750150918960571, 'learning_rate': 0.00012946818641294868, 'epoch': 0.46}
{'loss': 1.0075, 'grad_norm': 1.257935643196106, 'learning_rate': 0.00012941686436209688, 'epoch': 0.46}
{'loss': 0.8232, 'grad_norm': 1.1447899341583252, 'learning_rate': 0.0001293655338275631, 'epoch': 0.46}
{'loss': 0.9559, 'grad_norm': 1.2973781824111938, 'learning_rate': 0.00012931419482415077, 'epoch': 0.46}
{'loss': 0.987, 'grad_norm': 1.4204339981079102, 'learning_rate': 0.00012926284736666585, 'epoch': 0.46}
{'loss': 1.0233, 'grad_norm': 1.01433265209198, 'learning_rate': 0.0001292114914699167, 'epoch': 0.47}
{'loss': 0.8944, 'grad_norm': 1.3415905237197876, 'learning_rate': 0.0001291601271487141, 'epoch': 0.47}
{'loss': 0.7812, 'grad_norm': 1.4428223371505737, 'learning_rate': 0.00012910875441787128, 'epoch': 0.47}
{'loss': 0.8784, 'grad_norm': 1.1294924020767212, 'learning_rate': 0.00012905737329220392, 'epoch': 0.47}
{'loss': 1.367, 'grad_norm': 0.9521068930625916, 'learning_rate': 0.00012900598378653007, 'epoch': 0.47}
{'loss': 0.8057, 'grad_norm': 1.2375599145889282, 'learning_rate': 0.00012895458591567015, 'epoch': 0.47}
{'loss': 0.968, 'grad_norm': 1.0191363096237183, 'learning_rate': 0.00012890317969444716, 'epoch': 0.47}
{'loss': 1.1037, 'grad_norm': 1.1566755771636963, 'learning_rate': 0.00012885176513768637, 'epoch': 0.47}
{'loss': 0.7849, 'grad_norm': 0.9263859391212463, 'learning_rate': 0.00012880034226021546, 'epoch': 0.47}
{'loss': 0.9762, 'grad_norm': 1.0310460329055786, 'learning_rate': 0.00012874891107686457, 'epoch': 0.47}
{'loss': 1.1692, 'grad_norm': 0.9555992484092712, 'learning_rate': 0.00012869747160246616, 'epoch': 0.47}
{'loss': 1.0858, 'grad_norm': 1.0145758390426636, 'learning_rate': 0.0001286460238518552, 'epoch': 0.47}
{'loss': 0.84, 'grad_norm': 1.216544270515442, 'learning_rate': 0.00012859456783986893, 'epoch': 0.47}
{'loss': 0.8706, 'grad_norm': 1.3686511516571045, 'learning_rate': 0.00012854310358134704, 'epoch': 0.47}
{'loss': 0.6539, 'grad_norm': 1.4890391826629639, 'learning_rate': 0.0001284916310911315, 'epoch': 0.47}
{'loss': 0.8761, 'grad_norm': 1.2958375215530396, 'learning_rate': 0.00012844015038406684, 'epoch': 0.47}
{'loss': 1.1218, 'grad_norm': 1.1171571016311646, 'learning_rate': 0.0001283886614749998, 'epoch': 0.47}
{'loss': 0.7589, 'grad_norm': 1.4223251342773438, 'learning_rate': 0.0001283371643787795, 'epoch': 0.47}
{'loss': 1.1869, 'grad_norm': 1.5778344869613647, 'learning_rate': 0.0001282856591102575, 'epoch': 0.47}
{'loss': 1.017, 'grad_norm': 0.9560646414756775, 'learning_rate': 0.00012823414568428768, 'epoch': 0.47}
{'loss': 0.9081, 'grad_norm': 1.088741421699524, 'learning_rate': 0.00012818262411572618, 'epoch': 0.47}
{'loss': 1.0779, 'grad_norm': 1.0837547779083252, 'learning_rate': 0.00012813109441943166, 'epoch': 0.47}
{'loss': 1.0014, 'grad_norm': 1.1275079250335693, 'learning_rate': 0.00012807955661026498, 'epoch': 0.47}
{'loss': 0.9459, 'grad_norm': 1.2804410457611084, 'learning_rate': 0.00012802801070308946, 'epoch': 0.47}
{'loss': 1.0814, 'grad_norm': 0.9478957056999207, 'learning_rate': 0.0001279764567127706, 'epoch': 0.47}
{'loss': 0.9869, 'grad_norm': 1.3063400983810425, 'learning_rate': 0.00012792489465417644, 'epoch': 0.47}
{'loss': 1.0581, 'grad_norm': 1.0563827753067017, 'learning_rate': 0.00012787332454217712, 'epoch': 0.47}
{'loss': 0.6659, 'grad_norm': 0.8665907979011536, 'learning_rate': 0.0001278217463916453, 'epoch': 0.47}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.083, 'grad_norm': 1.5395671129226685, 'learning_rate': 0.0001277701602174558, 'epoch': 0.47}
{'loss': 0.7908, 'grad_norm': 1.2728283405303955, 'learning_rate': 0.00012771856603448583, 'epoch': 0.47}
{'loss': 0.8445, 'grad_norm': 1.2382497787475586, 'learning_rate': 0.00012766696385761494, 'epoch': 0.47}
{'loss': 1.1392, 'grad_norm': 1.3862011432647705, 'learning_rate': 0.00012761535370172492, 'epoch': 0.47}
{'loss': 0.9905, 'grad_norm': 1.09572172164917, 'learning_rate': 0.0001275637355816999, 'epoch': 0.47}
{'loss': 0.9827, 'grad_norm': 0.9559653997421265, 'learning_rate': 0.00012751210951242637, 'epoch': 0.47}
{'loss': 0.7216, 'grad_norm': 1.526296854019165, 'learning_rate': 0.0001274604755087929, 'epoch': 0.47}
{'loss': 0.7948, 'grad_norm': 1.4882309436798096, 'learning_rate': 0.00012740883358569057, 'epoch': 0.47}
{'loss': 1.0167, 'grad_norm': 1.1361247301101685, 'learning_rate': 0.0001273571837580127, 'epoch': 0.47}
{'loss': 0.6927, 'grad_norm': 1.10939621925354, 'learning_rate': 0.00012730552604065475, 'epoch': 0.47}
{'loss': 1.3361, 'grad_norm': 1.1943988800048828, 'learning_rate': 0.00012725386044851463, 'epoch': 0.47}
{'loss': 1.0308, 'grad_norm': 1.0358668565750122, 'learning_rate': 0.00012720218699649243, 'epoch': 0.47}
{'loss': 1.1434, 'grad_norm': 0.9994547367095947, 'learning_rate': 0.00012715050569949053, 'epoch': 0.47}
{'loss': 1.0525, 'grad_norm': 1.1885708570480347, 'learning_rate': 0.00012709881657241355, 'epoch': 0.47}
{'loss': 0.9312, 'grad_norm': 1.133029818534851, 'learning_rate': 0.0001270471196301684, 'epoch': 0.47}
{'loss': 1.0716, 'grad_norm': 1.1241555213928223, 'learning_rate': 0.0001269954148876642, 'epoch': 0.47}
{'loss': 0.8953, 'grad_norm': 1.3987592458724976, 'learning_rate': 0.00012694370235981237, 'epoch': 0.47}
{'loss': 0.7504, 'grad_norm': 1.3268437385559082, 'learning_rate': 0.00012689198206152657, 'epoch': 0.47}
{'loss': 1.2893, 'grad_norm': 1.0387938022613525, 'learning_rate': 0.00012684025400772268, 'epoch': 0.47}
{'loss': 1.3731, 'grad_norm': 1.4340006113052368, 'learning_rate': 0.00012678851821331882, 'epoch': 0.47}
{'loss': 1.0422, 'grad_norm': 1.1250927448272705, 'learning_rate': 0.0001267367746932353, 'epoch': 0.47}
{'loss': 1.1093, 'grad_norm': 1.7864704132080078, 'learning_rate': 0.00012668502346239478, 'epoch': 0.47}
{'loss': 0.9057, 'grad_norm': 1.0674489736557007, 'learning_rate': 0.00012663326453572201, 'epoch': 0.47}
{'loss': 0.9577, 'grad_norm': 1.3335734605789185, 'learning_rate': 0.00012658149792814404, 'epoch': 0.47}
{'loss': 0.8796, 'grad_norm': 1.1591979265213013, 'learning_rate': 0.0001265297236545901, 'epoch': 0.47}
{'loss': 1.1403, 'grad_norm': 1.1932761669158936, 'learning_rate': 0.00012647794172999162, 'epoch': 0.47}
{'loss': 1.3667, 'grad_norm': 1.4146766662597656, 'learning_rate': 0.00012642615216928226, 'epoch': 0.47}
{'loss': 0.7982, 'grad_norm': 1.141255497932434, 'learning_rate': 0.00012637435498739794, 'epoch': 0.47}
{'loss': 0.8724, 'grad_norm': 0.9935621023178101, 'learning_rate': 0.00012632255019927668, 'epoch': 0.47}
{'loss': 0.8311, 'grad_norm': 1.2140470743179321, 'learning_rate': 0.0001262707378198587, 'epoch': 0.47}
{'loss': 0.7652, 'grad_norm': 0.9624917507171631, 'learning_rate': 0.00012621891786408648, 'epoch': 0.47}
{'loss': 1.1442, 'grad_norm': 1.3534270524978638, 'learning_rate': 0.00012616709034690463, 'epoch': 0.47}
{'loss': 0.809, 'grad_norm': 1.2590408325195312, 'learning_rate': 0.00012611525528326, 'epoch': 0.47}
{'loss': 1.29, 'grad_norm': 0.9971252679824829, 'learning_rate': 0.00012606341268810145, 'epoch': 0.47}
{'loss': 0.9058, 'grad_norm': 1.1850870847702026, 'learning_rate': 0.0001260115625763803, 'epoch': 0.47}
{'loss': 0.944, 'grad_norm': 1.03399658203125, 'learning_rate': 0.00012595970496304975, 'epoch': 0.47}
{'loss': 0.7932, 'grad_norm': 1.0057166814804077, 'learning_rate': 0.00012590783986306532, 'epoch': 0.47}
{'loss': 0.962, 'grad_norm': 1.1838464736938477, 'learning_rate': 0.00012585596729138468, 'epoch': 0.48}
{'loss': 0.8865, 'grad_norm': 1.5842665433883667, 'learning_rate': 0.0001258040872629676, 'epoch': 0.48}
{'loss': 1.1556, 'grad_norm': 1.20513117313385, 'learning_rate': 0.00012575219979277602, 'epoch': 0.48}
{'loss': 1.304, 'grad_norm': 1.2055257558822632, 'learning_rate': 0.0001257003048957741, 'epoch': 0.48}
{'loss': 1.0796, 'grad_norm': 1.4032093286514282, 'learning_rate': 0.00012564840258692803, 'epoch': 0.48}
{'loss': 0.9815, 'grad_norm': 1.39373779296875, 'learning_rate': 0.00012559649288120618, 'epoch': 0.48}
{'loss': 1.3048, 'grad_norm': 1.4617997407913208, 'learning_rate': 0.00012554457579357905, 'epoch': 0.48}
{'loss': 0.9339, 'grad_norm': 1.0977712869644165, 'learning_rate': 0.00012549265133901934, 'epoch': 0.48}
{'loss': 0.9747, 'grad_norm': 1.897651195526123, 'learning_rate': 0.00012544071953250177, 'epoch': 0.48}
{'loss': 1.1964, 'grad_norm': 1.185106873512268, 'learning_rate': 0.0001253887803890032, 'epoch': 0.48}
{'loss': 0.9173, 'grad_norm': 0.9642559289932251, 'learning_rate': 0.00012533683392350263, 'epoch': 0.48}
{'loss': 1.4075, 'grad_norm': 1.0038533210754395, 'learning_rate': 0.00012528488015098126, 'epoch': 0.48}
{'loss': 0.7338, 'grad_norm': 1.0338901281356812, 'learning_rate': 0.00012523291908642217, 'epoch': 0.48}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0668, 'grad_norm': 1.2122290134429932, 'learning_rate': 0.0001251809507448108, 'epoch': 0.48}
{'loss': 0.9894, 'grad_norm': 1.2518200874328613, 'learning_rate': 0.00012512897514113452, 'epoch': 0.48}
{'loss': 0.7926, 'grad_norm': 1.0699238777160645, 'learning_rate': 0.00012507699229038284, 'epoch': 0.48}
{'loss': 0.7751, 'grad_norm': 1.1905920505523682, 'learning_rate': 0.00012502500220754737, 'epoch': 0.48}
{'loss': 1.177, 'grad_norm': 1.1816647052764893, 'learning_rate': 0.0001249730049076218, 'epoch': 0.48}
{'loss': 0.828, 'grad_norm': 1.516862392425537, 'learning_rate': 0.00012492100040560188, 'epoch': 0.48}
{'loss': 0.928, 'grad_norm': 1.508937954902649, 'learning_rate': 0.0001248689887164855, 'epoch': 0.48}
{'loss': 1.2102, 'grad_norm': 1.0026564598083496, 'learning_rate': 0.00012481696985527254, 'epoch': 0.48}
{'loss': 0.8382, 'grad_norm': 1.1823006868362427, 'learning_rate': 0.00012476494383696502, 'epoch': 0.48}
{'loss': 0.8786, 'grad_norm': 1.2824933528900146, 'learning_rate': 0.00012471291067656697, 'epoch': 0.48}
{'loss': 0.9069, 'grad_norm': 1.0912667512893677, 'learning_rate': 0.00012466087038908453, 'epoch': 0.48}
{'loss': 1.1526, 'grad_norm': 1.0566312074661255, 'learning_rate': 0.00012460882298952582, 'epoch': 0.48}
{'loss': 0.9594, 'grad_norm': 1.3221031427383423, 'learning_rate': 0.00012455676849290113, 'epoch': 0.48}
{'loss': 1.138, 'grad_norm': 1.225130319595337, 'learning_rate': 0.00012450470691422266, 'epoch': 0.48}
{'loss': 1.2216, 'grad_norm': 1.0157582759857178, 'learning_rate': 0.00012445263826850475, 'epoch': 0.48}
{'loss': 0.9294, 'grad_norm': 1.1948952674865723, 'learning_rate': 0.00012440056257076375, 'epoch': 0.48}
{'loss': 0.8195, 'grad_norm': 1.5659570693969727, 'learning_rate': 0.00012434847983601804, 'epoch': 0.48}
{'loss': 1.0725, 'grad_norm': 1.016383171081543, 'learning_rate': 0.000124296390079288, 'epoch': 0.48}
{'loss': 0.7316, 'grad_norm': 1.2660903930664062, 'learning_rate': 0.0001242442933155961, 'epoch': 0.48}
{'loss': 0.9105, 'grad_norm': 1.2754136323928833, 'learning_rate': 0.00012419218955996676, 'epoch': 0.48}
{'loss': 1.22, 'grad_norm': 0.9209396243095398, 'learning_rate': 0.0001241400788274265, 'epoch': 0.48}
{'loss': 0.8642, 'grad_norm': 1.0197651386260986, 'learning_rate': 0.0001240879611330038, 'epoch': 0.48}
{'loss': 1.1642, 'grad_norm': 1.3185091018676758, 'learning_rate': 0.0001240358364917291, 'epoch': 0.48}
{'loss': 1.0225, 'grad_norm': 1.0863492488861084, 'learning_rate': 0.00012398370491863495, 'epoch': 0.48}
{'loss': 1.0383, 'grad_norm': 1.1211261749267578, 'learning_rate': 0.0001239315664287558, 'epoch': 0.48}
{'loss': 1.149, 'grad_norm': 1.1581246852874756, 'learning_rate': 0.00012387942103712815, 'epoch': 0.48}
{'loss': 0.8569, 'grad_norm': 1.3988065719604492, 'learning_rate': 0.00012382726875879052, 'epoch': 0.48}
{'loss': 1.1383, 'grad_norm': 1.063328742980957, 'learning_rate': 0.00012377510960878333, 'epoch': 0.48}
{'loss': 1.1496, 'grad_norm': 1.0823605060577393, 'learning_rate': 0.00012372294360214905, 'epoch': 0.48}
{'loss': 0.994, 'grad_norm': 0.94118732213974, 'learning_rate': 0.0001236707707539321, 'epoch': 0.48}
{'loss': 1.0242, 'grad_norm': 1.1763720512390137, 'learning_rate': 0.00012361859107917886, 'epoch': 0.48}
{'loss': 0.9439, 'grad_norm': 1.501261591911316, 'learning_rate': 0.00012356640459293771, 'epoch': 0.48}
{'loss': 0.9757, 'grad_norm': 1.2236378192901611, 'learning_rate': 0.000123514211310259, 'epoch': 0.48}
{'loss': 0.8014, 'grad_norm': 1.1567105054855347, 'learning_rate': 0.00012346201124619502, 'epoch': 0.48}
{'loss': 0.9379, 'grad_norm': 1.093124508857727, 'learning_rate': 0.0001234098044158, 'epoch': 0.48}
{'loss': 0.9063, 'grad_norm': 1.2763906717300415, 'learning_rate': 0.00012335759083413015, 'epoch': 0.48}
{'loss': 0.7693, 'grad_norm': 1.2909399271011353, 'learning_rate': 0.00012330537051624357, 'epoch': 0.48}
{'loss': 1.1168, 'grad_norm': 1.0829989910125732, 'learning_rate': 0.0001232531434772004, 'epoch': 0.48}
{'loss': 0.9725, 'grad_norm': 1.0713976621627808, 'learning_rate': 0.0001232009097320627, 'epoch': 0.48}
{'loss': 0.8902, 'grad_norm': 1.3830093145370483, 'learning_rate': 0.00012314866929589432, 'epoch': 0.48}
{'loss': 0.8884, 'grad_norm': 1.1487889289855957, 'learning_rate': 0.00012309642218376123, 'epoch': 0.48}
{'loss': 0.7311, 'grad_norm': 1.4799292087554932, 'learning_rate': 0.0001230441684107312, 'epoch': 0.48}
{'loss': 0.8907, 'grad_norm': 0.945763349533081, 'learning_rate': 0.00012299190799187405, 'epoch': 0.48}
{'loss': 0.9373, 'grad_norm': 1.3662563562393188, 'learning_rate': 0.00012293964094226137, 'epoch': 0.48}
{'loss': 1.0066, 'grad_norm': 1.2058935165405273, 'learning_rate': 0.00012288736727696672, 'epoch': 0.48}
{'loss': 0.8727, 'grad_norm': 0.9793169498443604, 'learning_rate': 0.00012283508701106557, 'epoch': 0.48}
{'loss': 1.1079, 'grad_norm': 1.2047311067581177, 'learning_rate': 0.00012278280015963535, 'epoch': 0.48}
{'loss': 0.9058, 'grad_norm': 1.0401486158370972, 'learning_rate': 0.00012273050673775529, 'epoch': 0.48}
{'loss': 1.0521, 'grad_norm': 1.523148775100708, 'learning_rate': 0.00012267820676050656, 'epoch': 0.48}
{'loss': 0.982, 'grad_norm': 0.9353325963020325, 'learning_rate': 0.00012262590024297225, 'epoch': 0.48}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.971, 'grad_norm': 1.0743255615234375, 'learning_rate': 0.00012257358720023723, 'epoch': 0.48}
{'loss': 0.9936, 'grad_norm': 1.2931444644927979, 'learning_rate': 0.00012252126764738844, 'epoch': 0.48}
{'loss': 1.034, 'grad_norm': 1.3476948738098145, 'learning_rate': 0.0001224689415995145, 'epoch': 0.49}
{'loss': 1.0033, 'grad_norm': 1.1232943534851074, 'learning_rate': 0.00012241660907170608, 'epoch': 0.49}
{'loss': 0.9404, 'grad_norm': 1.0894231796264648, 'learning_rate': 0.00012236427007905558, 'epoch': 0.49}
{'loss': 0.738, 'grad_norm': 1.2517874240875244, 'learning_rate': 0.00012231192463665725, 'epoch': 0.49}
{'loss': 1.1912, 'grad_norm': 1.9927772283554077, 'learning_rate': 0.00012225957275960734, 'epoch': 0.49}
{'loss': 1.0445, 'grad_norm': 1.1782904863357544, 'learning_rate': 0.0001222072144630039, 'epoch': 0.49}
{'loss': 0.869, 'grad_norm': 1.62613844871521, 'learning_rate': 0.00012215484976194676, 'epoch': 0.49}
{'loss': 0.8343, 'grad_norm': 1.030131220817566, 'learning_rate': 0.00012210247867153765, 'epoch': 0.49}
{'loss': 0.8526, 'grad_norm': 1.049051284790039, 'learning_rate': 0.00012205010120688011, 'epoch': 0.49}
{'loss': 1.2441, 'grad_norm': 1.0352015495300293, 'learning_rate': 0.00012199771738307963, 'epoch': 0.49}
{'loss': 1.0043, 'grad_norm': 1.2107226848602295, 'learning_rate': 0.00012194532721524341, 'epoch': 0.49}
{'loss': 0.938, 'grad_norm': 1.2392005920410156, 'learning_rate': 0.00012189293071848051, 'epoch': 0.49}
{'loss': 0.9935, 'grad_norm': 1.03372323513031, 'learning_rate': 0.00012184052790790184, 'epoch': 0.49}
{'loss': 0.9288, 'grad_norm': 1.0101072788238525, 'learning_rate': 0.00012178811879862013, 'epoch': 0.49}
{'loss': 0.8666, 'grad_norm': 1.3178406953811646, 'learning_rate': 0.00012173570340574992, 'epoch': 0.49}
{'loss': 1.1047, 'grad_norm': 1.1306569576263428, 'learning_rate': 0.00012168328174440753, 'epoch': 0.49}
{'loss': 1.1402, 'grad_norm': 1.113071084022522, 'learning_rate': 0.00012163085382971112, 'epoch': 0.49}
{'loss': 0.9264, 'grad_norm': 0.9850815534591675, 'learning_rate': 0.00012157841967678063, 'epoch': 0.49}
{'loss': 1.0241, 'grad_norm': 1.281821608543396, 'learning_rate': 0.00012152597930073786, 'epoch': 0.49}
{'loss': 1.0172, 'grad_norm': 1.0135940313339233, 'learning_rate': 0.00012147353271670634, 'epoch': 0.49}
{'loss': 0.962, 'grad_norm': 1.1652745008468628, 'learning_rate': 0.0001214210799398114, 'epoch': 0.49}
{'loss': 0.8371, 'grad_norm': 1.2426494359970093, 'learning_rate': 0.00012136862098518021, 'epoch': 0.49}
{'loss': 1.0255, 'grad_norm': 1.0386561155319214, 'learning_rate': 0.0001213161558679416, 'epoch': 0.49}
{'loss': 1.0258, 'grad_norm': 1.0760841369628906, 'learning_rate': 0.00012126368460322637, 'epoch': 0.49}
{'loss': 0.9042, 'grad_norm': 1.6063917875289917, 'learning_rate': 0.0001212112072061669, 'epoch': 0.49}
{'loss': 1.0536, 'grad_norm': 1.507609248161316, 'learning_rate': 0.00012115872369189742, 'epoch': 0.49}
{'loss': 1.3618, 'grad_norm': 1.436998724937439, 'learning_rate': 0.00012110623407555397, 'epoch': 0.49}
{'loss': 0.9847, 'grad_norm': 1.191036343574524, 'learning_rate': 0.00012105373837227425, 'epoch': 0.49}
{'loss': 0.9698, 'grad_norm': 1.3743120431900024, 'learning_rate': 0.0001210012365971978, 'epoch': 0.49}
{'loss': 1.021, 'grad_norm': 1.0638337135314941, 'learning_rate': 0.00012094872876546586, 'epoch': 0.49}
{'loss': 1.0493, 'grad_norm': 1.1994718313217163, 'learning_rate': 0.00012089621489222147, 'epoch': 0.49}
{'loss': 1.0106, 'grad_norm': 1.118863821029663, 'learning_rate': 0.00012084369499260933, 'epoch': 0.49}
{'loss': 0.9601, 'grad_norm': 1.1735343933105469, 'learning_rate': 0.00012079116908177593, 'epoch': 0.49}
{'loss': 1.1547, 'grad_norm': 1.1757878065109253, 'learning_rate': 0.00012073863717486956, 'epoch': 0.49}
{'loss': 0.9837, 'grad_norm': 1.403214931488037, 'learning_rate': 0.00012068609928704009, 'epoch': 0.49}
{'loss': 0.8929, 'grad_norm': 1.3173149824142456, 'learning_rate': 0.00012063355543343924, 'epoch': 0.49}
{'loss': 1.0921, 'grad_norm': 1.396033525466919, 'learning_rate': 0.00012058100562922035, 'epoch': 0.49}
{'loss': 1.1207, 'grad_norm': 1.5524730682373047, 'learning_rate': 0.0001205284498895386, 'epoch': 0.49}
{'loss': 0.738, 'grad_norm': 1.1531007289886475, 'learning_rate': 0.00012047588822955076, 'epoch': 0.49}
{'loss': 1.0029, 'grad_norm': 0.9906806945800781, 'learning_rate': 0.0001204233206644154, 'epoch': 0.49}
{'loss': 1.1917, 'grad_norm': 1.3198518753051758, 'learning_rate': 0.00012037074720929273, 'epoch': 0.49}
{'loss': 1.2413, 'grad_norm': 1.139729380607605, 'learning_rate': 0.00012031816787934464, 'epoch': 0.49}
{'loss': 1.0499, 'grad_norm': 1.059187650680542, 'learning_rate': 0.00012026558268973484, 'epoch': 0.49}
{'loss': 1.0572, 'grad_norm': 1.143088698387146, 'learning_rate': 0.00012021299165562858, 'epoch': 0.49}
{'loss': 0.882, 'grad_norm': 1.161970853805542, 'learning_rate': 0.00012016039479219292, 'epoch': 0.49}
{'loss': 0.8446, 'grad_norm': 1.1437281370162964, 'learning_rate': 0.00012010779211459648, 'epoch': 0.49}
{'loss': 0.9788, 'grad_norm': 1.2899928092956543, 'learning_rate': 0.00012005518363800963, 'epoch': 0.49}
{'loss': 0.8401, 'grad_norm': 0.9695962071418762, 'learning_rate': 0.00012000256937760445, 'epoch': 0.49}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2572, 'grad_norm': 1.2103691101074219, 'learning_rate': 0.00011994994934855459, 'epoch': 0.49}
{'loss': 0.9154, 'grad_norm': 1.271511197090149, 'learning_rate': 0.00011989732356603545, 'epoch': 0.49}
{'loss': 0.8708, 'grad_norm': 1.0613086223602295, 'learning_rate': 0.000119844692045224, 'epoch': 0.49}
{'loss': 0.7833, 'grad_norm': 1.3516199588775635, 'learning_rate': 0.00011979205480129896, 'epoch': 0.49}
{'loss': 1.0391, 'grad_norm': 0.9865981936454773, 'learning_rate': 0.00011973941184944066, 'epoch': 0.49}
{'loss': 1.2725, 'grad_norm': 1.3548365831375122, 'learning_rate': 0.00011968676320483103, 'epoch': 0.49}
{'loss': 1.1599, 'grad_norm': 0.9237341284751892, 'learning_rate': 0.00011963410888265373, 'epoch': 0.49}
{'loss': 0.885, 'grad_norm': 1.188170313835144, 'learning_rate': 0.00011958144889809397, 'epoch': 0.49}
{'loss': 0.805, 'grad_norm': 1.1350256204605103, 'learning_rate': 0.00011952878326633872, 'epoch': 0.49}
{'loss': 0.7765, 'grad_norm': 1.2807062864303589, 'learning_rate': 0.0001194761120025764, 'epoch': 0.49}
{'loss': 0.971, 'grad_norm': 1.0485830307006836, 'learning_rate': 0.0001194234351219972, 'epoch': 0.49}
{'loss': 0.6613, 'grad_norm': 1.4753899574279785, 'learning_rate': 0.00011937075263979289, 'epoch': 0.49}
{'loss': 0.8947, 'grad_norm': 1.212398886680603, 'learning_rate': 0.00011931806457115677, 'epoch': 0.49}
{'loss': 1.3076, 'grad_norm': 1.255976915359497, 'learning_rate': 0.00011926537093128392, 'epoch': 0.49}
{'loss': 1.0568, 'grad_norm': 1.2597993612289429, 'learning_rate': 0.00011921267173537086, 'epoch': 0.49}
{'loss': 1.4472, 'grad_norm': 1.3365634679794312, 'learning_rate': 0.0001191599669986158, 'epoch': 0.49}
{'loss': 0.8665, 'grad_norm': 1.098752498626709, 'learning_rate': 0.00011910725673621856, 'epoch': 0.49}
{'loss': 0.9988, 'grad_norm': 1.29127037525177, 'learning_rate': 0.00011905454096338048, 'epoch': 0.5}
{'loss': 0.7791, 'grad_norm': 1.1750527620315552, 'learning_rate': 0.00011900181969530461, 'epoch': 0.5}
{'loss': 0.6407, 'grad_norm': 2.044013500213623, 'learning_rate': 0.00011894909294719547, 'epoch': 0.5}
{'loss': 0.7762, 'grad_norm': 1.1975404024124146, 'learning_rate': 0.00011889636073425918, 'epoch': 0.5}
{'loss': 1.0404, 'grad_norm': 1.462204098701477, 'learning_rate': 0.00011884362307170344, 'epoch': 0.5}
{'loss': 0.9286, 'grad_norm': 1.0940078496932983, 'learning_rate': 0.00011879087997473759, 'epoch': 0.5}
{'loss': 1.0565, 'grad_norm': 1.1696735620498657, 'learning_rate': 0.00011873813145857249, 'epoch': 0.5}
{'loss': 0.967, 'grad_norm': 1.2919107675552368, 'learning_rate': 0.00011868537753842051, 'epoch': 0.5}
{'loss': 0.8956, 'grad_norm': 1.2741581201553345, 'learning_rate': 0.00011863261822949563, 'epoch': 0.5}
{'loss': 1.0297, 'grad_norm': 1.2578860521316528, 'learning_rate': 0.0001185798535470134, 'epoch': 0.5}
{'loss': 1.1625, 'grad_norm': 0.9614523649215698, 'learning_rate': 0.0001185270835061909, 'epoch': 0.5}
{'loss': 1.2879, 'grad_norm': 0.9357653856277466, 'learning_rate': 0.00011847430812224678, 'epoch': 0.5}
{'loss': 0.9345, 'grad_norm': 1.0314522981643677, 'learning_rate': 0.00011842152741040116, 'epoch': 0.5}
{'loss': 0.8117, 'grad_norm': 1.3935941457748413, 'learning_rate': 0.00011836874138587579, 'epoch': 0.5}
{'loss': 1.1313, 'grad_norm': 1.079206943511963, 'learning_rate': 0.00011831595006389384, 'epoch': 0.5}
{'loss': 0.964, 'grad_norm': 1.2447731494903564, 'learning_rate': 0.00011826315345968013, 'epoch': 0.5}
{'loss': 1.1345, 'grad_norm': 1.206973910331726, 'learning_rate': 0.00011821035158846095, 'epoch': 0.5}
{'loss': 0.9393, 'grad_norm': 1.3663462400436401, 'learning_rate': 0.00011815754446546405, 'epoch': 0.5}
{'loss': 0.8035, 'grad_norm': 1.1517586708068848, 'learning_rate': 0.00011810473210591881, 'epoch': 0.5}
{'loss': 1.1495, 'grad_norm': 1.1697378158569336, 'learning_rate': 0.00011805191452505602, 'epoch': 0.5}
{'loss': 1.0813, 'grad_norm': 1.2387053966522217, 'learning_rate': 0.00011799909173810802, 'epoch': 0.5}
{'loss': 1.0801, 'grad_norm': 1.7428526878356934, 'learning_rate': 0.00011794626376030866, 'epoch': 0.5}
{'loss': 1.0471, 'grad_norm': 1.048652172088623, 'learning_rate': 0.00011789343060689329, 'epoch': 0.5}
{'loss': 1.0675, 'grad_norm': 1.086682677268982, 'learning_rate': 0.00011784059229309869, 'epoch': 0.5}
{'loss': 0.7734, 'grad_norm': 1.2231663465499878, 'learning_rate': 0.00011778774883416323, 'epoch': 0.5}
{'loss': 1.259, 'grad_norm': 1.312605381011963, 'learning_rate': 0.0001177349002453267, 'epoch': 0.5}
{'loss': 1.0473, 'grad_norm': 1.910897135734558, 'learning_rate': 0.00011768204654183033, 'epoch': 0.5}
{'loss': 0.9528, 'grad_norm': 1.127380132675171, 'learning_rate': 0.00011762918773891691, 'epoch': 0.5}
{'loss': 1.073, 'grad_norm': 1.7994015216827393, 'learning_rate': 0.00011757632385183063, 'epoch': 0.5}
{'loss': 0.9067, 'grad_norm': 1.1586344242095947, 'learning_rate': 0.00011752345489581727, 'epoch': 0.5}
{'loss': 1.1841, 'grad_norm': 1.130432367324829, 'learning_rate': 0.00011747058088612388, 'epoch': 0.5}
{'loss': 1.3343, 'grad_norm': 1.2276099920272827, 'learning_rate': 0.00011741770183799911, 'epoch': 0.5}
{'loss': 1.2848, 'grad_norm': 1.305748701095581, 'learning_rate': 0.00011736481776669306, 'epoch': 0.5}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8658, 'grad_norm': 1.0700167417526245, 'learning_rate': 0.00011731192868745715, 'epoch': 0.5}
{'loss': 1.0598, 'grad_norm': 1.1667556762695312, 'learning_rate': 0.00011725903461554444, 'epoch': 0.5}
{'loss': 1.2015, 'grad_norm': 0.9398633241653442, 'learning_rate': 0.00011720613556620925, 'epoch': 0.5}
{'loss': 0.8848, 'grad_norm': 1.3449715375900269, 'learning_rate': 0.00011715323155470745, 'epoch': 0.5}
{'loss': 1.0274, 'grad_norm': 1.0935947895050049, 'learning_rate': 0.0001171003225962963, 'epoch': 0.5}
{'loss': 0.8992, 'grad_norm': 1.0803033113479614, 'learning_rate': 0.00011704740870623446, 'epoch': 0.5}
{'loss': 1.2806, 'grad_norm': 1.198338270187378, 'learning_rate': 0.00011699448989978208, 'epoch': 0.5}
{'loss': 0.7379, 'grad_norm': 1.5327855348587036, 'learning_rate': 0.00011694156619220064, 'epoch': 0.5}
{'loss': 1.0945, 'grad_norm': 1.357467770576477, 'learning_rate': 0.00011688863759875313, 'epoch': 0.5}
{'loss': 1.0046, 'grad_norm': 2.1922647953033447, 'learning_rate': 0.00011683570413470383, 'epoch': 0.5}
{'loss': 1.0458, 'grad_norm': 1.0753071308135986, 'learning_rate': 0.00011678276581531861, 'epoch': 0.5}
{'loss': 0.9362, 'grad_norm': 1.2478368282318115, 'learning_rate': 0.00011672982265586456, 'epoch': 0.5}
{'loss': 1.147, 'grad_norm': 1.0476274490356445, 'learning_rate': 0.00011667687467161024, 'epoch': 0.5}
{'loss': 1.058, 'grad_norm': 0.9322709441184998, 'learning_rate': 0.00011662392187782558, 'epoch': 0.5}
{'loss': 1.3423, 'grad_norm': 1.7978402376174927, 'learning_rate': 0.00011657096428978191, 'epoch': 0.5}
{'loss': 1.111, 'grad_norm': 1.0409119129180908, 'learning_rate': 0.00011651800192275197, 'epoch': 0.5}
{'loss': 1.2086, 'grad_norm': 1.1927316188812256, 'learning_rate': 0.00011646503479200983, 'epoch': 0.5}
{'loss': 1.0238, 'grad_norm': 1.397908329963684, 'learning_rate': 0.00011641206291283097, 'epoch': 0.5}
{'loss': 0.8571, 'grad_norm': 1.2283449172973633, 'learning_rate': 0.0001163590863004922, 'epoch': 0.5}
{'loss': 1.1216, 'grad_norm': 0.9995788335800171, 'learning_rate': 0.00011630610497027174, 'epoch': 0.5}
{'loss': 1.0876, 'grad_norm': 1.223608374595642, 'learning_rate': 0.00011625311893744912, 'epoch': 0.5}
{'loss': 0.9312, 'grad_norm': 1.0658562183380127, 'learning_rate': 0.00011620012821730532, 'epoch': 0.5}
{'loss': 0.9782, 'grad_norm': 1.4721077680587769, 'learning_rate': 0.00011614713282512259, 'epoch': 0.5}
{'loss': 1.1305, 'grad_norm': 1.0067119598388672, 'learning_rate': 0.00011609413277618449, 'epoch': 0.5}
{'loss': 0.6598, 'grad_norm': 1.1602667570114136, 'learning_rate': 0.00011604112808577603, 'epoch': 0.5}
{'loss': 1.0902, 'grad_norm': 1.240570306777954, 'learning_rate': 0.0001159881187691835, 'epoch': 0.5}
{'loss': 0.7911, 'grad_norm': 1.1539443731307983, 'learning_rate': 0.00011593510484169452, 'epoch': 0.5}
{'loss': 0.9202, 'grad_norm': 1.1542463302612305, 'learning_rate': 0.00011588208631859807, 'epoch': 0.5}
{'loss': 0.9928, 'grad_norm': 1.2442631721496582, 'learning_rate': 0.00011582906321518438, 'epoch': 0.5}
{'loss': 1.1144, 'grad_norm': 1.556412696838379, 'learning_rate': 0.00011577603554674514, 'epoch': 0.5}
{'loss': 0.7502, 'grad_norm': 1.2859177589416504, 'learning_rate': 0.00011572300332857322, 'epoch': 0.5}
{'loss': 1.0946, 'grad_norm': 0.9858627319335938, 'learning_rate': 0.00011566996657596287, 'epoch': 0.5}
{'loss': 1.133, 'grad_norm': 1.0765501260757446, 'learning_rate': 0.00011561692530420965, 'epoch': 0.51}
{'loss': 0.923, 'grad_norm': 1.023611307144165, 'learning_rate': 0.00011556387952861036, 'epoch': 0.51}
{'loss': 1.0717, 'grad_norm': 1.3620284795761108, 'learning_rate': 0.00011551082926446322, 'epoch': 0.51}
{'loss': 0.9175, 'grad_norm': 1.0348304510116577, 'learning_rate': 0.0001154577745270676, 'epoch': 0.51}
{'loss': 1.0588, 'grad_norm': 1.3424224853515625, 'learning_rate': 0.0001154047153317243, 'epoch': 0.51}
{'loss': 1.0721, 'grad_norm': 1.1253002882003784, 'learning_rate': 0.00011535165169373527, 'epoch': 0.51}
{'loss': 1.1042, 'grad_norm': 1.221854329109192, 'learning_rate': 0.00011529858362840382, 'epoch': 0.51}
{'loss': 0.6894, 'grad_norm': 1.1180682182312012, 'learning_rate': 0.00011524551115103454, 'epoch': 0.51}
{'loss': 0.9655, 'grad_norm': 0.9838895201683044, 'learning_rate': 0.0001151924342769333, 'epoch': 0.51}
{'loss': 0.8657, 'grad_norm': 1.2351154088974, 'learning_rate': 0.00011513935302140717, 'epoch': 0.51}
{'loss': 1.084, 'grad_norm': 1.1038678884506226, 'learning_rate': 0.00011508626739976454, 'epoch': 0.51}
{'loss': 1.0066, 'grad_norm': 1.4611059427261353, 'learning_rate': 0.00011503317742731508, 'epoch': 0.51}
{'loss': 0.7723, 'grad_norm': 0.984919011592865, 'learning_rate': 0.00011498008311936965, 'epoch': 0.51}
{'loss': 0.9048, 'grad_norm': 1.1839505434036255, 'learning_rate': 0.00011492698449124042, 'epoch': 0.51}
{'loss': 1.1977, 'grad_norm': 0.9058681726455688, 'learning_rate': 0.00011487388155824075, 'epoch': 0.51}
{'loss': 0.9154, 'grad_norm': 1.109523057937622, 'learning_rate': 0.00011482077433568525, 'epoch': 0.51}
{'loss': 1.3171, 'grad_norm': 1.0063639879226685, 'learning_rate': 0.00011476766283888986, 'epoch': 0.51}
{'loss': 0.7392, 'grad_norm': 1.2805901765823364, 'learning_rate': 0.00011471454708317162, 'epoch': 0.51}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1799, 'grad_norm': 1.1923867464065552, 'learning_rate': 0.00011466142708384888, 'epoch': 0.51}
{'loss': 0.9426, 'grad_norm': 1.1395174264907837, 'learning_rate': 0.00011460830285624118, 'epoch': 0.51}
{'loss': 0.9508, 'grad_norm': 1.4804126024246216, 'learning_rate': 0.00011455517441566928, 'epoch': 0.51}
{'loss': 1.0396, 'grad_norm': 0.9781366586685181, 'learning_rate': 0.0001145020417774552, 'epoch': 0.51}
{'loss': 1.6573, 'grad_norm': 2.737166404724121, 'learning_rate': 0.00011444890495692213, 'epoch': 0.51}
{'loss': 1.2532, 'grad_norm': 1.1973692178726196, 'learning_rate': 0.00011439576396939449, 'epoch': 0.51}
{'loss': 0.9917, 'grad_norm': 0.9512979388237, 'learning_rate': 0.00011434261883019783, 'epoch': 0.51}
{'loss': 1.0684, 'grad_norm': 1.005533218383789, 'learning_rate': 0.00011428946955465895, 'epoch': 0.51}
{'loss': 0.9749, 'grad_norm': 1.1114572286605835, 'learning_rate': 0.00011423631615810593, 'epoch': 0.51}
{'loss': 1.2901, 'grad_norm': 1.1516774892807007, 'learning_rate': 0.00011418315865586788, 'epoch': 0.51}
{'loss': 1.2446, 'grad_norm': 0.9739663600921631, 'learning_rate': 0.00011412999706327521, 'epoch': 0.51}
{'loss': 0.6935, 'grad_norm': 0.935463011264801, 'learning_rate': 0.00011407683139565941, 'epoch': 0.51}
{'loss': 1.0149, 'grad_norm': 0.8771196007728577, 'learning_rate': 0.00011402366166835325, 'epoch': 0.51}
{'loss': 0.7873, 'grad_norm': 1.1732844114303589, 'learning_rate': 0.0001139704878966906, 'epoch': 0.51}
{'loss': 1.1863, 'grad_norm': 0.9466180801391602, 'learning_rate': 0.00011391731009600654, 'epoch': 0.51}
{'loss': 1.1213, 'grad_norm': 1.1021333932876587, 'learning_rate': 0.0001138641282816373, 'epoch': 0.51}
{'loss': 1.0626, 'grad_norm': 1.2983816862106323, 'learning_rate': 0.00011381094246892022, 'epoch': 0.51}
{'loss': 1.0382, 'grad_norm': 0.9974827766418457, 'learning_rate': 0.00011375775267319386, 'epoch': 0.51}
{'loss': 1.473, 'grad_norm': 1.2098426818847656, 'learning_rate': 0.00011370455890979793, 'epoch': 0.51}
{'loss': 1.0727, 'grad_norm': 1.0620630979537964, 'learning_rate': 0.00011365136119407319, 'epoch': 0.51}
{'loss': 0.7611, 'grad_norm': 1.3042359352111816, 'learning_rate': 0.00011359815954136164, 'epoch': 0.51}
{'loss': 0.9045, 'grad_norm': 1.420583963394165, 'learning_rate': 0.00011354495396700632, 'epoch': 0.51}
{'loss': 0.8757, 'grad_norm': 1.195580244064331, 'learning_rate': 0.00011349174448635158, 'epoch': 0.51}
{'loss': 0.5178, 'grad_norm': 1.0204269886016846, 'learning_rate': 0.0001134385311147427, 'epoch': 0.51}
{'loss': 1.0039, 'grad_norm': 1.4761956930160522, 'learning_rate': 0.00011338531386752618, 'epoch': 0.51}
{'loss': 0.9063, 'grad_norm': 0.9717167615890503, 'learning_rate': 0.00011333209276004959, 'epoch': 0.51}
{'loss': 1.0106, 'grad_norm': 1.2134743928909302, 'learning_rate': 0.00011327886780766168, 'epoch': 0.51}
{'loss': 1.0637, 'grad_norm': 1.0972182750701904, 'learning_rate': 0.00011322563902571226, 'epoch': 0.51}
{'loss': 0.9558, 'grad_norm': 1.3010114431381226, 'learning_rate': 0.00011317240642955225, 'epoch': 0.51}
{'loss': 0.9269, 'grad_norm': 1.151142954826355, 'learning_rate': 0.00011311917003453365, 'epoch': 0.51}
{'loss': 1.0653, 'grad_norm': 1.1334116458892822, 'learning_rate': 0.00011306592985600962, 'epoch': 0.51}
{'loss': 0.9963, 'grad_norm': 1.1234997510910034, 'learning_rate': 0.00011301268590933434, 'epoch': 0.51}
{'loss': 1.1187, 'grad_norm': 1.3551535606384277, 'learning_rate': 0.00011295943820986312, 'epoch': 0.51}
{'loss': 0.8066, 'grad_norm': 1.3882074356079102, 'learning_rate': 0.00011290618677295235, 'epoch': 0.51}
{'loss': 0.7153, 'grad_norm': 0.8966795802116394, 'learning_rate': 0.00011285293161395946, 'epoch': 0.51}
{'loss': 0.8855, 'grad_norm': 1.2310494184494019, 'learning_rate': 0.00011279967274824301, 'epoch': 0.51}
{'loss': 1.0975, 'grad_norm': 1.4673385620117188, 'learning_rate': 0.0001127464101911626, 'epoch': 0.51}
{'loss': 0.96, 'grad_norm': 1.4107532501220703, 'learning_rate': 0.00011269314395807888, 'epoch': 0.51}
{'loss': 0.8045, 'grad_norm': 1.3751713037490845, 'learning_rate': 0.00011263987406435359, 'epoch': 0.51}
{'loss': 0.8683, 'grad_norm': 1.5517358779907227, 'learning_rate': 0.00011258660052534951, 'epoch': 0.51}
{'loss': 0.6088, 'grad_norm': 1.0858441591262817, 'learning_rate': 0.00011253332335643043, 'epoch': 0.51}
{'loss': 1.3186, 'grad_norm': 1.0287420749664307, 'learning_rate': 0.0001124800425729613, 'epoch': 0.51}
{'loss': 1.2759, 'grad_norm': 1.3695201873779297, 'learning_rate': 0.00011242675819030797, 'epoch': 0.51}
{'loss': 0.9159, 'grad_norm': 1.1154770851135254, 'learning_rate': 0.00011237347022383746, 'epoch': 0.51}
{'loss': 0.9501, 'grad_norm': 1.3591392040252686, 'learning_rate': 0.00011232017868891769, 'epoch': 0.51}
{'loss': 0.7154, 'grad_norm': 1.0512292385101318, 'learning_rate': 0.00011226688360091775, 'epoch': 0.51}
{'loss': 1.0399, 'grad_norm': 1.0674570798873901, 'learning_rate': 0.00011221358497520768, 'epoch': 0.51}
{'loss': 0.8885, 'grad_norm': 1.2796064615249634, 'learning_rate': 0.00011216028282715854, 'epoch': 0.52}
{'loss': 0.7876, 'grad_norm': 1.0854902267456055, 'learning_rate': 0.00011210697717214238, 'epoch': 0.52}
{'loss': 0.9019, 'grad_norm': 1.615662932395935, 'learning_rate': 0.0001120536680255323, 'epoch': 0.52}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9504, 'grad_norm': 1.346914291381836, 'learning_rate': 0.00011200035540270246, 'epoch': 0.52}
{'loss': 1.2402, 'grad_norm': 1.4021689891815186, 'learning_rate': 0.0001119470393190279, 'epoch': 0.52}
{'loss': 1.1632, 'grad_norm': 1.6768537759780884, 'learning_rate': 0.00011189371978988477, 'epoch': 0.52}
{'loss': 1.2921, 'grad_norm': 1.184462070465088, 'learning_rate': 0.00011184039683065013, 'epoch': 0.52}
{'loss': 0.9568, 'grad_norm': 0.976800262928009, 'learning_rate': 0.00011178707045670204, 'epoch': 0.52}
{'loss': 1.004, 'grad_norm': 1.1413089036941528, 'learning_rate': 0.00011173374068341962, 'epoch': 0.52}
{'loss': 1.0321, 'grad_norm': 2.333085060119629, 'learning_rate': 0.00011168040752618291, 'epoch': 0.52}
{'loss': 0.9018, 'grad_norm': 1.052014708518982, 'learning_rate': 0.00011162707100037295, 'epoch': 0.52}
{'loss': 0.9394, 'grad_norm': 1.1597553491592407, 'learning_rate': 0.00011157373112137171, 'epoch': 0.52}
{'loss': 0.7709, 'grad_norm': 1.1259336471557617, 'learning_rate': 0.00011152038790456211, 'epoch': 0.52}
{'loss': 1.1057, 'grad_norm': 1.4667961597442627, 'learning_rate': 0.00011146704136532818, 'epoch': 0.52}
{'loss': 1.1056, 'grad_norm': 1.0470865964889526, 'learning_rate': 0.00011141369151905475, 'epoch': 0.52}
{'loss': 0.9858, 'grad_norm': 1.1690374612808228, 'learning_rate': 0.00011136033838112765, 'epoch': 0.52}
{'loss': 0.9844, 'grad_norm': 0.9629248976707458, 'learning_rate': 0.0001113069819669337, 'epoch': 0.52}
{'loss': 1.1338, 'grad_norm': 1.0395909547805786, 'learning_rate': 0.00011125362229186057, 'epoch': 0.52}
{'loss': 1.1939, 'grad_norm': 1.0302711725234985, 'learning_rate': 0.00011120025937129699, 'epoch': 0.52}
{'loss': 1.0895, 'grad_norm': 1.4290400743484497, 'learning_rate': 0.00011114689322063255, 'epoch': 0.52}
{'loss': 0.7843, 'grad_norm': 1.434517502784729, 'learning_rate': 0.00011109352385525783, 'epoch': 0.52}
{'loss': 1.0008, 'grad_norm': 1.1279609203338623, 'learning_rate': 0.0001110401512905642, 'epoch': 0.52}
{'loss': 0.9174, 'grad_norm': 1.4788875579833984, 'learning_rate': 0.00011098677554194417, 'epoch': 0.52}
{'loss': 1.1661, 'grad_norm': 1.3295150995254517, 'learning_rate': 0.00011093339662479098, 'epoch': 0.52}
{'loss': 0.9816, 'grad_norm': 1.1840282678604126, 'learning_rate': 0.00011088001455449885, 'epoch': 0.52}
{'loss': 1.1671, 'grad_norm': 1.4431755542755127, 'learning_rate': 0.00011082662934646295, 'epoch': 0.52}
{'loss': 1.0962, 'grad_norm': 1.9768651723861694, 'learning_rate': 0.00011077324101607929, 'epoch': 0.52}
{'loss': 0.7783, 'grad_norm': 1.4064624309539795, 'learning_rate': 0.00011071984957874479, 'epoch': 0.52}
{'loss': 0.7769, 'grad_norm': 1.2868342399597168, 'learning_rate': 0.00011066645504985733, 'epoch': 0.52}
{'loss': 1.3435, 'grad_norm': 1.193581223487854, 'learning_rate': 0.00011061305744481558, 'epoch': 0.52}
{'loss': 1.0555, 'grad_norm': 1.3168220520019531, 'learning_rate': 0.0001105596567790192, 'epoch': 0.52}
{'loss': 1.3559, 'grad_norm': 1.114495873451233, 'learning_rate': 0.00011050625306786866, 'epoch': 0.52}
{'loss': 0.9484, 'grad_norm': 1.4033658504486084, 'learning_rate': 0.00011045284632676536, 'epoch': 0.52}
{'loss': 1.3021, 'grad_norm': 0.9987555146217346, 'learning_rate': 0.00011039943657111151, 'epoch': 0.52}
{'loss': 0.9153, 'grad_norm': 1.8350396156311035, 'learning_rate': 0.00011034602381631026, 'epoch': 0.52}
{'loss': 0.8915, 'grad_norm': 0.9912970662117004, 'learning_rate': 0.00011029260807776556, 'epoch': 0.52}
{'loss': 0.9544, 'grad_norm': 0.9162352681159973, 'learning_rate': 0.00011023918937088224, 'epoch': 0.52}
{'loss': 0.9306, 'grad_norm': 1.2581093311309814, 'learning_rate': 0.00011018576771106605, 'epoch': 0.52}
{'loss': 0.8317, 'grad_norm': 1.2647442817687988, 'learning_rate': 0.00011013234311372353, 'epoch': 0.52}
{'loss': 0.7964, 'grad_norm': 1.2051070928573608, 'learning_rate': 0.00011007891559426203, 'epoch': 0.52}
{'loss': 0.7649, 'grad_norm': 0.8024423718452454, 'learning_rate': 0.00011002548516808983, 'epoch': 0.52}
{'loss': 1.1642, 'grad_norm': 1.203382134437561, 'learning_rate': 0.00010997205185061599, 'epoch': 0.52}
{'loss': 0.9582, 'grad_norm': 1.0107983350753784, 'learning_rate': 0.00010991861565725044, 'epoch': 0.52}
{'loss': 0.9504, 'grad_norm': 1.1732392311096191, 'learning_rate': 0.00010986517660340389, 'epoch': 0.52}
{'loss': 0.8954, 'grad_norm': 1.4872632026672363, 'learning_rate': 0.00010981173470448796, 'epoch': 0.52}
{'loss': 0.7684, 'grad_norm': 1.1543997526168823, 'learning_rate': 0.00010975828997591495, 'epoch': 0.52}
{'loss': 1.2396, 'grad_norm': 0.9926273226737976, 'learning_rate': 0.00010970484243309815, 'epoch': 0.52}
{'loss': 1.2432, 'grad_norm': 1.0926403999328613, 'learning_rate': 0.00010965139209145152, 'epoch': 0.52}
{'loss': 1.0514, 'grad_norm': 1.0131207704544067, 'learning_rate': 0.00010959793896638993, 'epoch': 0.52}
{'loss': 0.8329, 'grad_norm': 1.2895420789718628, 'learning_rate': 0.00010954448307332894, 'epoch': 0.52}
{'loss': 0.7571, 'grad_norm': 1.1814420223236084, 'learning_rate': 0.000109491024427685, 'epoch': 0.52}
{'loss': 0.9668, 'grad_norm': 1.1856552362442017, 'learning_rate': 0.00010943756304487531, 'epoch': 0.52}
{'loss': 0.6684, 'grad_norm': 1.565361738204956, 'learning_rate': 0.00010938409894031794, 'epoch': 0.52}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.014, 'grad_norm': 0.968472957611084, 'learning_rate': 0.00010933063212943162, 'epoch': 0.52}
{'loss': 1.0309, 'grad_norm': 1.0446476936340332, 'learning_rate': 0.00010927716262763592, 'epoch': 0.52}
{'loss': 1.0403, 'grad_norm': 1.0937917232513428, 'learning_rate': 0.00010922369045035121, 'epoch': 0.52}
{'loss': 0.878, 'grad_norm': 1.489182949066162, 'learning_rate': 0.00010917021561299863, 'epoch': 0.52}
{'loss': 0.9523, 'grad_norm': 1.011880874633789, 'learning_rate': 0.00010911673813100001, 'epoch': 0.52}
{'loss': 1.0689, 'grad_norm': 1.1324853897094727, 'learning_rate': 0.00010906325801977804, 'epoch': 0.52}
{'loss': 1.1995, 'grad_norm': 1.0252174139022827, 'learning_rate': 0.00010900977529475609, 'epoch': 0.52}
{'loss': 0.7594, 'grad_norm': 1.4648264646530151, 'learning_rate': 0.00010895628997135836, 'epoch': 0.52}
{'loss': 1.0523, 'grad_norm': 0.9112682938575745, 'learning_rate': 0.00010890280206500971, 'epoch': 0.52}
{'loss': 0.7956, 'grad_norm': 1.131844401359558, 'learning_rate': 0.00010884931159113586, 'epoch': 0.52}
{'loss': 1.1437, 'grad_norm': 1.104663610458374, 'learning_rate': 0.00010879581856516317, 'epoch': 0.52}
{'loss': 1.0483, 'grad_norm': 1.186415672302246, 'learning_rate': 0.00010874232300251874, 'epoch': 0.52}
{'loss': 1.1618, 'grad_norm': 1.1990528106689453, 'learning_rate': 0.00010868882491863049, 'epoch': 0.53}
{'loss': 1.2974, 'grad_norm': 1.4922138452529907, 'learning_rate': 0.00010863532432892698, 'epoch': 0.53}
{'loss': 0.947, 'grad_norm': 1.0446470975875854, 'learning_rate': 0.00010858182124883754, 'epoch': 0.53}
{'loss': 0.9683, 'grad_norm': 1.4874778985977173, 'learning_rate': 0.00010852831569379215, 'epoch': 0.53}
{'loss': 0.7374, 'grad_norm': 1.3197070360183716, 'learning_rate': 0.00010847480767922162, 'epoch': 0.53}
{'loss': 0.934, 'grad_norm': 1.233553171157837, 'learning_rate': 0.00010842129722055738, 'epoch': 0.53}
{'loss': 1.0997, 'grad_norm': 1.1121867895126343, 'learning_rate': 0.00010836778433323158, 'epoch': 0.53}
{'loss': 0.8257, 'grad_norm': 1.017775058746338, 'learning_rate': 0.00010831426903267706, 'epoch': 0.53}
{'loss': 0.8439, 'grad_norm': 1.3542407751083374, 'learning_rate': 0.00010826075133432738, 'epoch': 0.53}
{'loss': 1.1318, 'grad_norm': 1.2382339239120483, 'learning_rate': 0.00010820723125361684, 'epoch': 0.53}
{'loss': 0.7795, 'grad_norm': 1.9009672403335571, 'learning_rate': 0.00010815370880598035, 'epoch': 0.53}
{'loss': 1.112, 'grad_norm': 1.5132049322128296, 'learning_rate': 0.00010810018400685351, 'epoch': 0.53}
{'loss': 0.7506, 'grad_norm': 1.6694003343582153, 'learning_rate': 0.00010804665687167262, 'epoch': 0.53}
{'loss': 0.7847, 'grad_norm': 1.5392420291900635, 'learning_rate': 0.0001079931274158746, 'epoch': 0.53}
{'loss': 1.0255, 'grad_norm': 1.0652321577072144, 'learning_rate': 0.00010793959565489718, 'epoch': 0.53}
{'loss': 1.0914, 'grad_norm': 1.4898523092269897, 'learning_rate': 0.0001078860616041786, 'epoch': 0.53}
{'loss': 0.9918, 'grad_norm': 1.1447359323501587, 'learning_rate': 0.00010783252527915784, 'epoch': 0.53}
{'loss': 0.9955, 'grad_norm': 1.226931095123291, 'learning_rate': 0.00010777898669527449, 'epoch': 0.53}
{'loss': 0.9551, 'grad_norm': 1.0152021646499634, 'learning_rate': 0.00010772544586796889, 'epoch': 0.53}
{'loss': 1.1205, 'grad_norm': 0.9869353175163269, 'learning_rate': 0.00010767190281268187, 'epoch': 0.53}
{'loss': 0.8539, 'grad_norm': 0.9371354579925537, 'learning_rate': 0.00010761835754485505, 'epoch': 0.53}
{'loss': 0.8093, 'grad_norm': 1.3866881132125854, 'learning_rate': 0.00010756481007993063, 'epoch': 0.53}
{'loss': 0.8817, 'grad_norm': 1.1218421459197998, 'learning_rate': 0.0001075112604333514, 'epoch': 0.53}
{'loss': 0.9884, 'grad_norm': 1.3747118711471558, 'learning_rate': 0.00010745770862056081, 'epoch': 0.53}
{'loss': 0.9343, 'grad_norm': 1.3301538228988647, 'learning_rate': 0.00010740415465700301, 'epoch': 0.53}
{'loss': 1.0175, 'grad_norm': 1.179103136062622, 'learning_rate': 0.00010735059855812268, 'epoch': 0.53}
{'loss': 0.8339, 'grad_norm': 1.511040449142456, 'learning_rate': 0.00010729704033936512, 'epoch': 0.53}
{'loss': 1.2235, 'grad_norm': 0.9651513695716858, 'learning_rate': 0.00010724348001617625, 'epoch': 0.53}
{'loss': 0.9677, 'grad_norm': 1.0044615268707275, 'learning_rate': 0.00010718991760400265, 'epoch': 0.53}
{'loss': 1.008, 'grad_norm': 1.0688531398773193, 'learning_rate': 0.00010713635311829141, 'epoch': 0.53}
{'loss': 1.0811, 'grad_norm': 1.1762518882751465, 'learning_rate': 0.00010708278657449036, 'epoch': 0.53}
{'loss': 0.8757, 'grad_norm': 1.2066534757614136, 'learning_rate': 0.00010702921798804777, 'epoch': 0.53}
{'loss': 1.0486, 'grad_norm': 0.9447522759437561, 'learning_rate': 0.00010697564737441252, 'epoch': 0.53}
{'loss': 0.773, 'grad_norm': 1.1445280313491821, 'learning_rate': 0.00010692207474903422, 'epoch': 0.53}
{'loss': 1.0409, 'grad_norm': 1.5613980293273926, 'learning_rate': 0.0001068685001273629, 'epoch': 0.53}
{'loss': 0.7342, 'grad_norm': 1.1332297325134277, 'learning_rate': 0.00010681492352484919, 'epoch': 0.53}
{'loss': 0.9599, 'grad_norm': 1.0227978229522705, 'learning_rate': 0.00010676134495694439, 'epoch': 0.53}
{'loss': 1.0285, 'grad_norm': 1.0374456644058228, 'learning_rate': 0.00010670776443910024, 'epoch': 0.53}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9538, 'grad_norm': 1.2639509439468384, 'learning_rate': 0.00010665418198676917, 'epoch': 0.53}
{'loss': 0.9076, 'grad_norm': 1.6135176420211792, 'learning_rate': 0.00010660059761540404, 'epoch': 0.53}
{'loss': 0.8619, 'grad_norm': 0.9721671938896179, 'learning_rate': 0.00010654701134045835, 'epoch': 0.53}
{'loss': 0.9705, 'grad_norm': 1.109047770500183, 'learning_rate': 0.00010649342317738616, 'epoch': 0.53}
{'loss': 1.278, 'grad_norm': 1.1488734483718872, 'learning_rate': 0.00010643983314164194, 'epoch': 0.53}
{'loss': 1.2041, 'grad_norm': 1.0652016401290894, 'learning_rate': 0.00010638624124868091, 'epoch': 0.53}
{'loss': 0.9594, 'grad_norm': 1.0944311618804932, 'learning_rate': 0.00010633264751395865, 'epoch': 0.53}
{'loss': 1.1015, 'grad_norm': 0.9760683178901672, 'learning_rate': 0.00010627905195293135, 'epoch': 0.53}
{'loss': 1.1738, 'grad_norm': 1.389969825744629, 'learning_rate': 0.0001062254545810557, 'epoch': 0.53}
{'loss': 0.9536, 'grad_norm': 0.9979814887046814, 'learning_rate': 0.00010617185541378895, 'epoch': 0.53}
{'loss': 1.1906, 'grad_norm': 2.2599263191223145, 'learning_rate': 0.00010611825446658884, 'epoch': 0.53}
{'loss': 0.9482, 'grad_norm': 1.1504795551300049, 'learning_rate': 0.00010606465175491358, 'epoch': 0.53}
{'loss': 1.0967, 'grad_norm': 0.9535033106803894, 'learning_rate': 0.00010601104729422195, 'epoch': 0.53}
{'loss': 0.9867, 'grad_norm': 1.2491300106048584, 'learning_rate': 0.00010595744109997325, 'epoch': 0.53}
{'loss': 1.1876, 'grad_norm': 1.5879099369049072, 'learning_rate': 0.00010590383318762724, 'epoch': 0.53}
{'loss': 0.9743, 'grad_norm': 1.1279160976409912, 'learning_rate': 0.00010585022357264417, 'epoch': 0.53}
{'loss': 0.9346, 'grad_norm': 1.1663000583648682, 'learning_rate': 0.00010579661227048483, 'epoch': 0.53}
{'loss': 0.9777, 'grad_norm': 1.0654566287994385, 'learning_rate': 0.0001057429992966104, 'epoch': 0.53}
{'loss': 0.9741, 'grad_norm': 1.0955302715301514, 'learning_rate': 0.00010568938466648264, 'epoch': 0.53}
{'loss': 0.9289, 'grad_norm': 1.0923258066177368, 'learning_rate': 0.00010563576839556374, 'epoch': 0.53}
{'loss': 0.9012, 'grad_norm': 1.1618127822875977, 'learning_rate': 0.00010558215049931638, 'epoch': 0.53}
{'loss': 0.7379, 'grad_norm': 0.8199729323387146, 'learning_rate': 0.00010552853099320375, 'epoch': 0.53}
{'loss': 0.9965, 'grad_norm': 2.063141345977783, 'learning_rate': 0.00010547490989268934, 'epoch': 0.53}
{'loss': 0.7358, 'grad_norm': 1.4676787853240967, 'learning_rate': 0.0001054212872132373, 'epoch': 0.53}
{'loss': 0.9974, 'grad_norm': 1.0852810144424438, 'learning_rate': 0.00010536766297031215, 'epoch': 0.53}
{'loss': 1.1572, 'grad_norm': 1.128868818283081, 'learning_rate': 0.00010531403717937887, 'epoch': 0.53}
{'loss': 0.9993, 'grad_norm': 2.8649353981018066, 'learning_rate': 0.00010526040985590287, 'epoch': 0.53}
{'loss': 0.7644, 'grad_norm': 1.278855323791504, 'learning_rate': 0.00010520678101534998, 'epoch': 0.54}
{'loss': 0.659, 'grad_norm': 1.5792635679244995, 'learning_rate': 0.00010515315067318652, 'epoch': 0.54}
{'loss': 1.0431, 'grad_norm': 1.4111425876617432, 'learning_rate': 0.00010509951884487926, 'epoch': 0.54}
{'loss': 0.9655, 'grad_norm': 1.2108314037322998, 'learning_rate': 0.0001050458855458953, 'epoch': 0.54}
{'loss': 1.2404, 'grad_norm': 1.1145164966583252, 'learning_rate': 0.00010499225079170228, 'epoch': 0.54}
{'loss': 1.1316, 'grad_norm': 1.1889424324035645, 'learning_rate': 0.00010493861459776812, 'epoch': 0.54}
{'loss': 0.8825, 'grad_norm': 1.2497375011444092, 'learning_rate': 0.00010488497697956135, 'epoch': 0.54}
{'loss': 0.8126, 'grad_norm': 1.1774777173995972, 'learning_rate': 0.00010483133795255071, 'epoch': 0.54}
{'loss': 0.8799, 'grad_norm': 1.327301025390625, 'learning_rate': 0.0001047776975322055, 'epoch': 0.54}
{'loss': 0.9008, 'grad_norm': 1.2824852466583252, 'learning_rate': 0.00010472405573399534, 'epoch': 0.54}
{'loss': 1.1821, 'grad_norm': 1.9749202728271484, 'learning_rate': 0.00010467041257339023, 'epoch': 0.54}
{'loss': 1.0596, 'grad_norm': 1.1284559965133667, 'learning_rate': 0.00010461676806586064, 'epoch': 0.54}
{'loss': 1.0137, 'grad_norm': 1.1485366821289062, 'learning_rate': 0.0001045631222268774, 'epoch': 0.54}
{'loss': 1.0992, 'grad_norm': 1.03581964969635, 'learning_rate': 0.00010450947507191169, 'epoch': 0.54}
{'loss': 0.7094, 'grad_norm': 1.6581437587738037, 'learning_rate': 0.00010445582661643509, 'epoch': 0.54}
{'loss': 1.2568, 'grad_norm': 0.8945942521095276, 'learning_rate': 0.0001044021768759195, 'epoch': 0.54}
{'loss': 0.7699, 'grad_norm': 1.0456546545028687, 'learning_rate': 0.00010434852586583736, 'epoch': 0.54}
{'loss': 0.8299, 'grad_norm': 1.065727949142456, 'learning_rate': 0.00010429487360166128, 'epoch': 0.54}
{'loss': 0.7512, 'grad_norm': 1.0528473854064941, 'learning_rate': 0.00010424122009886434, 'epoch': 0.54}
{'loss': 1.0848, 'grad_norm': 0.915876567363739, 'learning_rate': 0.00010418756537291996, 'epoch': 0.54}
{'loss': 1.0353, 'grad_norm': 1.1045852899551392, 'learning_rate': 0.0001041339094393019, 'epoch': 0.54}
{'loss': 1.037, 'grad_norm': 0.9714899063110352, 'learning_rate': 0.00010408025231348428, 'epoch': 0.54}
{'loss': 0.8186, 'grad_norm': 1.3517049551010132, 'learning_rate': 0.00010402659401094152, 'epoch': 0.54}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9377, 'grad_norm': 1.2120146751403809, 'learning_rate': 0.00010397293454714848, 'epoch': 0.54}
{'loss': 0.9584, 'grad_norm': 1.110361099243164, 'learning_rate': 0.00010391927393758021, 'epoch': 0.54}
{'loss': 0.8408, 'grad_norm': 1.2568894624710083, 'learning_rate': 0.00010386561219771222, 'epoch': 0.54}
{'loss': 0.9875, 'grad_norm': 1.0801914930343628, 'learning_rate': 0.00010381194934302032, 'epoch': 0.54}
{'loss': 1.0856, 'grad_norm': 1.014975666999817, 'learning_rate': 0.00010375828538898053, 'epoch': 0.54}
{'loss': 0.9971, 'grad_norm': 0.9999037981033325, 'learning_rate': 0.0001037046203510694, 'epoch': 0.54}
{'loss': 1.0508, 'grad_norm': 1.5706161260604858, 'learning_rate': 0.00010365095424476357, 'epoch': 0.54}
{'loss': 1.0486, 'grad_norm': 1.3272042274475098, 'learning_rate': 0.00010359728708554013, 'epoch': 0.54}
{'loss': 0.8611, 'grad_norm': 1.0764377117156982, 'learning_rate': 0.00010354361888887642, 'epoch': 0.54}
{'loss': 1.1997, 'grad_norm': 0.9818750023841858, 'learning_rate': 0.00010348994967025012, 'epoch': 0.54}
{'loss': 0.8588, 'grad_norm': 1.3544784784317017, 'learning_rate': 0.00010343627944513911, 'epoch': 0.54}
{'loss': 0.8417, 'grad_norm': 0.9892833828926086, 'learning_rate': 0.00010338260822902167, 'epoch': 0.54}
{'loss': 0.7155, 'grad_norm': 1.8001995086669922, 'learning_rate': 0.0001033289360373763, 'epoch': 0.54}
{'loss': 1.0847, 'grad_norm': 1.11605966091156, 'learning_rate': 0.00010327526288568183, 'epoch': 0.54}
{'loss': 0.8255, 'grad_norm': 0.9712984561920166, 'learning_rate': 0.00010322158878941732, 'epoch': 0.54}
{'loss': 0.8195, 'grad_norm': 1.1232492923736572, 'learning_rate': 0.0001031679137640621, 'epoch': 0.54}
{'loss': 0.8579, 'grad_norm': 1.4040385484695435, 'learning_rate': 0.00010311423782509579, 'epoch': 0.54}
{'loss': 1.1105, 'grad_norm': 0.8883432745933533, 'learning_rate': 0.00010306056098799832, 'epoch': 0.54}
{'loss': 0.924, 'grad_norm': 1.416720986366272, 'learning_rate': 0.00010300688326824981, 'epoch': 0.54}
{'loss': 1.1569, 'grad_norm': 0.9959108233451843, 'learning_rate': 0.00010295320468133066, 'epoch': 0.54}
{'loss': 0.9145, 'grad_norm': 1.2434853315353394, 'learning_rate': 0.00010289952524272146, 'epoch': 0.54}
{'loss': 1.131, 'grad_norm': 1.5736942291259766, 'learning_rate': 0.00010284584496790318, 'epoch': 0.54}
{'loss': 1.1805, 'grad_norm': 1.411677598953247, 'learning_rate': 0.0001027921638723569, 'epoch': 0.54}
{'loss': 1.3342, 'grad_norm': 1.0977122783660889, 'learning_rate': 0.00010273848197156401, 'epoch': 0.54}
{'loss': 0.981, 'grad_norm': 1.0953088998794556, 'learning_rate': 0.00010268479928100614, 'epoch': 0.54}
{'loss': 0.902, 'grad_norm': 1.2238746881484985, 'learning_rate': 0.00010263111581616505, 'epoch': 0.54}
{'loss': 0.8927, 'grad_norm': 0.9492847919464111, 'learning_rate': 0.00010257743159252285, 'epoch': 0.54}
{'loss': 1.1639, 'grad_norm': 1.26566743850708, 'learning_rate': 0.00010252374662556177, 'epoch': 0.54}
{'loss': 1.0851, 'grad_norm': 1.6370694637298584, 'learning_rate': 0.00010247006093076434, 'epoch': 0.54}
{'loss': 0.9389, 'grad_norm': 1.0197311639785767, 'learning_rate': 0.00010241637452361323, 'epoch': 0.54}
{'loss': 1.1208, 'grad_norm': 1.051445484161377, 'learning_rate': 0.00010236268741959134, 'epoch': 0.54}
{'loss': 1.2425, 'grad_norm': 1.373001217842102, 'learning_rate': 0.00010230899963418181, 'epoch': 0.54}
{'loss': 1.0527, 'grad_norm': 0.9438292980194092, 'learning_rate': 0.00010225531118286789, 'epoch': 0.54}
{'loss': 0.9487, 'grad_norm': 0.9100978374481201, 'learning_rate': 0.00010220162208113309, 'epoch': 0.54}
{'loss': 1.357, 'grad_norm': 1.1677638292312622, 'learning_rate': 0.00010214793234446104, 'epoch': 0.54}
{'loss': 1.007, 'grad_norm': 0.9050766825675964, 'learning_rate': 0.0001020942419883357, 'epoch': 0.54}
{'loss': 0.8413, 'grad_norm': 1.3877133131027222, 'learning_rate': 0.00010204055102824105, 'epoch': 0.54}
{'loss': 1.0794, 'grad_norm': 1.1132360696792603, 'learning_rate': 0.00010198685947966129, 'epoch': 0.54}
{'loss': 0.8458, 'grad_norm': 1.0185908079147339, 'learning_rate': 0.00010193316735808085, 'epoch': 0.54}
{'loss': 1.0849, 'grad_norm': 1.0167349576950073, 'learning_rate': 0.00010187947467898425, 'epoch': 0.54}
{'loss': 0.9473, 'grad_norm': 1.3015477657318115, 'learning_rate': 0.0001018257814578562, 'epoch': 0.54}
{'loss': 1.2416, 'grad_norm': 1.34804105758667, 'learning_rate': 0.00010177208771018158, 'epoch': 0.54}
{'loss': 0.9541, 'grad_norm': 1.5445940494537354, 'learning_rate': 0.0001017183934514454, 'epoch': 0.55}
{'loss': 0.9015, 'grad_norm': 1.059468150138855, 'learning_rate': 0.00010166469869713282, 'epoch': 0.55}
{'loss': 1.3286, 'grad_norm': 1.1569156646728516, 'learning_rate': 0.00010161100346272914, 'epoch': 0.55}
{'loss': 1.0333, 'grad_norm': 1.6139190196990967, 'learning_rate': 0.00010155730776371983, 'epoch': 0.55}
{'loss': 0.8224, 'grad_norm': 1.1984243392944336, 'learning_rate': 0.00010150361161559048, 'epoch': 0.55}
{'loss': 0.7635, 'grad_norm': 1.0329134464263916, 'learning_rate': 0.00010144991503382674, 'epoch': 0.55}
{'loss': 0.9063, 'grad_norm': 1.1097893714904785, 'learning_rate': 0.00010139621803391455, 'epoch': 0.55}
{'loss': 1.1549, 'grad_norm': 1.1396838426589966, 'learning_rate': 0.00010134252063133975, 'epoch': 0.55}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9885, 'grad_norm': 1.270715355873108, 'learning_rate': 0.0001012888228415885, 'epoch': 0.55}
{'loss': 1.0156, 'grad_norm': 1.3577860593795776, 'learning_rate': 0.00010123512468014693, 'epoch': 0.55}
{'loss': 1.0406, 'grad_norm': 1.28438138961792, 'learning_rate': 0.00010118142616250137, 'epoch': 0.55}
{'loss': 0.8595, 'grad_norm': 1.3567872047424316, 'learning_rate': 0.00010112772730413815, 'epoch': 0.55}
{'loss': 0.901, 'grad_norm': 1.0423356294631958, 'learning_rate': 0.00010107402812054385, 'epoch': 0.55}
{'loss': 0.7885, 'grad_norm': 1.0960886478424072, 'learning_rate': 0.00010102032862720503, 'epoch': 0.55}
{'loss': 0.8063, 'grad_norm': 1.289110779762268, 'learning_rate': 0.00010096662883960832, 'epoch': 0.55}
{'loss': 0.9942, 'grad_norm': 1.0770885944366455, 'learning_rate': 0.0001009129287732405, 'epoch': 0.55}
{'loss': 0.9479, 'grad_norm': 1.137649416923523, 'learning_rate': 0.00010085922844358843, 'epoch': 0.55}
{'loss': 0.9972, 'grad_norm': 1.0069355964660645, 'learning_rate': 0.00010080552786613899, 'epoch': 0.55}
{'loss': 0.8794, 'grad_norm': 1.350286602973938, 'learning_rate': 0.00010075182705637925, 'epoch': 0.55}
{'loss': 1.1777, 'grad_norm': 1.142608880996704, 'learning_rate': 0.00010069812602979615, 'epoch': 0.55}
{'loss': 1.0261, 'grad_norm': 1.2450364828109741, 'learning_rate': 0.00010064442480187692, 'epoch': 0.55}
{'loss': 0.9943, 'grad_norm': 1.2696365118026733, 'learning_rate': 0.00010059072338810865, 'epoch': 0.55}
{'loss': 1.0812, 'grad_norm': 1.2382495403289795, 'learning_rate': 0.00010053702180397859, 'epoch': 0.55}
{'loss': 0.579, 'grad_norm': 1.007493257522583, 'learning_rate': 0.00010048332006497406, 'epoch': 0.55}
{'loss': 0.9018, 'grad_norm': 1.07900869846344, 'learning_rate': 0.00010042961818658234, 'epoch': 0.55}
{'loss': 0.7993, 'grad_norm': 1.2510546445846558, 'learning_rate': 0.00010037591618429076, 'epoch': 0.55}
{'loss': 1.0063, 'grad_norm': 1.7872653007507324, 'learning_rate': 0.00010032221407358681, 'epoch': 0.55}
{'loss': 0.7276, 'grad_norm': 1.329086422920227, 'learning_rate': 0.00010026851186995785, 'epoch': 0.55}
{'loss': 0.8823, 'grad_norm': 1.4529106616973877, 'learning_rate': 0.00010021480958889138, 'epoch': 0.55}
{'loss': 0.9947, 'grad_norm': 1.0493143796920776, 'learning_rate': 0.00010016110724587487, 'epoch': 0.55}
{'loss': 1.245, 'grad_norm': 1.5503716468811035, 'learning_rate': 0.00010010740485639579, 'epoch': 0.55}
{'loss': 0.8401, 'grad_norm': 1.1894246339797974, 'learning_rate': 0.00010005370243594166, 'epoch': 0.55}
{'loss': 0.8813, 'grad_norm': 1.3360971212387085, 'learning_rate': 0.0001, 'epoch': 0.55}
{'loss': 1.1018, 'grad_norm': 1.494073748588562, 'learning_rate': 9.994629756405835e-05, 'epoch': 0.55}
{'loss': 1.0509, 'grad_norm': 1.0403612852096558, 'learning_rate': 9.989259514360425e-05, 'epoch': 0.55}
{'loss': 1.2538, 'grad_norm': 0.9527773261070251, 'learning_rate': 9.983889275412516e-05, 'epoch': 0.55}
{'loss': 1.0881, 'grad_norm': 1.4616273641586304, 'learning_rate': 9.978519041110863e-05, 'epoch': 0.55}
{'loss': 1.1055, 'grad_norm': 1.260491967201233, 'learning_rate': 9.973148813004216e-05, 'epoch': 0.55}
{'loss': 0.8555, 'grad_norm': 1.0921329259872437, 'learning_rate': 9.96777859264132e-05, 'epoch': 0.55}
{'loss': 0.9388, 'grad_norm': 1.3437327146530151, 'learning_rate': 9.962408381570925e-05, 'epoch': 0.55}
{'loss': 1.1081, 'grad_norm': 1.191609501838684, 'learning_rate': 9.957038181341769e-05, 'epoch': 0.55}
{'loss': 0.812, 'grad_norm': 1.2139984369277954, 'learning_rate': 9.9516679935026e-05, 'epoch': 0.55}
{'loss': 1.0769, 'grad_norm': 1.1023284196853638, 'learning_rate': 9.946297819602143e-05, 'epoch': 0.55}
{'loss': 0.6112, 'grad_norm': 1.455790400505066, 'learning_rate': 9.940927661189136e-05, 'epoch': 0.55}
{'loss': 0.8734, 'grad_norm': 0.9579130411148071, 'learning_rate': 9.935557519812313e-05, 'epoch': 0.55}
{'loss': 0.9631, 'grad_norm': 1.4508986473083496, 'learning_rate': 9.930187397020386e-05, 'epoch': 0.55}
{'loss': 0.8707, 'grad_norm': 1.1278544664382935, 'learning_rate': 9.924817294362079e-05, 'epoch': 0.55}
{'loss': 1.0442, 'grad_norm': 1.233603835105896, 'learning_rate': 9.919447213386103e-05, 'epoch': 0.55}
{'loss': 0.7084, 'grad_norm': 0.9707848429679871, 'learning_rate': 9.914077155641159e-05, 'epoch': 0.55}
{'loss': 1.0716, 'grad_norm': 1.033037781715393, 'learning_rate': 9.908707122675953e-05, 'epoch': 0.55}
{'loss': 1.2588, 'grad_norm': 1.225391149520874, 'learning_rate': 9.903337116039171e-05, 'epoch': 0.55}
{'loss': 0.9866, 'grad_norm': 1.0811541080474854, 'learning_rate': 9.897967137279498e-05, 'epoch': 0.55}
{'loss': 0.9483, 'grad_norm': 1.5725160837173462, 'learning_rate': 9.892597187945616e-05, 'epoch': 0.55}
{'loss': 0.9504, 'grad_norm': 1.2723326683044434, 'learning_rate': 9.887227269586183e-05, 'epoch': 0.55}
{'loss': 0.7359, 'grad_norm': 0.8828505873680115, 'learning_rate': 9.881857383749868e-05, 'epoch': 0.55}
{'loss': 0.9529, 'grad_norm': 1.410971760749817, 'learning_rate': 9.876487531985309e-05, 'epoch': 0.55}
{'loss': 0.7857, 'grad_norm': 0.9889012575149536, 'learning_rate': 9.871117715841151e-05, 'epoch': 0.55}
{'loss': 0.8323, 'grad_norm': 1.0368293523788452, 'learning_rate': 9.865747936866027e-05, 'epoch': 0.55}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8014, 'grad_norm': 1.5664610862731934, 'learning_rate': 9.860378196608549e-05, 'epoch': 0.55}
{'loss': 1.0995, 'grad_norm': 1.0749990940093994, 'learning_rate': 9.855008496617327e-05, 'epoch': 0.55}
{'loss': 0.9308, 'grad_norm': 1.9251165390014648, 'learning_rate': 9.849638838440953e-05, 'epoch': 0.55}
{'loss': 0.8439, 'grad_norm': 1.1672037839889526, 'learning_rate': 9.844269223628016e-05, 'epoch': 0.55}
{'loss': 0.8967, 'grad_norm': 0.9651229381561279, 'learning_rate': 9.838899653727088e-05, 'epoch': 0.55}
{'loss': 1.0542, 'grad_norm': 1.3179726600646973, 'learning_rate': 9.83353013028672e-05, 'epoch': 0.55}
{'loss': 1.1992, 'grad_norm': 1.2292510271072388, 'learning_rate': 9.828160654855464e-05, 'epoch': 0.55}
{'loss': 0.9418, 'grad_norm': 0.987608790397644, 'learning_rate': 9.822791228981844e-05, 'epoch': 0.56}
{'loss': 0.9298, 'grad_norm': 1.3967909812927246, 'learning_rate': 9.81742185421438e-05, 'epoch': 0.56}
{'loss': 0.6764, 'grad_norm': 1.0924708843231201, 'learning_rate': 9.812052532101578e-05, 'epoch': 0.56}
{'loss': 1.065, 'grad_norm': 1.1787406206130981, 'learning_rate': 9.806683264191916e-05, 'epoch': 0.56}
{'loss': 1.0171, 'grad_norm': 1.0372790098190308, 'learning_rate': 9.801314052033872e-05, 'epoch': 0.56}
{'loss': 1.1776, 'grad_norm': 1.000720739364624, 'learning_rate': 9.795944897175896e-05, 'epoch': 0.56}
{'loss': 0.9979, 'grad_norm': 1.0059224367141724, 'learning_rate': 9.790575801166432e-05, 'epoch': 0.56}
{'loss': 1.1701, 'grad_norm': 1.2898814678192139, 'learning_rate': 9.785206765553896e-05, 'epoch': 0.56}
{'loss': 1.0057, 'grad_norm': 1.541524887084961, 'learning_rate': 9.779837791886694e-05, 'epoch': 0.56}
{'loss': 1.0874, 'grad_norm': 1.2712022066116333, 'learning_rate': 9.774468881713216e-05, 'epoch': 0.56}
{'loss': 0.9864, 'grad_norm': 1.1745643615722656, 'learning_rate': 9.769100036581823e-05, 'epoch': 0.56}
{'loss': 0.9432, 'grad_norm': 1.3454833030700684, 'learning_rate': 9.763731258040865e-05, 'epoch': 0.56}
{'loss': 0.9683, 'grad_norm': 1.3720343112945557, 'learning_rate': 9.75836254763868e-05, 'epoch': 0.56}
{'loss': 0.8828, 'grad_norm': 1.1946138143539429, 'learning_rate': 9.752993906923567e-05, 'epoch': 0.56}
{'loss': 1.2826, 'grad_norm': 1.6343721151351929, 'learning_rate': 9.747625337443825e-05, 'epoch': 0.56}
{'loss': 1.0788, 'grad_norm': 1.552286148071289, 'learning_rate': 9.742256840747718e-05, 'epoch': 0.56}
{'loss': 0.9513, 'grad_norm': 1.4147958755493164, 'learning_rate': 9.736888418383497e-05, 'epoch': 0.56}
{'loss': 1.1082, 'grad_norm': 1.0911614894866943, 'learning_rate': 9.73152007189939e-05, 'epoch': 0.56}
{'loss': 0.9223, 'grad_norm': 0.9746224880218506, 'learning_rate': 9.7261518028436e-05, 'epoch': 0.56}
{'loss': 0.92, 'grad_norm': 1.3474633693695068, 'learning_rate': 9.720783612764314e-05, 'epoch': 0.56}
{'loss': 1.1986, 'grad_norm': 1.1065547466278076, 'learning_rate': 9.715415503209685e-05, 'epoch': 0.56}
{'loss': 0.7056, 'grad_norm': 1.088751196861267, 'learning_rate': 9.710047475727855e-05, 'epoch': 0.56}
{'loss': 0.8801, 'grad_norm': 0.8771107792854309, 'learning_rate': 9.704679531866941e-05, 'epoch': 0.56}
{'loss': 0.6807, 'grad_norm': 0.9248294234275818, 'learning_rate': 9.699311673175021e-05, 'epoch': 0.56}
{'loss': 0.8706, 'grad_norm': 1.178431749343872, 'learning_rate': 9.693943901200168e-05, 'epoch': 0.56}
{'loss': 0.7702, 'grad_norm': 0.9648942351341248, 'learning_rate': 9.688576217490424e-05, 'epoch': 0.56}
{'loss': 0.6139, 'grad_norm': 1.2127410173416138, 'learning_rate': 9.683208623593793e-05, 'epoch': 0.56}
{'loss': 0.8416, 'grad_norm': 1.0651922225952148, 'learning_rate': 9.677841121058273e-05, 'epoch': 0.56}
{'loss': 0.7762, 'grad_norm': 1.4130159616470337, 'learning_rate': 9.67247371143182e-05, 'epoch': 0.56}
{'loss': 1.0815, 'grad_norm': 1.1789315938949585, 'learning_rate': 9.66710639626237e-05, 'epoch': 0.56}
{'loss': 1.1039, 'grad_norm': 1.1048157215118408, 'learning_rate': 9.661739177097836e-05, 'epoch': 0.56}
{'loss': 1.2085, 'grad_norm': 1.2314002513885498, 'learning_rate': 9.65637205548609e-05, 'epoch': 0.56}
{'loss': 0.9065, 'grad_norm': 1.3378660678863525, 'learning_rate': 9.651005032974994e-05, 'epoch': 0.56}
{'loss': 1.0209, 'grad_norm': 1.1231695413589478, 'learning_rate': 9.645638111112359e-05, 'epoch': 0.56}
{'loss': 0.8945, 'grad_norm': 1.1792560815811157, 'learning_rate': 9.640271291445986e-05, 'epoch': 0.56}
{'loss': 1.0268, 'grad_norm': 1.5901418924331665, 'learning_rate': 9.634904575523645e-05, 'epoch': 0.56}
{'loss': 1.0537, 'grad_norm': 1.0827571153640747, 'learning_rate': 9.629537964893063e-05, 'epoch': 0.56}
{'loss': 0.8521, 'grad_norm': 1.355983018875122, 'learning_rate': 9.624171461101948e-05, 'epoch': 0.56}
{'loss': 1.0107, 'grad_norm': 0.89499431848526, 'learning_rate': 9.618805065697972e-05, 'epoch': 0.56}
{'loss': 0.9059, 'grad_norm': 0.9746277332305908, 'learning_rate': 9.613438780228777e-05, 'epoch': 0.56}
{'loss': 1.0522, 'grad_norm': 0.9791368246078491, 'learning_rate': 9.608072606241982e-05, 'epoch': 0.56}
{'loss': 0.9279, 'grad_norm': 1.288428544998169, 'learning_rate': 9.602706545285155e-05, 'epoch': 0.56}
{'loss': 1.1351, 'grad_norm': 1.2113566398620605, 'learning_rate': 9.597340598905852e-05, 'epoch': 0.56}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9598, 'grad_norm': 1.389871597290039, 'learning_rate': 9.591974768651574e-05, 'epoch': 0.56}
{'loss': 0.674, 'grad_norm': 1.419278860092163, 'learning_rate': 9.58660905606981e-05, 'epoch': 0.56}
{'loss': 0.8763, 'grad_norm': 1.4374696016311646, 'learning_rate': 9.581243462708006e-05, 'epoch': 0.56}
{'loss': 0.6863, 'grad_norm': 1.5616686344146729, 'learning_rate': 9.575877990113567e-05, 'epoch': 0.56}
{'loss': 0.9764, 'grad_norm': 1.152136206626892, 'learning_rate': 9.570512639833874e-05, 'epoch': 0.56}
{'loss': 0.7556, 'grad_norm': 1.2264918088912964, 'learning_rate': 9.565147413416266e-05, 'epoch': 0.56}
{'loss': 0.4271, 'grad_norm': 0.8942738771438599, 'learning_rate': 9.559782312408048e-05, 'epoch': 0.56}
{'loss': 1.1354, 'grad_norm': 1.0580157041549683, 'learning_rate': 9.554417338356496e-05, 'epoch': 0.56}
{'loss': 1.028, 'grad_norm': 1.0736525058746338, 'learning_rate': 9.549052492808834e-05, 'epoch': 0.56}
{'loss': 0.8704, 'grad_norm': 0.9680569767951965, 'learning_rate': 9.543687777312263e-05, 'epoch': 0.56}
{'loss': 1.2472, 'grad_norm': 1.2151657342910767, 'learning_rate': 9.538323193413937e-05, 'epoch': 0.56}
{'loss': 1.099, 'grad_norm': 1.2538340091705322, 'learning_rate': 9.532958742660976e-05, 'epoch': 0.56}
{'loss': 0.8936, 'grad_norm': 1.1836751699447632, 'learning_rate': 9.52759442660047e-05, 'epoch': 0.56}
{'loss': 0.8195, 'grad_norm': 1.436605453491211, 'learning_rate': 9.522230246779452e-05, 'epoch': 0.56}
{'loss': 0.6105, 'grad_norm': 1.1218523979187012, 'learning_rate': 9.516866204744931e-05, 'epoch': 0.56}
{'loss': 0.8189, 'grad_norm': 1.3198888301849365, 'learning_rate': 9.511502302043868e-05, 'epoch': 0.56}
{'loss': 0.9638, 'grad_norm': 0.945763349533081, 'learning_rate': 9.50613854022319e-05, 'epoch': 0.56}
{'loss': 1.1682, 'grad_norm': 1.5838006734848022, 'learning_rate': 9.500774920829778e-05, 'epoch': 0.56}
{'loss': 0.8376, 'grad_norm': 1.3713411092758179, 'learning_rate': 9.495411445410472e-05, 'epoch': 0.56}
{'loss': 0.9938, 'grad_norm': 1.034001350402832, 'learning_rate': 9.490048115512074e-05, 'epoch': 0.56}
{'loss': 1.2322, 'grad_norm': 1.4042470455169678, 'learning_rate': 9.484684932681349e-05, 'epoch': 0.56}
{'loss': 1.0571, 'grad_norm': 0.9389389753341675, 'learning_rate': 9.479321898465003e-05, 'epoch': 0.56}
{'loss': 0.8986, 'grad_norm': 1.0106538534164429, 'learning_rate': 9.473959014409718e-05, 'epoch': 0.57}
{'loss': 1.2864, 'grad_norm': 1.1217780113220215, 'learning_rate': 9.468596282062114e-05, 'epoch': 0.57}
{'loss': 0.812, 'grad_norm': 0.9025201797485352, 'learning_rate': 9.463233702968783e-05, 'epoch': 0.57}
{'loss': 0.7928, 'grad_norm': 1.10855233669281, 'learning_rate': 9.457871278676272e-05, 'epoch': 0.57}
{'loss': 0.9612, 'grad_norm': 1.0772310495376587, 'learning_rate': 9.452509010731069e-05, 'epoch': 0.57}
{'loss': 0.8583, 'grad_norm': 1.5496914386749268, 'learning_rate': 9.447146900679631e-05, 'epoch': 0.57}
{'loss': 0.7876, 'grad_norm': 0.978119432926178, 'learning_rate': 9.441784950068362e-05, 'epoch': 0.57}
{'loss': 1.1469, 'grad_norm': 1.4563183784484863, 'learning_rate': 9.436423160443625e-05, 'epoch': 0.57}
{'loss': 0.7997, 'grad_norm': 1.114121437072754, 'learning_rate': 9.431061533351739e-05, 'epoch': 0.57}
{'loss': 1.2516, 'grad_norm': 1.2528446912765503, 'learning_rate': 9.42570007033896e-05, 'epoch': 0.57}
{'loss': 0.928, 'grad_norm': 0.9621039032936096, 'learning_rate': 9.420338772951521e-05, 'epoch': 0.57}
{'loss': 0.9626, 'grad_norm': 1.0768353939056396, 'learning_rate': 9.414977642735584e-05, 'epoch': 0.57}
{'loss': 1.1091, 'grad_norm': 1.0282373428344727, 'learning_rate': 9.409616681237275e-05, 'epoch': 0.57}
{'loss': 1.0463, 'grad_norm': 1.5666170120239258, 'learning_rate': 9.404255890002677e-05, 'epoch': 0.57}
{'loss': 1.2014, 'grad_norm': 1.1584450006484985, 'learning_rate': 9.398895270577806e-05, 'epoch': 0.57}
{'loss': 1.1114, 'grad_norm': 1.2585066556930542, 'learning_rate': 9.393534824508647e-05, 'epoch': 0.57}
{'loss': 1.3506, 'grad_norm': 1.7497470378875732, 'learning_rate': 9.38817455334112e-05, 'epoch': 0.57}
{'loss': 0.865, 'grad_norm': 1.306092381477356, 'learning_rate': 9.382814458621106e-05, 'epoch': 0.57}
{'loss': 0.8974, 'grad_norm': 1.0103821754455566, 'learning_rate': 9.377454541894433e-05, 'epoch': 0.57}
{'loss': 1.078, 'grad_norm': 1.4357784986495972, 'learning_rate': 9.372094804706867e-05, 'epoch': 0.57}
{'loss': 0.6144, 'grad_norm': 1.203019142150879, 'learning_rate': 9.36673524860414e-05, 'epoch': 0.57}
{'loss': 0.947, 'grad_norm': 1.1476492881774902, 'learning_rate': 9.361375875131911e-05, 'epoch': 0.57}
{'loss': 0.9008, 'grad_norm': 1.2258024215698242, 'learning_rate': 9.356016685835806e-05, 'epoch': 0.57}
{'loss': 0.6499, 'grad_norm': 0.9581902027130127, 'learning_rate': 9.350657682261391e-05, 'epoch': 0.57}
{'loss': 0.8341, 'grad_norm': 1.258234977722168, 'learning_rate': 9.345298865954166e-05, 'epoch': 0.57}
{'loss': 1.4831, 'grad_norm': 1.1186741590499878, 'learning_rate': 9.339940238459599e-05, 'epoch': 0.57}
{'loss': 0.867, 'grad_norm': 1.0375810861587524, 'learning_rate': 9.334581801323085e-05, 'epoch': 0.57}
{'loss': 1.1129, 'grad_norm': 1.1061733961105347, 'learning_rate': 9.329223556089975e-05, 'epoch': 0.57}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1167, 'grad_norm': 0.9485500454902649, 'learning_rate': 9.323865504305565e-05, 'epoch': 0.57}
{'loss': 1.0035, 'grad_norm': 0.9319688677787781, 'learning_rate': 9.318507647515082e-05, 'epoch': 0.57}
{'loss': 0.8966, 'grad_norm': 1.0057780742645264, 'learning_rate': 9.313149987263711e-05, 'epoch': 0.57}
{'loss': 0.7254, 'grad_norm': 1.153540015220642, 'learning_rate': 9.307792525096581e-05, 'epoch': 0.57}
{'loss': 0.9249, 'grad_norm': 1.1174852848052979, 'learning_rate': 9.302435262558747e-05, 'epoch': 0.57}
{'loss': 1.1137, 'grad_norm': 1.1905796527862549, 'learning_rate': 9.297078201195229e-05, 'epoch': 0.57}
{'loss': 0.844, 'grad_norm': 1.043161392211914, 'learning_rate': 9.291721342550967e-05, 'epoch': 0.57}
{'loss': 1.3384, 'grad_norm': 2.860071897506714, 'learning_rate': 9.286364688170857e-05, 'epoch': 0.57}
{'loss': 1.0519, 'grad_norm': 1.0436776876449585, 'learning_rate': 9.281008239599736e-05, 'epoch': 0.57}
{'loss': 1.0757, 'grad_norm': 1.08644437789917, 'learning_rate': 9.275651998382377e-05, 'epoch': 0.57}
{'loss': 0.8653, 'grad_norm': 1.2080085277557373, 'learning_rate': 9.270295966063493e-05, 'epoch': 0.57}
{'loss': 0.933, 'grad_norm': 0.9461731910705566, 'learning_rate': 9.264940144187735e-05, 'epoch': 0.57}
{'loss': 1.3713, 'grad_norm': 1.4239097833633423, 'learning_rate': 9.2595845342997e-05, 'epoch': 0.57}
{'loss': 0.8423, 'grad_norm': 1.2102900743484497, 'learning_rate': 9.254229137943921e-05, 'epoch': 0.57}
{'loss': 1.2331, 'grad_norm': 0.9607061743736267, 'learning_rate': 9.248873956664863e-05, 'epoch': 0.57}
{'loss': 1.0937, 'grad_norm': 1.313398003578186, 'learning_rate': 9.243518992006944e-05, 'epoch': 0.57}
{'loss': 0.8594, 'grad_norm': 1.0589085817337036, 'learning_rate': 9.238164245514497e-05, 'epoch': 0.57}
{'loss': 1.0128, 'grad_norm': 0.918613612651825, 'learning_rate': 9.232809718731814e-05, 'epoch': 0.57}
{'loss': 1.0323, 'grad_norm': 1.2622710466384888, 'learning_rate': 9.227455413203115e-05, 'epoch': 0.57}
{'loss': 0.8824, 'grad_norm': 1.1130280494689941, 'learning_rate': 9.222101330472552e-05, 'epoch': 0.57}
{'loss': 0.735, 'grad_norm': 1.1747088432312012, 'learning_rate': 9.216747472084221e-05, 'epoch': 0.57}
{'loss': 0.7601, 'grad_norm': 1.310880422592163, 'learning_rate': 9.211393839582142e-05, 'epoch': 0.57}
{'loss': 0.8684, 'grad_norm': 1.1595122814178467, 'learning_rate': 9.206040434510282e-05, 'epoch': 0.57}
{'loss': 0.9589, 'grad_norm': 1.136432409286499, 'learning_rate': 9.200687258412541e-05, 'epoch': 0.57}
{'loss': 0.7955, 'grad_norm': 1.1914483308792114, 'learning_rate': 9.195334312832742e-05, 'epoch': 0.57}
{'loss': 0.6486, 'grad_norm': 1.1852083206176758, 'learning_rate': 9.189981599314654e-05, 'epoch': 0.57}
{'loss': 0.8806, 'grad_norm': 1.1172689199447632, 'learning_rate': 9.184629119401968e-05, 'epoch': 0.57}
{'loss': 1.0169, 'grad_norm': 1.0068758726119995, 'learning_rate': 9.179276874638315e-05, 'epoch': 0.57}
{'loss': 0.7148, 'grad_norm': 1.2120544910430908, 'learning_rate': 9.173924866567263e-05, 'epoch': 0.57}
{'loss': 0.8606, 'grad_norm': 0.9773293137550354, 'learning_rate': 9.168573096732297e-05, 'epoch': 0.57}
{'loss': 1.272, 'grad_norm': 1.8671001195907593, 'learning_rate': 9.163221566676847e-05, 'epoch': 0.57}
{'loss': 1.007, 'grad_norm': 1.1692627668380737, 'learning_rate': 9.157870277944266e-05, 'epoch': 0.57}
{'loss': 0.9567, 'grad_norm': 1.6632108688354492, 'learning_rate': 9.152519232077839e-05, 'epoch': 0.57}
{'loss': 0.9458, 'grad_norm': 1.185763955116272, 'learning_rate': 9.147168430620787e-05, 'epoch': 0.57}
{'loss': 1.1153, 'grad_norm': 1.2747466564178467, 'learning_rate': 9.141817875116248e-05, 'epoch': 0.57}
{'loss': 0.9408, 'grad_norm': 0.9902237057685852, 'learning_rate': 9.136467567107306e-05, 'epoch': 0.57}
{'loss': 0.9526, 'grad_norm': 1.2168160676956177, 'learning_rate': 9.131117508136953e-05, 'epoch': 0.57}
{'loss': 1.08, 'grad_norm': 1.1265279054641724, 'learning_rate': 9.125767699748127e-05, 'epoch': 0.58}
{'loss': 1.05, 'grad_norm': 0.9519414305686951, 'learning_rate': 9.120418143483688e-05, 'epoch': 0.58}
{'loss': 0.933, 'grad_norm': 1.4713996648788452, 'learning_rate': 9.115068840886417e-05, 'epoch': 0.58}
{'loss': 1.2029, 'grad_norm': 1.3236957788467407, 'learning_rate': 9.10971979349903e-05, 'epoch': 0.58}
{'loss': 1.1235, 'grad_norm': 1.1161407232284546, 'learning_rate': 9.104371002864166e-05, 'epoch': 0.58}
{'loss': 0.7105, 'grad_norm': 1.4501672983169556, 'learning_rate': 9.099022470524392e-05, 'epoch': 0.58}
{'loss': 1.0848, 'grad_norm': 1.1816065311431885, 'learning_rate': 9.093674198022201e-05, 'epoch': 0.58}
{'loss': 1.0488, 'grad_norm': 1.206612467765808, 'learning_rate': 9.0883261869e-05, 'epoch': 0.58}
{'loss': 0.9374, 'grad_norm': 1.1076077222824097, 'learning_rate': 9.082978438700138e-05, 'epoch': 0.58}
{'loss': 0.8133, 'grad_norm': 1.074454426765442, 'learning_rate': 9.07763095496488e-05, 'epoch': 0.58}
{'loss': 0.9902, 'grad_norm': 0.9278172254562378, 'learning_rate': 9.072283737236409e-05, 'epoch': 0.58}
{'loss': 1.5012, 'grad_norm': 1.2527350187301636, 'learning_rate': 9.066936787056842e-05, 'epoch': 0.58}
{'loss': 0.9994, 'grad_norm': 1.5630154609680176, 'learning_rate': 9.061590105968208e-05, 'epoch': 0.58}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.2056, 'grad_norm': 1.1300936937332153, 'learning_rate': 9.056243695512469e-05, 'epoch': 0.58}
{'loss': 0.7532, 'grad_norm': 1.0833693742752075, 'learning_rate': 9.050897557231504e-05, 'epoch': 0.58}
{'loss': 0.8355, 'grad_norm': 1.105078101158142, 'learning_rate': 9.045551692667108e-05, 'epoch': 0.58}
{'loss': 1.1727, 'grad_norm': 1.293277382850647, 'learning_rate': 9.040206103361012e-05, 'epoch': 0.58}
{'loss': 0.617, 'grad_norm': 1.353035807609558, 'learning_rate': 9.034860790854849e-05, 'epoch': 0.58}
{'loss': 0.9365, 'grad_norm': 1.4717762470245361, 'learning_rate': 9.029515756690186e-05, 'epoch': 0.58}
{'loss': 0.7945, 'grad_norm': 1.168912649154663, 'learning_rate': 9.024171002408506e-05, 'epoch': 0.58}
{'loss': 0.9428, 'grad_norm': 1.2255487442016602, 'learning_rate': 9.018826529551208e-05, 'epoch': 0.58}
{'loss': 0.8565, 'grad_norm': 1.1898374557495117, 'learning_rate': 9.013482339659615e-05, 'epoch': 0.58}
{'loss': 0.7975, 'grad_norm': 1.3589928150177002, 'learning_rate': 9.00813843427496e-05, 'epoch': 0.58}
{'loss': 1.2492, 'grad_norm': 1.3243374824523926, 'learning_rate': 9.002794814938402e-05, 'epoch': 0.58}
{'loss': 1.025, 'grad_norm': 2.137096881866455, 'learning_rate': 8.997451483191019e-05, 'epoch': 0.58}
{'loss': 1.1513, 'grad_norm': 1.3143460750579834, 'learning_rate': 8.9921084405738e-05, 'epoch': 0.58}
{'loss': 1.0131, 'grad_norm': 1.2034543752670288, 'learning_rate': 8.986765688627652e-05, 'epoch': 0.58}
{'loss': 0.8943, 'grad_norm': 1.5167814493179321, 'learning_rate': 8.981423228893397e-05, 'epoch': 0.58}
{'loss': 1.2659, 'grad_norm': 1.1798409223556519, 'learning_rate': 8.976081062911775e-05, 'epoch': 0.58}
{'loss': 0.8197, 'grad_norm': 1.1341280937194824, 'learning_rate': 8.970739192223448e-05, 'epoch': 0.58}
{'loss': 0.9844, 'grad_norm': 2.0208494663238525, 'learning_rate': 8.965397618368977e-05, 'epoch': 0.58}
{'loss': 1.2053, 'grad_norm': 0.9996521472930908, 'learning_rate': 8.960056342888854e-05, 'epoch': 0.58}
{'loss': 1.0157, 'grad_norm': 1.0381122827529907, 'learning_rate': 8.954715367323468e-05, 'epoch': 0.58}
{'loss': 0.9504, 'grad_norm': 1.4182904958724976, 'learning_rate': 8.949374693213134e-05, 'epoch': 0.58}
{'loss': 0.7779, 'grad_norm': 0.9850794672966003, 'learning_rate': 8.944034322098082e-05, 'epoch': 0.58}
{'loss': 1.2014, 'grad_norm': 1.027966856956482, 'learning_rate': 8.938694255518444e-05, 'epoch': 0.58}
{'loss': 1.0656, 'grad_norm': 1.2881603240966797, 'learning_rate': 8.933354495014272e-05, 'epoch': 0.58}
{'loss': 0.9806, 'grad_norm': 1.090152382850647, 'learning_rate': 8.928015042125523e-05, 'epoch': 0.58}
{'loss': 0.7116, 'grad_norm': 1.1708917617797852, 'learning_rate': 8.922675898392072e-05, 'epoch': 0.58}
{'loss': 0.7278, 'grad_norm': 1.0953454971313477, 'learning_rate': 8.917337065353709e-05, 'epoch': 0.58}
{'loss': 0.781, 'grad_norm': 1.2891954183578491, 'learning_rate': 8.911998544550116e-05, 'epoch': 0.58}
{'loss': 0.6337, 'grad_norm': 1.0518444776535034, 'learning_rate': 8.906660337520903e-05, 'epoch': 0.58}
{'loss': 1.1334, 'grad_norm': 1.1299526691436768, 'learning_rate': 8.901322445805586e-05, 'epoch': 0.58}
{'loss': 1.0306, 'grad_norm': 1.261749029159546, 'learning_rate': 8.895984870943579e-05, 'epoch': 0.58}
{'loss': 0.9377, 'grad_norm': 1.3491888046264648, 'learning_rate': 8.890647614474224e-05, 'epoch': 0.58}
{'loss': 0.8306, 'grad_norm': 1.3239142894744873, 'learning_rate': 8.885310677936746e-05, 'epoch': 0.58}
{'loss': 0.8398, 'grad_norm': 1.0259793996810913, 'learning_rate': 8.879974062870302e-05, 'epoch': 0.58}
{'loss': 1.0087, 'grad_norm': 1.1241790056228638, 'learning_rate': 8.874637770813946e-05, 'epoch': 0.58}
{'loss': 0.7093, 'grad_norm': 1.060457468032837, 'learning_rate': 8.869301803306633e-05, 'epoch': 0.58}
{'loss': 1.1162, 'grad_norm': 1.1466153860092163, 'learning_rate': 8.863966161887239e-05, 'epoch': 0.58}
{'loss': 0.7332, 'grad_norm': 1.1080329418182373, 'learning_rate': 8.858630848094526e-05, 'epoch': 0.58}
{'loss': 0.8409, 'grad_norm': 0.8565617799758911, 'learning_rate': 8.853295863467182e-05, 'epoch': 0.58}
{'loss': 0.9366, 'grad_norm': 1.117425799369812, 'learning_rate': 8.84796120954379e-05, 'epoch': 0.58}
{'loss': 1.0252, 'grad_norm': 1.3337516784667969, 'learning_rate': 8.842626887862833e-05, 'epoch': 0.58}
{'loss': 1.1877, 'grad_norm': 1.1074901819229126, 'learning_rate': 8.83729289996271e-05, 'epoch': 0.58}
{'loss': 1.1846, 'grad_norm': 1.06610906124115, 'learning_rate': 8.831959247381711e-05, 'epoch': 0.58}
{'loss': 1.0981, 'grad_norm': 1.2323083877563477, 'learning_rate': 8.826625931658039e-05, 'epoch': 0.58}
{'loss': 0.9252, 'grad_norm': 0.8889973759651184, 'learning_rate': 8.821292954329798e-05, 'epoch': 0.58}
{'loss': 0.7994, 'grad_norm': 1.1425070762634277, 'learning_rate': 8.81596031693499e-05, 'epoch': 0.58}
{'loss': 0.8376, 'grad_norm': 0.9123789072036743, 'learning_rate': 8.810628021011528e-05, 'epoch': 0.58}
{'loss': 0.9372, 'grad_norm': 1.6823952198028564, 'learning_rate': 8.805296068097211e-05, 'epoch': 0.58}
{'loss': 1.1808, 'grad_norm': 1.0822274684906006, 'learning_rate': 8.799964459729754e-05, 'epoch': 0.58}
{'loss': 0.9175, 'grad_norm': 1.2778067588806152, 'learning_rate': 8.79463319744677e-05, 'epoch': 0.58}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8097, 'grad_norm': 1.2057229280471802, 'learning_rate': 8.789302282785764e-05, 'epoch': 0.58}
{'loss': 0.899, 'grad_norm': 1.2211564779281616, 'learning_rate': 8.783971717284152e-05, 'epoch': 0.58}
{'loss': 0.9365, 'grad_norm': 1.1988954544067383, 'learning_rate': 8.778641502479234e-05, 'epoch': 0.59}
{'loss': 1.2968, 'grad_norm': 1.063059687614441, 'learning_rate': 8.773311639908225e-05, 'epoch': 0.59}
{'loss': 0.9845, 'grad_norm': 1.2400544881820679, 'learning_rate': 8.767982131108232e-05, 'epoch': 0.59}
{'loss': 0.8859, 'grad_norm': 1.0680547952651978, 'learning_rate': 8.762652977616257e-05, 'epoch': 0.59}
{'loss': 1.1674, 'grad_norm': 1.500157356262207, 'learning_rate': 8.757324180969207e-05, 'epoch': 0.59}
{'loss': 0.9316, 'grad_norm': 1.5555720329284668, 'learning_rate': 8.751995742703873e-05, 'epoch': 0.59}
{'loss': 0.8741, 'grad_norm': 1.0337213277816772, 'learning_rate': 8.746667664356956e-05, 'epoch': 0.59}
{'loss': 0.9837, 'grad_norm': 0.9112688899040222, 'learning_rate': 8.741339947465054e-05, 'epoch': 0.59}
{'loss': 1.1461, 'grad_norm': 0.9310992360115051, 'learning_rate': 8.736012593564642e-05, 'epoch': 0.59}
{'loss': 0.8748, 'grad_norm': 1.2136789560317993, 'learning_rate': 8.730685604192115e-05, 'epoch': 0.59}
{'loss': 1.0473, 'grad_norm': 1.2097746133804321, 'learning_rate': 8.725358980883742e-05, 'epoch': 0.59}
{'loss': 0.8425, 'grad_norm': 1.0127726793289185, 'learning_rate': 8.720032725175699e-05, 'epoch': 0.59}
{'loss': 0.9745, 'grad_norm': 1.0527987480163574, 'learning_rate': 8.714706838604055e-05, 'epoch': 0.59}
{'loss': 1.0504, 'grad_norm': 1.337808609008789, 'learning_rate': 8.709381322704769e-05, 'epoch': 0.59}
{'loss': 1.1226, 'grad_norm': 1.2358379364013672, 'learning_rate': 8.704056179013689e-05, 'epoch': 0.59}
{'loss': 1.0806, 'grad_norm': 1.420304298400879, 'learning_rate': 8.698731409066568e-05, 'epoch': 0.59}
{'loss': 0.892, 'grad_norm': 1.0667104721069336, 'learning_rate': 8.693407014399039e-05, 'epoch': 0.59}
{'loss': 1.0396, 'grad_norm': 1.4811434745788574, 'learning_rate': 8.68808299654664e-05, 'epoch': 0.59}
{'loss': 0.8816, 'grad_norm': 1.3128358125686646, 'learning_rate': 8.682759357044779e-05, 'epoch': 0.59}
{'loss': 0.9772, 'grad_norm': 1.2713159322738647, 'learning_rate': 8.677436097428775e-05, 'epoch': 0.59}
{'loss': 0.7106, 'grad_norm': 1.3423341512680054, 'learning_rate': 8.672113219233835e-05, 'epoch': 0.59}
{'loss': 0.7889, 'grad_norm': 1.4013651609420776, 'learning_rate': 8.666790723995042e-05, 'epoch': 0.59}
{'loss': 1.1682, 'grad_norm': 1.1077444553375244, 'learning_rate': 8.661468613247387e-05, 'epoch': 0.59}
{'loss': 1.0745, 'grad_norm': 1.1576827764511108, 'learning_rate': 8.656146888525734e-05, 'epoch': 0.59}
{'loss': 1.1285, 'grad_norm': 1.2478736639022827, 'learning_rate': 8.650825551364843e-05, 'epoch': 0.59}
{'loss': 1.0509, 'grad_norm': 1.0176838636398315, 'learning_rate': 8.645504603299369e-05, 'epoch': 0.59}
{'loss': 1.0729, 'grad_norm': 1.130486249923706, 'learning_rate': 8.64018404586384e-05, 'epoch': 0.59}
{'loss': 0.9113, 'grad_norm': 1.2973610162734985, 'learning_rate': 8.634863880592686e-05, 'epoch': 0.59}
{'loss': 0.7089, 'grad_norm': 0.953165590763092, 'learning_rate': 8.629544109020211e-05, 'epoch': 0.59}
{'loss': 1.0288, 'grad_norm': 1.4619511365890503, 'learning_rate': 8.624224732680612e-05, 'epoch': 0.59}
{'loss': 0.9329, 'grad_norm': 1.4241244792938232, 'learning_rate': 8.618905753107979e-05, 'epoch': 0.59}
{'loss': 1.1525, 'grad_norm': 1.0769776105880737, 'learning_rate': 8.613587171836271e-05, 'epoch': 0.59}
{'loss': 0.8802, 'grad_norm': 1.2258228063583374, 'learning_rate': 8.608268990399349e-05, 'epoch': 0.59}
{'loss': 0.8637, 'grad_norm': 1.1906414031982422, 'learning_rate': 8.602951210330942e-05, 'epoch': 0.59}
{'loss': 1.0166, 'grad_norm': 1.1899007558822632, 'learning_rate': 8.597633833164676e-05, 'epoch': 0.59}
{'loss': 0.9234, 'grad_norm': 1.4001513719558716, 'learning_rate': 8.592316860434062e-05, 'epoch': 0.59}
{'loss': 1.1403, 'grad_norm': 1.4331262111663818, 'learning_rate': 8.587000293672481e-05, 'epoch': 0.59}
{'loss': 1.0382, 'grad_norm': 1.0288653373718262, 'learning_rate': 8.581684134413216e-05, 'epoch': 0.59}
{'loss': 0.9049, 'grad_norm': 1.2299436330795288, 'learning_rate': 8.57636838418941e-05, 'epoch': 0.59}
{'loss': 0.9623, 'grad_norm': 1.082292079925537, 'learning_rate': 8.571053044534103e-05, 'epoch': 0.59}
{'loss': 0.9942, 'grad_norm': 1.0758801698684692, 'learning_rate': 8.565738116980222e-05, 'epoch': 0.59}
{'loss': 1.0059, 'grad_norm': 1.2329769134521484, 'learning_rate': 8.560423603060554e-05, 'epoch': 0.59}
{'loss': 0.6291, 'grad_norm': 1.176408290863037, 'learning_rate': 8.55510950430779e-05, 'epoch': 0.59}
{'loss': 1.1226, 'grad_norm': 1.0810133218765259, 'learning_rate': 8.549795822254482e-05, 'epoch': 0.59}
{'loss': 1.0493, 'grad_norm': 1.0944503545761108, 'learning_rate': 8.544482558433072e-05, 'epoch': 0.59}
{'loss': 0.8468, 'grad_norm': 0.9412977695465088, 'learning_rate': 8.539169714375885e-05, 'epoch': 0.59}
{'loss': 1.1178, 'grad_norm': 1.0965098142623901, 'learning_rate': 8.533857291615114e-05, 'epoch': 0.59}
{'loss': 1.1453, 'grad_norm': 1.6815153360366821, 'learning_rate': 8.528545291682838e-05, 'epoch': 0.59}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0291, 'grad_norm': 0.9485343098640442, 'learning_rate': 8.523233716111016e-05, 'epoch': 0.59}
{'loss': 0.8803, 'grad_norm': 0.9154196977615356, 'learning_rate': 8.517922566431473e-05, 'epoch': 0.59}
{'loss': 1.0627, 'grad_norm': 1.5739489793777466, 'learning_rate': 8.512611844175929e-05, 'epoch': 0.59}
{'loss': 1.0, 'grad_norm': 1.1631040573120117, 'learning_rate': 8.50730155087596e-05, 'epoch': 0.59}
{'loss': 1.1176, 'grad_norm': 1.486796259880066, 'learning_rate': 8.501991688063033e-05, 'epoch': 0.59}
{'loss': 0.804, 'grad_norm': 1.5679285526275635, 'learning_rate': 8.496682257268493e-05, 'epoch': 0.59}
{'loss': 0.8554, 'grad_norm': 1.3503600358963013, 'learning_rate': 8.491373260023545e-05, 'epoch': 0.59}
{'loss': 0.7092, 'grad_norm': 1.6546550989151, 'learning_rate': 8.486064697859285e-05, 'epoch': 0.59}
{'loss': 0.8959, 'grad_norm': 0.9961467385292053, 'learning_rate': 8.480756572306674e-05, 'epoch': 0.59}
{'loss': 0.9767, 'grad_norm': 1.202604055404663, 'learning_rate': 8.475448884896547e-05, 'epoch': 0.59}
{'loss': 0.9749, 'grad_norm': 1.5885732173919678, 'learning_rate': 8.47014163715962e-05, 'epoch': 0.59}
{'loss': 0.932, 'grad_norm': 1.156915307044983, 'learning_rate': 8.464834830626476e-05, 'epoch': 0.59}
{'loss': 0.9445, 'grad_norm': 1.3089089393615723, 'learning_rate': 8.459528466827575e-05, 'epoch': 0.59}
{'loss': 1.1246, 'grad_norm': 0.9790986776351929, 'learning_rate': 8.45422254729324e-05, 'epoch': 0.59}
{'loss': 0.8108, 'grad_norm': 1.2515075206756592, 'learning_rate': 8.448917073553678e-05, 'epoch': 0.59}
{'loss': 0.7142, 'grad_norm': 1.1124718189239502, 'learning_rate': 8.443612047138966e-05, 'epoch': 0.59}
{'loss': 0.8129, 'grad_norm': 1.160217046737671, 'learning_rate': 8.438307469579037e-05, 'epoch': 0.59}
{'loss': 1.2727, 'grad_norm': 0.9840361475944519, 'learning_rate': 8.433003342403715e-05, 'epoch': 0.6}
{'loss': 0.8968, 'grad_norm': 1.2416489124298096, 'learning_rate': 8.427699667142682e-05, 'epoch': 0.6}
{'loss': 1.1622, 'grad_norm': 0.9645034670829773, 'learning_rate': 8.422396445325487e-05, 'epoch': 0.6}
{'loss': 0.924, 'grad_norm': 1.5073057413101196, 'learning_rate': 8.417093678481563e-05, 'epoch': 0.6}
{'loss': 1.1698, 'grad_norm': 1.055645227432251, 'learning_rate': 8.411791368140196e-05, 'epoch': 0.6}
{'loss': 0.9319, 'grad_norm': 1.3646957874298096, 'learning_rate': 8.406489515830553e-05, 'epoch': 0.6}
{'loss': 1.0549, 'grad_norm': 1.1120049953460693, 'learning_rate': 8.401188123081653e-05, 'epoch': 0.6}
{'loss': 1.1683, 'grad_norm': 1.6412193775177002, 'learning_rate': 8.395887191422397e-05, 'epoch': 0.6}
{'loss': 1.0298, 'grad_norm': 1.231330394744873, 'learning_rate': 8.390586722381554e-05, 'epoch': 0.6}
{'loss': 1.0171, 'grad_norm': 1.2595512866973877, 'learning_rate': 8.385286717487743e-05, 'epoch': 0.6}
{'loss': 0.7965, 'grad_norm': 0.9938129782676697, 'learning_rate': 8.37998717826947e-05, 'epoch': 0.6}
{'loss': 1.0269, 'grad_norm': 1.2670187950134277, 'learning_rate': 8.374688106255089e-05, 'epoch': 0.6}
{'loss': 0.9589, 'grad_norm': 1.1218479871749878, 'learning_rate': 8.369389502972828e-05, 'epoch': 0.6}
{'loss': 0.9481, 'grad_norm': 1.255146861076355, 'learning_rate': 8.364091369950782e-05, 'epoch': 0.6}
{'loss': 0.9358, 'grad_norm': 1.5853067636489868, 'learning_rate': 8.358793708716905e-05, 'epoch': 0.6}
{'loss': 0.6385, 'grad_norm': 1.0759172439575195, 'learning_rate': 8.353496520799021e-05, 'epoch': 0.6}
{'loss': 1.2091, 'grad_norm': 1.2741938829421997, 'learning_rate': 8.348199807724806e-05, 'epoch': 0.6}
{'loss': 1.078, 'grad_norm': 1.1544526815414429, 'learning_rate': 8.34290357102181e-05, 'epoch': 0.6}
{'loss': 0.8078, 'grad_norm': 1.0459989309310913, 'learning_rate': 8.337607812217446e-05, 'epoch': 0.6}
{'loss': 0.8868, 'grad_norm': 1.1818054914474487, 'learning_rate': 8.332312532838978e-05, 'epoch': 0.6}
{'loss': 0.9685, 'grad_norm': 1.3292773962020874, 'learning_rate': 8.327017734413544e-05, 'epoch': 0.6}
{'loss': 1.0863, 'grad_norm': 1.0255780220031738, 'learning_rate': 8.32172341846814e-05, 'epoch': 0.6}
{'loss': 0.9809, 'grad_norm': 1.0295871496200562, 'learning_rate': 8.316429586529615e-05, 'epoch': 0.6}
{'loss': 1.0457, 'grad_norm': 1.1360875368118286, 'learning_rate': 8.311136240124691e-05, 'epoch': 0.6}
{'loss': 0.8309, 'grad_norm': 0.9925447702407837, 'learning_rate': 8.305843380779938e-05, 'epoch': 0.6}
{'loss': 1.0929, 'grad_norm': 1.2448736429214478, 'learning_rate': 8.300551010021795e-05, 'epoch': 0.6}
{'loss': 1.0147, 'grad_norm': 0.9978705644607544, 'learning_rate': 8.295259129376557e-05, 'epoch': 0.6}
{'loss': 1.0799, 'grad_norm': 0.959662914276123, 'learning_rate': 8.289967740370371e-05, 'epoch': 0.6}
{'loss': 0.7904, 'grad_norm': 1.5140275955200195, 'learning_rate': 8.284676844529257e-05, 'epoch': 0.6}
{'loss': 0.7597, 'grad_norm': 1.121706247329712, 'learning_rate': 8.279386443379076e-05, 'epoch': 0.6}
{'loss': 0.7242, 'grad_norm': 1.1841890811920166, 'learning_rate': 8.274096538445557e-05, 'epoch': 0.6}
{'loss': 1.1827, 'grad_norm': 1.1714757680892944, 'learning_rate': 8.268807131254287e-05, 'epoch': 0.6}
{'loss': 0.9431, 'grad_norm': 1.18414306640625, 'learning_rate': 8.263518223330697e-05, 'epoch': 0.6}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0069, 'grad_norm': 1.487922191619873, 'learning_rate': 8.25822981620009e-05, 'epoch': 0.6}
{'loss': 1.0376, 'grad_norm': 1.6301852464675903, 'learning_rate': 8.252941911387613e-05, 'epoch': 0.6}
{'loss': 0.9617, 'grad_norm': 1.052531123161316, 'learning_rate': 8.247654510418275e-05, 'epoch': 0.6}
{'loss': 0.9329, 'grad_norm': 1.4430991411209106, 'learning_rate': 8.242367614816938e-05, 'epoch': 0.6}
{'loss': 0.7752, 'grad_norm': 1.5042835474014282, 'learning_rate': 8.237081226108311e-05, 'epoch': 0.6}
{'loss': 1.0058, 'grad_norm': 1.0595020055770874, 'learning_rate': 8.231795345816972e-05, 'epoch': 0.6}
{'loss': 0.8689, 'grad_norm': 1.126283884048462, 'learning_rate': 8.226509975467334e-05, 'epoch': 0.6}
{'loss': 0.7596, 'grad_norm': 1.1645464897155762, 'learning_rate': 8.221225116583678e-05, 'epoch': 0.6}
{'loss': 0.791, 'grad_norm': 1.1768410205841064, 'learning_rate': 8.215940770690133e-05, 'epoch': 0.6}
{'loss': 0.7889, 'grad_norm': 1.0734541416168213, 'learning_rate': 8.210656939310672e-05, 'epoch': 0.6}
{'loss': 0.7979, 'grad_norm': 1.2655712366104126, 'learning_rate': 8.205373623969136e-05, 'epoch': 0.6}
{'loss': 0.7025, 'grad_norm': 1.2364197969436646, 'learning_rate': 8.200090826189202e-05, 'epoch': 0.6}
{'loss': 0.8438, 'grad_norm': 1.6347707509994507, 'learning_rate': 8.194808547494401e-05, 'epoch': 0.6}
{'loss': 0.8514, 'grad_norm': 1.309319257736206, 'learning_rate': 8.189526789408123e-05, 'epoch': 0.6}
{'loss': 0.8505, 'grad_norm': 1.0058025121688843, 'learning_rate': 8.184245553453596e-05, 'epoch': 0.6}
{'loss': 0.8048, 'grad_norm': 1.1871721744537354, 'learning_rate': 8.17896484115391e-05, 'epoch': 0.6}
{'loss': 0.8781, 'grad_norm': 1.1037365198135376, 'learning_rate': 8.173684654031989e-05, 'epoch': 0.6}
{'loss': 0.9488, 'grad_norm': 1.6581521034240723, 'learning_rate': 8.168404993610617e-05, 'epoch': 0.6}
{'loss': 0.8269, 'grad_norm': 0.9612873196601868, 'learning_rate': 8.163125861412426e-05, 'epoch': 0.6}
{'loss': 0.7991, 'grad_norm': 1.167040228843689, 'learning_rate': 8.157847258959885e-05, 'epoch': 0.6}
{'loss': 0.9533, 'grad_norm': 1.1600861549377441, 'learning_rate': 8.152569187775325e-05, 'epoch': 0.6}
{'loss': 1.0768, 'grad_norm': 1.1463500261306763, 'learning_rate': 8.147291649380912e-05, 'epoch': 0.6}
{'loss': 1.0399, 'grad_norm': 0.9978739023208618, 'learning_rate': 8.142014645298661e-05, 'epoch': 0.6}
{'loss': 1.1588, 'grad_norm': 0.9423593282699585, 'learning_rate': 8.13673817705044e-05, 'epoch': 0.6}
{'loss': 0.9447, 'grad_norm': 0.9019728899002075, 'learning_rate': 8.131462246157953e-05, 'epoch': 0.6}
{'loss': 0.7151, 'grad_norm': 1.180550217628479, 'learning_rate': 8.126186854142752e-05, 'epoch': 0.6}
{'loss': 0.7042, 'grad_norm': 1.2914750576019287, 'learning_rate': 8.120912002526242e-05, 'epoch': 0.6}
{'loss': 0.9452, 'grad_norm': 1.1310205459594727, 'learning_rate': 8.115637692829657e-05, 'epoch': 0.6}
{'loss': 0.8234, 'grad_norm': 1.369296908378601, 'learning_rate': 8.110363926574087e-05, 'epoch': 0.6}
{'loss': 0.9845, 'grad_norm': 0.9506216645240784, 'learning_rate': 8.105090705280456e-05, 'epoch': 0.6}
{'loss': 1.0788, 'grad_norm': 0.9826281070709229, 'learning_rate': 8.099818030469538e-05, 'epoch': 0.6}
{'loss': 0.8598, 'grad_norm': 1.3121880292892456, 'learning_rate': 8.094545903661953e-05, 'epoch': 0.6}
{'loss': 1.0408, 'grad_norm': 1.1486042737960815, 'learning_rate': 8.089274326378145e-05, 'epoch': 0.61}
{'loss': 0.872, 'grad_norm': 1.0811678171157837, 'learning_rate': 8.084003300138422e-05, 'epoch': 0.61}
{'loss': 1.1909, 'grad_norm': 1.0158239603042603, 'learning_rate': 8.078732826462915e-05, 'epoch': 0.61}
{'loss': 1.0306, 'grad_norm': 1.299704909324646, 'learning_rate': 8.07346290687161e-05, 'epoch': 0.61}
{'loss': 0.8073, 'grad_norm': 0.9856046438217163, 'learning_rate': 8.068193542884325e-05, 'epoch': 0.61}
{'loss': 0.9665, 'grad_norm': 0.9995006918907166, 'learning_rate': 8.062924736020714e-05, 'epoch': 0.61}
{'loss': 0.7934, 'grad_norm': 1.0998854637145996, 'learning_rate': 8.057656487800282e-05, 'epoch': 0.61}
{'loss': 1.0805, 'grad_norm': 1.1511085033416748, 'learning_rate': 8.052388799742361e-05, 'epoch': 0.61}
{'loss': 1.0673, 'grad_norm': 1.6042606830596924, 'learning_rate': 8.047121673366129e-05, 'epoch': 0.61}
{'loss': 1.036, 'grad_norm': 1.4677603244781494, 'learning_rate': 8.041855110190604e-05, 'epoch': 0.61}
{'loss': 1.11, 'grad_norm': 0.9871199727058411, 'learning_rate': 8.036589111734629e-05, 'epoch': 0.61}
{'loss': 0.9133, 'grad_norm': 1.5653623342514038, 'learning_rate': 8.0313236795169e-05, 'epoch': 0.61}
{'loss': 0.9404, 'grad_norm': 1.1051608324050903, 'learning_rate': 8.026058815055936e-05, 'epoch': 0.61}
{'loss': 0.8022, 'grad_norm': 1.397518277168274, 'learning_rate': 8.020794519870106e-05, 'epoch': 0.61}
{'loss': 0.9093, 'grad_norm': 1.2316884994506836, 'learning_rate': 8.015530795477603e-05, 'epoch': 0.61}
{'loss': 1.0858, 'grad_norm': 1.1620577573776245, 'learning_rate': 8.010267643396457e-05, 'epoch': 0.61}
{'loss': 0.7966, 'grad_norm': 0.9393616914749146, 'learning_rate': 8.005005065144543e-05, 'epoch': 0.61}
{'loss': 0.6466, 'grad_norm': 0.8651003837585449, 'learning_rate': 7.999743062239557e-05, 'epoch': 0.61}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0769, 'grad_norm': 1.386361002922058, 'learning_rate': 7.994481636199036e-05, 'epoch': 0.61}
{'loss': 0.6502, 'grad_norm': 1.3480361700057983, 'learning_rate': 7.989220788540355e-05, 'epoch': 0.61}
{'loss': 1.1282, 'grad_norm': 1.3178855180740356, 'learning_rate': 7.98396052078071e-05, 'epoch': 0.61}
{'loss': 0.7254, 'grad_norm': 1.1800692081451416, 'learning_rate': 7.978700834437143e-05, 'epoch': 0.61}
{'loss': 1.1287, 'grad_norm': 1.3044060468673706, 'learning_rate': 7.973441731026518e-05, 'epoch': 0.61}
{'loss': 1.1551, 'grad_norm': 1.1058993339538574, 'learning_rate': 7.968183212065537e-05, 'epoch': 0.61}
{'loss': 1.4326, 'grad_norm': 0.9367811679840088, 'learning_rate': 7.962925279070732e-05, 'epoch': 0.61}
{'loss': 0.6647, 'grad_norm': 1.4462693929672241, 'learning_rate': 7.957667933558461e-05, 'epoch': 0.61}
{'loss': 0.907, 'grad_norm': 1.4451578855514526, 'learning_rate': 7.952411177044923e-05, 'epoch': 0.61}
{'loss': 0.8168, 'grad_norm': 1.089320182800293, 'learning_rate': 7.947155011046144e-05, 'epoch': 0.61}
{'loss': 1.172, 'grad_norm': 1.2502882480621338, 'learning_rate': 7.941899437077965e-05, 'epoch': 0.61}
{'loss': 1.2119, 'grad_norm': 1.3150261640548706, 'learning_rate': 7.936644456656081e-05, 'epoch': 0.61}
{'loss': 0.944, 'grad_norm': 1.0806690454483032, 'learning_rate': 7.931390071295994e-05, 'epoch': 0.61}
{'loss': 0.9738, 'grad_norm': 1.2244970798492432, 'learning_rate': 7.926136282513045e-05, 'epoch': 0.61}
{'loss': 0.6641, 'grad_norm': 1.0886101722717285, 'learning_rate': 7.920883091822408e-05, 'epoch': 0.61}
{'loss': 0.9703, 'grad_norm': 1.2314831018447876, 'learning_rate': 7.915630500739071e-05, 'epoch': 0.61}
{'loss': 0.9774, 'grad_norm': 1.4466224908828735, 'learning_rate': 7.910378510777857e-05, 'epoch': 0.61}
{'loss': 1.0019, 'grad_norm': 1.3297371864318848, 'learning_rate': 7.905127123453415e-05, 'epoch': 0.61}
{'loss': 1.0795, 'grad_norm': 1.0509672164916992, 'learning_rate': 7.89987634028022e-05, 'epoch': 0.61}
{'loss': 1.1235, 'grad_norm': 1.0095280408859253, 'learning_rate': 7.894626162772578e-05, 'epoch': 0.61}
{'loss': 1.1283, 'grad_norm': 1.136308193206787, 'learning_rate': 7.889376592444605e-05, 'epoch': 0.61}
{'loss': 1.1536, 'grad_norm': 1.1209598779678345, 'learning_rate': 7.88412763081026e-05, 'epoch': 0.61}
{'loss': 0.9238, 'grad_norm': 1.0898669958114624, 'learning_rate': 7.878879279383314e-05, 'epoch': 0.61}
{'loss': 0.8047, 'grad_norm': 1.2465049028396606, 'learning_rate': 7.873631539677364e-05, 'epoch': 0.61}
{'loss': 0.7839, 'grad_norm': 1.5990111827850342, 'learning_rate': 7.868384413205842e-05, 'epoch': 0.61}
{'loss': 1.3934, 'grad_norm': 1.5812331438064575, 'learning_rate': 7.863137901481983e-05, 'epoch': 0.61}
{'loss': 0.9364, 'grad_norm': 1.145137071609497, 'learning_rate': 7.857892006018862e-05, 'epoch': 0.61}
{'loss': 1.1794, 'grad_norm': 0.9501686096191406, 'learning_rate': 7.852646728329368e-05, 'epoch': 0.61}
{'loss': 1.1598, 'grad_norm': 1.5146652460098267, 'learning_rate': 7.847402069926215e-05, 'epoch': 0.61}
{'loss': 0.896, 'grad_norm': 1.0022692680358887, 'learning_rate': 7.84215803232194e-05, 'epoch': 0.61}
{'loss': 0.7813, 'grad_norm': 1.6937856674194336, 'learning_rate': 7.836914617028891e-05, 'epoch': 0.61}
{'loss': 1.0017, 'grad_norm': 1.1028451919555664, 'learning_rate': 7.831671825559252e-05, 'epoch': 0.61}
{'loss': 0.8457, 'grad_norm': 1.007906436920166, 'learning_rate': 7.82642965942501e-05, 'epoch': 0.61}
{'loss': 0.8904, 'grad_norm': 1.104949951171875, 'learning_rate': 7.821188120137986e-05, 'epoch': 0.61}
{'loss': 1.0263, 'grad_norm': 1.3066093921661377, 'learning_rate': 7.815947209209817e-05, 'epoch': 0.61}
{'loss': 1.2417, 'grad_norm': 1.1574631929397583, 'learning_rate': 7.81070692815195e-05, 'epoch': 0.61}
{'loss': 0.7971, 'grad_norm': 1.3936522006988525, 'learning_rate': 7.805467278475663e-05, 'epoch': 0.61}
{'loss': 1.17, 'grad_norm': 0.9608418345451355, 'learning_rate': 7.800228261692037e-05, 'epoch': 0.61}
{'loss': 0.8434, 'grad_norm': 1.1140236854553223, 'learning_rate': 7.794989879311991e-05, 'epoch': 0.61}
{'loss': 0.9805, 'grad_norm': 0.9458047151565552, 'learning_rate': 7.789752132846239e-05, 'epoch': 0.61}
{'loss': 1.0755, 'grad_norm': 1.2020699977874756, 'learning_rate': 7.784515023805328e-05, 'epoch': 0.61}
{'loss': 0.9744, 'grad_norm': 1.0372700691223145, 'learning_rate': 7.779278553699614e-05, 'epoch': 0.61}
{'loss': 1.0997, 'grad_norm': 1.0910671949386597, 'learning_rate': 7.774042724039267e-05, 'epoch': 0.61}
{'loss': 1.0542, 'grad_norm': 1.231514573097229, 'learning_rate': 7.768807536334276e-05, 'epoch': 0.61}
{'loss': 1.1385, 'grad_norm': 1.5619057416915894, 'learning_rate': 7.763572992094447e-05, 'epoch': 0.61}
{'loss': 1.0843, 'grad_norm': 1.1748104095458984, 'learning_rate': 7.758339092829395e-05, 'epoch': 0.61}
{'loss': 0.6329, 'grad_norm': 1.0203197002410889, 'learning_rate': 7.753105840048548e-05, 'epoch': 0.61}
{'loss': 0.8909, 'grad_norm': 1.3643054962158203, 'learning_rate': 7.747873235261157e-05, 'epoch': 0.62}
{'loss': 1.066, 'grad_norm': 0.9076372981071472, 'learning_rate': 7.742641279976278e-05, 'epoch': 0.62}
{'loss': 1.2713, 'grad_norm': 0.9802577495574951, 'learning_rate': 7.73740997570278e-05, 'epoch': 0.62}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1729, 'grad_norm': 1.1605812311172485, 'learning_rate': 7.732179323949347e-05, 'epoch': 0.62}
{'loss': 0.9423, 'grad_norm': 1.1470215320587158, 'learning_rate': 7.726949326224472e-05, 'epoch': 0.62}
{'loss': 0.8136, 'grad_norm': 1.1030800342559814, 'learning_rate': 7.721719984036469e-05, 'epoch': 0.62}
{'loss': 0.7654, 'grad_norm': 1.1892507076263428, 'learning_rate': 7.716491298893442e-05, 'epoch': 0.62}
{'loss': 0.8686, 'grad_norm': 1.1971962451934814, 'learning_rate': 7.711263272303331e-05, 'epoch': 0.62}
{'loss': 0.8135, 'grad_norm': 0.9865109324455261, 'learning_rate': 7.706035905773866e-05, 'epoch': 0.62}
{'loss': 0.8869, 'grad_norm': 1.1292182207107544, 'learning_rate': 7.700809200812595e-05, 'epoch': 0.62}
{'loss': 0.9871, 'grad_norm': 1.4127360582351685, 'learning_rate': 7.69558315892688e-05, 'epoch': 0.62}
{'loss': 0.8792, 'grad_norm': 1.2113131284713745, 'learning_rate': 7.690357781623879e-05, 'epoch': 0.62}
{'loss': 0.87, 'grad_norm': 1.295660376548767, 'learning_rate': 7.685133070410571e-05, 'epoch': 0.62}
{'loss': 1.2154, 'grad_norm': 1.4841015338897705, 'learning_rate': 7.679909026793735e-05, 'epoch': 0.62}
{'loss': 1.2157, 'grad_norm': 1.3436133861541748, 'learning_rate': 7.67468565227996e-05, 'epoch': 0.62}
{'loss': 0.7428, 'grad_norm': 1.3040963411331177, 'learning_rate': 7.669462948375646e-05, 'epoch': 0.62}
{'loss': 1.0043, 'grad_norm': 1.0008360147476196, 'learning_rate': 7.664240916586989e-05, 'epoch': 0.62}
{'loss': 0.8466, 'grad_norm': 1.0781222581863403, 'learning_rate': 7.659019558420004e-05, 'epoch': 0.62}
{'loss': 0.9967, 'grad_norm': 1.3263671398162842, 'learning_rate': 7.6537988753805e-05, 'epoch': 0.62}
{'loss': 0.9481, 'grad_norm': 1.2263739109039307, 'learning_rate': 7.6485788689741e-05, 'epoch': 0.62}
{'loss': 0.8354, 'grad_norm': 1.1440043449401855, 'learning_rate': 7.643359540706232e-05, 'epoch': 0.62}
{'loss': 1.1312, 'grad_norm': 1.2535696029663086, 'learning_rate': 7.638140892082117e-05, 'epoch': 0.62}
{'loss': 1.1028, 'grad_norm': 1.4199355840682983, 'learning_rate': 7.632922924606795e-05, 'epoch': 0.62}
{'loss': 0.8288, 'grad_norm': 0.948434591293335, 'learning_rate': 7.627705639785098e-05, 'epoch': 0.62}
{'loss': 0.5805, 'grad_norm': 1.0738849639892578, 'learning_rate': 7.622489039121667e-05, 'epoch': 0.62}
{'loss': 1.1329, 'grad_norm': 1.3132199048995972, 'learning_rate': 7.617273124120951e-05, 'epoch': 0.62}
{'loss': 1.092, 'grad_norm': 1.6914992332458496, 'learning_rate': 7.612057896287186e-05, 'epoch': 0.62}
{'loss': 1.0098, 'grad_norm': 1.9117265939712524, 'learning_rate': 7.606843357124426e-05, 'epoch': 0.62}
{'loss': 0.948, 'grad_norm': 1.2692891359329224, 'learning_rate': 7.60162950813651e-05, 'epoch': 0.62}
{'loss': 0.9366, 'grad_norm': 1.1227843761444092, 'learning_rate': 7.596416350827091e-05, 'epoch': 0.62}
{'loss': 1.1207, 'grad_norm': 1.2175508737564087, 'learning_rate': 7.591203886699625e-05, 'epoch': 0.62}
{'loss': 1.0344, 'grad_norm': 1.1681427955627441, 'learning_rate': 7.58599211725735e-05, 'epoch': 0.62}
{'loss': 1.0933, 'grad_norm': 1.3332350254058838, 'learning_rate': 7.580781044003324e-05, 'epoch': 0.62}
{'loss': 1.201, 'grad_norm': 1.1790117025375366, 'learning_rate': 7.575570668440392e-05, 'epoch': 0.62}
{'loss': 0.8934, 'grad_norm': 1.1269272565841675, 'learning_rate': 7.570360992071199e-05, 'epoch': 0.62}
{'loss': 1.1097, 'grad_norm': 0.9407868981361389, 'learning_rate': 7.5651520163982e-05, 'epoch': 0.62}
{'loss': 0.9461, 'grad_norm': 1.2804982662200928, 'learning_rate': 7.559943742923626e-05, 'epoch': 0.62}
{'loss': 1.0836, 'grad_norm': 1.0504202842712402, 'learning_rate': 7.554736173149524e-05, 'epoch': 0.62}
{'loss': 0.8743, 'grad_norm': 1.2200632095336914, 'learning_rate': 7.549529308577736e-05, 'epoch': 0.62}
{'loss': 1.2292, 'grad_norm': 0.8944576978683472, 'learning_rate': 7.54432315070989e-05, 'epoch': 0.62}
{'loss': 0.7954, 'grad_norm': 1.3559595346450806, 'learning_rate': 7.53911770104742e-05, 'epoch': 0.62}
{'loss': 1.1843, 'grad_norm': 1.5194814205169678, 'learning_rate': 7.533912961091551e-05, 'epoch': 0.62}
{'loss': 1.165, 'grad_norm': 1.0993695259094238, 'learning_rate': 7.528708932343304e-05, 'epoch': 0.62}
{'loss': 0.8883, 'grad_norm': 1.240325927734375, 'learning_rate': 7.5235056163035e-05, 'epoch': 0.62}
{'loss': 0.869, 'grad_norm': 1.0875012874603271, 'learning_rate': 7.518303014472748e-05, 'epoch': 0.62}
{'loss': 0.9911, 'grad_norm': 1.1070520877838135, 'learning_rate': 7.513101128351454e-05, 'epoch': 0.62}
{'loss': 1.19, 'grad_norm': 1.2757978439331055, 'learning_rate': 7.507899959439814e-05, 'epoch': 0.62}
{'loss': 0.9687, 'grad_norm': 2.1379892826080322, 'learning_rate': 7.502699509237822e-05, 'epoch': 0.62}
{'loss': 0.9243, 'grad_norm': 1.194813847541809, 'learning_rate': 7.497499779245267e-05, 'epoch': 0.62}
{'loss': 0.8851, 'grad_norm': 1.141711711883545, 'learning_rate': 7.492300770961718e-05, 'epoch': 0.62}
{'loss': 0.8354, 'grad_norm': 1.0087097883224487, 'learning_rate': 7.487102485886552e-05, 'epoch': 0.62}
{'loss': 0.8224, 'grad_norm': 1.0703880786895752, 'learning_rate': 7.48190492551892e-05, 'epoch': 0.62}
{'loss': 0.9721, 'grad_norm': 1.0940269231796265, 'learning_rate': 7.476708091357782e-05, 'epoch': 0.62}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6527, 'grad_norm': 1.3041633367538452, 'learning_rate': 7.471511984901878e-05, 'epoch': 0.62}
{'loss': 0.9013, 'grad_norm': 2.1556642055511475, 'learning_rate': 7.466316607649738e-05, 'epoch': 0.62}
{'loss': 1.0822, 'grad_norm': 1.1385325193405151, 'learning_rate': 7.461121961099684e-05, 'epoch': 0.62}
{'loss': 1.1653, 'grad_norm': 1.5616811513900757, 'learning_rate': 7.455928046749827e-05, 'epoch': 0.62}
{'loss': 1.2089, 'grad_norm': 1.2343323230743408, 'learning_rate': 7.450734866098066e-05, 'epoch': 0.62}
{'loss': 0.9719, 'grad_norm': 1.00877845287323, 'learning_rate': 7.445542420642097e-05, 'epoch': 0.62}
{'loss': 1.0369, 'grad_norm': 1.080116868019104, 'learning_rate': 7.440350711879384e-05, 'epoch': 0.62}
{'loss': 1.0202, 'grad_norm': 1.0149646997451782, 'learning_rate': 7.435159741307202e-05, 'epoch': 0.62}
{'loss': 1.1617, 'grad_norm': 1.4403058290481567, 'learning_rate': 7.429969510422592e-05, 'epoch': 0.62}
{'loss': 1.1929, 'grad_norm': 1.2458992004394531, 'learning_rate': 7.424780020722397e-05, 'epoch': 0.62}
{'loss': 1.1535, 'grad_norm': 0.9517732262611389, 'learning_rate': 7.419591273703244e-05, 'epoch': 0.62}
{'loss': 0.8521, 'grad_norm': 1.0748854875564575, 'learning_rate': 7.414403270861536e-05, 'epoch': 0.62}
{'loss': 1.0792, 'grad_norm': 1.320778250694275, 'learning_rate': 7.409216013693471e-05, 'epoch': 0.63}
{'loss': 0.9955, 'grad_norm': 1.523905873298645, 'learning_rate': 7.404029503695028e-05, 'epoch': 0.63}
{'loss': 0.8888, 'grad_norm': 1.3819537162780762, 'learning_rate': 7.398843742361972e-05, 'epoch': 0.63}
{'loss': 0.867, 'grad_norm': 1.0893126726150513, 'learning_rate': 7.393658731189857e-05, 'epoch': 0.63}
{'loss': 1.2594, 'grad_norm': 1.263266682624817, 'learning_rate': 7.388474471674006e-05, 'epoch': 0.63}
{'loss': 0.9357, 'grad_norm': 1.0518112182617188, 'learning_rate': 7.383290965309541e-05, 'epoch': 0.63}
{'loss': 0.9776, 'grad_norm': 0.9648042917251587, 'learning_rate': 7.378108213591355e-05, 'epoch': 0.63}
{'loss': 1.239, 'grad_norm': 1.35763680934906, 'learning_rate': 7.372926218014131e-05, 'epoch': 0.63}
{'loss': 1.0342, 'grad_norm': 1.0295380353927612, 'learning_rate': 7.367744980072338e-05, 'epoch': 0.63}
{'loss': 0.9468, 'grad_norm': 1.3549144268035889, 'learning_rate': 7.362564501260207e-05, 'epoch': 0.63}
{'loss': 0.7875, 'grad_norm': 1.4879462718963623, 'learning_rate': 7.357384783071772e-05, 'epoch': 0.63}
{'loss': 1.2079, 'grad_norm': 0.881409227848053, 'learning_rate': 7.35220582700084e-05, 'epoch': 0.63}
{'loss': 1.0633, 'grad_norm': 1.392810583114624, 'learning_rate': 7.347027634540993e-05, 'epoch': 0.63}
{'loss': 0.7942, 'grad_norm': 1.0834565162658691, 'learning_rate': 7.3418502071856e-05, 'epoch': 0.63}
{'loss': 0.7015, 'grad_norm': 1.0458461046218872, 'learning_rate': 7.336673546427801e-05, 'epoch': 0.63}
{'loss': 0.7293, 'grad_norm': 1.2732564210891724, 'learning_rate': 7.331497653760521e-05, 'epoch': 0.63}
{'loss': 0.9716, 'grad_norm': 1.3433825969696045, 'learning_rate': 7.32632253067647e-05, 'epoch': 0.63}
{'loss': 0.9522, 'grad_norm': 0.9824850559234619, 'learning_rate': 7.32114817866812e-05, 'epoch': 0.63}
{'loss': 0.8849, 'grad_norm': 1.2367645502090454, 'learning_rate': 7.315974599227735e-05, 'epoch': 0.63}
{'loss': 0.8178, 'grad_norm': 1.0653611421585083, 'learning_rate': 7.310801793847344e-05, 'epoch': 0.63}
{'loss': 1.1013, 'grad_norm': 1.0731480121612549, 'learning_rate': 7.305629764018764e-05, 'epoch': 0.63}
{'loss': 1.0269, 'grad_norm': 1.1651006937026978, 'learning_rate': 7.300458511233583e-05, 'epoch': 0.63}
{'loss': 0.8121, 'grad_norm': 1.0913797616958618, 'learning_rate': 7.295288036983163e-05, 'epoch': 0.63}
{'loss': 1.1219, 'grad_norm': 1.4358044862747192, 'learning_rate': 7.29011834275865e-05, 'epoch': 0.63}
{'loss': 1.1931, 'grad_norm': 1.1915884017944336, 'learning_rate': 7.28494943005095e-05, 'epoch': 0.63}
{'loss': 0.6587, 'grad_norm': 1.5698224306106567, 'learning_rate': 7.279781300350758e-05, 'epoch': 0.63}
{'loss': 0.8679, 'grad_norm': 1.1880240440368652, 'learning_rate': 7.27461395514854e-05, 'epoch': 0.63}
{'loss': 1.0339, 'grad_norm': 1.6428653001785278, 'learning_rate': 7.269447395934526e-05, 'epoch': 0.63}
{'loss': 1.2201, 'grad_norm': 1.0901013612747192, 'learning_rate': 7.264281624198736e-05, 'epoch': 0.63}
{'loss': 0.8234, 'grad_norm': 1.1074928045272827, 'learning_rate': 7.259116641430943e-05, 'epoch': 0.63}
{'loss': 0.9679, 'grad_norm': 1.264352560043335, 'learning_rate': 7.253952449120712e-05, 'epoch': 0.63}
{'loss': 0.8152, 'grad_norm': 0.9386022090911865, 'learning_rate': 7.248789048757368e-05, 'epoch': 0.63}
{'loss': 0.9726, 'grad_norm': 1.0207105875015259, 'learning_rate': 7.243626441830009e-05, 'epoch': 0.63}
{'loss': 0.8626, 'grad_norm': 1.4249670505523682, 'learning_rate': 7.23846462982751e-05, 'epoch': 0.63}
{'loss': 0.8244, 'grad_norm': 1.080662727355957, 'learning_rate': 7.233303614238509e-05, 'epoch': 0.63}
{'loss': 0.7873, 'grad_norm': 1.72348952293396, 'learning_rate': 7.22814339655142e-05, 'epoch': 0.63}
{'loss': 0.9566, 'grad_norm': 0.8973951935768127, 'learning_rate': 7.222983978254426e-05, 'epoch': 0.63}
{'loss': 0.9275, 'grad_norm': 1.6723049879074097, 'learning_rate': 7.217825360835473e-05, 'epoch': 0.63}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6218, 'grad_norm': 1.5372614860534668, 'learning_rate': 7.212667545782293e-05, 'epoch': 0.63}
{'loss': 1.491, 'grad_norm': 1.0768065452575684, 'learning_rate': 7.207510534582361e-05, 'epoch': 0.63}
{'loss': 0.8847, 'grad_norm': 1.0368256568908691, 'learning_rate': 7.202354328722938e-05, 'epoch': 0.63}
{'loss': 0.7806, 'grad_norm': 0.9669329524040222, 'learning_rate': 7.197198929691059e-05, 'epoch': 0.63}
{'loss': 0.7534, 'grad_norm': 1.5096158981323242, 'learning_rate': 7.192044338973503e-05, 'epoch': 0.63}
{'loss': 0.8034, 'grad_norm': 0.9525661468505859, 'learning_rate': 7.186890558056836e-05, 'epoch': 0.63}
{'loss': 1.0804, 'grad_norm': 1.1448462009429932, 'learning_rate': 7.181737588427384e-05, 'epoch': 0.63}
{'loss': 0.7618, 'grad_norm': 1.1930434703826904, 'learning_rate': 7.176585431571235e-05, 'epoch': 0.63}
{'loss': 0.9839, 'grad_norm': 0.9665997624397278, 'learning_rate': 7.171434088974251e-05, 'epoch': 0.63}
{'loss': 0.8829, 'grad_norm': 1.1601063013076782, 'learning_rate': 7.166283562122049e-05, 'epoch': 0.63}
{'loss': 0.7036, 'grad_norm': 1.1375558376312256, 'learning_rate': 7.161133852500019e-05, 'epoch': 0.63}
{'loss': 0.6397, 'grad_norm': 1.1693089008331299, 'learning_rate': 7.155984961593316e-05, 'epoch': 0.63}
{'loss': 1.0961, 'grad_norm': 1.361313819885254, 'learning_rate': 7.150836890886847e-05, 'epoch': 0.63}
{'loss': 0.7796, 'grad_norm': 1.079880952835083, 'learning_rate': 7.145689641865301e-05, 'epoch': 0.63}
{'loss': 0.6973, 'grad_norm': 1.0706169605255127, 'learning_rate': 7.14054321601311e-05, 'epoch': 0.63}
{'loss': 1.2563, 'grad_norm': 1.1671414375305176, 'learning_rate': 7.135397614814481e-05, 'epoch': 0.63}
{'loss': 0.8173, 'grad_norm': 1.1487339735031128, 'learning_rate': 7.130252839753385e-05, 'epoch': 0.63}
{'loss': 0.8825, 'grad_norm': 1.4451242685317993, 'learning_rate': 7.125108892313546e-05, 'epoch': 0.63}
{'loss': 1.1701, 'grad_norm': 1.1596624851226807, 'learning_rate': 7.119965773978459e-05, 'epoch': 0.63}
{'loss': 0.7747, 'grad_norm': 1.3322833776474, 'learning_rate': 7.114823486231366e-05, 'epoch': 0.63}
{'loss': 1.0122, 'grad_norm': 0.9016848802566528, 'learning_rate': 7.109682030555283e-05, 'epoch': 0.63}
{'loss': 1.0597, 'grad_norm': 1.4349788427352905, 'learning_rate': 7.104541408432985e-05, 'epoch': 0.63}
{'loss': 0.9789, 'grad_norm': 1.7655892372131348, 'learning_rate': 7.099401621346994e-05, 'epoch': 0.63}
{'loss': 1.1079, 'grad_norm': 1.1297156810760498, 'learning_rate': 7.094262670779612e-05, 'epoch': 0.63}
{'loss': 1.0423, 'grad_norm': 1.1421539783477783, 'learning_rate': 7.089124558212871e-05, 'epoch': 0.63}
{'loss': 0.8072, 'grad_norm': 0.8649436235427856, 'learning_rate': 7.083987285128591e-05, 'epoch': 0.63}
{'loss': 1.0425, 'grad_norm': 1.1069140434265137, 'learning_rate': 7.078850853008332e-05, 'epoch': 0.63}
{'loss': 0.7664, 'grad_norm': 1.116001009941101, 'learning_rate': 7.073715263333416e-05, 'epoch': 0.64}
{'loss': 0.817, 'grad_norm': 1.0197936296463013, 'learning_rate': 7.068580517584927e-05, 'epoch': 0.64}
{'loss': 0.9713, 'grad_norm': 0.949528157711029, 'learning_rate': 7.063446617243694e-05, 'epoch': 0.64}
{'loss': 1.0498, 'grad_norm': 0.9138144254684448, 'learning_rate': 7.05831356379031e-05, 'epoch': 0.64}
{'loss': 1.0852, 'grad_norm': 1.1071009635925293, 'learning_rate': 7.053181358705132e-05, 'epoch': 0.64}
{'loss': 1.1095, 'grad_norm': 1.1536858081817627, 'learning_rate': 7.048050003468251e-05, 'epoch': 0.64}
{'loss': 1.1859, 'grad_norm': 1.0042409896850586, 'learning_rate': 7.042919499559537e-05, 'epoch': 0.64}
{'loss': 0.8153, 'grad_norm': 0.9226633906364441, 'learning_rate': 7.037789848458589e-05, 'epoch': 0.64}
{'loss': 1.005, 'grad_norm': 1.2257908582687378, 'learning_rate': 7.032661051644783e-05, 'epoch': 0.64}
{'loss': 0.8794, 'grad_norm': 1.1623550653457642, 'learning_rate': 7.027533110597239e-05, 'epoch': 0.64}
{'loss': 0.8981, 'grad_norm': 1.1716138124465942, 'learning_rate': 7.022406026794828e-05, 'epoch': 0.64}
{'loss': 1.0798, 'grad_norm': 1.2800657749176025, 'learning_rate': 7.017279801716177e-05, 'epoch': 0.64}
{'loss': 1.0124, 'grad_norm': 1.3267520666122437, 'learning_rate': 7.012154436839663e-05, 'epoch': 0.64}
{'loss': 0.9714, 'grad_norm': 1.7884358167648315, 'learning_rate': 7.007029933643419e-05, 'epoch': 0.64}
{'loss': 1.1731, 'grad_norm': 1.4242063760757446, 'learning_rate': 7.00190629360533e-05, 'epoch': 0.64}
{'loss': 0.8603, 'grad_norm': 1.4358209371566772, 'learning_rate': 6.996783518203019e-05, 'epoch': 0.64}
{'loss': 0.9373, 'grad_norm': 1.0401265621185303, 'learning_rate': 6.991661608913877e-05, 'epoch': 0.64}
{'loss': 0.8325, 'grad_norm': 0.9890429377555847, 'learning_rate': 6.986540567215044e-05, 'epoch': 0.64}
{'loss': 0.906, 'grad_norm': 0.9862611889839172, 'learning_rate': 6.981420394583389e-05, 'epoch': 0.64}
{'loss': 0.8354, 'grad_norm': 1.1938722133636475, 'learning_rate': 6.976301092495556e-05, 'epoch': 0.64}
{'loss': 0.8958, 'grad_norm': 1.5774016380310059, 'learning_rate': 6.971182662427924e-05, 'epoch': 0.64}
{'loss': 0.9319, 'grad_norm': 1.303990125656128, 'learning_rate': 6.966065105856622e-05, 'epoch': 0.64}
{'loss': 0.7417, 'grad_norm': 1.4553523063659668, 'learning_rate': 6.960948424257532e-05, 'epoch': 0.64}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.631, 'grad_norm': 1.0809589624404907, 'learning_rate': 6.955832619106277e-05, 'epoch': 0.64}
{'loss': 0.7404, 'grad_norm': 1.0949485301971436, 'learning_rate': 6.950717691878236e-05, 'epoch': 0.64}
{'loss': 0.966, 'grad_norm': 1.2598605155944824, 'learning_rate': 6.945603644048521e-05, 'epoch': 0.64}
{'loss': 0.957, 'grad_norm': 1.2659660577774048, 'learning_rate': 6.940490477092004e-05, 'epoch': 0.64}
{'loss': 1.3024, 'grad_norm': 0.989738404750824, 'learning_rate': 6.9353781924833e-05, 'epoch': 0.64}
{'loss': 0.7413, 'grad_norm': 1.1664096117019653, 'learning_rate': 6.93026679169676e-05, 'epoch': 0.64}
{'loss': 0.906, 'grad_norm': 1.3925446271896362, 'learning_rate': 6.925156276206497e-05, 'epoch': 0.64}
{'loss': 0.8487, 'grad_norm': 1.1150245666503906, 'learning_rate': 6.920046647486349e-05, 'epoch': 0.64}
{'loss': 0.9332, 'grad_norm': 1.2031422853469849, 'learning_rate': 6.914937907009913e-05, 'epoch': 0.64}
{'loss': 1.2032, 'grad_norm': 1.2241883277893066, 'learning_rate': 6.909830056250527e-05, 'epoch': 0.64}
{'loss': 0.8894, 'grad_norm': 1.3074145317077637, 'learning_rate': 6.904723096681267e-05, 'epoch': 0.64}
{'loss': 1.0649, 'grad_norm': 1.1889843940734863, 'learning_rate': 6.89961702977496e-05, 'epoch': 0.64}
{'loss': 1.2004, 'grad_norm': 1.4158421754837036, 'learning_rate': 6.894511857004165e-05, 'epoch': 0.64}
{'loss': 0.9384, 'grad_norm': 1.0429439544677734, 'learning_rate': 6.889407579841193e-05, 'epoch': 0.64}
{'loss': 1.0901, 'grad_norm': 1.183279037475586, 'learning_rate': 6.884304199758095e-05, 'epoch': 0.64}
{'loss': 1.0284, 'grad_norm': 1.1138885021209717, 'learning_rate': 6.879201718226658e-05, 'epoch': 0.64}
{'loss': 0.9334, 'grad_norm': 1.076798677444458, 'learning_rate': 6.874100136718416e-05, 'epoch': 0.64}
{'loss': 0.9145, 'grad_norm': 1.037646770477295, 'learning_rate': 6.868999456704633e-05, 'epoch': 0.64}
{'loss': 0.8214, 'grad_norm': 1.0711119174957275, 'learning_rate': 6.863899679656328e-05, 'epoch': 0.64}
{'loss': 1.1339, 'grad_norm': 0.9548181295394897, 'learning_rate': 6.85880080704425e-05, 'epoch': 0.64}
{'loss': 1.2054, 'grad_norm': 1.2184817790985107, 'learning_rate': 6.853702840338889e-05, 'epoch': 0.64}
{'loss': 0.8044, 'grad_norm': 1.1028298139572144, 'learning_rate': 6.848605781010477e-05, 'epoch': 0.64}
{'loss': 1.0352, 'grad_norm': 1.113269567489624, 'learning_rate': 6.843509630528977e-05, 'epoch': 0.64}
{'loss': 0.7898, 'grad_norm': 1.056701898574829, 'learning_rate': 6.838414390364094e-05, 'epoch': 0.64}
{'loss': 1.1167, 'grad_norm': 1.5956672430038452, 'learning_rate': 6.833320061985277e-05, 'epoch': 0.64}
{'loss': 1.0412, 'grad_norm': 1.068646788597107, 'learning_rate': 6.828226646861697e-05, 'epoch': 0.64}
{'loss': 1.2194, 'grad_norm': 1.044090986251831, 'learning_rate': 6.82313414646228e-05, 'epoch': 0.64}
{'loss': 0.998, 'grad_norm': 1.0855270624160767, 'learning_rate': 6.81804256225567e-05, 'epoch': 0.64}
{'loss': 1.2723, 'grad_norm': 1.735515832901001, 'learning_rate': 6.812951895710258e-05, 'epoch': 0.64}
{'loss': 0.9624, 'grad_norm': 1.1938011646270752, 'learning_rate': 6.807862148294171e-05, 'epoch': 0.64}
{'loss': 0.751, 'grad_norm': 1.3622300624847412, 'learning_rate': 6.802773321475262e-05, 'epoch': 0.64}
{'loss': 1.0751, 'grad_norm': 1.2842389345169067, 'learning_rate': 6.79768541672113e-05, 'epoch': 0.64}
{'loss': 0.8982, 'grad_norm': 1.2079190015792847, 'learning_rate': 6.7925984354991e-05, 'epoch': 0.64}
{'loss': 1.1368, 'grad_norm': 1.6040806770324707, 'learning_rate': 6.787512379276229e-05, 'epoch': 0.64}
{'loss': 0.8373, 'grad_norm': 1.2157171964645386, 'learning_rate': 6.78242724951932e-05, 'epoch': 0.64}
{'loss': 0.843, 'grad_norm': 0.9203768968582153, 'learning_rate': 6.77734304769489e-05, 'epoch': 0.64}
{'loss': 0.8216, 'grad_norm': 1.1520904302597046, 'learning_rate': 6.772259775269203e-05, 'epoch': 0.64}
{'loss': 0.8768, 'grad_norm': 1.3078263998031616, 'learning_rate': 6.767177433708254e-05, 'epoch': 0.64}
{'loss': 0.9908, 'grad_norm': 1.0806137323379517, 'learning_rate': 6.762096024477758e-05, 'epoch': 0.64}
{'loss': 1.1153, 'grad_norm': 1.4496151208877563, 'learning_rate': 6.757015549043175e-05, 'epoch': 0.64}
{'loss': 0.7248, 'grad_norm': 1.8868106603622437, 'learning_rate': 6.751936008869686e-05, 'epoch': 0.64}
{'loss': 0.9312, 'grad_norm': 1.2028470039367676, 'learning_rate': 6.746857405422207e-05, 'epoch': 0.64}
{'loss': 0.9927, 'grad_norm': 1.095662236213684, 'learning_rate': 6.741779740165384e-05, 'epoch': 0.65}
{'loss': 0.9384, 'grad_norm': 1.0843605995178223, 'learning_rate': 6.73670301456359e-05, 'epoch': 0.65}
{'loss': 1.1119, 'grad_norm': 1.1510673761367798, 'learning_rate': 6.731627230080932e-05, 'epoch': 0.65}
{'loss': 0.8525, 'grad_norm': 1.6055939197540283, 'learning_rate': 6.726552388181233e-05, 'epoch': 0.65}
{'loss': 1.195, 'grad_norm': 1.1764219999313354, 'learning_rate': 6.72147849032806e-05, 'epoch': 0.65}
{'loss': 0.9258, 'grad_norm': 1.382832646369934, 'learning_rate': 6.716405537984703e-05, 'epoch': 0.65}
{'loss': 0.9236, 'grad_norm': 1.33293616771698, 'learning_rate': 6.711333532614168e-05, 'epoch': 0.65}
{'loss': 0.9479, 'grad_norm': 1.3374595642089844, 'learning_rate': 6.706262475679205e-05, 'epoch': 0.65}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0141, 'grad_norm': 1.4526863098144531, 'learning_rate': 6.70119236864228e-05, 'epoch': 0.65}
{'loss': 1.5113, 'grad_norm': 1.5108708143234253, 'learning_rate': 6.696123212965583e-05, 'epoch': 0.65}
{'loss': 0.8446, 'grad_norm': 0.8305061459541321, 'learning_rate': 6.69105501011104e-05, 'epoch': 0.65}
{'loss': 0.7974, 'grad_norm': 1.3466230630874634, 'learning_rate': 6.685987761540294e-05, 'epoch': 0.65}
{'loss': 1.0904, 'grad_norm': 0.9555007815361023, 'learning_rate': 6.680921468714719e-05, 'epoch': 0.65}
{'loss': 0.8347, 'grad_norm': 1.3829197883605957, 'learning_rate': 6.6758561330954e-05, 'epoch': 0.65}
{'loss': 0.9756, 'grad_norm': 1.3499916791915894, 'learning_rate': 6.670791756143162e-05, 'epoch': 0.65}
{'loss': 0.8529, 'grad_norm': 0.9188746213912964, 'learning_rate': 6.665728339318551e-05, 'epoch': 0.65}
{'loss': 0.7369, 'grad_norm': 1.7332247495651245, 'learning_rate': 6.660665884081822e-05, 'epoch': 0.65}
{'loss': 0.6677, 'grad_norm': 1.1932289600372314, 'learning_rate': 6.655604391892972e-05, 'epoch': 0.65}
{'loss': 0.9567, 'grad_norm': 1.3396673202514648, 'learning_rate': 6.650543864211703e-05, 'epoch': 0.65}
{'loss': 0.835, 'grad_norm': 1.2103934288024902, 'learning_rate': 6.64548430249745e-05, 'epoch': 0.65}
{'loss': 1.0399, 'grad_norm': 1.10623037815094, 'learning_rate': 6.64042570820937e-05, 'epoch': 0.65}
{'loss': 1.0796, 'grad_norm': 1.0529307126998901, 'learning_rate': 6.63536808280633e-05, 'epoch': 0.65}
{'loss': 0.8878, 'grad_norm': 1.1887511014938354, 'learning_rate': 6.630311427746932e-05, 'epoch': 0.65}
{'loss': 1.2324, 'grad_norm': 0.9991205930709839, 'learning_rate': 6.625255744489489e-05, 'epoch': 0.65}
{'loss': 0.9583, 'grad_norm': 1.3316084146499634, 'learning_rate': 6.620201034492032e-05, 'epoch': 0.65}
{'loss': 0.8854, 'grad_norm': 1.104432225227356, 'learning_rate': 6.615147299212321e-05, 'epoch': 0.65}
{'loss': 0.7336, 'grad_norm': 1.2240667343139648, 'learning_rate': 6.61009454010782e-05, 'epoch': 0.65}
{'loss': 0.8694, 'grad_norm': 1.0829638242721558, 'learning_rate': 6.605042758635729e-05, 'epoch': 0.65}
{'loss': 1.2315, 'grad_norm': 1.4222490787506104, 'learning_rate': 6.599991956252956e-05, 'epoch': 0.65}
{'loss': 0.9944, 'grad_norm': 1.2505931854248047, 'learning_rate': 6.594942134416122e-05, 'epoch': 0.65}
{'loss': 0.9113, 'grad_norm': 1.108236312866211, 'learning_rate': 6.58989329458158e-05, 'epoch': 0.65}
{'loss': 1.1748, 'grad_norm': 1.094552755355835, 'learning_rate': 6.584845438205383e-05, 'epoch': 0.65}
{'loss': 1.2033, 'grad_norm': 1.1008111238479614, 'learning_rate': 6.579798566743314e-05, 'epoch': 0.65}
{'loss': 0.713, 'grad_norm': 1.0527607202529907, 'learning_rate': 6.574752681650864e-05, 'epoch': 0.65}
{'loss': 0.9156, 'grad_norm': 1.0550217628479004, 'learning_rate': 6.56970778438324e-05, 'epoch': 0.65}
{'loss': 1.489, 'grad_norm': 1.1884026527404785, 'learning_rate': 6.564663876395376e-05, 'epoch': 0.65}
{'loss': 0.7017, 'grad_norm': 1.0767537355422974, 'learning_rate': 6.559620959141897e-05, 'epoch': 0.65}
{'loss': 0.8123, 'grad_norm': 1.3616470098495483, 'learning_rate': 6.554579034077164e-05, 'epoch': 0.65}
{'loss': 0.9121, 'grad_norm': 1.434104084968567, 'learning_rate': 6.549538102655245e-05, 'epoch': 0.65}
{'loss': 0.8628, 'grad_norm': 0.9594712853431702, 'learning_rate': 6.544498166329913e-05, 'epoch': 0.65}
{'loss': 0.9494, 'grad_norm': 1.010328769683838, 'learning_rate': 6.53945922655467e-05, 'epoch': 0.65}
{'loss': 1.0448, 'grad_norm': 1.5781025886535645, 'learning_rate': 6.534421284782715e-05, 'epoch': 0.65}
{'loss': 0.7368, 'grad_norm': 1.0455677509307861, 'learning_rate': 6.52938434246697e-05, 'epoch': 0.65}
{'loss': 0.6559, 'grad_norm': 1.168712854385376, 'learning_rate': 6.524348401060067e-05, 'epoch': 0.65}
{'loss': 1.116, 'grad_norm': 0.9874698519706726, 'learning_rate': 6.51931346201434e-05, 'epoch': 0.65}
{'loss': 0.9255, 'grad_norm': 1.2224900722503662, 'learning_rate': 6.51427952678185e-05, 'epoch': 0.65}
{'loss': 1.0139, 'grad_norm': 0.9991565942764282, 'learning_rate': 6.509246596814351e-05, 'epoch': 0.65}
{'loss': 0.7002, 'grad_norm': 1.1915478706359863, 'learning_rate': 6.50421467356332e-05, 'epoch': 0.65}
{'loss': 0.7941, 'grad_norm': 0.9414689540863037, 'learning_rate': 6.499183758479944e-05, 'epoch': 0.65}
{'loss': 0.7255, 'grad_norm': 1.5728037357330322, 'learning_rate': 6.494153853015106e-05, 'epoch': 0.65}
{'loss': 0.9961, 'grad_norm': 1.648698091506958, 'learning_rate': 6.489124958619412e-05, 'epoch': 0.65}
{'loss': 0.9552, 'grad_norm': 0.991710901260376, 'learning_rate': 6.48409707674317e-05, 'epoch': 0.65}
{'loss': 1.0184, 'grad_norm': 1.083971619606018, 'learning_rate': 6.479070208836394e-05, 'epoch': 0.65}
{'loss': 0.8569, 'grad_norm': 1.0601627826690674, 'learning_rate': 6.474044356348811e-05, 'epoch': 0.65}
{'loss': 1.1689, 'grad_norm': 1.0580265522003174, 'learning_rate': 6.469019520729853e-05, 'epoch': 0.65}
{'loss': 1.283, 'grad_norm': 0.9771411418914795, 'learning_rate': 6.463995703428662e-05, 'epoch': 0.65}
{'loss': 0.912, 'grad_norm': 2.0315403938293457, 'learning_rate': 6.458972905894074e-05, 'epoch': 0.65}
{'loss': 0.9269, 'grad_norm': 1.0506274700164795, 'learning_rate': 6.453951129574644e-05, 'epoch': 0.65}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.718, 'grad_norm': 1.2027947902679443, 'learning_rate': 6.448930375918631e-05, 'epoch': 0.65}
{'loss': 0.759, 'grad_norm': 1.0762919187545776, 'learning_rate': 6.443910646373992e-05, 'epoch': 0.65}
{'loss': 0.9991, 'grad_norm': 1.0655242204666138, 'learning_rate': 6.438891942388393e-05, 'epoch': 0.65}
{'loss': 0.8452, 'grad_norm': 0.9878843426704407, 'learning_rate': 6.43387426540921e-05, 'epoch': 0.65}
{'loss': 1.0787, 'grad_norm': 1.3050605058670044, 'learning_rate': 6.428857616883507e-05, 'epoch': 0.65}
{'loss': 1.0853, 'grad_norm': 1.3622394800186157, 'learning_rate': 6.423841998258069e-05, 'epoch': 0.65}
{'loss': 1.029, 'grad_norm': 1.093679428100586, 'learning_rate': 6.418827410979373e-05, 'epoch': 0.65}
{'loss': 0.9729, 'grad_norm': 1.357998251914978, 'learning_rate': 6.413813856493602e-05, 'epoch': 0.66}
{'loss': 0.9216, 'grad_norm': 1.1718227863311768, 'learning_rate': 6.408801336246645e-05, 'epoch': 0.66}
{'loss': 1.0293, 'grad_norm': 1.3060760498046875, 'learning_rate': 6.403789851684082e-05, 'epoch': 0.66}
{'loss': 0.8081, 'grad_norm': 1.5929127931594849, 'learning_rate': 6.398779404251208e-05, 'epoch': 0.66}
{'loss': 0.9462, 'grad_norm': 1.0256280899047852, 'learning_rate': 6.393769995393003e-05, 'epoch': 0.66}
{'loss': 0.9229, 'grad_norm': 1.2989623546600342, 'learning_rate': 6.388761626554163e-05, 'epoch': 0.66}
{'loss': 1.2888, 'grad_norm': 1.1078156232833862, 'learning_rate': 6.383754299179079e-05, 'epoch': 0.66}
{'loss': 0.9187, 'grad_norm': 1.524654746055603, 'learning_rate': 6.378748014711834e-05, 'epoch': 0.66}
{'loss': 0.8122, 'grad_norm': 1.3545992374420166, 'learning_rate': 6.37374277459622e-05, 'epoch': 0.66}
{'loss': 0.8268, 'grad_norm': 1.2823165655136108, 'learning_rate': 6.368738580275724e-05, 'epoch': 0.66}
{'loss': 0.7078, 'grad_norm': 1.0332366228103638, 'learning_rate': 6.36373543319353e-05, 'epoch': 0.66}
{'loss': 1.1071, 'grad_norm': 1.3860152959823608, 'learning_rate': 6.358733334792526e-05, 'epoch': 0.66}
{'loss': 0.9358, 'grad_norm': 1.071115255355835, 'learning_rate': 6.353732286515286e-05, 'epoch': 0.66}
{'loss': 1.0984, 'grad_norm': 1.4584964513778687, 'learning_rate': 6.348732289804096e-05, 'epoch': 0.66}
{'loss': 1.0387, 'grad_norm': 0.9545044898986816, 'learning_rate': 6.343733346100924e-05, 'epoch': 0.66}
{'loss': 0.9423, 'grad_norm': 1.0916986465454102, 'learning_rate': 6.338735456847442e-05, 'epoch': 0.66}
{'loss': 0.7792, 'grad_norm': 1.119847059249878, 'learning_rate': 6.333738623485025e-05, 'epoch': 0.66}
{'loss': 1.1098, 'grad_norm': 1.1500211954116821, 'learning_rate': 6.328742847454724e-05, 'epoch': 0.66}
{'loss': 1.0419, 'grad_norm': 1.3422536849975586, 'learning_rate': 6.323748130197308e-05, 'epoch': 0.66}
{'loss': 0.9957, 'grad_norm': 0.9888418912887573, 'learning_rate': 6.318754473153221e-05, 'epoch': 0.66}
{'loss': 1.1586, 'grad_norm': 1.5811127424240112, 'learning_rate': 6.313761877762615e-05, 'epoch': 0.66}
{'loss': 0.8396, 'grad_norm': 1.0355417728424072, 'learning_rate': 6.308770345465327e-05, 'epoch': 0.66}
{'loss': 0.6957, 'grad_norm': 0.9157042503356934, 'learning_rate': 6.30377987770089e-05, 'epoch': 0.66}
{'loss': 0.7213, 'grad_norm': 0.9862143397331238, 'learning_rate': 6.298790475908539e-05, 'epoch': 0.66}
{'loss': 1.0366, 'grad_norm': 1.0453969240188599, 'learning_rate': 6.293802141527183e-05, 'epoch': 0.66}
{'loss': 0.9351, 'grad_norm': 1.6926989555358887, 'learning_rate': 6.288814875995437e-05, 'epoch': 0.66}
{'loss': 1.065, 'grad_norm': 1.1120476722717285, 'learning_rate': 6.283828680751612e-05, 'epoch': 0.66}
{'loss': 0.9786, 'grad_norm': 1.1349244117736816, 'learning_rate': 6.278843557233689e-05, 'epoch': 0.66}
{'loss': 0.8613, 'grad_norm': 1.325321078300476, 'learning_rate': 6.273859506879365e-05, 'epoch': 0.66}
{'loss': 0.9965, 'grad_norm': 1.1954929828643799, 'learning_rate': 6.268876531126012e-05, 'epoch': 0.66}
{'loss': 1.0579, 'grad_norm': 0.8868059515953064, 'learning_rate': 6.263894631410692e-05, 'epoch': 0.66}
{'loss': 0.9285, 'grad_norm': 1.0145076513290405, 'learning_rate': 6.258913809170168e-05, 'epoch': 0.66}
{'loss': 1.2206, 'grad_norm': 1.1627904176712036, 'learning_rate': 6.25393406584088e-05, 'epoch': 0.66}
{'loss': 0.8736, 'grad_norm': 1.135421633720398, 'learning_rate': 6.248955402858965e-05, 'epoch': 0.66}
{'loss': 0.7915, 'grad_norm': 1.1956888437271118, 'learning_rate': 6.243977821660248e-05, 'epoch': 0.66}
{'loss': 1.0219, 'grad_norm': 1.0580171346664429, 'learning_rate': 6.239001323680231e-05, 'epoch': 0.66}
{'loss': 0.9535, 'grad_norm': 1.1689106225967407, 'learning_rate': 6.234025910354122e-05, 'epoch': 0.66}
{'loss': 0.9684, 'grad_norm': 1.248520016670227, 'learning_rate': 6.229051583116796e-05, 'epoch': 0.66}
{'loss': 0.9974, 'grad_norm': 1.1412112712860107, 'learning_rate': 6.224078343402833e-05, 'epoch': 0.66}
{'loss': 0.8367, 'grad_norm': 1.2471638917922974, 'learning_rate': 6.21910619264649e-05, 'epoch': 0.66}
{'loss': 1.0172, 'grad_norm': 0.9511216878890991, 'learning_rate': 6.214135132281707e-05, 'epoch': 0.66}
{'loss': 0.9242, 'grad_norm': 1.0306832790374756, 'learning_rate': 6.209165163742119e-05, 'epoch': 0.66}
{'loss': 1.0206, 'grad_norm': 1.0479105710983276, 'learning_rate': 6.204196288461037e-05, 'epoch': 0.66}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9146, 'grad_norm': 1.1797806024551392, 'learning_rate': 6.199228507871462e-05, 'epoch': 0.66}
{'loss': 0.8935, 'grad_norm': 1.118374228477478, 'learning_rate': 6.194261823406083e-05, 'epoch': 0.66}
{'loss': 1.0945, 'grad_norm': 1.2742749452590942, 'learning_rate': 6.18929623649726e-05, 'epoch': 0.66}
{'loss': 1.0797, 'grad_norm': 1.047261357307434, 'learning_rate': 6.18433174857705e-05, 'epoch': 0.66}
{'loss': 0.9604, 'grad_norm': 1.096826195716858, 'learning_rate': 6.179368361077183e-05, 'epoch': 0.66}
{'loss': 1.0442, 'grad_norm': 1.0453157424926758, 'learning_rate': 6.174406075429076e-05, 'epoch': 0.66}
{'loss': 0.9762, 'grad_norm': 1.0998485088348389, 'learning_rate': 6.169444893063836e-05, 'epoch': 0.66}
{'loss': 0.9517, 'grad_norm': 1.0663777589797974, 'learning_rate': 6.164484815412234e-05, 'epoch': 0.66}
{'loss': 1.1446, 'grad_norm': 1.0507171154022217, 'learning_rate': 6.159525843904741e-05, 'epoch': 0.66}
{'loss': 1.0851, 'grad_norm': 1.0956283807754517, 'learning_rate': 6.154567979971493e-05, 'epoch': 0.66}
{'loss': 0.8377, 'grad_norm': 1.2954200506210327, 'learning_rate': 6.149611225042317e-05, 'epoch': 0.66}
{'loss': 1.0498, 'grad_norm': 1.5337598323822021, 'learning_rate': 6.144655580546724e-05, 'epoch': 0.66}
{'loss': 0.9921, 'grad_norm': 1.0982451438903809, 'learning_rate': 6.139701047913885e-05, 'epoch': 0.66}
{'loss': 1.1776, 'grad_norm': 0.9382277131080627, 'learning_rate': 6.134747628572677e-05, 'epoch': 0.66}
{'loss': 1.1312, 'grad_norm': 0.9372498393058777, 'learning_rate': 6.12979532395163e-05, 'epoch': 0.66}
{'loss': 0.8804, 'grad_norm': 1.1037707328796387, 'learning_rate': 6.12484413547897e-05, 'epoch': 0.66}
{'loss': 0.7615, 'grad_norm': 1.4481496810913086, 'learning_rate': 6.119894064582601e-05, 'epoch': 0.66}
{'loss': 0.8989, 'grad_norm': 1.09603750705719, 'learning_rate': 6.114945112690091e-05, 'epoch': 0.66}
{'loss': 1.045, 'grad_norm': 1.2789227962493896, 'learning_rate': 6.1099972812287e-05, 'epoch': 0.66}
{'loss': 0.8858, 'grad_norm': 1.1832237243652344, 'learning_rate': 6.105050571625353e-05, 'epoch': 0.66}
{'loss': 0.7549, 'grad_norm': 1.7437885999679565, 'learning_rate': 6.100104985306665e-05, 'epoch': 0.66}
{'loss': 0.7705, 'grad_norm': 1.1073225736618042, 'learning_rate': 6.095160523698913e-05, 'epoch': 0.66}
{'loss': 1.2285, 'grad_norm': 1.0890276432037354, 'learning_rate': 6.090217188228058e-05, 'epoch': 0.67}
{'loss': 0.9065, 'grad_norm': 1.206442952156067, 'learning_rate': 6.0852749803197354e-05, 'epoch': 0.67}
{'loss': 0.877, 'grad_norm': 1.4895519018173218, 'learning_rate': 6.080333901399251e-05, 'epoch': 0.67}
{'loss': 1.1024, 'grad_norm': 1.3486714363098145, 'learning_rate': 6.075393952891588e-05, 'epoch': 0.67}
{'loss': 1.1715, 'grad_norm': 1.1049227714538574, 'learning_rate': 6.070455136221411e-05, 'epoch': 0.67}
{'loss': 1.1405, 'grad_norm': 0.8917640447616577, 'learning_rate': 6.065517452813042e-05, 'epoch': 0.67}
{'loss': 0.9176, 'grad_norm': 1.6893826723098755, 'learning_rate': 6.0605809040904894e-05, 'epoch': 0.67}
{'loss': 0.9141, 'grad_norm': 1.4943064451217651, 'learning_rate': 6.0556454914774295e-05, 'epoch': 0.67}
{'loss': 0.7482, 'grad_norm': 1.0198941230773926, 'learning_rate': 6.050711216397212e-05, 'epoch': 0.67}
{'loss': 0.9121, 'grad_norm': 1.8193329572677612, 'learning_rate': 6.045778080272859e-05, 'epoch': 0.67}
{'loss': 0.9513, 'grad_norm': 0.9035303592681885, 'learning_rate': 6.040846084527059e-05, 'epoch': 0.67}
{'loss': 1.0615, 'grad_norm': 1.1124423742294312, 'learning_rate': 6.0359152305821766e-05, 'epoch': 0.67}
{'loss': 0.9142, 'grad_norm': 1.4234877824783325, 'learning_rate': 6.030985519860255e-05, 'epoch': 0.67}
{'loss': 0.822, 'grad_norm': 1.3752515316009521, 'learning_rate': 6.026056953782987e-05, 'epoch': 0.67}
{'loss': 0.9762, 'grad_norm': 0.9654955267906189, 'learning_rate': 6.021129533771757e-05, 'epoch': 0.67}
{'loss': 1.0603, 'grad_norm': 1.069439172744751, 'learning_rate': 6.0162032612476004e-05, 'epoch': 0.67}
{'loss': 1.1612, 'grad_norm': 1.2853353023529053, 'learning_rate': 6.011278137631236e-05, 'epoch': 0.67}
{'loss': 0.544, 'grad_norm': 1.0105866193771362, 'learning_rate': 6.006354164343046e-05, 'epoch': 0.67}
{'loss': 0.7098, 'grad_norm': 1.1058928966522217, 'learning_rate': 6.0014313428030774e-05, 'epoch': 0.67}
{'loss': 0.9111, 'grad_norm': 1.085315465927124, 'learning_rate': 5.9965096744310526e-05, 'epoch': 0.67}
{'loss': 0.7338, 'grad_norm': 1.2841377258300781, 'learning_rate': 5.991589160646352e-05, 'epoch': 0.67}
{'loss': 0.8855, 'grad_norm': 1.0975356101989746, 'learning_rate': 5.9866698028680293e-05, 'epoch': 0.67}
{'loss': 0.8836, 'grad_norm': 1.0782873630523682, 'learning_rate': 5.981751602514809e-05, 'epoch': 0.67}
{'loss': 1.3113, 'grad_norm': 1.100578784942627, 'learning_rate': 5.976834561005069e-05, 'epoch': 0.67}
{'loss': 1.0774, 'grad_norm': 1.002498984336853, 'learning_rate': 5.9719186797568674e-05, 'epoch': 0.67}
{'loss': 0.8416, 'grad_norm': 1.2498843669891357, 'learning_rate': 5.967003960187914e-05, 'epoch': 0.67}
{'loss': 0.9652, 'grad_norm': 1.338529348373413, 'learning_rate': 5.962090403715592e-05, 'epoch': 0.67}
{'loss': 0.7198, 'grad_norm': 1.0975507497787476, 'learning_rate': 5.957178011756952e-05, 'epoch': 0.67}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1203, 'grad_norm': 1.3125360012054443, 'learning_rate': 5.9522667857286974e-05, 'epoch': 0.67}
{'loss': 0.7475, 'grad_norm': 1.5228464603424072, 'learning_rate': 5.947356727047208e-05, 'epoch': 0.67}
{'loss': 1.0753, 'grad_norm': 1.5342687368392944, 'learning_rate': 5.942447837128516e-05, 'epoch': 0.67}
{'loss': 0.9108, 'grad_norm': 0.9901469349861145, 'learning_rate': 5.937540117388325e-05, 'epoch': 0.67}
{'loss': 1.2564, 'grad_norm': 0.9964553713798523, 'learning_rate': 5.9326335692419995e-05, 'epoch': 0.67}
{'loss': 1.248, 'grad_norm': 1.0920219421386719, 'learning_rate': 5.927728194104558e-05, 'epoch': 0.67}
{'loss': 1.0231, 'grad_norm': 1.0066248178482056, 'learning_rate': 5.922823993390696e-05, 'epoch': 0.67}
{'loss': 0.9024, 'grad_norm': 1.1249586343765259, 'learning_rate': 5.917920968514752e-05, 'epoch': 0.67}
{'loss': 1.0845, 'grad_norm': 1.1792182922363281, 'learning_rate': 5.913019120890737e-05, 'epoch': 0.67}
{'loss': 0.9371, 'grad_norm': 1.0908452272415161, 'learning_rate': 5.9081184519323275e-05, 'epoch': 0.67}
{'loss': 0.6282, 'grad_norm': 1.0025638341903687, 'learning_rate': 5.9032189630528436e-05, 'epoch': 0.67}
{'loss': 0.8835, 'grad_norm': 1.1620205640792847, 'learning_rate': 5.898320655665279e-05, 'epoch': 0.67}
{'loss': 1.065, 'grad_norm': 1.2065869569778442, 'learning_rate': 5.8934235311822824e-05, 'epoch': 0.67}
{'loss': 0.9729, 'grad_norm': 1.3172829151153564, 'learning_rate': 5.8885275910161576e-05, 'epoch': 0.67}
{'loss': 1.1651, 'grad_norm': 1.0473212003707886, 'learning_rate': 5.883632836578876e-05, 'epoch': 0.67}
{'loss': 0.9318, 'grad_norm': 1.0103522539138794, 'learning_rate': 5.8787392692820533e-05, 'epoch': 0.67}
{'loss': 0.9518, 'grad_norm': 1.08355712890625, 'learning_rate': 5.873846890536976e-05, 'epoch': 0.67}
{'loss': 1.0013, 'grad_norm': 1.4429270029067993, 'learning_rate': 5.868955701754584e-05, 'epoch': 0.67}
{'loss': 0.5118, 'grad_norm': 0.9834809303283691, 'learning_rate': 5.864065704345467e-05, 'epoch': 0.67}
{'loss': 1.201, 'grad_norm': 1.3305487632751465, 'learning_rate': 5.859176899719883e-05, 'epoch': 0.67}
{'loss': 0.9758, 'grad_norm': 1.0016661882400513, 'learning_rate': 5.8542892892877335e-05, 'epoch': 0.67}
{'loss': 0.9586, 'grad_norm': 1.1564432382583618, 'learning_rate': 5.8494028744585826e-05, 'epoch': 0.67}
{'loss': 1.1937, 'grad_norm': 1.073879599571228, 'learning_rate': 5.844517656641655e-05, 'epoch': 0.67}
{'loss': 0.7479, 'grad_norm': 1.3722962141036987, 'learning_rate': 5.8396336372458195e-05, 'epoch': 0.67}
{'loss': 0.8449, 'grad_norm': 1.2562611103057861, 'learning_rate': 5.834750817679606e-05, 'epoch': 0.67}
{'loss': 0.9105, 'grad_norm': 1.2538914680480957, 'learning_rate': 5.829869199351188e-05, 'epoch': 0.67}
{'loss': 1.4497, 'grad_norm': 2.116095542907715, 'learning_rate': 5.8249887836684126e-05, 'epoch': 0.67}
{'loss': 1.146, 'grad_norm': 1.1563620567321777, 'learning_rate': 5.820109572038761e-05, 'epoch': 0.67}
{'loss': 0.7715, 'grad_norm': 1.1244786977767944, 'learning_rate': 5.8152315658693765e-05, 'epoch': 0.67}
{'loss': 1.2171, 'grad_norm': 1.0536184310913086, 'learning_rate': 5.810354766567052e-05, 'epoch': 0.67}
{'loss': 1.1062, 'grad_norm': 1.0687581300735474, 'learning_rate': 5.805479175538229e-05, 'epoch': 0.67}
{'loss': 1.1425, 'grad_norm': 1.2372941970825195, 'learning_rate': 5.8006047941890116e-05, 'epoch': 0.67}
{'loss': 0.8206, 'grad_norm': 1.0756385326385498, 'learning_rate': 5.795731623925146e-05, 'epoch': 0.67}
{'loss': 1.1165, 'grad_norm': 1.1317369937896729, 'learning_rate': 5.790859666152029e-05, 'epoch': 0.67}
{'loss': 0.9804, 'grad_norm': 1.5272825956344604, 'learning_rate': 5.785988922274711e-05, 'epoch': 0.67}
{'loss': 1.3075, 'grad_norm': 0.9380335211753845, 'learning_rate': 5.78111939369789e-05, 'epoch': 0.67}
{'loss': 0.9291, 'grad_norm': 0.934357762336731, 'learning_rate': 5.776251081825912e-05, 'epoch': 0.67}
{'loss': 1.3053, 'grad_norm': 1.2990143299102783, 'learning_rate': 5.771383988062786e-05, 'epoch': 0.68}
{'loss': 1.1189, 'grad_norm': 1.4050819873809814, 'learning_rate': 5.766518113812141e-05, 'epoch': 0.68}
{'loss': 0.8124, 'grad_norm': 1.309569001197815, 'learning_rate': 5.761653460477286e-05, 'epoch': 0.68}
{'loss': 0.9372, 'grad_norm': 1.5069658756256104, 'learning_rate': 5.7567900294611586e-05, 'epoch': 0.68}
{'loss': 1.0596, 'grad_norm': 1.00458824634552, 'learning_rate': 5.751927822166345e-05, 'epoch': 0.68}
{'loss': 1.2171, 'grad_norm': 1.4857465028762817, 'learning_rate': 5.747066839995093e-05, 'epoch': 0.68}
{'loss': 0.9858, 'grad_norm': 1.4616035223007202, 'learning_rate': 5.7422070843492734e-05, 'epoch': 0.68}
{'loss': 1.0523, 'grad_norm': 1.1646887063980103, 'learning_rate': 5.737348556630429e-05, 'epoch': 0.68}
{'loss': 0.9721, 'grad_norm': 1.4306951761245728, 'learning_rate': 5.7324912582397225e-05, 'epoch': 0.68}
{'loss': 0.7592, 'grad_norm': 0.8508483171463013, 'learning_rate': 5.727635190577986e-05, 'epoch': 0.68}
{'loss': 0.8491, 'grad_norm': 1.1611684560775757, 'learning_rate': 5.722780355045682e-05, 'epoch': 0.68}
{'loss': 1.1654, 'grad_norm': 1.168590784072876, 'learning_rate': 5.7179267530429216e-05, 'epoch': 0.68}
{'loss': 1.1642, 'grad_norm': 1.0959453582763672, 'learning_rate': 5.713074385969457e-05, 'epoch': 0.68}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.163, 'grad_norm': 1.2288060188293457, 'learning_rate': 5.708223255224695e-05, 'epoch': 0.68}
{'loss': 0.8079, 'grad_norm': 1.1945840120315552, 'learning_rate': 5.7033733622076744e-05, 'epoch': 0.68}
{'loss': 0.8879, 'grad_norm': 1.0714473724365234, 'learning_rate': 5.698524708317081e-05, 'epoch': 0.68}
{'loss': 0.8586, 'grad_norm': 1.288001537322998, 'learning_rate': 5.693677294951244e-05, 'epoch': 0.68}
{'loss': 1.037, 'grad_norm': 1.1977628469467163, 'learning_rate': 5.6888311235081295e-05, 'epoch': 0.68}
{'loss': 0.9189, 'grad_norm': 1.0356736183166504, 'learning_rate': 5.6839861953853605e-05, 'epoch': 0.68}
{'loss': 0.7729, 'grad_norm': 1.081726312637329, 'learning_rate': 5.679142511980175e-05, 'epoch': 0.68}
{'loss': 1.232, 'grad_norm': 0.9998500943183899, 'learning_rate': 5.674300074689488e-05, 'epoch': 0.68}
{'loss': 1.2139, 'grad_norm': 1.139086127281189, 'learning_rate': 5.6694588849098154e-05, 'epoch': 0.68}
{'loss': 1.0901, 'grad_norm': 1.2782621383666992, 'learning_rate': 5.6646189440373456e-05, 'epoch': 0.68}
{'loss': 0.8743, 'grad_norm': 1.2526289224624634, 'learning_rate': 5.65978025346789e-05, 'epoch': 0.68}
{'loss': 0.9244, 'grad_norm': 1.0617624521255493, 'learning_rate': 5.654942814596902e-05, 'epoch': 0.68}
{'loss': 0.9112, 'grad_norm': 1.383702039718628, 'learning_rate': 5.650106628819485e-05, 'epoch': 0.68}
{'loss': 1.0392, 'grad_norm': 1.3782442808151245, 'learning_rate': 5.645271697530356e-05, 'epoch': 0.68}
{'loss': 1.0791, 'grad_norm': 1.0910776853561401, 'learning_rate': 5.6404380221238985e-05, 'epoch': 0.68}
{'loss': 1.1387, 'grad_norm': 0.8905488848686218, 'learning_rate': 5.6356056039941175e-05, 'epoch': 0.68}
{'loss': 0.839, 'grad_norm': 0.9947760701179504, 'learning_rate': 5.630774444534659e-05, 'epoch': 0.68}
{'loss': 1.1071, 'grad_norm': 1.2862391471862793, 'learning_rate': 5.625944545138805e-05, 'epoch': 0.68}
{'loss': 0.7159, 'grad_norm': 1.2332075834274292, 'learning_rate': 5.621115907199477e-05, 'epoch': 0.68}
{'loss': 0.7246, 'grad_norm': 1.110465168952942, 'learning_rate': 5.616288532109225e-05, 'epoch': 0.68}
{'loss': 0.9058, 'grad_norm': 1.8483108282089233, 'learning_rate': 5.6114624212602504e-05, 'epoch': 0.68}
{'loss': 1.0518, 'grad_norm': 1.2385419607162476, 'learning_rate': 5.606637576044376e-05, 'epoch': 0.68}
{'loss': 1.0115, 'grad_norm': 1.3688703775405884, 'learning_rate': 5.601813997853062e-05, 'epoch': 0.68}
{'loss': 0.6672, 'grad_norm': 0.9650223851203918, 'learning_rate': 5.596991688077409e-05, 'epoch': 0.68}
{'loss': 0.694, 'grad_norm': 1.024898886680603, 'learning_rate': 5.59217064810814e-05, 'epoch': 0.68}
{'loss': 1.1783, 'grad_norm': 1.0934362411499023, 'learning_rate': 5.587350879335634e-05, 'epoch': 0.68}
{'loss': 0.8232, 'grad_norm': 1.0545251369476318, 'learning_rate': 5.582532383149872e-05, 'epoch': 0.68}
{'loss': 1.0608, 'grad_norm': 1.0073251724243164, 'learning_rate': 5.577715160940502e-05, 'epoch': 0.68}
{'loss': 0.9968, 'grad_norm': 1.2734496593475342, 'learning_rate': 5.5728992140967715e-05, 'epoch': 0.68}
{'loss': 1.0288, 'grad_norm': 1.2888494729995728, 'learning_rate': 5.568084544007588e-05, 'epoch': 0.68}
{'loss': 0.8906, 'grad_norm': 1.1192396879196167, 'learning_rate': 5.563271152061476e-05, 'epoch': 0.68}
{'loss': 0.8276, 'grad_norm': 1.1438015699386597, 'learning_rate': 5.5584590396465916e-05, 'epoch': 0.68}
{'loss': 1.1454, 'grad_norm': 1.3532283306121826, 'learning_rate': 5.553648208150728e-05, 'epoch': 0.68}
{'loss': 0.7235, 'grad_norm': 1.068520188331604, 'learning_rate': 5.548838658961302e-05, 'epoch': 0.68}
{'loss': 1.0784, 'grad_norm': 1.1241211891174316, 'learning_rate': 5.5440303934653694e-05, 'epoch': 0.68}
{'loss': 1.1264, 'grad_norm': 1.0249804258346558, 'learning_rate': 5.53922341304961e-05, 'epoch': 0.68}
{'loss': 0.9429, 'grad_norm': 1.1518596410751343, 'learning_rate': 5.534417719100332e-05, 'epoch': 0.68}
{'loss': 1.1723, 'grad_norm': 1.2255138158798218, 'learning_rate': 5.5296133130034713e-05, 'epoch': 0.68}
{'loss': 0.9255, 'grad_norm': 1.83881413936615, 'learning_rate': 5.5248101961446065e-05, 'epoch': 0.68}
{'loss': 1.055, 'grad_norm': 1.1554611921310425, 'learning_rate': 5.520008369908918e-05, 'epoch': 0.68}
{'loss': 0.8745, 'grad_norm': 0.9935183525085449, 'learning_rate': 5.5152078356812456e-05, 'epoch': 0.68}
{'loss': 1.0144, 'grad_norm': 1.155617356300354, 'learning_rate': 5.5104085948460235e-05, 'epoch': 0.68}
{'loss': 1.2317, 'grad_norm': 1.0409740209579468, 'learning_rate': 5.5056106487873426e-05, 'epoch': 0.68}
{'loss': 0.8798, 'grad_norm': 1.0581165552139282, 'learning_rate': 5.500813998888903e-05, 'epoch': 0.68}
{'loss': 1.1403, 'grad_norm': 1.1983140707015991, 'learning_rate': 5.4960186465340316e-05, 'epoch': 0.68}
{'loss': 0.795, 'grad_norm': 1.256491780281067, 'learning_rate': 5.491224593105695e-05, 'epoch': 0.68}
{'loss': 0.8375, 'grad_norm': 1.012791395187378, 'learning_rate': 5.486431839986462e-05, 'epoch': 0.68}
{'loss': 0.7631, 'grad_norm': 0.9828767776489258, 'learning_rate': 5.481640388558551e-05, 'epoch': 0.68}
{'loss': 0.7992, 'grad_norm': 1.1115361452102661, 'learning_rate': 5.476850240203788e-05, 'epoch': 0.68}
{'loss': 1.1779, 'grad_norm': 1.0224181413650513, 'learning_rate': 5.472061396303629e-05, 'epoch': 0.68}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8686, 'grad_norm': 1.2023311853408813, 'learning_rate': 5.467273858239155e-05, 'epoch': 0.68}
{'loss': 1.1917, 'grad_norm': 1.1253350973129272, 'learning_rate': 5.462487627391066e-05, 'epoch': 0.68}
{'loss': 0.9572, 'grad_norm': 1.0455313920974731, 'learning_rate': 5.457702705139689e-05, 'epoch': 0.69}
{'loss': 0.7516, 'grad_norm': 1.427388072013855, 'learning_rate': 5.4529190928649754e-05, 'epoch': 0.69}
{'loss': 1.2309, 'grad_norm': 1.654969334602356, 'learning_rate': 5.448136791946494e-05, 'epoch': 0.69}
{'loss': 1.0067, 'grad_norm': 1.2934582233428955, 'learning_rate': 5.443355803763438e-05, 'epoch': 0.69}
{'loss': 0.6415, 'grad_norm': 1.1803497076034546, 'learning_rate': 5.43857612969462e-05, 'epoch': 0.69}
{'loss': 0.6422, 'grad_norm': 1.303410291671753, 'learning_rate': 5.4337977711184717e-05, 'epoch': 0.69}
{'loss': 0.9384, 'grad_norm': 1.3292545080184937, 'learning_rate': 5.4290207294130615e-05, 'epoch': 0.69}
{'loss': 1.1527, 'grad_norm': 1.2718424797058105, 'learning_rate': 5.424245005956048e-05, 'epoch': 0.69}
{'loss': 1.141, 'grad_norm': 1.2779512405395508, 'learning_rate': 5.419470602124742e-05, 'epoch': 0.69}
{'loss': 0.9227, 'grad_norm': 1.098777174949646, 'learning_rate': 5.414697519296046e-05, 'epoch': 0.69}
{'loss': 1.1934, 'grad_norm': 0.9212691783905029, 'learning_rate': 5.4099257588465015e-05, 'epoch': 0.69}
{'loss': 1.1798, 'grad_norm': 1.5759410858154297, 'learning_rate': 5.405155322152261e-05, 'epoch': 0.69}
{'loss': 0.8112, 'grad_norm': 1.1454708576202393, 'learning_rate': 5.40038621058909e-05, 'epoch': 0.69}
{'loss': 0.8431, 'grad_norm': 1.2163448333740234, 'learning_rate': 5.395618425532389e-05, 'epoch': 0.69}
{'loss': 1.0574, 'grad_norm': 1.0733333826065063, 'learning_rate': 5.390851968357149e-05, 'epoch': 0.69}
{'loss': 1.1606, 'grad_norm': 1.0824000835418701, 'learning_rate': 5.386086840438004e-05, 'epoch': 0.69}
{'loss': 0.6975, 'grad_norm': 0.8711572885513306, 'learning_rate': 5.381323043149191e-05, 'epoch': 0.69}
{'loss': 1.1566, 'grad_norm': 1.2542771100997925, 'learning_rate': 5.376560577864567e-05, 'epoch': 0.69}
{'loss': 1.0178, 'grad_norm': 1.134481430053711, 'learning_rate': 5.371799445957598e-05, 'epoch': 0.69}
{'loss': 0.8399, 'grad_norm': 1.3831777572631836, 'learning_rate': 5.3670396488013854e-05, 'epoch': 0.69}
{'loss': 1.0361, 'grad_norm': 0.9821842312812805, 'learning_rate': 5.362281187768614e-05, 'epoch': 0.69}
{'loss': 1.0246, 'grad_norm': 1.0159571170806885, 'learning_rate': 5.357524064231616e-05, 'epoch': 0.69}
{'loss': 0.8177, 'grad_norm': 1.2933493852615356, 'learning_rate': 5.3527682795623146e-05, 'epoch': 0.69}
{'loss': 1.1079, 'grad_norm': 1.5280076265335083, 'learning_rate': 5.3480138351322596e-05, 'epoch': 0.69}
{'loss': 0.9396, 'grad_norm': 1.206539511680603, 'learning_rate': 5.343260732312606e-05, 'epoch': 0.69}
{'loss': 0.791, 'grad_norm': 1.2568117380142212, 'learning_rate': 5.3385089724741246e-05, 'epoch': 0.69}
{'loss': 0.8535, 'grad_norm': 1.3076164722442627, 'learning_rate': 5.3337585569872106e-05, 'epoch': 0.69}
{'loss': 0.9958, 'grad_norm': 1.117309331893921, 'learning_rate': 5.329009487221845e-05, 'epoch': 0.69}
{'loss': 1.0956, 'grad_norm': 1.1153850555419922, 'learning_rate': 5.32426176454765e-05, 'epoch': 0.69}
{'loss': 1.1577, 'grad_norm': 0.9859808087348938, 'learning_rate': 5.31951539033384e-05, 'epoch': 0.69}
{'loss': 1.1187, 'grad_norm': 0.8767820000648499, 'learning_rate': 5.314770365949249e-05, 'epoch': 0.69}
{'loss': 0.9526, 'grad_norm': 1.121841549873352, 'learning_rate': 5.3100266927623156e-05, 'epoch': 0.69}
{'loss': 0.6681, 'grad_norm': 1.3313125371932983, 'learning_rate': 5.305284372141095e-05, 'epoch': 0.69}
{'loss': 0.8122, 'grad_norm': 0.9753615856170654, 'learning_rate': 5.300543405453244e-05, 'epoch': 0.69}
{'loss': 0.9876, 'grad_norm': 1.0400623083114624, 'learning_rate': 5.295803794066046e-05, 'epoch': 0.69}
{'loss': 0.917, 'grad_norm': 1.037491798400879, 'learning_rate': 5.2910655393463736e-05, 'epoch': 0.69}
{'loss': 0.9031, 'grad_norm': 2.106903314590454, 'learning_rate': 5.286328642660718e-05, 'epoch': 0.69}
{'loss': 0.9666, 'grad_norm': 1.2813438177108765, 'learning_rate': 5.28159310537518e-05, 'epoch': 0.69}
{'loss': 0.9328, 'grad_norm': 1.1753560304641724, 'learning_rate': 5.276858928855458e-05, 'epoch': 0.69}
{'loss': 1.0975, 'grad_norm': 1.08158278465271, 'learning_rate': 5.27212611446688e-05, 'epoch': 0.69}
{'loss': 0.9883, 'grad_norm': 1.0520271062850952, 'learning_rate': 5.267394663574351e-05, 'epoch': 0.69}
{'loss': 1.0416, 'grad_norm': 1.1661559343338013, 'learning_rate': 5.262664577542413e-05, 'epoch': 0.69}
{'loss': 1.0703, 'grad_norm': 1.1005935668945312, 'learning_rate': 5.257935857735186e-05, 'epoch': 0.69}
{'loss': 1.1005, 'grad_norm': 0.9973819255828857, 'learning_rate': 5.2532085055164204e-05, 'epoch': 0.69}
{'loss': 1.0202, 'grad_norm': 1.0934065580368042, 'learning_rate': 5.248482522249458e-05, 'epoch': 0.69}
{'loss': 0.837, 'grad_norm': 1.173911213874817, 'learning_rate': 5.243757909297247e-05, 'epoch': 0.69}
{'loss': 1.0423, 'grad_norm': 1.1590702533721924, 'learning_rate': 5.2390346680223535e-05, 'epoch': 0.69}
{'loss': 0.7374, 'grad_norm': 1.065851092338562, 'learning_rate': 5.234312799786921e-05, 'epoch': 0.69}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.817, 'grad_norm': 1.405023217201233, 'learning_rate': 5.2295923059527285e-05, 'epoch': 0.69}
{'loss': 0.9357, 'grad_norm': 1.310469388961792, 'learning_rate': 5.2248731878811365e-05, 'epoch': 0.69}
{'loss': 1.2701, 'grad_norm': 1.2613264322280884, 'learning_rate': 5.220155446933117e-05, 'epoch': 0.69}
{'loss': 0.8953, 'grad_norm': 1.4910937547683716, 'learning_rate': 5.215439084469245e-05, 'epoch': 0.69}
{'loss': 0.6964, 'grad_norm': 1.0319041013717651, 'learning_rate': 5.210724101849696e-05, 'epoch': 0.69}
{'loss': 1.0054, 'grad_norm': 1.0914984941482544, 'learning_rate': 5.206010500434243e-05, 'epoch': 0.69}
{'loss': 0.8787, 'grad_norm': 1.0170643329620361, 'learning_rate': 5.201298281582277e-05, 'epoch': 0.69}
{'loss': 0.9946, 'grad_norm': 1.212822437286377, 'learning_rate': 5.1965874466527694e-05, 'epoch': 0.69}
{'loss': 1.1375, 'grad_norm': 1.1691466569900513, 'learning_rate': 5.191877997004309e-05, 'epoch': 0.69}
{'loss': 0.9319, 'grad_norm': 1.349336862564087, 'learning_rate': 5.1871699339950755e-05, 'epoch': 0.69}
{'loss': 0.9904, 'grad_norm': 1.025144338607788, 'learning_rate': 5.182463258982846e-05, 'epoch': 0.69}
{'loss': 0.9202, 'grad_norm': 0.936945378780365, 'learning_rate': 5.177757973325017e-05, 'epoch': 0.69}
{'loss': 1.0219, 'grad_norm': 1.3316384553909302, 'learning_rate': 5.1730540783785544e-05, 'epoch': 0.69}
{'loss': 0.7286, 'grad_norm': 1.157609224319458, 'learning_rate': 5.168351575500049e-05, 'epoch': 0.69}
{'loss': 1.0689, 'grad_norm': 1.1131517887115479, 'learning_rate': 5.163650466045677e-05, 'epoch': 0.69}
{'loss': 1.0517, 'grad_norm': 1.3094183206558228, 'learning_rate': 5.1589507513712164e-05, 'epoch': 0.69}
{'loss': 1.0381, 'grad_norm': 1.4970574378967285, 'learning_rate': 5.154252432832043e-05, 'epoch': 0.69}
{'loss': 0.8499, 'grad_norm': 1.2687138319015503, 'learning_rate': 5.149555511783126e-05, 'epoch': 0.7}
{'loss': 1.0784, 'grad_norm': 1.3386379480361938, 'learning_rate': 5.144859989579034e-05, 'epoch': 0.7}
{'loss': 1.0621, 'grad_norm': 1.3015069961547852, 'learning_rate': 5.14016586757394e-05, 'epoch': 0.7}
{'loss': 0.8952, 'grad_norm': 1.158664584159851, 'learning_rate': 5.135473147121601e-05, 'epoch': 0.7}
{'loss': 1.1387, 'grad_norm': 1.4175342321395874, 'learning_rate': 5.130781829575375e-05, 'epoch': 0.7}
{'loss': 0.9893, 'grad_norm': 1.311354637145996, 'learning_rate': 5.126091916288218e-05, 'epoch': 0.7}
{'loss': 1.2944, 'grad_norm': 1.1019518375396729, 'learning_rate': 5.121403408612672e-05, 'epoch': 0.7}
{'loss': 0.7168, 'grad_norm': 1.262393832206726, 'learning_rate': 5.116716307900893e-05, 'epoch': 0.7}
{'loss': 0.6191, 'grad_norm': 1.4737516641616821, 'learning_rate': 5.1120306155046015e-05, 'epoch': 0.7}
{'loss': 0.9289, 'grad_norm': 1.0414198637008667, 'learning_rate': 5.10734633277514e-05, 'epoch': 0.7}
{'loss': 0.8418, 'grad_norm': 1.1030281782150269, 'learning_rate': 5.102663461063433e-05, 'epoch': 0.7}
{'loss': 1.3621, 'grad_norm': 0.9164855480194092, 'learning_rate': 5.097982001719993e-05, 'epoch': 0.7}
{'loss': 1.0088, 'grad_norm': 1.1275115013122559, 'learning_rate': 5.093301956094934e-05, 'epoch': 0.7}
{'loss': 0.871, 'grad_norm': 1.3380411863327026, 'learning_rate': 5.088623325537952e-05, 'epoch': 0.7}
{'loss': 0.7911, 'grad_norm': 1.0729069709777832, 'learning_rate': 5.083946111398356e-05, 'epoch': 0.7}
{'loss': 0.9627, 'grad_norm': 1.2772835493087769, 'learning_rate': 5.0792703150250134e-05, 'epoch': 0.7}
{'loss': 0.9654, 'grad_norm': 1.0289232730865479, 'learning_rate': 5.0745959377664154e-05, 'epoch': 0.7}
{'loss': 0.8604, 'grad_norm': 1.0314584970474243, 'learning_rate': 5.069922980970626e-05, 'epoch': 0.7}
{'loss': 1.0747, 'grad_norm': 1.2195032835006714, 'learning_rate': 5.0652514459853016e-05, 'epoch': 0.7}
{'loss': 0.9007, 'grad_norm': 0.9104990363121033, 'learning_rate': 5.0605813341576924e-05, 'epoch': 0.7}
{'loss': 0.8081, 'grad_norm': 1.0299403667449951, 'learning_rate': 5.055912646834635e-05, 'epoch': 0.7}
{'loss': 1.0596, 'grad_norm': 1.0729994773864746, 'learning_rate': 5.051245385362553e-05, 'epoch': 0.7}
{'loss': 0.7762, 'grad_norm': 1.0304127931594849, 'learning_rate': 5.046579551087469e-05, 'epoch': 0.7}
{'loss': 0.8914, 'grad_norm': 1.1122674942016602, 'learning_rate': 5.041915145354983e-05, 'epoch': 0.7}
{'loss': 0.8981, 'grad_norm': 1.1005936861038208, 'learning_rate': 5.0372521695102894e-05, 'epoch': 0.7}
{'loss': 1.0953, 'grad_norm': 0.9511322975158691, 'learning_rate': 5.032590624898166e-05, 'epoch': 0.7}
{'loss': 0.8927, 'grad_norm': 1.0954387187957764, 'learning_rate': 5.027930512862976e-05, 'epoch': 0.7}
{'loss': 0.8546, 'grad_norm': 1.4906030893325806, 'learning_rate': 5.0232718347486864e-05, 'epoch': 0.7}
{'loss': 1.0321, 'grad_norm': 1.0750658512115479, 'learning_rate': 5.018614591898821e-05, 'epoch': 0.7}
{'loss': 0.7885, 'grad_norm': 1.3370391130447388, 'learning_rate': 5.013958785656516e-05, 'epoch': 0.7}
{'loss': 1.136, 'grad_norm': 1.4872651100158691, 'learning_rate': 5.0093044173644834e-05, 'epoch': 0.7}
{'loss': 0.702, 'grad_norm': 1.2731053829193115, 'learning_rate': 5.0046514883650176e-05, 'epoch': 0.7}
{'loss': 0.907, 'grad_norm': 1.0673059225082397, 'learning_rate': 5.000000000000002e-05, 'epoch': 0.7}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1821, 'grad_norm': 1.6464217901229858, 'learning_rate': 4.9953499536109005e-05, 'epoch': 0.7}
{'loss': 0.779, 'grad_norm': 1.295382022857666, 'learning_rate': 4.99070135053877e-05, 'epoch': 0.7}
{'loss': 0.7967, 'grad_norm': 1.179407000541687, 'learning_rate': 4.986054192124242e-05, 'epoch': 0.7}
{'loss': 1.1189, 'grad_norm': 1.245913028717041, 'learning_rate': 4.9814084797075355e-05, 'epoch': 0.7}
{'loss': 0.9083, 'grad_norm': 0.9963353872299194, 'learning_rate': 4.97676421462845e-05, 'epoch': 0.7}
{'loss': 0.6293, 'grad_norm': 1.3851605653762817, 'learning_rate': 4.972121398226371e-05, 'epoch': 0.7}
{'loss': 0.7569, 'grad_norm': 0.801591694355011, 'learning_rate': 4.967480031840259e-05, 'epoch': 0.7}
{'loss': 1.0575, 'grad_norm': 1.0606088638305664, 'learning_rate': 4.9628401168086746e-05, 'epoch': 0.7}
{'loss': 0.8133, 'grad_norm': 1.3026692867279053, 'learning_rate': 4.958201654469731e-05, 'epoch': 0.7}
{'loss': 1.0421, 'grad_norm': 1.2014319896697998, 'learning_rate': 4.953564646161148e-05, 'epoch': 0.7}
{'loss': 1.0575, 'grad_norm': 1.1467442512512207, 'learning_rate': 4.948929093220216e-05, 'epoch': 0.7}
{'loss': 0.8634, 'grad_norm': 0.903199315071106, 'learning_rate': 4.944294996983805e-05, 'epoch': 0.7}
{'loss': 1.0201, 'grad_norm': 1.5353999137878418, 'learning_rate': 4.939662358788364e-05, 'epoch': 0.7}
{'loss': 0.8515, 'grad_norm': 1.1185181140899658, 'learning_rate': 4.9350311799699226e-05, 'epoch': 0.7}
{'loss': 0.8401, 'grad_norm': 1.2766356468200684, 'learning_rate': 4.9304014618640995e-05, 'epoch': 0.7}
{'loss': 0.846, 'grad_norm': 1.3325393199920654, 'learning_rate': 4.925773205806069e-05, 'epoch': 0.7}
{'loss': 0.9855, 'grad_norm': 1.4919078350067139, 'learning_rate': 4.9211464131306095e-05, 'epoch': 0.7}
{'loss': 0.729, 'grad_norm': 1.4834883213043213, 'learning_rate': 4.916521085172062e-05, 'epoch': 0.7}
{'loss': 0.6888, 'grad_norm': 1.0676523447036743, 'learning_rate': 4.911897223264348e-05, 'epoch': 0.7}
{'loss': 1.0092, 'grad_norm': 1.329041600227356, 'learning_rate': 4.9072748287409677e-05, 'epoch': 0.7}
{'loss': 0.7491, 'grad_norm': 0.9714968204498291, 'learning_rate': 4.902653902934997e-05, 'epoch': 0.7}
{'loss': 0.794, 'grad_norm': 2.9028828144073486, 'learning_rate': 4.898034447179085e-05, 'epoch': 0.7}
{'loss': 1.0585, 'grad_norm': 1.2190309762954712, 'learning_rate': 4.893416462805469e-05, 'epoch': 0.7}
{'loss': 0.7153, 'grad_norm': 1.2918529510498047, 'learning_rate': 4.888799951145948e-05, 'epoch': 0.7}
{'loss': 0.9942, 'grad_norm': 1.0087144374847412, 'learning_rate': 4.884184913531902e-05, 'epoch': 0.7}
{'loss': 1.028, 'grad_norm': 1.684682011604309, 'learning_rate': 4.8795713512942865e-05, 'epoch': 0.7}
{'loss': 0.7501, 'grad_norm': 1.044265866279602, 'learning_rate': 4.874959265763627e-05, 'epoch': 0.7}
{'loss': 1.1327, 'grad_norm': 1.2894530296325684, 'learning_rate': 4.8703486582700365e-05, 'epoch': 0.7}
{'loss': 1.1885, 'grad_norm': 1.0240952968597412, 'learning_rate': 4.8657395301431784e-05, 'epoch': 0.7}
{'loss': 0.9657, 'grad_norm': 1.1501692533493042, 'learning_rate': 4.861131882712314e-05, 'epoch': 0.7}
{'loss': 1.0854, 'grad_norm': 1.6767348051071167, 'learning_rate': 4.856525717306263e-05, 'epoch': 0.7}
{'loss': 0.7953, 'grad_norm': 0.9161284565925598, 'learning_rate': 4.8519210352534207e-05, 'epoch': 0.7}
{'loss': 0.898, 'grad_norm': 1.0803933143615723, 'learning_rate': 4.8473178378817564e-05, 'epoch': 0.71}
{'loss': 0.9777, 'grad_norm': 0.9987163543701172, 'learning_rate': 4.8427161265188034e-05, 'epoch': 0.71}
{'loss': 1.0064, 'grad_norm': 1.135384202003479, 'learning_rate': 4.838115902491685e-05, 'epoch': 0.71}
{'loss': 0.9671, 'grad_norm': 1.2311491966247559, 'learning_rate': 4.833517167127076e-05, 'epoch': 0.71}
{'loss': 0.9109, 'grad_norm': 1.1491377353668213, 'learning_rate': 4.8289199217512324e-05, 'epoch': 0.71}
{'loss': 1.0916, 'grad_norm': 1.1211260557174683, 'learning_rate': 4.824324167689975e-05, 'epoch': 0.71}
{'loss': 1.1627, 'grad_norm': 1.1794432401657104, 'learning_rate': 4.8197299062686995e-05, 'epoch': 0.71}
{'loss': 0.9525, 'grad_norm': 1.176235556602478, 'learning_rate': 4.8151371388123644e-05, 'epoch': 0.71}
{'loss': 1.0245, 'grad_norm': 0.9732958078384399, 'learning_rate': 4.810545866645512e-05, 'epoch': 0.71}
{'loss': 0.8602, 'grad_norm': 1.0733007192611694, 'learning_rate': 4.805956091092227e-05, 'epoch': 0.71}
{'loss': 0.9511, 'grad_norm': 1.1596273183822632, 'learning_rate': 4.801367813476194e-05, 'epoch': 0.71}
{'loss': 1.1862, 'grad_norm': 0.8553217649459839, 'learning_rate': 4.796781035120642e-05, 'epoch': 0.71}
{'loss': 0.7728, 'grad_norm': 1.4209668636322021, 'learning_rate': 4.7921957573483754e-05, 'epoch': 0.71}
{'loss': 1.1506, 'grad_norm': 1.1301053762435913, 'learning_rate': 4.7876119814817756e-05, 'epoch': 0.71}
{'loss': 0.87, 'grad_norm': 0.8982501029968262, 'learning_rate': 4.783029708842766e-05, 'epoch': 0.71}
{'loss': 0.9935, 'grad_norm': 2.876115560531616, 'learning_rate': 4.7784489407528686e-05, 'epoch': 0.71}
{'loss': 1.0119, 'grad_norm': 1.3106390237808228, 'learning_rate': 4.773869678533138e-05, 'epoch': 0.71}
{'loss': 0.8283, 'grad_norm': 0.9929876327514648, 'learning_rate': 4.7692919235042255e-05, 'epoch': 0.71}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8257, 'grad_norm': 1.2917691469192505, 'learning_rate': 4.7647156769863265e-05, 'epoch': 0.71}
{'loss': 0.9724, 'grad_norm': 1.038339376449585, 'learning_rate': 4.7601409402992106e-05, 'epoch': 0.71}
{'loss': 0.7657, 'grad_norm': 1.0340542793273926, 'learning_rate': 4.755567714762209e-05, 'epoch': 0.71}
{'loss': 0.8299, 'grad_norm': 1.0715000629425049, 'learning_rate': 4.7509960016942144e-05, 'epoch': 0.71}
{'loss': 0.6981, 'grad_norm': 1.0747487545013428, 'learning_rate': 4.746425802413694e-05, 'epoch': 0.71}
{'loss': 0.9671, 'grad_norm': 1.4480270147323608, 'learning_rate': 4.7418571182386686e-05, 'epoch': 0.71}
{'loss': 0.7069, 'grad_norm': 1.0427441596984863, 'learning_rate': 4.737289950486723e-05, 'epoch': 0.71}
{'loss': 1.2341, 'grad_norm': 1.3612606525421143, 'learning_rate': 4.732724300475008e-05, 'epoch': 0.71}
{'loss': 1.0447, 'grad_norm': 1.203423261642456, 'learning_rate': 4.7281601695202347e-05, 'epoch': 0.71}
{'loss': 0.871, 'grad_norm': 1.2492923736572266, 'learning_rate': 4.723597558938672e-05, 'epoch': 0.71}
{'loss': 0.8301, 'grad_norm': 1.7202695608139038, 'learning_rate': 4.7190364700461664e-05, 'epoch': 0.71}
{'loss': 1.0065, 'grad_norm': 1.2881633043289185, 'learning_rate': 4.714476904158098e-05, 'epoch': 0.71}
{'loss': 0.9282, 'grad_norm': 1.2431113719940186, 'learning_rate': 4.7099188625894375e-05, 'epoch': 0.71}
{'loss': 0.9692, 'grad_norm': 1.118807315826416, 'learning_rate': 4.7053623466546956e-05, 'epoch': 0.71}
{'loss': 1.0918, 'grad_norm': 1.2279547452926636, 'learning_rate': 4.700807357667952e-05, 'epoch': 0.71}
{'loss': 1.1261, 'grad_norm': 1.1676280498504639, 'learning_rate': 4.6962538969428416e-05, 'epoch': 0.71}
{'loss': 1.1137, 'grad_norm': 1.8331372737884521, 'learning_rate': 4.691701965792559e-05, 'epoch': 0.71}
{'loss': 0.9375, 'grad_norm': 1.4652061462402344, 'learning_rate': 4.687151565529864e-05, 'epoch': 0.71}
{'loss': 0.655, 'grad_norm': 1.538374662399292, 'learning_rate': 4.682602697467067e-05, 'epoch': 0.71}
{'loss': 0.9597, 'grad_norm': 0.9727694988250732, 'learning_rate': 4.678055362916041e-05, 'epoch': 0.71}
{'loss': 0.8748, 'grad_norm': 1.280234932899475, 'learning_rate': 4.673509563188214e-05, 'epoch': 0.71}
{'loss': 0.8463, 'grad_norm': 0.9442874193191528, 'learning_rate': 4.668965299594574e-05, 'epoch': 0.71}
{'loss': 0.7438, 'grad_norm': 1.6390050649642944, 'learning_rate': 4.66442257344566e-05, 'epoch': 0.71}
{'loss': 0.9277, 'grad_norm': 1.1378194093704224, 'learning_rate': 4.6598813860515856e-05, 'epoch': 0.71}
{'loss': 0.915, 'grad_norm': 1.0467617511749268, 'learning_rate': 4.6553417387219886e-05, 'epoch': 0.71}
{'loss': 0.8874, 'grad_norm': 1.165305495262146, 'learning_rate': 4.650803632766096e-05, 'epoch': 0.71}
{'loss': 0.8562, 'grad_norm': 0.987935483455658, 'learning_rate': 4.6462670694926704e-05, 'epoch': 0.71}
{'loss': 1.0288, 'grad_norm': 0.9963720440864563, 'learning_rate': 4.6417320502100316e-05, 'epoch': 0.71}
{'loss': 1.2406, 'grad_norm': 1.3313711881637573, 'learning_rate': 4.6371985762260685e-05, 'epoch': 0.71}
{'loss': 0.6634, 'grad_norm': 1.3238582611083984, 'learning_rate': 4.6326666488481975e-05, 'epoch': 0.71}
{'loss': 0.7553, 'grad_norm': 2.1995363235473633, 'learning_rate': 4.628136269383421e-05, 'epoch': 0.71}
{'loss': 1.0311, 'grad_norm': 1.1879315376281738, 'learning_rate': 4.6236074391382624e-05, 'epoch': 0.71}
{'loss': 0.8804, 'grad_norm': 1.0870496034622192, 'learning_rate': 4.619080159418826e-05, 'epoch': 0.71}
{'loss': 0.9038, 'grad_norm': 1.2888062000274658, 'learning_rate': 4.6145544315307534e-05, 'epoch': 0.71}
{'loss': 1.1185, 'grad_norm': 1.2640413045883179, 'learning_rate': 4.6100302567792444e-05, 'epoch': 0.71}
{'loss': 0.8336, 'grad_norm': 1.0720354318618774, 'learning_rate': 4.6055076364690476e-05, 'epoch': 0.71}
{'loss': 0.8477, 'grad_norm': 1.0486822128295898, 'learning_rate': 4.600986571904461e-05, 'epoch': 0.71}
{'loss': 0.9832, 'grad_norm': 1.1368204355239868, 'learning_rate': 4.596467064389346e-05, 'epoch': 0.71}
{'loss': 0.9861, 'grad_norm': 0.8763224482536316, 'learning_rate': 4.5919491152271033e-05, 'epoch': 0.71}
{'loss': 1.1506, 'grad_norm': 1.0759791135787964, 'learning_rate': 4.587432725720687e-05, 'epoch': 0.71}
{'loss': 0.9476, 'grad_norm': 1.2740225791931152, 'learning_rate': 4.582917897172603e-05, 'epoch': 0.71}
{'loss': 1.0388, 'grad_norm': 1.097101092338562, 'learning_rate': 4.578404630884905e-05, 'epoch': 0.71}
{'loss': 0.8115, 'grad_norm': 1.3921891450881958, 'learning_rate': 4.5738929281591946e-05, 'epoch': 0.71}
{'loss': 0.9255, 'grad_norm': 1.293918251991272, 'learning_rate': 4.5693827902966355e-05, 'epoch': 0.71}
{'loss': 0.9067, 'grad_norm': 1.1014131307601929, 'learning_rate': 4.564874218597915e-05, 'epoch': 0.71}
{'loss': 0.8223, 'grad_norm': 1.2018722295761108, 'learning_rate': 4.5603672143632944e-05, 'epoch': 0.71}
{'loss': 0.7618, 'grad_norm': 0.9743920564651489, 'learning_rate': 4.5558617788925695e-05, 'epoch': 0.71}
{'loss': 0.9777, 'grad_norm': 1.0463346242904663, 'learning_rate': 4.5513579134850816e-05, 'epoch': 0.72}
{'loss': 0.9409, 'grad_norm': 0.9588589668273926, 'learning_rate': 4.546855619439734e-05, 'epoch': 0.72}
{'loss': 1.0009, 'grad_norm': 1.4152661561965942, 'learning_rate': 4.542354898054953e-05, 'epoch': 0.72}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0701, 'grad_norm': 0.9980465173721313, 'learning_rate': 4.5378557506287404e-05, 'epoch': 0.72}
{'loss': 1.082, 'grad_norm': 1.1218231916427612, 'learning_rate': 4.533358178458612e-05, 'epoch': 0.72}
{'loss': 1.0393, 'grad_norm': 0.9750823974609375, 'learning_rate': 4.528862182841659e-05, 'epoch': 0.72}
{'loss': 0.8294, 'grad_norm': 0.9099851250648499, 'learning_rate': 4.524367765074499e-05, 'epoch': 0.72}
{'loss': 0.7953, 'grad_norm': 1.0302633047103882, 'learning_rate': 4.519874926453302e-05, 'epoch': 0.72}
{'loss': 1.0229, 'grad_norm': 0.9729360342025757, 'learning_rate': 4.5153836682737774e-05, 'epoch': 0.72}
{'loss': 1.145, 'grad_norm': 0.8821084499359131, 'learning_rate': 4.5108939918311886e-05, 'epoch': 0.72}
{'loss': 1.0395, 'grad_norm': 1.325304627418518, 'learning_rate': 4.506405898420335e-05, 'epoch': 0.72}
{'loss': 0.8626, 'grad_norm': 1.0459848642349243, 'learning_rate': 4.5019193893355585e-05, 'epoch': 0.72}
{'loss': 0.6205, 'grad_norm': 1.110992670059204, 'learning_rate': 4.497434465870749e-05, 'epoch': 0.72}
{'loss': 1.2148, 'grad_norm': 1.2875503301620483, 'learning_rate': 4.492951129319331e-05, 'epoch': 0.72}
{'loss': 1.0557, 'grad_norm': 1.136763572692871, 'learning_rate': 4.4884693809742906e-05, 'epoch': 0.72}
{'loss': 0.7494, 'grad_norm': 1.1196421384811401, 'learning_rate': 4.483989222128127e-05, 'epoch': 0.72}
{'loss': 1.0437, 'grad_norm': 1.020865797996521, 'learning_rate': 4.479510654072909e-05, 'epoch': 0.72}
{'loss': 0.907, 'grad_norm': 1.1844284534454346, 'learning_rate': 4.4750336781002225e-05, 'epoch': 0.72}
{'loss': 0.9767, 'grad_norm': 1.5577024221420288, 'learning_rate': 4.4705582955012137e-05, 'epoch': 0.72}
{'loss': 0.9316, 'grad_norm': 1.1683598756790161, 'learning_rate': 4.46608450756656e-05, 'epoch': 0.72}
{'loss': 0.9379, 'grad_norm': 1.249416470527649, 'learning_rate': 4.461612315586481e-05, 'epoch': 0.72}
{'loss': 0.8929, 'grad_norm': 1.0655498504638672, 'learning_rate': 4.457141720850733e-05, 'epoch': 0.72}
{'loss': 0.6907, 'grad_norm': 1.1859365701675415, 'learning_rate': 4.452672724648611e-05, 'epoch': 0.72}
{'loss': 0.7781, 'grad_norm': 1.1825429201126099, 'learning_rate': 4.448205328268959e-05, 'epoch': 0.72}
{'loss': 0.8605, 'grad_norm': 1.3320209980010986, 'learning_rate': 4.443739533000151e-05, 'epoch': 0.72}
{'loss': 0.7672, 'grad_norm': 1.3996555805206299, 'learning_rate': 4.439275340130099e-05, 'epoch': 0.72}
{'loss': 0.9358, 'grad_norm': 1.0023777484893799, 'learning_rate': 4.434812750946256e-05, 'epoch': 0.72}
{'loss': 1.3285, 'grad_norm': 1.2335695028305054, 'learning_rate': 4.430351766735609e-05, 'epoch': 0.72}
{'loss': 0.9313, 'grad_norm': 1.0997145175933838, 'learning_rate': 4.425892388784682e-05, 'epoch': 0.72}
{'loss': 1.2924, 'grad_norm': 1.254737138748169, 'learning_rate': 4.42143461837955e-05, 'epoch': 0.72}
{'loss': 0.7375, 'grad_norm': 1.172236442565918, 'learning_rate': 4.416978456805796e-05, 'epoch': 0.72}
{'loss': 1.1839, 'grad_norm': 1.1268166303634644, 'learning_rate': 4.412523905348568e-05, 'epoch': 0.72}
{'loss': 1.1493, 'grad_norm': 1.7350417375564575, 'learning_rate': 4.4080709652925336e-05, 'epoch': 0.72}
{'loss': 0.9339, 'grad_norm': 0.8639190196990967, 'learning_rate': 4.403619637921894e-05, 'epoch': 0.72}
{'loss': 0.811, 'grad_norm': 1.256120204925537, 'learning_rate': 4.399169924520403e-05, 'epoch': 0.72}
{'loss': 1.1093, 'grad_norm': 1.2798322439193726, 'learning_rate': 4.394721826371322e-05, 'epoch': 0.72}
{'loss': 0.9722, 'grad_norm': 1.2138055562973022, 'learning_rate': 4.390275344757475e-05, 'epoch': 0.72}
{'loss': 1.2052, 'grad_norm': 1.1057162284851074, 'learning_rate': 4.385830480961192e-05, 'epoch': 0.72}
{'loss': 0.8519, 'grad_norm': 1.3129128217697144, 'learning_rate': 4.381387236264359e-05, 'epoch': 0.72}
{'loss': 0.7428, 'grad_norm': 0.9854874610900879, 'learning_rate': 4.376945611948385e-05, 'epoch': 0.72}
{'loss': 0.9689, 'grad_norm': 1.032096028327942, 'learning_rate': 4.372505609294214e-05, 'epoch': 0.72}
{'loss': 1.0015, 'grad_norm': 1.3574564456939697, 'learning_rate': 4.368067229582319e-05, 'epoch': 0.72}
{'loss': 0.7537, 'grad_norm': 1.3762390613555908, 'learning_rate': 4.3636304740927046e-05, 'epoch': 0.72}
{'loss': 1.0371, 'grad_norm': 0.9547312259674072, 'learning_rate': 4.359195344104916e-05, 'epoch': 0.72}
{'loss': 0.7781, 'grad_norm': 0.8965569138526917, 'learning_rate': 4.3547618408980215e-05, 'epoch': 0.72}
{'loss': 0.7974, 'grad_norm': 1.0181092023849487, 'learning_rate': 4.350329965750621e-05, 'epoch': 0.72}
{'loss': 0.9834, 'grad_norm': 0.9360120892524719, 'learning_rate': 4.345899719940843e-05, 'epoch': 0.72}
{'loss': 0.9558, 'grad_norm': 1.5752731561660767, 'learning_rate': 4.34147110474636e-05, 'epoch': 0.72}
{'loss': 1.0095, 'grad_norm': 1.3256816864013672, 'learning_rate': 4.3370441214443466e-05, 'epoch': 0.72}
{'loss': 0.848, 'grad_norm': 1.2229722738265991, 'learning_rate': 4.3326187713115415e-05, 'epoch': 0.72}
{'loss': 0.9422, 'grad_norm': 1.029103398323059, 'learning_rate': 4.3281950556241766e-05, 'epoch': 0.72}
{'loss': 1.1969, 'grad_norm': 1.6252260208129883, 'learning_rate': 4.3237729756580434e-05, 'epoch': 0.72}
{'loss': 1.2312, 'grad_norm': 1.0089681148529053, 'learning_rate': 4.3193525326884435e-05, 'epoch': 0.72}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8023, 'grad_norm': 1.1828076839447021, 'learning_rate': 4.314933727990211e-05, 'epoch': 0.72}
{'loss': 0.8034, 'grad_norm': 1.2275272607803345, 'learning_rate': 4.31051656283771e-05, 'epoch': 0.72}
{'loss': 0.8649, 'grad_norm': 1.4102046489715576, 'learning_rate': 4.306101038504824e-05, 'epoch': 0.72}
{'loss': 1.263, 'grad_norm': 1.5694352388381958, 'learning_rate': 4.301687156264977e-05, 'epoch': 0.72}
{'loss': 0.9064, 'grad_norm': 1.6192265748977661, 'learning_rate': 4.2972749173911074e-05, 'epoch': 0.72}
{'loss': 0.8593, 'grad_norm': 1.211133599281311, 'learning_rate': 4.2928643231556844e-05, 'epoch': 0.72}
{'loss': 0.9058, 'grad_norm': 1.2518078088760376, 'learning_rate': 4.288455374830701e-05, 'epoch': 0.72}
{'loss': 1.1678, 'grad_norm': 1.15215265750885, 'learning_rate': 4.2840480736876776e-05, 'epoch': 0.72}
{'loss': 0.9197, 'grad_norm': 1.657052755355835, 'learning_rate': 4.279642420997655e-05, 'epoch': 0.72}
{'loss': 0.9588, 'grad_norm': 1.6083780527114868, 'learning_rate': 4.275238418031209e-05, 'epoch': 0.72}
{'loss': 0.9465, 'grad_norm': 1.273720383644104, 'learning_rate': 4.270836066058429e-05, 'epoch': 0.72}
{'loss': 1.0647, 'grad_norm': 1.2150263786315918, 'learning_rate': 4.266435366348932e-05, 'epoch': 0.72}
{'loss': 0.7341, 'grad_norm': 1.0754804611206055, 'learning_rate': 4.2620363201718594e-05, 'epoch': 0.73}
{'loss': 1.0099, 'grad_norm': 1.0388840436935425, 'learning_rate': 4.257638928795872e-05, 'epoch': 0.73}
{'loss': 1.15, 'grad_norm': 1.1952072381973267, 'learning_rate': 4.253243193489165e-05, 'epoch': 0.73}
{'loss': 0.6889, 'grad_norm': 1.179861068725586, 'learning_rate': 4.2488491155194346e-05, 'epoch': 0.73}
{'loss': 1.4453, 'grad_norm': 1.0862348079681396, 'learning_rate': 4.244456696153924e-05, 'epoch': 0.73}
{'loss': 0.9241, 'grad_norm': 1.3923166990280151, 'learning_rate': 4.240065936659374e-05, 'epoch': 0.73}
{'loss': 0.9944, 'grad_norm': 0.8481255173683167, 'learning_rate': 4.235676838302068e-05, 'epoch': 0.73}
{'loss': 1.0177, 'grad_norm': 1.3706636428833008, 'learning_rate': 4.231289402347798e-05, 'epoch': 0.73}
{'loss': 1.0717, 'grad_norm': 1.0893038511276245, 'learning_rate': 4.226903630061878e-05, 'epoch': 0.73}
{'loss': 0.7536, 'grad_norm': 0.9763220548629761, 'learning_rate': 4.2225195227091454e-05, 'epoch': 0.73}
{'loss': 0.9533, 'grad_norm': 1.4927476644515991, 'learning_rate': 4.2181370815539504e-05, 'epoch': 0.73}
{'loss': 1.1844, 'grad_norm': 1.0254501104354858, 'learning_rate': 4.213756307860175e-05, 'epoch': 0.73}
{'loss': 0.7848, 'grad_norm': 1.0782493352890015, 'learning_rate': 4.209377202891212e-05, 'epoch': 0.73}
{'loss': 0.7573, 'grad_norm': 1.0246901512145996, 'learning_rate': 4.204999767909972e-05, 'epoch': 0.73}
{'loss': 1.1446, 'grad_norm': 0.9816033244132996, 'learning_rate': 4.2006240041788825e-05, 'epoch': 0.73}
{'loss': 0.8911, 'grad_norm': 0.981107234954834, 'learning_rate': 4.1962499129599044e-05, 'epoch': 0.73}
{'loss': 0.9265, 'grad_norm': 1.074997067451477, 'learning_rate': 4.191877495514489e-05, 'epoch': 0.73}
{'loss': 1.0552, 'grad_norm': 1.1110916137695312, 'learning_rate': 4.1875067531036374e-05, 'epoch': 0.73}
{'loss': 0.9412, 'grad_norm': 1.231555461883545, 'learning_rate': 4.1831376869878336e-05, 'epoch': 0.73}
{'loss': 0.8884, 'grad_norm': 1.3770921230316162, 'learning_rate': 4.1787702984271074e-05, 'epoch': 0.73}
{'loss': 0.9375, 'grad_norm': 1.1365392208099365, 'learning_rate': 4.174404588680988e-05, 'epoch': 0.73}
{'loss': 0.932, 'grad_norm': 0.88890141248703, 'learning_rate': 4.170040559008522e-05, 'epoch': 0.73}
{'loss': 0.943, 'grad_norm': 1.4701794385910034, 'learning_rate': 4.165678210668286e-05, 'epoch': 0.73}
{'loss': 0.9223, 'grad_norm': 1.1192431449890137, 'learning_rate': 4.161317544918345e-05, 'epoch': 0.73}
{'loss': 0.841, 'grad_norm': 1.2459683418273926, 'learning_rate': 4.1569585630163044e-05, 'epoch': 0.73}
{'loss': 0.9194, 'grad_norm': 1.2540338039398193, 'learning_rate': 4.1526012662192714e-05, 'epoch': 0.73}
{'loss': 0.817, 'grad_norm': 1.1631975173950195, 'learning_rate': 4.148245655783869e-05, 'epoch': 0.73}
{'loss': 0.9012, 'grad_norm': 1.3099170923233032, 'learning_rate': 4.143891732966233e-05, 'epoch': 0.73}
{'loss': 1.0371, 'grad_norm': 1.1448341608047485, 'learning_rate': 4.139539499022015e-05, 'epoch': 0.73}
{'loss': 0.7783, 'grad_norm': 1.12103271484375, 'learning_rate': 4.135188955206375e-05, 'epoch': 0.73}
{'loss': 0.9551, 'grad_norm': 1.6847119331359863, 'learning_rate': 4.130840102773995e-05, 'epoch': 0.73}
{'loss': 0.7343, 'grad_norm': 1.079216718673706, 'learning_rate': 4.12649294297906e-05, 'epoch': 0.73}
{'loss': 1.1057, 'grad_norm': 1.5127300024032593, 'learning_rate': 4.12214747707527e-05, 'epoch': 0.73}
{'loss': 0.8512, 'grad_norm': 0.9119893312454224, 'learning_rate': 4.117803706315836e-05, 'epoch': 0.73}
{'loss': 0.8293, 'grad_norm': 0.9417560696601868, 'learning_rate': 4.113461631953477e-05, 'epoch': 0.73}
{'loss': 0.7926, 'grad_norm': 1.0500853061676025, 'learning_rate': 4.109121255240438e-05, 'epoch': 0.73}
{'loss': 0.8516, 'grad_norm': 1.1585421562194824, 'learning_rate': 4.104782577428448e-05, 'epoch': 0.73}
{'loss': 1.0526, 'grad_norm': 1.658801794052124, 'learning_rate': 4.100445599768774e-05, 'epoch': 0.73}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.3176, 'grad_norm': 1.2516978979110718, 'learning_rate': 4.096110323512167e-05, 'epoch': 0.73}
{'loss': 1.3539, 'grad_norm': 0.9451846480369568, 'learning_rate': 4.09177674990891e-05, 'epoch': 0.73}
{'loss': 1.3171, 'grad_norm': 1.0629674196243286, 'learning_rate': 4.0874448802087794e-05, 'epoch': 0.73}
{'loss': 0.6786, 'grad_norm': 0.9462953805923462, 'learning_rate': 4.0831147156610684e-05, 'epoch': 0.73}
{'loss': 0.5743, 'grad_norm': 1.0508626699447632, 'learning_rate': 4.078786257514573e-05, 'epoch': 0.73}
{'loss': 0.8839, 'grad_norm': 1.3040192127227783, 'learning_rate': 4.074459507017597e-05, 'epoch': 0.73}
{'loss': 1.0154, 'grad_norm': 1.1495288610458374, 'learning_rate': 4.070134465417963e-05, 'epoch': 0.73}
{'loss': 0.6402, 'grad_norm': 1.0215462446212769, 'learning_rate': 4.065811133962987e-05, 'epoch': 0.73}
{'loss': 1.2241, 'grad_norm': 1.1216522455215454, 'learning_rate': 4.0614895138994965e-05, 'epoch': 0.73}
{'loss': 0.919, 'grad_norm': 1.0076786279678345, 'learning_rate': 4.057169606473827e-05, 'epoch': 0.73}
{'loss': 1.0405, 'grad_norm': 1.4570670127868652, 'learning_rate': 4.0528514129318195e-05, 'epoch': 0.73}
{'loss': 0.9751, 'grad_norm': 1.076264500617981, 'learning_rate': 4.048534934518816e-05, 'epoch': 0.73}
{'loss': 0.8578, 'grad_norm': 1.050437331199646, 'learning_rate': 4.044220172479675e-05, 'epoch': 0.73}
{'loss': 0.7231, 'grad_norm': 1.051305890083313, 'learning_rate': 4.039907128058749e-05, 'epoch': 0.73}
{'loss': 0.801, 'grad_norm': 1.491796851158142, 'learning_rate': 4.0355958024998995e-05, 'epoch': 0.73}
{'loss': 1.0819, 'grad_norm': 1.4145716428756714, 'learning_rate': 4.031286197046493e-05, 'epoch': 0.73}
{'loss': 0.9335, 'grad_norm': 1.1091364622116089, 'learning_rate': 4.0269783129413955e-05, 'epoch': 0.73}
{'loss': 0.7115, 'grad_norm': 0.8981150984764099, 'learning_rate': 4.022672151426989e-05, 'epoch': 0.73}
{'loss': 0.8124, 'grad_norm': 1.243179202079773, 'learning_rate': 4.018367713745137e-05, 'epoch': 0.73}
{'loss': 0.94, 'grad_norm': 0.9866925477981567, 'learning_rate': 4.0140650011372295e-05, 'epoch': 0.73}
{'loss': 0.9024, 'grad_norm': 1.204956293106079, 'learning_rate': 4.009764014844143e-05, 'epoch': 0.73}
{'loss': 0.6715, 'grad_norm': 1.0240507125854492, 'learning_rate': 4.005464756106262e-05, 'epoch': 0.73}
{'loss': 1.1019, 'grad_norm': 1.4160765409469604, 'learning_rate': 4.001167226163471e-05, 'epoch': 0.73}
{'loss': 0.8978, 'grad_norm': 1.2318663597106934, 'learning_rate': 3.99687142625516e-05, 'epoch': 0.73}
{'loss': 0.7733, 'grad_norm': 1.0569746494293213, 'learning_rate': 3.99257735762021e-05, 'epoch': 0.73}
{'loss': 1.076, 'grad_norm': 1.3571739196777344, 'learning_rate': 3.988285021497019e-05, 'epoch': 0.73}
{'loss': 1.2075, 'grad_norm': 3.698432683944702, 'learning_rate': 3.9839944191234715e-05, 'epoch': 0.73}
{'loss': 1.1126, 'grad_norm': 1.383305311203003, 'learning_rate': 3.9797055517369577e-05, 'epoch': 0.74}
{'loss': 1.2869, 'grad_norm': 0.8558135032653809, 'learning_rate': 3.975418420574364e-05, 'epoch': 0.74}
{'loss': 0.8974, 'grad_norm': 1.0347813367843628, 'learning_rate': 3.971133026872077e-05, 'epoch': 0.74}
{'loss': 0.8075, 'grad_norm': 1.159048318862915, 'learning_rate': 3.966849371865993e-05, 'epoch': 0.74}
{'loss': 1.0071, 'grad_norm': 1.2212504148483276, 'learning_rate': 3.962567456791484e-05, 'epoch': 0.74}
{'loss': 1.0111, 'grad_norm': 5.416647911071777, 'learning_rate': 3.9582872828834485e-05, 'epoch': 0.74}
{'loss': 0.778, 'grad_norm': 0.9474968314170837, 'learning_rate': 3.954008851376252e-05, 'epoch': 0.74}
{'loss': 0.9508, 'grad_norm': 1.0912102460861206, 'learning_rate': 3.9497321635037856e-05, 'epoch': 0.74}
{'loss': 1.1636, 'grad_norm': 1.183664321899414, 'learning_rate': 3.9454572204994215e-05, 'epoch': 0.74}
{'loss': 0.8857, 'grad_norm': 1.0266395807266235, 'learning_rate': 3.9411840235960296e-05, 'epoch': 0.74}
{'loss': 0.6608, 'grad_norm': 1.3554898500442505, 'learning_rate': 3.936912574025988e-05, 'epoch': 0.74}
{'loss': 1.0666, 'grad_norm': 1.2379509210586548, 'learning_rate': 3.9326428730211505e-05, 'epoch': 0.74}
{'loss': 0.8975, 'grad_norm': 1.515303134918213, 'learning_rate': 3.9283749218128885e-05, 'epoch': 0.74}
{'loss': 0.9589, 'grad_norm': 1.1874806880950928, 'learning_rate': 3.9241087216320536e-05, 'epoch': 0.74}
{'loss': 0.8742, 'grad_norm': 1.3677318096160889, 'learning_rate': 3.919844273708999e-05, 'epoch': 0.74}
{'loss': 1.1833, 'grad_norm': 1.6919422149658203, 'learning_rate': 3.915581579273569e-05, 'epoch': 0.74}
{'loss': 1.0544, 'grad_norm': 1.1105360984802246, 'learning_rate': 3.9113206395551064e-05, 'epoch': 0.74}
{'loss': 0.7062, 'grad_norm': 0.9869267344474792, 'learning_rate': 3.9070614557824406e-05, 'epoch': 0.74}
{'loss': 1.2415, 'grad_norm': 1.1968637704849243, 'learning_rate': 3.902804029183907e-05, 'epoch': 0.74}
{'loss': 0.8704, 'grad_norm': 0.9928430318832397, 'learning_rate': 3.8985483609873244e-05, 'epoch': 0.74}
{'loss': 0.607, 'grad_norm': 1.2865662574768066, 'learning_rate': 3.894294452420005e-05, 'epoch': 0.74}
{'loss': 0.8821, 'grad_norm': 1.2407654523849487, 'learning_rate': 3.8900423047087585e-05, 'epoch': 0.74}
{'loss': 0.9819, 'grad_norm': 1.0767583847045898, 'learning_rate': 3.885791919079878e-05, 'epoch': 0.74}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7902, 'grad_norm': 1.1402385234832764, 'learning_rate': 3.881543296759165e-05, 'epoch': 0.74}
{'loss': 1.1128, 'grad_norm': 1.1964257955551147, 'learning_rate': 3.877296438971888e-05, 'epoch': 0.74}
{'loss': 1.0148, 'grad_norm': 1.0896409749984741, 'learning_rate': 3.873051346942831e-05, 'epoch': 0.74}
{'loss': 1.1237, 'grad_norm': 1.2862662076950073, 'learning_rate': 3.868808021896254e-05, 'epoch': 0.74}
{'loss': 0.9976, 'grad_norm': 1.0447919368743896, 'learning_rate': 3.864566465055912e-05, 'epoch': 0.74}
{'loss': 1.1152, 'grad_norm': 1.3700039386749268, 'learning_rate': 3.860326677645051e-05, 'epoch': 0.74}
{'loss': 0.8651, 'grad_norm': 1.2915071249008179, 'learning_rate': 3.856088660886402e-05, 'epoch': 0.74}
{'loss': 1.1312, 'grad_norm': 1.1962522268295288, 'learning_rate': 3.851852416002187e-05, 'epoch': 0.74}
{'loss': 0.8946, 'grad_norm': 1.3885129690170288, 'learning_rate': 3.847617944214127e-05, 'epoch': 0.74}
{'loss': 1.1156, 'grad_norm': 1.6653597354888916, 'learning_rate': 3.843385246743417e-05, 'epoch': 0.74}
{'loss': 1.4374, 'grad_norm': 1.2386428117752075, 'learning_rate': 3.839154324810749e-05, 'epoch': 0.74}
{'loss': 0.9351, 'grad_norm': 1.0762337446212769, 'learning_rate': 3.8349251796363e-05, 'epoch': 0.74}
{'loss': 0.9929, 'grad_norm': 1.2106109857559204, 'learning_rate': 3.830697812439729e-05, 'epoch': 0.74}
{'loss': 1.1531, 'grad_norm': 1.5480186939239502, 'learning_rate': 3.826472224440202e-05, 'epoch': 0.74}
{'loss': 0.9906, 'grad_norm': 0.9213833808898926, 'learning_rate': 3.8222484168563424e-05, 'epoch': 0.74}
{'loss': 1.0495, 'grad_norm': 1.7276030778884888, 'learning_rate': 3.818026390906291e-05, 'epoch': 0.74}
{'loss': 1.0224, 'grad_norm': 0.8982635736465454, 'learning_rate': 3.813806147807645e-05, 'epoch': 0.74}
{'loss': 0.5919, 'grad_norm': 1.4547221660614014, 'learning_rate': 3.809587688777514e-05, 'epoch': 0.74}
{'loss': 1.0806, 'grad_norm': 1.108374834060669, 'learning_rate': 3.805371015032476e-05, 'epoch': 0.74}
{'loss': 0.9424, 'grad_norm': 1.1212595701217651, 'learning_rate': 3.8011561277885964e-05, 'epoch': 0.74}
{'loss': 1.1272, 'grad_norm': 1.0845180749893188, 'learning_rate': 3.7969430282614404e-05, 'epoch': 0.74}
{'loss': 0.8966, 'grad_norm': 1.0312861204147339, 'learning_rate': 3.792731717666029e-05, 'epoch': 0.74}
{'loss': 0.8428, 'grad_norm': 1.8716717958450317, 'learning_rate': 3.788522197216897e-05, 'epoch': 0.74}
{'loss': 0.8396, 'grad_norm': 1.3292804956436157, 'learning_rate': 3.784314468128044e-05, 'epoch': 0.74}
{'loss': 1.0444, 'grad_norm': 1.0611495971679688, 'learning_rate': 3.7801085316129615e-05, 'epoch': 0.74}
{'loss': 1.0061, 'grad_norm': 1.1141554117202759, 'learning_rate': 3.775904388884618e-05, 'epoch': 0.74}
{'loss': 0.7183, 'grad_norm': 0.9859512448310852, 'learning_rate': 3.77170204115547e-05, 'epoch': 0.74}
{'loss': 1.098, 'grad_norm': 1.1059767007827759, 'learning_rate': 3.767501489637452e-05, 'epoch': 0.74}
{'loss': 0.7715, 'grad_norm': 1.0790181159973145, 'learning_rate': 3.763302735541987e-05, 'epoch': 0.74}
{'loss': 0.9302, 'grad_norm': 1.1749507188796997, 'learning_rate': 3.759105780079974e-05, 'epoch': 0.74}
{'loss': 0.9322, 'grad_norm': 1.1070497035980225, 'learning_rate': 3.754910624461795e-05, 'epoch': 0.74}
{'loss': 0.571, 'grad_norm': 1.4658286571502686, 'learning_rate': 3.75071726989731e-05, 'epoch': 0.74}
{'loss': 0.7907, 'grad_norm': 1.0211032629013062, 'learning_rate': 3.7465257175958624e-05, 'epoch': 0.74}
{'loss': 0.9522, 'grad_norm': 1.0509827136993408, 'learning_rate': 3.742335968766283e-05, 'epoch': 0.74}
{'loss': 0.7411, 'grad_norm': 1.1228444576263428, 'learning_rate': 3.738148024616863e-05, 'epoch': 0.74}
{'loss': 0.6153, 'grad_norm': 1.249543309211731, 'learning_rate': 3.733961886355398e-05, 'epoch': 0.74}
{'loss': 0.8275, 'grad_norm': 0.9792025685310364, 'learning_rate': 3.729777555189139e-05, 'epoch': 0.74}
{'loss': 1.104, 'grad_norm': 1.0720062255859375, 'learning_rate': 3.725595032324833e-05, 'epoch': 0.74}
{'loss': 0.851, 'grad_norm': 0.9955185651779175, 'learning_rate': 3.7214143189687e-05, 'epoch': 0.74}
{'loss': 0.7787, 'grad_norm': 1.0804615020751953, 'learning_rate': 3.7172354163264324e-05, 'epoch': 0.74}
{'loss': 0.9121, 'grad_norm': 1.171859860420227, 'learning_rate': 3.713058325603213e-05, 'epoch': 0.74}
{'loss': 0.859, 'grad_norm': 1.102638602256775, 'learning_rate': 3.7088830480036894e-05, 'epoch': 0.74}
{'loss': 0.8942, 'grad_norm': 1.0372463464736938, 'learning_rate': 3.7047095847319935e-05, 'epoch': 0.75}
{'loss': 1.0531, 'grad_norm': 1.425702452659607, 'learning_rate': 3.7005379369917325e-05, 'epoch': 0.75}
{'loss': 0.9536, 'grad_norm': 1.1200320720672607, 'learning_rate': 3.696368105985988e-05, 'epoch': 0.75}
{'loss': 1.0042, 'grad_norm': 1.0034412145614624, 'learning_rate': 3.692200092917315e-05, 'epoch': 0.75}
{'loss': 1.1061, 'grad_norm': 1.224319338798523, 'learning_rate': 3.6880338989877604e-05, 'epoch': 0.75}
{'loss': 0.9807, 'grad_norm': 1.2645175457000732, 'learning_rate': 3.6838695253988206e-05, 'epoch': 0.75}
{'loss': 0.7793, 'grad_norm': 1.1245813369750977, 'learning_rate': 3.679706973351491e-05, 'epoch': 0.75}
{'loss': 0.8711, 'grad_norm': 1.214324951171875, 'learning_rate': 3.675546244046228e-05, 'epoch': 0.75}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0359, 'grad_norm': 1.065134048461914, 'learning_rate': 3.6713873386829654e-05, 'epoch': 0.75}
{'loss': 1.2323, 'grad_norm': 1.2419548034667969, 'learning_rate': 3.667230258461113e-05, 'epoch': 0.75}
{'loss': 0.776, 'grad_norm': 1.1110713481903076, 'learning_rate': 3.663075004579547e-05, 'epoch': 0.75}
{'loss': 0.6817, 'grad_norm': 1.1060919761657715, 'learning_rate': 3.6589215782366345e-05, 'epoch': 0.75}
{'loss': 0.9362, 'grad_norm': 1.6173906326293945, 'learning_rate': 3.65476998063019e-05, 'epoch': 0.75}
{'loss': 0.9914, 'grad_norm': 1.2077966928482056, 'learning_rate': 3.650620212957524e-05, 'epoch': 0.75}
{'loss': 0.9768, 'grad_norm': 1.208318829536438, 'learning_rate': 3.646472276415406e-05, 'epoch': 0.75}
{'loss': 0.8357, 'grad_norm': 1.4025723934173584, 'learning_rate': 3.64232617220008e-05, 'epoch': 0.75}
{'loss': 0.9405, 'grad_norm': 1.306542992591858, 'learning_rate': 3.638181901507265e-05, 'epoch': 0.75}
{'loss': 1.2173, 'grad_norm': 0.9524090886116028, 'learning_rate': 3.6340394655321465e-05, 'epoch': 0.75}
{'loss': 1.1205, 'grad_norm': 1.1343291997909546, 'learning_rate': 3.629898865469381e-05, 'epoch': 0.75}
{'loss': 0.9739, 'grad_norm': 1.4912091493606567, 'learning_rate': 3.6257601025131026e-05, 'epoch': 0.75}
{'loss': 0.9363, 'grad_norm': 1.0458834171295166, 'learning_rate': 3.6216231778569096e-05, 'epoch': 0.75}
{'loss': 0.8472, 'grad_norm': 1.0935087203979492, 'learning_rate': 3.6174880926938705e-05, 'epoch': 0.75}
{'loss': 0.7656, 'grad_norm': 1.2584247589111328, 'learning_rate': 3.6133548482165225e-05, 'epoch': 0.75}
{'loss': 1.0013, 'grad_norm': 1.4937689304351807, 'learning_rate': 3.6092234456168706e-05, 'epoch': 0.75}
{'loss': 1.1068, 'grad_norm': 1.175337314605713, 'learning_rate': 3.605093886086403e-05, 'epoch': 0.75}
{'loss': 1.0423, 'grad_norm': 1.119889497756958, 'learning_rate': 3.60096617081605e-05, 'epoch': 0.75}
{'loss': 0.9596, 'grad_norm': 1.180469036102295, 'learning_rate': 3.596840300996238e-05, 'epoch': 0.75}
{'loss': 0.9691, 'grad_norm': 1.2198952436447144, 'learning_rate': 3.5927162778168355e-05, 'epoch': 0.75}
{'loss': 1.094, 'grad_norm': 1.5274591445922852, 'learning_rate': 3.5885941024671996e-05, 'epoch': 0.75}
{'loss': 0.8278, 'grad_norm': 0.9740437865257263, 'learning_rate': 3.584473776136145e-05, 'epoch': 0.75}
{'loss': 0.6739, 'grad_norm': 1.0655587911605835, 'learning_rate': 3.5803553000119474e-05, 'epoch': 0.75}
{'loss': 0.8918, 'grad_norm': 1.3529281616210938, 'learning_rate': 3.576238675282364e-05, 'epoch': 0.75}
{'loss': 1.0814, 'grad_norm': 1.3531306982040405, 'learning_rate': 3.5721239031346066e-05, 'epoch': 0.75}
{'loss': 0.7309, 'grad_norm': 1.078713297843933, 'learning_rate': 3.568010984755354e-05, 'epoch': 0.75}
{'loss': 0.881, 'grad_norm': 1.0325441360473633, 'learning_rate': 3.563899921330755e-05, 'epoch': 0.75}
{'loss': 0.999, 'grad_norm': 1.089433193206787, 'learning_rate': 3.559790714046417e-05, 'epoch': 0.75}
{'loss': 0.7372, 'grad_norm': 1.2314972877502441, 'learning_rate': 3.555683364087414e-05, 'epoch': 0.75}
{'loss': 0.796, 'grad_norm': 1.0641634464263916, 'learning_rate': 3.5515778726382966e-05, 'epoch': 0.75}
{'loss': 1.5321, 'grad_norm': 2.0507395267486572, 'learning_rate': 3.5474742408830544e-05, 'epoch': 0.75}
{'loss': 0.8507, 'grad_norm': 1.0809941291809082, 'learning_rate': 3.543372470005164e-05, 'epoch': 0.75}
{'loss': 0.912, 'grad_norm': 1.065650463104248, 'learning_rate': 3.539272561187555e-05, 'epoch': 0.75}
{'loss': 0.9942, 'grad_norm': 1.2934956550598145, 'learning_rate': 3.5351745156126216e-05, 'epoch': 0.75}
{'loss': 1.0188, 'grad_norm': 1.1252639293670654, 'learning_rate': 3.531078334462218e-05, 'epoch': 0.75}
{'loss': 0.8894, 'grad_norm': 1.1766163110733032, 'learning_rate': 3.526984018917662e-05, 'epoch': 0.75}
{'loss': 0.9037, 'grad_norm': 0.9161454439163208, 'learning_rate': 3.522891570159743e-05, 'epoch': 0.75}
{'loss': 0.8785, 'grad_norm': 1.258979320526123, 'learning_rate': 3.518800989368691e-05, 'epoch': 0.75}
{'loss': 0.7305, 'grad_norm': 1.0704213380813599, 'learning_rate': 3.5147122777242204e-05, 'epoch': 0.75}
{'loss': 1.0931, 'grad_norm': 1.6005953550338745, 'learning_rate': 3.510625436405491e-05, 'epoch': 0.75}
{'loss': 0.8681, 'grad_norm': 1.0076541900634766, 'learning_rate': 3.506540466591129e-05, 'epoch': 0.75}
{'loss': 1.3067, 'grad_norm': 1.194517731666565, 'learning_rate': 3.5024573694592214e-05, 'epoch': 0.75}
{'loss': 1.0201, 'grad_norm': 0.9815396666526794, 'learning_rate': 3.4983761461873075e-05, 'epoch': 0.75}
{'loss': 0.9965, 'grad_norm': 1.1332532167434692, 'learning_rate': 3.494296797952401e-05, 'epoch': 0.75}
{'loss': 0.8864, 'grad_norm': 1.015120029449463, 'learning_rate': 3.490219325930962e-05, 'epoch': 0.75}
{'loss': 1.1769, 'grad_norm': 2.180011510848999, 'learning_rate': 3.486143731298916e-05, 'epoch': 0.75}
{'loss': 0.9378, 'grad_norm': 1.1388959884643555, 'learning_rate': 3.482070015231642e-05, 'epoch': 0.75}
{'loss': 1.0623, 'grad_norm': 1.0481514930725098, 'learning_rate': 3.477998178903982e-05, 'epoch': 0.75}
{'loss': 0.8984, 'grad_norm': 1.1399999856948853, 'learning_rate': 3.4739282234902306e-05, 'epoch': 0.75}
{'loss': 0.7514, 'grad_norm': 0.9971994757652283, 'learning_rate': 3.469860150164152e-05, 'epoch': 0.75}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.3116, 'grad_norm': 1.1594946384429932, 'learning_rate': 3.465793960098945e-05, 'epoch': 0.75}
{'loss': 0.9564, 'grad_norm': 1.1476894617080688, 'learning_rate': 3.461729654467293e-05, 'epoch': 0.75}
{'loss': 0.9431, 'grad_norm': 1.2605997323989868, 'learning_rate': 3.4576672344413145e-05, 'epoch': 0.75}
{'loss': 1.3276, 'grad_norm': 1.1849077939987183, 'learning_rate': 3.4536067011925945e-05, 'epoch': 0.75}
{'loss': 0.7861, 'grad_norm': 1.3651517629623413, 'learning_rate': 3.449548055892171e-05, 'epoch': 0.75}
{'loss': 0.7759, 'grad_norm': 1.186718225479126, 'learning_rate': 3.445491299710534e-05, 'epoch': 0.75}
{'loss': 0.8846, 'grad_norm': 1.3330416679382324, 'learning_rate': 3.441436433817641e-05, 'epoch': 0.75}
{'loss': 0.7658, 'grad_norm': 1.2746018171310425, 'learning_rate': 3.4373834593828845e-05, 'epoch': 0.76}
{'loss': 1.001, 'grad_norm': 1.2214144468307495, 'learning_rate': 3.4333323775751325e-05, 'epoch': 0.76}
{'loss': 1.006, 'grad_norm': 1.1154794692993164, 'learning_rate': 3.429283189562694e-05, 'epoch': 0.76}
{'loss': 0.6955, 'grad_norm': 1.329238772392273, 'learning_rate': 3.4252358965133344e-05, 'epoch': 0.76}
{'loss': 0.9914, 'grad_norm': 1.3025784492492676, 'learning_rate': 3.421190499594271e-05, 'epoch': 0.76}
{'loss': 0.9836, 'grad_norm': 1.1709538698196411, 'learning_rate': 3.417146999972187e-05, 'epoch': 0.76}
{'loss': 0.8222, 'grad_norm': 0.979669988155365, 'learning_rate': 3.413105398813195e-05, 'epoch': 0.76}
{'loss': 0.7778, 'grad_norm': 1.1714272499084473, 'learning_rate': 3.409065697282883e-05, 'epoch': 0.76}
{'loss': 0.82, 'grad_norm': 1.4254024028778076, 'learning_rate': 3.4050278965462764e-05, 'epoch': 0.76}
{'loss': 0.9912, 'grad_norm': 1.0818631649017334, 'learning_rate': 3.400991997767857e-05, 'epoch': 0.76}
{'loss': 0.9138, 'grad_norm': 1.0419094562530518, 'learning_rate': 3.396958002111568e-05, 'epoch': 0.76}
{'loss': 0.7479, 'grad_norm': 1.1688002347946167, 'learning_rate': 3.3929259107407784e-05, 'epoch': 0.76}
{'loss': 0.8396, 'grad_norm': 1.0718775987625122, 'learning_rate': 3.388895724818341e-05, 'epoch': 0.76}
{'loss': 1.13, 'grad_norm': 1.3763338327407837, 'learning_rate': 3.3848674455065255e-05, 'epoch': 0.76}
{'loss': 0.991, 'grad_norm': 1.1415923833847046, 'learning_rate': 3.38084107396708e-05, 'epoch': 0.76}
{'loss': 1.3692, 'grad_norm': 1.0866166353225708, 'learning_rate': 3.3768166113611865e-05, 'epoch': 0.76}
{'loss': 0.7609, 'grad_norm': 1.1022602319717407, 'learning_rate': 3.3727940588494824e-05, 'epoch': 0.76}
{'loss': 1.0589, 'grad_norm': 1.2216277122497559, 'learning_rate': 3.36877341759205e-05, 'epoch': 0.76}
{'loss': 1.2623, 'grad_norm': 1.0188299417495728, 'learning_rate': 3.364754688748422e-05, 'epoch': 0.76}
{'loss': 1.4069, 'grad_norm': 1.0715123414993286, 'learning_rate': 3.360737873477584e-05, 'epoch': 0.76}
{'loss': 1.2623, 'grad_norm': 0.9992324113845825, 'learning_rate': 3.3567229729379656e-05, 'epoch': 0.76}
{'loss': 1.2591, 'grad_norm': 1.0503419637680054, 'learning_rate': 3.352709988287444e-05, 'epoch': 0.76}
{'loss': 0.8258, 'grad_norm': 0.9962096810340881, 'learning_rate': 3.348698920683343e-05, 'epoch': 0.76}
{'loss': 1.0593, 'grad_norm': 0.9720331430435181, 'learning_rate': 3.3446897712824377e-05, 'epoch': 0.76}
{'loss': 1.1248, 'grad_norm': 1.353235125541687, 'learning_rate': 3.340682541240943e-05, 'epoch': 0.76}
{'loss': 0.9364, 'grad_norm': 1.3214964866638184, 'learning_rate': 3.336677231714535e-05, 'epoch': 0.76}
{'loss': 0.7771, 'grad_norm': 1.3270167112350464, 'learning_rate': 3.3326738438583114e-05, 'epoch': 0.76}
{'loss': 0.8538, 'grad_norm': 1.2219432592391968, 'learning_rate': 3.3286723788268414e-05, 'epoch': 0.76}
{'loss': 1.1406, 'grad_norm': 1.0841468572616577, 'learning_rate': 3.3246728377741246e-05, 'epoch': 0.76}
{'loss': 0.7123, 'grad_norm': 1.0780186653137207, 'learning_rate': 3.320675221853608e-05, 'epoch': 0.76}
{'loss': 1.1382, 'grad_norm': 1.1749234199523926, 'learning_rate': 3.3166795322181865e-05, 'epoch': 0.76}
{'loss': 0.8373, 'grad_norm': 0.9488069415092468, 'learning_rate': 3.312685770020193e-05, 'epoch': 0.76}
{'loss': 1.1646, 'grad_norm': 1.2365005016326904, 'learning_rate': 3.308693936411421e-05, 'epoch': 0.76}
{'loss': 0.8991, 'grad_norm': 0.9832807183265686, 'learning_rate': 3.304704032543081e-05, 'epoch': 0.76}
{'loss': 0.7206, 'grad_norm': 1.066359043121338, 'learning_rate': 3.300716059565853e-05, 'epoch': 0.76}
{'loss': 0.9069, 'grad_norm': 2.2410407066345215, 'learning_rate': 3.296730018629846e-05, 'epoch': 0.76}
{'loss': 0.7702, 'grad_norm': 1.1994166374206543, 'learning_rate': 3.292745910884614e-05, 'epoch': 0.76}
{'loss': 1.1168, 'grad_norm': 0.9854475259780884, 'learning_rate': 3.288763737479155e-05, 'epoch': 0.76}
{'loss': 0.9047, 'grad_norm': 1.2812085151672363, 'learning_rate': 3.284783499561907e-05, 'epoch': 0.76}
{'loss': 0.7072, 'grad_norm': 0.8638589978218079, 'learning_rate': 3.280805198280754e-05, 'epoch': 0.76}
{'loss': 0.7275, 'grad_norm': 1.2009103298187256, 'learning_rate': 3.27682883478302e-05, 'epoch': 0.76}
{'loss': 0.7141, 'grad_norm': 1.1509193181991577, 'learning_rate': 3.272854410215467e-05, 'epoch': 0.76}
{'loss': 1.1717, 'grad_norm': 1.036562442779541, 'learning_rate': 3.268881925724297e-05, 'epoch': 0.76}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.968, 'grad_norm': 1.0740569829940796, 'learning_rate': 3.264911382455164e-05, 'epoch': 0.76}
{'loss': 1.1807, 'grad_norm': 1.4160884618759155, 'learning_rate': 3.2609427815531426e-05, 'epoch': 0.76}
{'loss': 0.8196, 'grad_norm': 0.9393144249916077, 'learning_rate': 3.2569761241627696e-05, 'epoch': 0.76}
{'loss': 1.0312, 'grad_norm': 1.3491915464401245, 'learning_rate': 3.253011411427998e-05, 'epoch': 0.76}
{'loss': 0.8186, 'grad_norm': 1.173384189605713, 'learning_rate': 3.24904864449224e-05, 'epoch': 0.76}
{'loss': 1.0762, 'grad_norm': 1.0201318264007568, 'learning_rate': 3.245087824498336e-05, 'epoch': 0.76}
{'loss': 1.1229, 'grad_norm': 1.6198389530181885, 'learning_rate': 3.241128952588569e-05, 'epoch': 0.76}
{'loss': 0.9103, 'grad_norm': 1.1305289268493652, 'learning_rate': 3.237172029904657e-05, 'epoch': 0.76}
{'loss': 0.9882, 'grad_norm': 1.165479063987732, 'learning_rate': 3.233217057587754e-05, 'epoch': 0.76}
{'loss': 1.1142, 'grad_norm': 1.1410741806030273, 'learning_rate': 3.229264036778462e-05, 'epoch': 0.76}
{'loss': 0.8564, 'grad_norm': 1.0983495712280273, 'learning_rate': 3.2253129686168106e-05, 'epoch': 0.76}
{'loss': 0.866, 'grad_norm': 1.1654722690582275, 'learning_rate': 3.221363854242268e-05, 'epoch': 0.76}
{'loss': 0.974, 'grad_norm': 1.1229331493377686, 'learning_rate': 3.217416694793739e-05, 'epoch': 0.76}
{'loss': 0.5939, 'grad_norm': 1.0702800750732422, 'learning_rate': 3.213471491409568e-05, 'epoch': 0.76}
{'loss': 0.8861, 'grad_norm': 1.2447996139526367, 'learning_rate': 3.2095282452275256e-05, 'epoch': 0.76}
{'loss': 1.3095, 'grad_norm': 1.1777344942092896, 'learning_rate': 3.205586957384838e-05, 'epoch': 0.76}
{'loss': 1.1139, 'grad_norm': 1.1133396625518799, 'learning_rate': 3.201647629018139e-05, 'epoch': 0.76}
{'loss': 1.03, 'grad_norm': 1.0979772806167603, 'learning_rate': 3.1977102612635215e-05, 'epoch': 0.76}
{'loss': 0.9535, 'grad_norm': 1.1503219604492188, 'learning_rate': 3.1937748552565015e-05, 'epoch': 0.76}
{'loss': 1.161, 'grad_norm': 1.4466809034347534, 'learning_rate': 3.1898414121320276e-05, 'epoch': 0.76}
{'loss': 0.8745, 'grad_norm': 1.298624873161316, 'learning_rate': 3.185909933024494e-05, 'epoch': 0.76}
{'loss': 1.0296, 'grad_norm': 1.0152815580368042, 'learning_rate': 3.1819804190677085e-05, 'epoch': 0.76}
{'loss': 0.825, 'grad_norm': 1.2774618864059448, 'learning_rate': 3.1780528713949375e-05, 'epoch': 0.77}
{'loss': 0.992, 'grad_norm': 1.7868225574493408, 'learning_rate': 3.174127291138853e-05, 'epoch': 0.77}
{'loss': 0.8012, 'grad_norm': 0.9807239174842834, 'learning_rate': 3.170203679431584e-05, 'epoch': 0.77}
{'loss': 0.8637, 'grad_norm': 1.2138394117355347, 'learning_rate': 3.1662820374046774e-05, 'epoch': 0.77}
{'loss': 1.1051, 'grad_norm': 1.2319284677505493, 'learning_rate': 3.162362366189116e-05, 'epoch': 0.77}
{'loss': 0.8636, 'grad_norm': 1.010007619857788, 'learning_rate': 3.1584446669153136e-05, 'epoch': 0.77}
{'loss': 0.8177, 'grad_norm': 1.3235905170440674, 'learning_rate': 3.154528940713113e-05, 'epoch': 0.77}
{'loss': 1.2077, 'grad_norm': 1.0345529317855835, 'learning_rate': 3.1506151887117974e-05, 'epoch': 0.77}
{'loss': 1.0677, 'grad_norm': 1.071488618850708, 'learning_rate': 3.146703412040071e-05, 'epoch': 0.77}
{'loss': 0.811, 'grad_norm': 1.253374695777893, 'learning_rate': 3.142793611826069e-05, 'epoch': 0.77}
{'loss': 0.9544, 'grad_norm': 1.0934734344482422, 'learning_rate': 3.13888578919736e-05, 'epoch': 0.77}
{'loss': 0.9379, 'grad_norm': 1.285849690437317, 'learning_rate': 3.134979945280948e-05, 'epoch': 0.77}
{'loss': 0.9795, 'grad_norm': 1.440794825553894, 'learning_rate': 3.131076081203247e-05, 'epoch': 0.77}
{'loss': 0.8242, 'grad_norm': 1.0628348588943481, 'learning_rate': 3.127174198090126e-05, 'epoch': 0.77}
{'loss': 1.1202, 'grad_norm': 1.0640000104904175, 'learning_rate': 3.123274297066856e-05, 'epoch': 0.77}
{'loss': 0.9929, 'grad_norm': 1.1826626062393188, 'learning_rate': 3.11937637925816e-05, 'epoch': 0.77}
{'loss': 1.0172, 'grad_norm': 1.0051349401474, 'learning_rate': 3.115480445788174e-05, 'epoch': 0.77}
{'loss': 1.0519, 'grad_norm': 1.0653184652328491, 'learning_rate': 3.1115864977804676e-05, 'epoch': 0.77}
{'loss': 0.9015, 'grad_norm': 0.9990973472595215, 'learning_rate': 3.1076945363580365e-05, 'epoch': 0.77}
{'loss': 0.6847, 'grad_norm': 1.126895546913147, 'learning_rate': 3.103804562643302e-05, 'epoch': 0.77}
{'loss': 0.8088, 'grad_norm': 1.1819229125976562, 'learning_rate': 3.0999165777581176e-05, 'epoch': 0.77}
{'loss': 1.1861, 'grad_norm': 1.2730923891067505, 'learning_rate': 3.096030582823757e-05, 'epoch': 0.77}
{'loss': 1.0539, 'grad_norm': 0.9084790349006653, 'learning_rate': 3.0921465789609226e-05, 'epoch': 0.77}
{'loss': 1.1739, 'grad_norm': 1.1975356340408325, 'learning_rate': 3.0882645672897425e-05, 'epoch': 0.77}
{'loss': 0.768, 'grad_norm': 0.893635630607605, 'learning_rate': 3.08438454892977e-05, 'epoch': 0.77}
{'loss': 1.0945, 'grad_norm': 1.4563721418380737, 'learning_rate': 3.080506524999981e-05, 'epoch': 0.77}
{'loss': 0.765, 'grad_norm': 1.0828568935394287, 'learning_rate': 3.076630496618788e-05, 'epoch': 0.77}
{'loss': 1.0947, 'grad_norm': 1.1435613632202148, 'learning_rate': 3.072756464904006e-05, 'epoch': 0.77}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9152, 'grad_norm': 1.3269257545471191, 'learning_rate': 3.068884430972897e-05, 'epoch': 0.77}
{'loss': 0.9296, 'grad_norm': 1.1473933458328247, 'learning_rate': 3.065014395942134e-05, 'epoch': 0.77}
{'loss': 0.9426, 'grad_norm': 1.1677500009536743, 'learning_rate': 3.061146360927813e-05, 'epoch': 0.77}
{'loss': 0.885, 'grad_norm': 1.148792028427124, 'learning_rate': 3.057280327045467e-05, 'epoch': 0.77}
{'loss': 0.6558, 'grad_norm': 1.0344069004058838, 'learning_rate': 3.053416295410026e-05, 'epoch': 0.77}
{'loss': 0.9284, 'grad_norm': 0.9598199129104614, 'learning_rate': 3.0495542671358746e-05, 'epoch': 0.77}
{'loss': 0.553, 'grad_norm': 1.2841814756393433, 'learning_rate': 3.0456942433367875e-05, 'epoch': 0.77}
{'loss': 0.942, 'grad_norm': 0.9566966891288757, 'learning_rate': 3.0418362251259892e-05, 'epoch': 0.77}
{'loss': 0.9809, 'grad_norm': 1.2164915800094604, 'learning_rate': 3.0379802136161074e-05, 'epoch': 0.77}
{'loss': 1.0594, 'grad_norm': 1.02826726436615, 'learning_rate': 3.0341262099191993e-05, 'epoch': 0.77}
{'loss': 0.8694, 'grad_norm': 1.1582914590835571, 'learning_rate': 3.03027421514674e-05, 'epoch': 0.77}
{'loss': 1.0519, 'grad_norm': 1.1984379291534424, 'learning_rate': 3.0264242304096235e-05, 'epoch': 0.77}
{'loss': 1.208, 'grad_norm': 1.162844181060791, 'learning_rate': 3.0225762568181736e-05, 'epoch': 0.77}
{'loss': 0.9398, 'grad_norm': 1.2720983028411865, 'learning_rate': 3.018730295482124e-05, 'epoch': 0.77}
{'loss': 0.9555, 'grad_norm': 1.2100727558135986, 'learning_rate': 3.0148863475106314e-05, 'epoch': 0.77}
{'loss': 0.7933, 'grad_norm': 1.078748345375061, 'learning_rate': 3.0110444140122708e-05, 'epoch': 0.77}
{'loss': 0.7228, 'grad_norm': 1.4517685174942017, 'learning_rate': 3.0072044960950384e-05, 'epoch': 0.77}
{'loss': 1.0632, 'grad_norm': 1.3346049785614014, 'learning_rate': 3.0033665948663448e-05, 'epoch': 0.77}
{'loss': 1.0337, 'grad_norm': 1.014853596687317, 'learning_rate': 2.9995307114330318e-05, 'epoch': 0.77}
{'loss': 1.024, 'grad_norm': 0.9627321362495422, 'learning_rate': 2.9956968469013368e-05, 'epoch': 0.77}
{'loss': 1.0565, 'grad_norm': 1.0640220642089844, 'learning_rate': 2.991865002376937e-05, 'epoch': 0.77}
{'loss': 0.8089, 'grad_norm': 1.2386213541030884, 'learning_rate': 2.9880351789649154e-05, 'epoch': 0.77}
{'loss': 0.9512, 'grad_norm': 1.1851215362548828, 'learning_rate': 2.984207377769771e-05, 'epoch': 0.77}
{'loss': 1.1147, 'grad_norm': 1.1180596351623535, 'learning_rate': 2.9803815998954332e-05, 'epoch': 0.77}
{'loss': 1.0551, 'grad_norm': 1.0488734245300293, 'learning_rate': 2.976557846445225e-05, 'epoch': 0.77}
{'loss': 1.0887, 'grad_norm': 1.212134838104248, 'learning_rate': 2.972736118521908e-05, 'epoch': 0.77}
{'loss': 0.9783, 'grad_norm': 0.9326460361480713, 'learning_rate': 2.968916417227646e-05, 'epoch': 0.77}
{'loss': 1.1731, 'grad_norm': 1.3445608615875244, 'learning_rate': 2.9650987436640242e-05, 'epoch': 0.77}
{'loss': 0.701, 'grad_norm': 1.0201431512832642, 'learning_rate': 2.9612830989320406e-05, 'epoch': 0.77}
{'loss': 0.9956, 'grad_norm': 0.9989178776741028, 'learning_rate': 2.9574694841321082e-05, 'epoch': 0.77}
{'loss': 0.9403, 'grad_norm': 1.1993993520736694, 'learning_rate': 2.953657900364053e-05, 'epoch': 0.77}
{'loss': 0.6791, 'grad_norm': 1.0416667461395264, 'learning_rate': 2.9498483487271223e-05, 'epoch': 0.77}
{'loss': 1.1059, 'grad_norm': 0.8296339511871338, 'learning_rate': 2.9460408303199694e-05, 'epoch': 0.77}
{'loss': 0.7005, 'grad_norm': 1.2032285928726196, 'learning_rate': 2.942235346240666e-05, 'epoch': 0.77}
{'loss': 0.9816, 'grad_norm': 1.1519051790237427, 'learning_rate': 2.938431897586694e-05, 'epoch': 0.77}
{'loss': 1.2726, 'grad_norm': 1.0194216966629028, 'learning_rate': 2.934630485454948e-05, 'epoch': 0.77}
{'loss': 0.9803, 'grad_norm': 1.1414554119110107, 'learning_rate': 2.930831110941743e-05, 'epoch': 0.77}
{'loss': 1.0102, 'grad_norm': 1.1145468950271606, 'learning_rate': 2.9270337751427913e-05, 'epoch': 0.78}
{'loss': 1.0594, 'grad_norm': 1.2484984397888184, 'learning_rate': 2.9232384791532375e-05, 'epoch': 0.78}
{'loss': 0.7199, 'grad_norm': 1.1427760124206543, 'learning_rate': 2.919445224067614e-05, 'epoch': 0.78}
{'loss': 0.8575, 'grad_norm': 1.1835405826568604, 'learning_rate': 2.915654010979887e-05, 'epoch': 0.78}
{'loss': 0.7729, 'grad_norm': 0.9408997893333435, 'learning_rate': 2.9118648409834205e-05, 'epoch': 0.78}
{'loss': 1.0373, 'grad_norm': 1.1187008619308472, 'learning_rate': 2.9080777151709925e-05, 'epoch': 0.78}
{'loss': 1.1817, 'grad_norm': 1.274980902671814, 'learning_rate': 2.904292634634793e-05, 'epoch': 0.78}
{'loss': 0.8775, 'grad_norm': 1.5812453031539917, 'learning_rate': 2.9005096004664177e-05, 'epoch': 0.78}
{'loss': 0.9214, 'grad_norm': 1.3803220987319946, 'learning_rate': 2.8967286137568804e-05, 'epoch': 0.78}
{'loss': 1.0428, 'grad_norm': 0.91866534948349, 'learning_rate': 2.892949675596599e-05, 'epoch': 0.78}
{'loss': 0.9955, 'grad_norm': 0.9798567891120911, 'learning_rate': 2.8891727870753983e-05, 'epoch': 0.78}
{'loss': 0.9116, 'grad_norm': 1.0444509983062744, 'learning_rate': 2.8853979492825156e-05, 'epoch': 0.78}
{'loss': 1.0802, 'grad_norm': 0.949739933013916, 'learning_rate': 2.881625163306596e-05, 'epoch': 0.78}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.957, 'grad_norm': 1.2006547451019287, 'learning_rate': 2.8778544302356904e-05, 'epoch': 0.78}
{'loss': 0.9022, 'grad_norm': 1.0521879196166992, 'learning_rate': 2.8740857511572684e-05, 'epoch': 0.78}
{'loss': 1.0403, 'grad_norm': 1.2209126949310303, 'learning_rate': 2.8703191271581875e-05, 'epoch': 0.78}
{'loss': 0.9108, 'grad_norm': 1.6514538526535034, 'learning_rate': 2.8665545593247312e-05, 'epoch': 0.78}
{'loss': 1.0258, 'grad_norm': 0.9680690765380859, 'learning_rate': 2.862792048742583e-05, 'epoch': 0.78}
{'loss': 0.8207, 'grad_norm': 1.209436297416687, 'learning_rate': 2.8590315964968262e-05, 'epoch': 0.78}
{'loss': 0.869, 'grad_norm': 1.1971944570541382, 'learning_rate': 2.8552732036719687e-05, 'epoch': 0.78}
{'loss': 1.2412, 'grad_norm': 0.9458121657371521, 'learning_rate': 2.8515168713518993e-05, 'epoch': 0.78}
{'loss': 0.9666, 'grad_norm': 1.3150473833084106, 'learning_rate': 2.8477626006199364e-05, 'epoch': 0.78}
{'loss': 1.1603, 'grad_norm': 1.081365704536438, 'learning_rate': 2.84401039255879e-05, 'epoch': 0.78}
{'loss': 0.9222, 'grad_norm': 1.0326381921768188, 'learning_rate': 2.840260248250578e-05, 'epoch': 0.78}
{'loss': 0.8158, 'grad_norm': 0.8848124146461487, 'learning_rate': 2.836512168776826e-05, 'epoch': 0.78}
{'loss': 0.9105, 'grad_norm': 1.2531626224517822, 'learning_rate': 2.8327661552184603e-05, 'epoch': 0.78}
{'loss': 0.9012, 'grad_norm': 0.8094101548194885, 'learning_rate': 2.8290222086558106e-05, 'epoch': 0.78}
{'loss': 1.0162, 'grad_norm': 1.5001481771469116, 'learning_rate': 2.8252803301686193e-05, 'epoch': 0.78}
{'loss': 0.6961, 'grad_norm': 1.2539807558059692, 'learning_rate': 2.8215405208360233e-05, 'epoch': 0.78}
{'loss': 1.128, 'grad_norm': 0.9578835368156433, 'learning_rate': 2.817802781736565e-05, 'epoch': 0.78}
{'loss': 0.8789, 'grad_norm': 1.2575953006744385, 'learning_rate': 2.8140671139481912e-05, 'epoch': 0.78}
{'loss': 0.8625, 'grad_norm': 1.5599415302276611, 'learning_rate': 2.810333518548246e-05, 'epoch': 0.78}
{'loss': 1.0417, 'grad_norm': 1.0241104364395142, 'learning_rate': 2.8066019966134904e-05, 'epoch': 0.78}
{'loss': 0.9608, 'grad_norm': 1.4676721096038818, 'learning_rate': 2.8028725492200657e-05, 'epoch': 0.78}
{'loss': 0.9908, 'grad_norm': 1.0677518844604492, 'learning_rate': 2.7991451774435384e-05, 'epoch': 0.78}
{'loss': 1.5298, 'grad_norm': 1.1509790420532227, 'learning_rate': 2.7954198823588517e-05, 'epoch': 0.78}
{'loss': 1.0204, 'grad_norm': 1.085869550704956, 'learning_rate': 2.791696665040373e-05, 'epoch': 0.78}
{'loss': 0.9915, 'grad_norm': 0.888396143913269, 'learning_rate': 2.7879755265618555e-05, 'epoch': 0.78}
{'loss': 0.9593, 'grad_norm': 1.196946144104004, 'learning_rate': 2.7842564679964567e-05, 'epoch': 0.78}
{'loss': 0.6566, 'grad_norm': 1.1439425945281982, 'learning_rate': 2.7805394904167437e-05, 'epoch': 0.78}
{'loss': 0.8657, 'grad_norm': 0.9284037351608276, 'learning_rate': 2.7768245948946612e-05, 'epoch': 0.78}
{'loss': 0.7554, 'grad_norm': 1.1888511180877686, 'learning_rate': 2.7731117825015772e-05, 'epoch': 0.78}
{'loss': 1.0771, 'grad_norm': 1.1454858779907227, 'learning_rate': 2.7694010543082472e-05, 'epoch': 0.78}
{'loss': 0.7683, 'grad_norm': 1.0628684759140015, 'learning_rate': 2.765692411384826e-05, 'epoch': 0.78}
{'loss': 0.6956, 'grad_norm': 1.1406861543655396, 'learning_rate': 2.761985854800868e-05, 'epoch': 0.78}
{'loss': 0.766, 'grad_norm': 1.156181812286377, 'learning_rate': 2.7582813856253275e-05, 'epoch': 0.78}
{'loss': 1.0613, 'grad_norm': 1.1463522911071777, 'learning_rate': 2.754579004926551e-05, 'epoch': 0.78}
{'loss': 1.0255, 'grad_norm': 1.0040704011917114, 'learning_rate': 2.7508787137722947e-05, 'epoch': 0.78}
{'loss': 0.7763, 'grad_norm': 1.0391314029693604, 'learning_rate': 2.7471805132297013e-05, 'epoch': 0.78}
{'loss': 1.0555, 'grad_norm': 1.404329776763916, 'learning_rate': 2.743484404365314e-05, 'epoch': 0.78}
{'loss': 1.0606, 'grad_norm': 1.0891796350479126, 'learning_rate': 2.7397903882450728e-05, 'epoch': 0.78}
{'loss': 0.5976, 'grad_norm': 0.914604663848877, 'learning_rate': 2.73609846593431e-05, 'epoch': 0.78}
{'loss': 0.9271, 'grad_norm': 1.7020376920700073, 'learning_rate': 2.7324086384977698e-05, 'epoch': 0.78}
{'loss': 1.1684, 'grad_norm': 1.3221514225006104, 'learning_rate': 2.728720906999567e-05, 'epoch': 0.78}
{'loss': 1.1016, 'grad_norm': 1.1684741973876953, 'learning_rate': 2.725035272503238e-05, 'epoch': 0.78}
{'loss': 0.9608, 'grad_norm': 1.1287744045257568, 'learning_rate': 2.7213517360716888e-05, 'epoch': 0.78}
{'loss': 0.9252, 'grad_norm': 1.2319594621658325, 'learning_rate': 2.7176702987672443e-05, 'epoch': 0.78}
{'loss': 0.9275, 'grad_norm': 1.2803068161010742, 'learning_rate': 2.713990961651609e-05, 'epoch': 0.78}
{'loss': 0.9278, 'grad_norm': 1.1908477544784546, 'learning_rate': 2.7103137257858868e-05, 'epoch': 0.78}
{'loss': 1.071, 'grad_norm': 1.3399468660354614, 'learning_rate': 2.7066385922305714e-05, 'epoch': 0.78}
{'loss': 1.1059, 'grad_norm': 1.2799618244171143, 'learning_rate': 2.7029655620455608e-05, 'epoch': 0.78}
{'loss': 0.8009, 'grad_norm': 1.246590495109558, 'learning_rate': 2.699294636290134e-05, 'epoch': 0.78}
{'loss': 1.0249, 'grad_norm': 1.1673731803894043, 'learning_rate': 2.6956258160229695e-05, 'epoch': 0.78}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1638, 'grad_norm': 1.0205388069152832, 'learning_rate': 2.6919591023021384e-05, 'epoch': 0.78}
{'loss': 1.1038, 'grad_norm': 1.297129511833191, 'learning_rate': 2.6882944961850987e-05, 'epoch': 0.78}
{'loss': 1.1301, 'grad_norm': 1.032059907913208, 'learning_rate': 2.684631998728715e-05, 'epoch': 0.79}
{'loss': 0.8796, 'grad_norm': 1.0757532119750977, 'learning_rate': 2.6809716109892214e-05, 'epoch': 0.79}
{'loss': 0.8806, 'grad_norm': 1.4486198425292969, 'learning_rate': 2.677313334022268e-05, 'epoch': 0.79}
{'loss': 1.2007, 'grad_norm': 0.9464139342308044, 'learning_rate': 2.6736571688828726e-05, 'epoch': 0.79}
{'loss': 0.9439, 'grad_norm': 1.2921159267425537, 'learning_rate': 2.6700031166254658e-05, 'epoch': 0.79}
{'loss': 0.838, 'grad_norm': 1.2548433542251587, 'learning_rate': 2.666351178303853e-05, 'epoch': 0.79}
{'loss': 1.1285, 'grad_norm': 1.2379509210586548, 'learning_rate': 2.6627013549712355e-05, 'epoch': 0.79}
{'loss': 0.9884, 'grad_norm': 1.519225835800171, 'learning_rate': 2.6590536476802118e-05, 'epoch': 0.79}
{'loss': 0.9742, 'grad_norm': 1.0029854774475098, 'learning_rate': 2.655408057482751e-05, 'epoch': 0.79}
{'loss': 0.7582, 'grad_norm': 1.0934621095657349, 'learning_rate': 2.651764585430234e-05, 'epoch': 0.79}
{'loss': 1.0512, 'grad_norm': 1.2077611684799194, 'learning_rate': 2.6481232325734174e-05, 'epoch': 0.79}
{'loss': 0.9605, 'grad_norm': 0.9786145091056824, 'learning_rate': 2.6444839999624494e-05, 'epoch': 0.79}
{'loss': 0.9242, 'grad_norm': 1.1166424751281738, 'learning_rate': 2.6408468886468672e-05, 'epoch': 0.79}
{'loss': 1.3636, 'grad_norm': 0.9389783143997192, 'learning_rate': 2.637211899675596e-05, 'epoch': 0.79}
{'loss': 0.6414, 'grad_norm': 1.2159266471862793, 'learning_rate': 2.6335790340969456e-05, 'epoch': 0.79}
{'loss': 0.855, 'grad_norm': 1.2772258520126343, 'learning_rate': 2.629948292958625e-05, 'epoch': 0.79}
{'loss': 1.0337, 'grad_norm': 1.121539831161499, 'learning_rate': 2.626319677307718e-05, 'epoch': 0.79}
{'loss': 1.0328, 'grad_norm': 1.351096749305725, 'learning_rate': 2.622693188190699e-05, 'epoch': 0.79}
{'loss': 0.853, 'grad_norm': 0.9738631844520569, 'learning_rate': 2.6190688266534313e-05, 'epoch': 0.79}
{'loss': 0.9295, 'grad_norm': 1.1414031982421875, 'learning_rate': 2.615446593741161e-05, 'epoch': 0.79}
{'loss': 1.2132, 'grad_norm': 1.117379069328308, 'learning_rate': 2.61182649049853e-05, 'epoch': 0.79}
{'loss': 0.9095, 'grad_norm': 1.118729591369629, 'learning_rate': 2.608208517969547e-05, 'epoch': 0.79}
{'loss': 1.1033, 'grad_norm': 1.2607827186584473, 'learning_rate': 2.6045926771976303e-05, 'epoch': 0.79}
{'loss': 0.7762, 'grad_norm': 1.0064777135849, 'learning_rate': 2.6009789692255582e-05, 'epoch': 0.79}
{'loss': 1.0526, 'grad_norm': 1.045972466468811, 'learning_rate': 2.597367395095517e-05, 'epoch': 0.79}
{'loss': 0.9615, 'grad_norm': 0.9338002800941467, 'learning_rate': 2.593757955849063e-05, 'epoch': 0.79}
{'loss': 1.2212, 'grad_norm': 1.1472651958465576, 'learning_rate': 2.5901506525271425e-05, 'epoch': 0.79}
{'loss': 0.6071, 'grad_norm': 1.1029595136642456, 'learning_rate': 2.58654548617008e-05, 'epoch': 0.79}
{'loss': 0.9462, 'grad_norm': 1.145525336265564, 'learning_rate': 2.582942457817594e-05, 'epoch': 0.79}
{'loss': 1.0311, 'grad_norm': 1.179638385772705, 'learning_rate': 2.5793415685087797e-05, 'epoch': 0.79}
{'loss': 1.1309, 'grad_norm': 1.1192702054977417, 'learning_rate': 2.5757428192821133e-05, 'epoch': 0.79}
{'loss': 0.9311, 'grad_norm': 1.0573956966400146, 'learning_rate': 2.572146211175458e-05, 'epoch': 0.79}
{'loss': 0.9013, 'grad_norm': 1.4174412488937378, 'learning_rate': 2.5685517452260567e-05, 'epoch': 0.79}
{'loss': 1.1205, 'grad_norm': 1.099611520767212, 'learning_rate': 2.564959422470543e-05, 'epoch': 0.79}
{'loss': 1.0533, 'grad_norm': 0.8614161014556885, 'learning_rate': 2.5613692439449143e-05, 'epoch': 0.79}
{'loss': 0.7715, 'grad_norm': 1.0590420961380005, 'learning_rate': 2.55778121068457e-05, 'epoch': 0.79}
{'loss': 0.8218, 'grad_norm': 1.043411135673523, 'learning_rate': 2.5541953237242777e-05, 'epoch': 0.79}
{'loss': 1.2137, 'grad_norm': 1.2533568143844604, 'learning_rate': 2.5506115840981904e-05, 'epoch': 0.79}
{'loss': 0.9964, 'grad_norm': 1.1298741102218628, 'learning_rate': 2.5470299928398424e-05, 'epoch': 0.79}
{'loss': 1.0086, 'grad_norm': 1.2657841444015503, 'learning_rate': 2.543450550982143e-05, 'epoch': 0.79}
{'loss': 0.9366, 'grad_norm': 1.2011321783065796, 'learning_rate': 2.539873259557396e-05, 'epoch': 0.79}
{'loss': 1.0033, 'grad_norm': 1.5509635210037231, 'learning_rate': 2.5362981195972625e-05, 'epoch': 0.79}
{'loss': 0.7792, 'grad_norm': 1.0497124195098877, 'learning_rate': 2.5327251321328037e-05, 'epoch': 0.79}
{'loss': 0.9733, 'grad_norm': 1.2987147569656372, 'learning_rate': 2.5291542981944504e-05, 'epoch': 0.79}
{'loss': 0.796, 'grad_norm': 1.2470955848693848, 'learning_rate': 2.5255856188120132e-05, 'epoch': 0.79}
{'loss': 0.9514, 'grad_norm': 1.0617036819458008, 'learning_rate': 2.5220190950146827e-05, 'epoch': 0.79}
{'loss': 0.7197, 'grad_norm': 0.9684368968009949, 'learning_rate': 2.5184547278310267e-05, 'epoch': 0.79}
{'loss': 1.2126, 'grad_norm': 1.2325832843780518, 'learning_rate': 2.514892518288988e-05, 'epoch': 0.79}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7446, 'grad_norm': 1.0219088792800903, 'learning_rate': 2.511332467415899e-05, 'epoch': 0.79}
{'loss': 0.9803, 'grad_norm': 1.0204033851623535, 'learning_rate': 2.5077745762384542e-05, 'epoch': 0.79}
{'loss': 1.1284, 'grad_norm': 1.2757389545440674, 'learning_rate': 2.5042188457827365e-05, 'epoch': 0.79}
{'loss': 1.1057, 'grad_norm': 1.4041951894760132, 'learning_rate': 2.5006652770741978e-05, 'epoch': 0.79}
{'loss': 0.9255, 'grad_norm': 1.3963216543197632, 'learning_rate': 2.4971138711376697e-05, 'epoch': 0.79}
{'loss': 0.8327, 'grad_norm': 1.1050533056259155, 'learning_rate': 2.493564628997369e-05, 'epoch': 0.79}
{'loss': 1.1555, 'grad_norm': 1.798221468925476, 'learning_rate': 2.4900175516768676e-05, 'epoch': 0.79}
{'loss': 1.0202, 'grad_norm': 1.1303139925003052, 'learning_rate': 2.4864726401991378e-05, 'epoch': 0.79}
{'loss': 0.9741, 'grad_norm': 1.2891552448272705, 'learning_rate': 2.482929895586502e-05, 'epoch': 0.79}
{'loss': 0.8602, 'grad_norm': 0.932357668876648, 'learning_rate': 2.479389318860682e-05, 'epoch': 0.79}
{'loss': 0.8798, 'grad_norm': 1.0338724851608276, 'learning_rate': 2.4758509110427575e-05, 'epoch': 0.79}
{'loss': 1.0065, 'grad_norm': 1.0548752546310425, 'learning_rate': 2.4723146731531876e-05, 'epoch': 0.79}
{'loss': 1.1161, 'grad_norm': 1.3412264585494995, 'learning_rate': 2.4687806062118134e-05, 'epoch': 0.79}
{'loss': 1.0112, 'grad_norm': 1.2333412170410156, 'learning_rate': 2.465248711237831e-05, 'epoch': 0.79}
{'loss': 0.9257, 'grad_norm': 1.3323097229003906, 'learning_rate': 2.4617189892498327e-05, 'epoch': 0.79}
{'loss': 1.0938, 'grad_norm': 1.3527134656906128, 'learning_rate': 2.458191441265768e-05, 'epoch': 0.79}
{'loss': 0.9735, 'grad_norm': 1.2233198881149292, 'learning_rate': 2.4546660683029653e-05, 'epoch': 0.79}
{'loss': 0.8709, 'grad_norm': 1.1322966814041138, 'learning_rate': 2.4511428713781238e-05, 'epoch': 0.8}
{'loss': 1.0811, 'grad_norm': 1.1820294857025146, 'learning_rate': 2.4476218515073234e-05, 'epoch': 0.8}
{'loss': 0.8996, 'grad_norm': 1.3803616762161255, 'learning_rate': 2.444103009705999e-05, 'epoch': 0.8}
{'loss': 1.4404, 'grad_norm': 1.062683343887329, 'learning_rate': 2.4405863469889757e-05, 'epoch': 0.8}
{'loss': 0.9922, 'grad_norm': 1.2596375942230225, 'learning_rate': 2.4370718643704392e-05, 'epoch': 0.8}
{'loss': 0.8698, 'grad_norm': 1.1434800624847412, 'learning_rate': 2.4335595628639496e-05, 'epoch': 0.8}
{'loss': 0.9881, 'grad_norm': 1.0698256492614746, 'learning_rate': 2.4300494434824373e-05, 'epoch': 0.8}
{'loss': 0.9151, 'grad_norm': 0.9445606470108032, 'learning_rate': 2.4265415072382016e-05, 'epoch': 0.8}
{'loss': 0.8603, 'grad_norm': 1.0042709112167358, 'learning_rate': 2.4230357551429216e-05, 'epoch': 0.8}
{'loss': 1.1829, 'grad_norm': 1.047279715538025, 'learning_rate': 2.4195321882076295e-05, 'epoch': 0.8}
{'loss': 1.0588, 'grad_norm': 1.0502065420150757, 'learning_rate': 2.4160308074427452e-05, 'epoch': 0.8}
{'loss': 1.0163, 'grad_norm': 1.3894562721252441, 'learning_rate': 2.4125316138580467e-05, 'epoch': 0.8}
{'loss': 1.1089, 'grad_norm': 1.3480900526046753, 'learning_rate': 2.409034608462686e-05, 'epoch': 0.8}
{'loss': 1.1472, 'grad_norm': 0.912736713886261, 'learning_rate': 2.4055397922651802e-05, 'epoch': 0.8}
{'loss': 0.9995, 'grad_norm': 0.9073443412780762, 'learning_rate': 2.40204716627342e-05, 'epoch': 0.8}
{'loss': 0.9956, 'grad_norm': 0.9909777641296387, 'learning_rate': 2.3985567314946578e-05, 'epoch': 0.8}
{'loss': 1.0417, 'grad_norm': 1.1720688343048096, 'learning_rate': 2.3950684889355234e-05, 'epoch': 0.8}
{'loss': 0.7485, 'grad_norm': 1.0426480770111084, 'learning_rate': 2.391582439602007e-05, 'epoch': 0.8}
{'loss': 1.2588, 'grad_norm': 1.018578052520752, 'learning_rate': 2.3880985844994674e-05, 'epoch': 0.8}
{'loss': 0.7015, 'grad_norm': 1.0633842945098877, 'learning_rate': 2.3846169246326343e-05, 'epoch': 0.8}
{'loss': 1.1768, 'grad_norm': 1.3919254541397095, 'learning_rate': 2.3811374610055957e-05, 'epoch': 0.8}
{'loss': 1.1734, 'grad_norm': 1.116337776184082, 'learning_rate': 2.3776601946218223e-05, 'epoch': 0.8}
{'loss': 0.644, 'grad_norm': 1.1030305624008179, 'learning_rate': 2.3741851264841297e-05, 'epoch': 0.8}
{'loss': 0.7773, 'grad_norm': 1.2646865844726562, 'learning_rate': 2.3707122575947194e-05, 'epoch': 0.8}
{'loss': 0.8822, 'grad_norm': 0.9008404016494751, 'learning_rate': 2.367241588955146e-05, 'epoch': 0.8}
{'loss': 0.6652, 'grad_norm': 1.0939477682113647, 'learning_rate': 2.363773121566334e-05, 'epoch': 0.8}
{'loss': 0.6723, 'grad_norm': 1.1086639165878296, 'learning_rate': 2.3603068564285747e-05, 'epoch': 0.8}
{'loss': 0.8832, 'grad_norm': 0.9952362775802612, 'learning_rate': 2.356842794541516e-05, 'epoch': 0.8}
{'loss': 0.6862, 'grad_norm': 1.221642017364502, 'learning_rate': 2.353380936904188e-05, 'epoch': 0.8}
{'loss': 0.8675, 'grad_norm': 1.1417272090911865, 'learning_rate': 2.349921284514961e-05, 'epoch': 0.8}
{'loss': 0.8156, 'grad_norm': 1.340041160583496, 'learning_rate': 2.346463838371591e-05, 'epoch': 0.8}
{'loss': 1.0747, 'grad_norm': 1.2403939962387085, 'learning_rate': 2.343008599471187e-05, 'epoch': 0.8}
{'loss': 0.9161, 'grad_norm': 1.1297056674957275, 'learning_rate': 2.339555568810221e-05, 'epoch': 0.8}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9978, 'grad_norm': 1.6348603963851929, 'learning_rate': 2.3361047473845322e-05, 'epoch': 0.8}
{'loss': 0.8473, 'grad_norm': 1.0721988677978516, 'learning_rate': 2.3326561361893205e-05, 'epoch': 0.8}
{'loss': 1.2253, 'grad_norm': 1.4333268404006958, 'learning_rate': 2.3292097362191456e-05, 'epoch': 0.8}
{'loss': 1.0296, 'grad_norm': 1.0622977018356323, 'learning_rate': 2.3257655484679374e-05, 'epoch': 0.8}
{'loss': 0.9942, 'grad_norm': 1.359299898147583, 'learning_rate': 2.322323573928983e-05, 'epoch': 0.8}
{'loss': 0.8586, 'grad_norm': 1.1666851043701172, 'learning_rate': 2.3188838135949253e-05, 'epoch': 0.8}
{'loss': 0.6476, 'grad_norm': 1.424551248550415, 'learning_rate': 2.315446268457786e-05, 'epoch': 0.8}
{'loss': 0.8781, 'grad_norm': 1.2376958131790161, 'learning_rate': 2.312010939508923e-05, 'epoch': 0.8}
{'loss': 0.8608, 'grad_norm': 0.9657847285270691, 'learning_rate': 2.3085778277390803e-05, 'epoch': 0.8}
{'loss': 0.6983, 'grad_norm': 1.1481207609176636, 'learning_rate': 2.3051469341383402e-05, 'epoch': 0.8}
{'loss': 1.1798, 'grad_norm': 1.1955134868621826, 'learning_rate': 2.301718259696165e-05, 'epoch': 0.8}
{'loss': 0.7579, 'grad_norm': 1.2500019073486328, 'learning_rate': 2.2982918054013637e-05, 'epoch': 0.8}
{'loss': 0.8775, 'grad_norm': 1.4052705764770508, 'learning_rate': 2.2948675722421086e-05, 'epoch': 0.8}
{'loss': 0.8713, 'grad_norm': 0.8353891968727112, 'learning_rate': 2.291445561205934e-05, 'epoch': 0.8}
{'loss': 0.8628, 'grad_norm': 1.2552388906478882, 'learning_rate': 2.2880257732797284e-05, 'epoch': 0.8}
{'loss': 0.9685, 'grad_norm': 1.3461312055587769, 'learning_rate': 2.2846082094497458e-05, 'epoch': 0.8}
{'loss': 1.0191, 'grad_norm': 1.0694924592971802, 'learning_rate': 2.281192870701594e-05, 'epoch': 0.8}
{'loss': 1.1782, 'grad_norm': 1.2663029432296753, 'learning_rate': 2.277779758020241e-05, 'epoch': 0.8}
{'loss': 1.2585, 'grad_norm': 1.1386598348617554, 'learning_rate': 2.274368872390009e-05, 'epoch': 0.8}
{'loss': 1.0887, 'grad_norm': 1.4861054420471191, 'learning_rate': 2.270960214794584e-05, 'epoch': 0.8}
{'loss': 0.9924, 'grad_norm': 0.9526183009147644, 'learning_rate': 2.2675537862170027e-05, 'epoch': 0.8}
{'loss': 1.1252, 'grad_norm': 0.9965774416923523, 'learning_rate': 2.2641495876396713e-05, 'epoch': 0.8}
{'loss': 0.5717, 'grad_norm': 0.996097981929779, 'learning_rate': 2.2607476200443324e-05, 'epoch': 0.8}
{'loss': 0.924, 'grad_norm': 1.6127448081970215, 'learning_rate': 2.2573478844121054e-05, 'epoch': 0.8}
{'loss': 0.9231, 'grad_norm': 1.4689759016036987, 'learning_rate': 2.2539503817234553e-05, 'epoch': 0.8}
{'loss': 0.8955, 'grad_norm': 1.1132127046585083, 'learning_rate': 2.2505551129582047e-05, 'epoch': 0.8}
{'loss': 0.633, 'grad_norm': 1.112417459487915, 'learning_rate': 2.247162079095535e-05, 'epoch': 0.8}
{'loss': 0.9601, 'grad_norm': 1.0658533573150635, 'learning_rate': 2.2437712811139767e-05, 'epoch': 0.8}
{'loss': 0.6985, 'grad_norm': 1.1434797048568726, 'learning_rate': 2.240382719991426e-05, 'epoch': 0.8}
{'loss': 0.9349, 'grad_norm': 1.0765317678451538, 'learning_rate': 2.23699639670512e-05, 'epoch': 0.8}
{'loss': 1.2118, 'grad_norm': 1.4554188251495361, 'learning_rate': 2.2336123122316644e-05, 'epoch': 0.8}
{'loss': 0.7535, 'grad_norm': 1.217591643333435, 'learning_rate': 2.23023046754701e-05, 'epoch': 0.8}
{'loss': 1.2033, 'grad_norm': 1.2036259174346924, 'learning_rate': 2.2268508636264652e-05, 'epoch': 0.81}
{'loss': 1.0324, 'grad_norm': 1.0708085298538208, 'learning_rate': 2.2234735014446907e-05, 'epoch': 0.81}
{'loss': 1.0597, 'grad_norm': 1.3249057531356812, 'learning_rate': 2.2200983819757026e-05, 'epoch': 0.81}
{'loss': 0.8879, 'grad_norm': 1.0208206176757812, 'learning_rate': 2.216725506192865e-05, 'epoch': 0.81}
{'loss': 1.0651, 'grad_norm': 1.3651686906814575, 'learning_rate': 2.213354875068906e-05, 'epoch': 0.81}
{'loss': 1.1658, 'grad_norm': 5.363443374633789, 'learning_rate': 2.2099864895758948e-05, 'epoch': 0.81}
{'loss': 0.8922, 'grad_norm': 0.9537047147750854, 'learning_rate': 2.2066203506852566e-05, 'epoch': 0.81}
{'loss': 0.9791, 'grad_norm': 1.033311367034912, 'learning_rate': 2.2032564593677774e-05, 'epoch': 0.81}
{'loss': 0.8592, 'grad_norm': 0.9251187443733215, 'learning_rate': 2.1998948165935762e-05, 'epoch': 0.81}
{'loss': 0.967, 'grad_norm': 1.123096227645874, 'learning_rate': 2.1965354233321445e-05, 'epoch': 0.81}
{'loss': 0.5297, 'grad_norm': 0.9272288084030151, 'learning_rate': 2.193178280552306e-05, 'epoch': 0.81}
{'loss': 1.1235, 'grad_norm': 1.2677264213562012, 'learning_rate': 2.1898233892222507e-05, 'epoch': 0.81}
{'loss': 0.877, 'grad_norm': 1.1635445356369019, 'learning_rate': 2.1864707503095128e-05, 'epoch': 0.81}
{'loss': 1.1576, 'grad_norm': 1.0186688899993896, 'learning_rate': 2.183120364780975e-05, 'epoch': 0.81}
{'loss': 0.8412, 'grad_norm': 1.2048652172088623, 'learning_rate': 2.1797722336028724e-05, 'epoch': 0.81}
{'loss': 0.8722, 'grad_norm': 1.2322667837142944, 'learning_rate': 2.1764263577407896e-05, 'epoch': 0.81}
{'loss': 1.0186, 'grad_norm': 1.191440224647522, 'learning_rate': 2.1730827381596643e-05, 'epoch': 0.81}
{'loss': 0.8129, 'grad_norm': 1.2757680416107178, 'learning_rate': 2.1697413758237784e-05, 'epoch': 0.81}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.206, 'grad_norm': 1.029514193534851, 'learning_rate': 2.1664022716967635e-05, 'epoch': 0.81}
{'loss': 0.8298, 'grad_norm': 1.3911532163619995, 'learning_rate': 2.163065426741603e-05, 'epoch': 0.81}
{'loss': 0.9912, 'grad_norm': 1.3077569007873535, 'learning_rate': 2.1597308419206264e-05, 'epoch': 0.81}
{'loss': 0.8998, 'grad_norm': 1.2613946199417114, 'learning_rate': 2.1563985181955082e-05, 'epoch': 0.81}
{'loss': 1.0196, 'grad_norm': 1.0314708948135376, 'learning_rate': 2.153068456527283e-05, 'epoch': 0.81}
{'loss': 1.0228, 'grad_norm': 1.357134222984314, 'learning_rate': 2.1497406578763147e-05, 'epoch': 0.81}
{'loss': 1.1413, 'grad_norm': 0.9195458889007568, 'learning_rate': 2.146415123202331e-05, 'epoch': 0.81}
{'loss': 0.9332, 'grad_norm': 0.9202172160148621, 'learning_rate': 2.1430918534643996e-05, 'epoch': 0.81}
{'loss': 1.0625, 'grad_norm': 0.9660899043083191, 'learning_rate': 2.13977084962093e-05, 'epoch': 0.81}
{'loss': 1.0532, 'grad_norm': 1.4868308305740356, 'learning_rate': 2.136452112629693e-05, 'epoch': 0.81}
{'loss': 1.0897, 'grad_norm': 1.4772871732711792, 'learning_rate': 2.1331356434477866e-05, 'epoch': 0.81}
{'loss': 0.8458, 'grad_norm': 0.9536393880844116, 'learning_rate': 2.1298214430316743e-05, 'epoch': 0.81}
{'loss': 0.7676, 'grad_norm': 0.9992421269416809, 'learning_rate': 2.1265095123371447e-05, 'epoch': 0.81}
{'loss': 0.8982, 'grad_norm': 1.14622962474823, 'learning_rate': 2.123199852319352e-05, 'epoch': 0.81}
{'loss': 1.0753, 'grad_norm': 1.0831378698349, 'learning_rate': 2.119892463932781e-05, 'epoch': 0.81}
{'loss': 0.9518, 'grad_norm': 1.1286377906799316, 'learning_rate': 2.1165873481312692e-05, 'epoch': 0.81}
{'loss': 0.9171, 'grad_norm': 1.3313568830490112, 'learning_rate': 2.113284505867994e-05, 'epoch': 0.81}
{'loss': 1.1754, 'grad_norm': 1.2791881561279297, 'learning_rate': 2.109983938095479e-05, 'epoch': 0.81}
{'loss': 1.0522, 'grad_norm': 1.3162893056869507, 'learning_rate': 2.1066856457655946e-05, 'epoch': 0.81}
{'loss': 1.0897, 'grad_norm': 1.1205426454544067, 'learning_rate': 2.1033896298295508e-05, 'epoch': 0.81}
{'loss': 1.0978, 'grad_norm': 0.9285141229629517, 'learning_rate': 2.100095891237903e-05, 'epoch': 0.81}
{'loss': 1.3255, 'grad_norm': 1.032753586769104, 'learning_rate': 2.096804430940551e-05, 'epoch': 0.81}
{'loss': 0.8307, 'grad_norm': 1.4788881540298462, 'learning_rate': 2.0935152498867326e-05, 'epoch': 0.81}
{'loss': 0.7316, 'grad_norm': 1.1635667085647583, 'learning_rate': 2.090228349025032e-05, 'epoch': 0.81}
{'loss': 1.047, 'grad_norm': 1.085540533065796, 'learning_rate': 2.0869437293033835e-05, 'epoch': 0.81}
{'loss': 0.7739, 'grad_norm': 1.0203783512115479, 'learning_rate': 2.0836613916690428e-05, 'epoch': 0.81}
{'loss': 0.635, 'grad_norm': 1.1512800455093384, 'learning_rate': 2.0803813370686298e-05, 'epoch': 0.81}
{'loss': 1.2308, 'grad_norm': 1.354516625404358, 'learning_rate': 2.0771035664480942e-05, 'epoch': 0.81}
{'loss': 0.9359, 'grad_norm': 1.2954210042953491, 'learning_rate': 2.0738280807527276e-05, 'epoch': 0.81}
{'loss': 1.2127, 'grad_norm': 1.1464236974716187, 'learning_rate': 2.0705548809271658e-05, 'epoch': 0.81}
{'loss': 0.7334, 'grad_norm': 1.0696314573287964, 'learning_rate': 2.0672839679153812e-05, 'epoch': 0.81}
{'loss': 0.9019, 'grad_norm': 1.0672656297683716, 'learning_rate': 2.0640153426606935e-05, 'epoch': 0.81}
{'loss': 0.8864, 'grad_norm': 1.1397671699523926, 'learning_rate': 2.0607490061057565e-05, 'epoch': 0.81}
{'loss': 1.023, 'grad_norm': 1.556744933128357, 'learning_rate': 2.057484959192567e-05, 'epoch': 0.81}
{'loss': 0.5873, 'grad_norm': 1.175206184387207, 'learning_rate': 2.0542232028624586e-05, 'epoch': 0.81}
{'loss': 0.9422, 'grad_norm': 1.0458287000656128, 'learning_rate': 2.0509637380561063e-05, 'epoch': 0.81}
{'loss': 0.75, 'grad_norm': 1.2176244258880615, 'learning_rate': 2.047706565713523e-05, 'epoch': 0.81}
{'loss': 1.1194, 'grad_norm': 1.0153154134750366, 'learning_rate': 2.044451686774068e-05, 'epoch': 0.81}
{'loss': 0.8196, 'grad_norm': 1.1512176990509033, 'learning_rate': 2.041199102176422e-05, 'epoch': 0.81}
{'loss': 0.6408, 'grad_norm': 1.165385127067566, 'learning_rate': 2.0379488128586243e-05, 'epoch': 0.81}
{'loss': 0.7271, 'grad_norm': 1.3476018905639648, 'learning_rate': 2.0347008197580374e-05, 'epoch': 0.81}
{'loss': 0.7684, 'grad_norm': 1.1273868083953857, 'learning_rate': 2.0314551238113677e-05, 'epoch': 0.81}
{'loss': 0.8967, 'grad_norm': 1.0214908123016357, 'learning_rate': 2.028211725954663e-05, 'epoch': 0.81}
{'loss': 0.8022, 'grad_norm': 1.081911325454712, 'learning_rate': 2.024970627123295e-05, 'epoch': 0.81}
{'loss': 1.2013, 'grad_norm': 1.0563247203826904, 'learning_rate': 2.0217318282519904e-05, 'epoch': 0.81}
{'loss': 0.9098, 'grad_norm': 1.2398782968521118, 'learning_rate': 2.0184953302747934e-05, 'epoch': 0.81}
{'loss': 1.0003, 'grad_norm': 1.108379602432251, 'learning_rate': 2.0152611341251015e-05, 'epoch': 0.81}
{'loss': 0.9202, 'grad_norm': 1.5463181734085083, 'learning_rate': 2.01202924073564e-05, 'epoch': 0.82}
{'loss': 0.7705, 'grad_norm': 1.2557005882263184, 'learning_rate': 2.00879965103847e-05, 'epoch': 0.82}
{'loss': 1.1202, 'grad_norm': 1.22786283493042, 'learning_rate': 2.0055723659649904e-05, 'epoch': 0.82}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8502, 'grad_norm': 1.2242954969406128, 'learning_rate': 2.002347386445932e-05, 'epoch': 0.82}
{'loss': 1.2374, 'grad_norm': 1.2299187183380127, 'learning_rate': 1.999124713411368e-05, 'epoch': 0.82}
{'loss': 0.7204, 'grad_norm': 1.0588014125823975, 'learning_rate': 1.9959043477907e-05, 'epoch': 0.82}
{'loss': 0.9952, 'grad_norm': 1.012911319732666, 'learning_rate': 1.9926862905126665e-05, 'epoch': 0.82}
{'loss': 1.1049, 'grad_norm': 1.319995403289795, 'learning_rate': 1.9894705425053372e-05, 'epoch': 0.82}
{'loss': 0.8265, 'grad_norm': 1.0201289653778076, 'learning_rate': 1.986257104696121e-05, 'epoch': 0.82}
{'loss': 1.1025, 'grad_norm': 1.1430786848068237, 'learning_rate': 1.9830459780117538e-05, 'epoch': 0.82}
{'loss': 0.8814, 'grad_norm': 1.625775694847107, 'learning_rate': 1.9798371633783174e-05, 'epoch': 0.82}
{'loss': 0.987, 'grad_norm': 1.3672786951065063, 'learning_rate': 1.9766306617212072e-05, 'epoch': 0.82}
{'loss': 0.8892, 'grad_norm': 1.0743328332901, 'learning_rate': 1.973426473965172e-05, 'epoch': 0.82}
{'loss': 0.803, 'grad_norm': 1.166576862335205, 'learning_rate': 1.9702246010342816e-05, 'epoch': 0.82}
{'loss': 0.7149, 'grad_norm': 1.1983293294906616, 'learning_rate': 1.967025043851939e-05, 'epoch': 0.82}
{'loss': 1.0589, 'grad_norm': 0.9824930429458618, 'learning_rate': 1.963827803340883e-05, 'epoch': 0.82}
{'loss': 0.8824, 'grad_norm': 1.1870968341827393, 'learning_rate': 1.960632880423178e-05, 'epoch': 0.82}
{'loss': 0.8831, 'grad_norm': 1.0645166635513306, 'learning_rate': 1.9574402760202315e-05, 'epoch': 0.82}
{'loss': 1.1025, 'grad_norm': 1.2531436681747437, 'learning_rate': 1.95424999105277e-05, 'epoch': 0.82}
{'loss': 0.7814, 'grad_norm': 1.193218469619751, 'learning_rate': 1.9510620264408596e-05, 'epoch': 0.82}
{'loss': 0.7978, 'grad_norm': 1.356885552406311, 'learning_rate': 1.9478763831038915e-05, 'epoch': 0.82}
{'loss': 0.6544, 'grad_norm': 1.1322417259216309, 'learning_rate': 1.944693061960591e-05, 'epoch': 0.82}
{'loss': 0.7398, 'grad_norm': 1.205669641494751, 'learning_rate': 1.9415120639290085e-05, 'epoch': 0.82}
{'loss': 0.7773, 'grad_norm': 1.092752456665039, 'learning_rate': 1.9383333899265364e-05, 'epoch': 0.82}
{'loss': 0.9028, 'grad_norm': 1.1286293268203735, 'learning_rate': 1.935157040869884e-05, 'epoch': 0.82}
{'loss': 1.0499, 'grad_norm': 1.54148268699646, 'learning_rate': 1.9319830176750964e-05, 'epoch': 0.82}
{'loss': 0.6824, 'grad_norm': 1.1449053287506104, 'learning_rate': 1.9288113212575452e-05, 'epoch': 0.82}
{'loss': 0.7127, 'grad_norm': 1.2651416063308716, 'learning_rate': 1.9256419525319313e-05, 'epoch': 0.82}
{'loss': 0.9728, 'grad_norm': 1.472375750541687, 'learning_rate': 1.922474912412292e-05, 'epoch': 0.82}
{'loss': 0.8461, 'grad_norm': 1.1863881349563599, 'learning_rate': 1.919310201811977e-05, 'epoch': 0.82}
{'loss': 0.8966, 'grad_norm': 1.29506516456604, 'learning_rate': 1.9161478216436836e-05, 'epoch': 0.82}
{'loss': 0.9472, 'grad_norm': 1.4115326404571533, 'learning_rate': 1.912987772819417e-05, 'epoch': 0.82}
{'loss': 1.1999, 'grad_norm': 1.1586685180664062, 'learning_rate': 1.9098300562505266e-05, 'epoch': 0.82}
{'loss': 1.4507, 'grad_norm': 1.3819072246551514, 'learning_rate': 1.9066746728476813e-05, 'epoch': 0.82}
{'loss': 1.0372, 'grad_norm': 1.0060478448867798, 'learning_rate': 1.9035216235208775e-05, 'epoch': 0.82}
{'loss': 0.9122, 'grad_norm': 1.2723870277404785, 'learning_rate': 1.9003709091794408e-05, 'epoch': 0.82}
{'loss': 1.133, 'grad_norm': 1.4088972806930542, 'learning_rate': 1.8972225307320178e-05, 'epoch': 0.82}
{'loss': 1.0887, 'grad_norm': 1.319514513015747, 'learning_rate': 1.8940764890865914e-05, 'epoch': 0.82}
{'loss': 1.1145, 'grad_norm': 1.0496103763580322, 'learning_rate': 1.8909327851504633e-05, 'epoch': 0.82}
{'loss': 0.9488, 'grad_norm': 1.2196391820907593, 'learning_rate': 1.8877914198302626e-05, 'epoch': 0.82}
{'loss': 1.2032, 'grad_norm': 1.0316616296768188, 'learning_rate': 1.8846523940319416e-05, 'epoch': 0.82}
{'loss': 1.0696, 'grad_norm': 1.1117380857467651, 'learning_rate': 1.8815157086607826e-05, 'epoch': 0.82}
{'loss': 1.1326, 'grad_norm': 0.9902523159980774, 'learning_rate': 1.8783813646213867e-05, 'epoch': 0.82}
{'loss': 0.788, 'grad_norm': 1.1503801345825195, 'learning_rate': 1.8752493628176925e-05, 'epoch': 0.82}
{'loss': 1.0435, 'grad_norm': 1.185089111328125, 'learning_rate': 1.8721197041529427e-05, 'epoch': 0.82}
{'loss': 1.0121, 'grad_norm': 1.1854041814804077, 'learning_rate': 1.8689923895297245e-05, 'epoch': 0.82}
{'loss': 0.7359, 'grad_norm': 1.057544231414795, 'learning_rate': 1.865867419849937e-05, 'epoch': 0.82}
{'loss': 0.9745, 'grad_norm': 1.078777551651001, 'learning_rate': 1.8627447960148037e-05, 'epoch': 0.82}
{'loss': 1.0273, 'grad_norm': 1.2803361415863037, 'learning_rate': 1.8596245189248827e-05, 'epoch': 0.82}
{'loss': 0.6936, 'grad_norm': 1.285294532775879, 'learning_rate': 1.856506589480037e-05, 'epoch': 0.82}
{'loss': 0.7137, 'grad_norm': 1.1260120868682861, 'learning_rate': 1.8533910085794713e-05, 'epoch': 0.82}
{'loss': 0.7101, 'grad_norm': 1.2768537998199463, 'learning_rate': 1.850277777121696e-05, 'epoch': 0.82}
{'loss': 1.0645, 'grad_norm': 1.0325840711593628, 'learning_rate': 1.8471668960045574e-05, 'epoch': 0.82}
{'loss': 0.8458, 'grad_norm': 1.0981439352035522, 'learning_rate': 1.8440583661252165e-05, 'epoch': 0.82}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8, 'grad_norm': 1.2373838424682617, 'learning_rate': 1.8409521883801595e-05, 'epoch': 0.82}
{'loss': 0.888, 'grad_norm': 1.0213953256607056, 'learning_rate': 1.83784836366519e-05, 'epoch': 0.82}
{'loss': 0.5897, 'grad_norm': 0.9812019467353821, 'learning_rate': 1.8347468928754407e-05, 'epoch': 0.82}
{'loss': 0.9554, 'grad_norm': 1.0920299291610718, 'learning_rate': 1.8316477769053597e-05, 'epoch': 0.82}
{'loss': 1.4115, 'grad_norm': 1.0896157026290894, 'learning_rate': 1.8285510166487152e-05, 'epoch': 0.82}
{'loss': 0.9914, 'grad_norm': 1.0244157314300537, 'learning_rate': 1.8254566129985995e-05, 'epoch': 0.82}
{'loss': 0.9116, 'grad_norm': 1.6353598833084106, 'learning_rate': 1.82236456684742e-05, 'epoch': 0.82}
{'loss': 0.8748, 'grad_norm': 1.3693641424179077, 'learning_rate': 1.819274879086916e-05, 'epoch': 0.82}
{'loss': 0.7522, 'grad_norm': 1.1200754642486572, 'learning_rate': 1.8161875506081293e-05, 'epoch': 0.82}
{'loss': 0.8356, 'grad_norm': 1.0857830047607422, 'learning_rate': 1.81310258230144e-05, 'epoch': 0.82}
{'loss': 0.933, 'grad_norm': 1.2259843349456787, 'learning_rate': 1.8100199750565273e-05, 'epoch': 0.82}
{'loss': 1.233, 'grad_norm': 1.1856987476348877, 'learning_rate': 1.806939729762409e-05, 'epoch': 0.83}
{'loss': 1.2186, 'grad_norm': 1.0485410690307617, 'learning_rate': 1.8038618473074087e-05, 'epoch': 0.83}
{'loss': 0.8226, 'grad_norm': 1.386795163154602, 'learning_rate': 1.800786328579176e-05, 'epoch': 0.83}
{'loss': 0.8157, 'grad_norm': 1.586344838142395, 'learning_rate': 1.7977131744646724e-05, 'epoch': 0.83}
{'loss': 1.3527, 'grad_norm': 1.282212257385254, 'learning_rate': 1.794642385850179e-05, 'epoch': 0.83}
{'loss': 0.8855, 'grad_norm': 1.3019726276397705, 'learning_rate': 1.7915739636213035e-05, 'epoch': 0.83}
{'loss': 0.9674, 'grad_norm': 1.1057969331741333, 'learning_rate': 1.78850790866296e-05, 'epoch': 0.83}
{'loss': 1.0802, 'grad_norm': 1.1858255863189697, 'learning_rate': 1.7854442218593838e-05, 'epoch': 0.83}
{'loss': 0.9144, 'grad_norm': 0.8724748492240906, 'learning_rate': 1.7823829040941286e-05, 'epoch': 0.83}
{'loss': 0.7891, 'grad_norm': 1.0817643404006958, 'learning_rate': 1.779323956250062e-05, 'epoch': 0.83}
{'loss': 1.1531, 'grad_norm': 1.3809341192245483, 'learning_rate': 1.7762673792093687e-05, 'epoch': 0.83}
{'loss': 0.9456, 'grad_norm': 1.309517741203308, 'learning_rate': 1.773213173853554e-05, 'epoch': 0.83}
{'loss': 1.0716, 'grad_norm': 1.2835121154785156, 'learning_rate': 1.7701613410634365e-05, 'epoch': 0.83}
{'loss': 1.1162, 'grad_norm': 1.1893442869186401, 'learning_rate': 1.767111881719148e-05, 'epoch': 0.83}
{'loss': 0.911, 'grad_norm': 1.4589860439300537, 'learning_rate': 1.7640647967001378e-05, 'epoch': 0.83}
{'loss': 1.0738, 'grad_norm': 1.060823678970337, 'learning_rate': 1.761020086885169e-05, 'epoch': 0.83}
{'loss': 1.1308, 'grad_norm': 1.1539720296859741, 'learning_rate': 1.757977753152328e-05, 'epoch': 0.83}
{'loss': 1.0294, 'grad_norm': 1.0757383108139038, 'learning_rate': 1.7549377963789994e-05, 'epoch': 0.83}
{'loss': 1.1663, 'grad_norm': 1.161207675933838, 'learning_rate': 1.751900217441902e-05, 'epoch': 0.83}
{'loss': 0.9337, 'grad_norm': 1.221757173538208, 'learning_rate': 1.7488650172170496e-05, 'epoch': 0.83}
{'loss': 1.1365, 'grad_norm': 1.1192858219146729, 'learning_rate': 1.7458321965797852e-05, 'epoch': 0.83}
{'loss': 1.0126, 'grad_norm': 1.2030041217803955, 'learning_rate': 1.7428017564047594e-05, 'epoch': 0.83}
{'loss': 0.7047, 'grad_norm': 1.009121060371399, 'learning_rate': 1.7397736975659352e-05, 'epoch': 0.83}
{'loss': 0.8681, 'grad_norm': 1.17791748046875, 'learning_rate': 1.736748020936587e-05, 'epoch': 0.83}
{'loss': 1.0165, 'grad_norm': 1.1043585538864136, 'learning_rate': 1.733724727389313e-05, 'epoch': 0.83}
{'loss': 0.8509, 'grad_norm': 1.0060961246490479, 'learning_rate': 1.730703817796011e-05, 'epoch': 0.83}
{'loss': 0.6099, 'grad_norm': 0.7731919288635254, 'learning_rate': 1.7276852930278985e-05, 'epoch': 0.83}
{'loss': 0.7011, 'grad_norm': 0.9755818843841553, 'learning_rate': 1.7246691539555028e-05, 'epoch': 0.83}
{'loss': 0.8983, 'grad_norm': 1.1422171592712402, 'learning_rate': 1.721655401448662e-05, 'epoch': 0.83}
{'loss': 0.8375, 'grad_norm': 1.2021982669830322, 'learning_rate': 1.7186440363765355e-05, 'epoch': 0.83}
{'loss': 0.8621, 'grad_norm': 1.0109614133834839, 'learning_rate': 1.7156350596075744e-05, 'epoch': 0.83}
{'loss': 0.8199, 'grad_norm': 1.0702345371246338, 'learning_rate': 1.7126284720095653e-05, 'epoch': 0.83}
{'loss': 0.8238, 'grad_norm': 0.8996791839599609, 'learning_rate': 1.7096242744495837e-05, 'epoch': 0.83}
{'loss': 0.9233, 'grad_norm': 1.0200507640838623, 'learning_rate': 1.7066224677940314e-05, 'epoch': 0.83}
{'loss': 1.0394, 'grad_norm': 1.2030937671661377, 'learning_rate': 1.703623052908614e-05, 'epoch': 0.83}
{'loss': 1.1706, 'grad_norm': 1.0797446966171265, 'learning_rate': 1.7006260306583454e-05, 'epoch': 0.83}
{'loss': 0.7653, 'grad_norm': 1.321965217590332, 'learning_rate': 1.697631401907559e-05, 'epoch': 0.83}
{'loss': 0.918, 'grad_norm': 0.9274489283561707, 'learning_rate': 1.6946391675198836e-05, 'epoch': 0.83}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0332, 'grad_norm': 1.0524271726608276, 'learning_rate': 1.69164932835827e-05, 'epoch': 0.83}
{'loss': 0.9913, 'grad_norm': 0.9771437644958496, 'learning_rate': 1.6886618852849724e-05, 'epoch': 0.83}
{'loss': 1.1017, 'grad_norm': 1.294750690460205, 'learning_rate': 1.685676839161554e-05, 'epoch': 0.83}
{'loss': 1.1513, 'grad_norm': 1.067458987236023, 'learning_rate': 1.6826941908488893e-05, 'epoch': 0.83}
{'loss': 1.1526, 'grad_norm': 1.000443696975708, 'learning_rate': 1.6797139412071584e-05, 'epoch': 0.83}
{'loss': 0.953, 'grad_norm': 1.0056217908859253, 'learning_rate': 1.6767360910958495e-05, 'epoch': 0.83}
{'loss': 1.0689, 'grad_norm': 0.96148681640625, 'learning_rate': 1.6737606413737638e-05, 'epoch': 0.83}
{'loss': 0.9981, 'grad_norm': 1.1152526140213013, 'learning_rate': 1.6707875928990058e-05, 'epoch': 0.83}
{'loss': 0.8982, 'grad_norm': 1.332870602607727, 'learning_rate': 1.6678169465289863e-05, 'epoch': 0.83}
{'loss': 0.9352, 'grad_norm': 1.1525341272354126, 'learning_rate': 1.6648487031204273e-05, 'epoch': 0.83}
{'loss': 1.071, 'grad_norm': 1.298027515411377, 'learning_rate': 1.661882863529354e-05, 'epoch': 0.83}
{'loss': 1.0551, 'grad_norm': 1.2060625553131104, 'learning_rate': 1.6589194286111044e-05, 'epoch': 0.83}
{'loss': 1.0152, 'grad_norm': 1.249030590057373, 'learning_rate': 1.6559583992203122e-05, 'epoch': 0.83}
{'loss': 1.0765, 'grad_norm': 1.4055413007736206, 'learning_rate': 1.6529997762109317e-05, 'epoch': 0.83}
{'loss': 1.1134, 'grad_norm': 1.3688186407089233, 'learning_rate': 1.6500435604362075e-05, 'epoch': 0.83}
{'loss': 0.7897, 'grad_norm': 0.9607630372047424, 'learning_rate': 1.647089752748704e-05, 'epoch': 0.83}
{'loss': 0.9502, 'grad_norm': 1.2874759435653687, 'learning_rate': 1.644138354000284e-05, 'epoch': 0.83}
{'loss': 1.1054, 'grad_norm': 1.2071690559387207, 'learning_rate': 1.641189365042115e-05, 'epoch': 0.83}
{'loss': 0.5642, 'grad_norm': 1.1690239906311035, 'learning_rate': 1.638242786724672e-05, 'epoch': 0.83}
{'loss': 1.1679, 'grad_norm': 1.0349704027175903, 'learning_rate': 1.6352986198977325e-05, 'epoch': 0.83}
{'loss': 0.9039, 'grad_norm': 1.2728551626205444, 'learning_rate': 1.632356865410384e-05, 'epoch': 0.83}
{'loss': 0.9713, 'grad_norm': 1.0836261510849, 'learning_rate': 1.629417524111011e-05, 'epoch': 0.83}
{'loss': 1.004, 'grad_norm': 1.0986591577529907, 'learning_rate': 1.626480596847306e-05, 'epoch': 0.83}
{'loss': 1.0525, 'grad_norm': 1.1785119771957397, 'learning_rate': 1.6235460844662642e-05, 'epoch': 0.83}
{'loss': 0.9156, 'grad_norm': 1.0631120204925537, 'learning_rate': 1.620613987814189e-05, 'epoch': 0.83}
{'loss': 0.8855, 'grad_norm': 0.9898456335067749, 'learning_rate': 1.6176843077366754e-05, 'epoch': 0.83}
{'loss': 1.0188, 'grad_norm': 1.4523335695266724, 'learning_rate': 1.6147570450786387e-05, 'epoch': 0.83}
{'loss': 0.9554, 'grad_norm': 1.2476632595062256, 'learning_rate': 1.6118322006842757e-05, 'epoch': 0.84}
{'loss': 1.0196, 'grad_norm': 1.1868467330932617, 'learning_rate': 1.608909775397106e-05, 'epoch': 0.84}
{'loss': 0.7803, 'grad_norm': 1.0564091205596924, 'learning_rate': 1.6059897700599413e-05, 'epoch': 0.84}
{'loss': 0.9243, 'grad_norm': 1.0869907140731812, 'learning_rate': 1.6030721855148932e-05, 'epoch': 0.84}
{'loss': 0.8148, 'grad_norm': 1.6067757606506348, 'learning_rate': 1.600157022603388e-05, 'epoch': 0.84}
{'loss': 0.8005, 'grad_norm': 0.9948728084564209, 'learning_rate': 1.5972442821661337e-05, 'epoch': 0.84}
{'loss': 0.9622, 'grad_norm': 2.272468328475952, 'learning_rate': 1.5943339650431576e-05, 'epoch': 0.84}
{'loss': 1.0466, 'grad_norm': 1.186341643333435, 'learning_rate': 1.5914260720737795e-05, 'epoch': 0.84}
{'loss': 0.9196, 'grad_norm': 1.248827576637268, 'learning_rate': 1.5885206040966215e-05, 'epoch': 0.84}
{'loss': 0.6635, 'grad_norm': 1.13991379737854, 'learning_rate': 1.585617561949606e-05, 'epoch': 0.84}
{'loss': 0.9928, 'grad_norm': 1.0040053129196167, 'learning_rate': 1.5827169464699576e-05, 'epoch': 0.84}
{'loss': 0.9761, 'grad_norm': 1.001140832901001, 'learning_rate': 1.579818758494197e-05, 'epoch': 0.84}
{'loss': 0.9126, 'grad_norm': 1.3489354848861694, 'learning_rate': 1.576922998858151e-05, 'epoch': 0.84}
{'loss': 0.8317, 'grad_norm': 1.1155756711959839, 'learning_rate': 1.5740296683969423e-05, 'epoch': 0.84}
{'loss': 1.0784, 'grad_norm': 1.1970933675765991, 'learning_rate': 1.571138767944993e-05, 'epoch': 0.84}
{'loss': 0.9886, 'grad_norm': 1.6434236764907837, 'learning_rate': 1.5682502983360247e-05, 'epoch': 0.84}
{'loss': 0.9252, 'grad_norm': 0.9755846858024597, 'learning_rate': 1.565364260403055e-05, 'epoch': 0.84}
{'loss': 0.8731, 'grad_norm': 1.052451491355896, 'learning_rate': 1.562480654978411e-05, 'epoch': 0.84}
{'loss': 1.0343, 'grad_norm': 1.1586531400680542, 'learning_rate': 1.559599482893701e-05, 'epoch': 0.84}
{'loss': 0.9337, 'grad_norm': 1.347334384918213, 'learning_rate': 1.5567207449798515e-05, 'epoch': 0.84}
{'loss': 1.1238, 'grad_norm': 0.9973365068435669, 'learning_rate': 1.553844442067066e-05, 'epoch': 0.84}
{'loss': 0.8699, 'grad_norm': 1.1205347776412964, 'learning_rate': 1.5509705749848648e-05, 'epoch': 0.84}
{'loss': 0.9465, 'grad_norm': 0.9635637998580933, 'learning_rate': 1.5480991445620542e-05, 'epoch': 0.84}
{'loss': 0.961, 'grad_norm': 1.1483569145202637, 'learning_rate': 1.5452301516267375e-05, 'epoch': 0.84}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9052, 'grad_norm': 1.2996331453323364, 'learning_rate': 1.5423635970063265e-05, 'epoch': 0.84}
{'loss': 0.7845, 'grad_norm': 0.9535441398620605, 'learning_rate': 1.5394994815275132e-05, 'epoch': 0.84}
{'loss': 0.8164, 'grad_norm': 1.1020262241363525, 'learning_rate': 1.536637806016301e-05, 'epoch': 0.84}
{'loss': 0.8621, 'grad_norm': 1.505370020866394, 'learning_rate': 1.5337785712979802e-05, 'epoch': 0.84}
{'loss': 1.0077, 'grad_norm': 1.1899453401565552, 'learning_rate': 1.530921778197142e-05, 'epoch': 0.84}
{'loss': 0.9249, 'grad_norm': 1.3239846229553223, 'learning_rate': 1.5280674275376693e-05, 'epoch': 0.84}
{'loss': 1.0038, 'grad_norm': 1.089181900024414, 'learning_rate': 1.5252155201427453e-05, 'epoch': 0.84}
{'loss': 1.05, 'grad_norm': 0.9650552868843079, 'learning_rate': 1.5223660568348442e-05, 'epoch': 0.84}
{'loss': 0.8487, 'grad_norm': 1.1719111204147339, 'learning_rate': 1.5195190384357404e-05, 'epoch': 0.84}
{'loss': 0.8601, 'grad_norm': 0.9075127840042114, 'learning_rate': 1.5166744657664988e-05, 'epoch': 0.84}
{'loss': 1.0021, 'grad_norm': 0.9935442805290222, 'learning_rate': 1.5138323396474808e-05, 'epoch': 0.84}
{'loss': 0.8977, 'grad_norm': 1.0455907583236694, 'learning_rate': 1.5109926608983416e-05, 'epoch': 0.84}
{'loss': 1.1509, 'grad_norm': 0.9600610733032227, 'learning_rate': 1.5081554303380285e-05, 'epoch': 0.84}
{'loss': 1.006, 'grad_norm': 1.3810802698135376, 'learning_rate': 1.5053206487847914e-05, 'epoch': 0.84}
{'loss': 1.163, 'grad_norm': 1.2018084526062012, 'learning_rate': 1.50248831705616e-05, 'epoch': 0.84}
{'loss': 1.0348, 'grad_norm': 1.164811372756958, 'learning_rate': 1.4996584359689703e-05, 'epoch': 0.84}
{'loss': 0.9549, 'grad_norm': 1.1002087593078613, 'learning_rate': 1.4968310063393453e-05, 'epoch': 0.84}
{'loss': 0.9541, 'grad_norm': 1.207911491394043, 'learning_rate': 1.4940060289827017e-05, 'epoch': 0.84}
{'loss': 0.7291, 'grad_norm': 1.051234483718872, 'learning_rate': 1.4911835047137501e-05, 'epoch': 0.84}
{'loss': 0.8918, 'grad_norm': 1.3896158933639526, 'learning_rate': 1.4883634343464913e-05, 'epoch': 0.84}
{'loss': 0.8469, 'grad_norm': 1.4429794549942017, 'learning_rate': 1.4855458186942184e-05, 'epoch': 0.84}
{'loss': 0.9519, 'grad_norm': 1.0239993333816528, 'learning_rate': 1.4827306585695234e-05, 'epoch': 0.84}
{'loss': 0.9123, 'grad_norm': 0.9426562190055847, 'learning_rate': 1.4799179547842822e-05, 'epoch': 0.84}
{'loss': 1.1226, 'grad_norm': 1.3555669784545898, 'learning_rate': 1.4771077081496654e-05, 'epoch': 0.84}
{'loss': 0.9679, 'grad_norm': 0.9033395648002625, 'learning_rate': 1.4742999194761342e-05, 'epoch': 0.84}
{'loss': 0.7772, 'grad_norm': 1.1437122821807861, 'learning_rate': 1.4714945895734377e-05, 'epoch': 0.84}
{'loss': 1.0006, 'grad_norm': 1.0899426937103271, 'learning_rate': 1.4686917192506289e-05, 'epoch': 0.84}
{'loss': 0.9939, 'grad_norm': 1.1901720762252808, 'learning_rate': 1.465891309316032e-05, 'epoch': 0.84}
{'loss': 0.9887, 'grad_norm': 1.2688102722167969, 'learning_rate': 1.4630933605772801e-05, 'epoch': 0.84}
{'loss': 1.0516, 'grad_norm': 1.1007661819458008, 'learning_rate': 1.4602978738412798e-05, 'epoch': 0.84}
{'loss': 0.8108, 'grad_norm': 1.271451473236084, 'learning_rate': 1.457504849914242e-05, 'epoch': 0.84}
{'loss': 1.1299, 'grad_norm': 1.1545287370681763, 'learning_rate': 1.4547142896016608e-05, 'epoch': 0.84}
{'loss': 0.9417, 'grad_norm': 1.0462926626205444, 'learning_rate': 1.4519261937083161e-05, 'epoch': 0.84}
{'loss': 0.873, 'grad_norm': 1.2253096103668213, 'learning_rate': 1.4491405630382892e-05, 'epoch': 0.84}
{'loss': 0.8163, 'grad_norm': 1.0212379693984985, 'learning_rate': 1.4463573983949341e-05, 'epoch': 0.84}
{'loss': 0.9335, 'grad_norm': 1.239473581314087, 'learning_rate': 1.4435767005809075e-05, 'epoch': 0.84}
{'loss': 0.688, 'grad_norm': 1.0795267820358276, 'learning_rate': 1.4407984703981469e-05, 'epoch': 0.84}
{'loss': 1.1049, 'grad_norm': 1.1725386381149292, 'learning_rate': 1.4380227086478815e-05, 'epoch': 0.84}
{'loss': 1.0155, 'grad_norm': 0.9925298094749451, 'learning_rate': 1.4352494161306285e-05, 'epoch': 0.84}
{'loss': 0.9109, 'grad_norm': 0.8852919936180115, 'learning_rate': 1.4324785936461892e-05, 'epoch': 0.84}
{'loss': 1.0984, 'grad_norm': 1.1225861310958862, 'learning_rate': 1.429710241993656e-05, 'epoch': 0.84}
{'loss': 0.904, 'grad_norm': 1.8226125240325928, 'learning_rate': 1.4269443619714117e-05, 'epoch': 0.85}
{'loss': 0.8065, 'grad_norm': 1.1439714431762695, 'learning_rate': 1.42418095437712e-05, 'epoch': 0.85}
{'loss': 1.0683, 'grad_norm': 1.4892953634262085, 'learning_rate': 1.4214200200077343e-05, 'epoch': 0.85}
{'loss': 1.0043, 'grad_norm': 1.6474601030349731, 'learning_rate': 1.4186615596594966e-05, 'epoch': 0.85}
{'loss': 0.8596, 'grad_norm': 1.3107497692108154, 'learning_rate': 1.4159055741279281e-05, 'epoch': 0.85}
{'loss': 0.8804, 'grad_norm': 1.0998319387435913, 'learning_rate': 1.4131520642078521e-05, 'epoch': 0.85}
{'loss': 1.0478, 'grad_norm': 1.484804630279541, 'learning_rate': 1.4104010306933557e-05, 'epoch': 0.85}
{'loss': 1.1149, 'grad_norm': 1.1437658071517944, 'learning_rate': 1.4076524743778319e-05, 'epoch': 0.85}
{'loss': 1.1137, 'grad_norm': 0.9328093528747559, 'learning_rate': 1.4049063960539488e-05, 'epoch': 0.85}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7868, 'grad_norm': 1.14031982421875, 'learning_rate': 1.4021627965136618e-05, 'epoch': 0.85}
{'loss': 0.792, 'grad_norm': 1.1575422286987305, 'learning_rate': 1.3994216765482127e-05, 'epoch': 0.85}
{'loss': 1.1337, 'grad_norm': 1.1531782150268555, 'learning_rate': 1.3966830369481232e-05, 'epoch': 0.85}
{'loss': 1.0704, 'grad_norm': 1.0602139234542847, 'learning_rate': 1.3939468785032106e-05, 'epoch': 0.85}
{'loss': 0.9417, 'grad_norm': 1.2622885704040527, 'learning_rate': 1.3912132020025647e-05, 'epoch': 0.85}
{'loss': 0.8838, 'grad_norm': 1.1094638109207153, 'learning_rate': 1.3884820082345673e-05, 'epoch': 0.85}
{'loss': 0.958, 'grad_norm': 1.0053428411483765, 'learning_rate': 1.3857532979868804e-05, 'epoch': 0.85}
{'loss': 0.9709, 'grad_norm': 1.4717230796813965, 'learning_rate': 1.383027072046451e-05, 'epoch': 0.85}
{'loss': 0.9209, 'grad_norm': 1.0697259902954102, 'learning_rate': 1.3803033311995072e-05, 'epoch': 0.85}
{'loss': 0.8228, 'grad_norm': 1.1600594520568848, 'learning_rate': 1.3775820762315694e-05, 'epoch': 0.85}
{'loss': 0.9745, 'grad_norm': 1.0911237001419067, 'learning_rate': 1.3748633079274253e-05, 'epoch': 0.85}
{'loss': 0.66, 'grad_norm': 1.2054094076156616, 'learning_rate': 1.3721470270711623e-05, 'epoch': 0.85}
{'loss': 0.9713, 'grad_norm': 0.959517240524292, 'learning_rate': 1.369433234446139e-05, 'epoch': 0.85}
{'loss': 0.6535, 'grad_norm': 1.0826163291931152, 'learning_rate': 1.366721930835001e-05, 'epoch': 0.85}
{'loss': 1.072, 'grad_norm': 1.1283482313156128, 'learning_rate': 1.3640131170196758e-05, 'epoch': 0.85}
{'loss': 1.018, 'grad_norm': 0.8951137661933899, 'learning_rate': 1.3613067937813685e-05, 'epoch': 0.85}
{'loss': 0.8272, 'grad_norm': 1.1253286600112915, 'learning_rate': 1.358602961900578e-05, 'epoch': 0.85}
{'loss': 0.9854, 'grad_norm': 1.5294584035873413, 'learning_rate': 1.3559016221570663e-05, 'epoch': 0.85}
{'loss': 0.7428, 'grad_norm': 1.1921508312225342, 'learning_rate': 1.3532027753298937e-05, 'epoch': 0.85}
{'loss': 0.8507, 'grad_norm': 1.3257085084915161, 'learning_rate': 1.3505064221973929e-05, 'epoch': 0.85}
{'loss': 1.1126, 'grad_norm': 1.1530953645706177, 'learning_rate': 1.3478125635371786e-05, 'epoch': 0.85}
{'loss': 0.9875, 'grad_norm': 1.0565705299377441, 'learning_rate': 1.345121200126146e-05, 'epoch': 0.85}
{'loss': 0.9096, 'grad_norm': 1.0139869451522827, 'learning_rate': 1.342432332740473e-05, 'epoch': 0.85}
{'loss': 0.832, 'grad_norm': 1.0736708641052246, 'learning_rate': 1.339745962155613e-05, 'epoch': 0.85}
{'loss': 0.865, 'grad_norm': 1.0155142545700073, 'learning_rate': 1.3370620891463071e-05, 'epoch': 0.85}
{'loss': 1.0922, 'grad_norm': 1.1388684511184692, 'learning_rate': 1.3343807144865683e-05, 'epoch': 0.85}
{'loss': 0.7841, 'grad_norm': 1.4064568281173706, 'learning_rate': 1.3317018389496927e-05, 'epoch': 0.85}
{'loss': 0.6897, 'grad_norm': 0.9442187547683716, 'learning_rate': 1.3290254633082555e-05, 'epoch': 0.85}
{'loss': 0.6228, 'grad_norm': 1.0651217699050903, 'learning_rate': 1.326351588334107e-05, 'epoch': 0.85}
{'loss': 1.2416, 'grad_norm': 1.5004136562347412, 'learning_rate': 1.3236802147983873e-05, 'epoch': 0.85}
{'loss': 1.0042, 'grad_norm': 1.4179213047027588, 'learning_rate': 1.3210113434715e-05, 'epoch': 0.85}
{'loss': 0.973, 'grad_norm': 1.2006117105484009, 'learning_rate': 1.3183449751231392e-05, 'epoch': 0.85}
{'loss': 0.9154, 'grad_norm': 0.9646598696708679, 'learning_rate': 1.3156811105222721e-05, 'epoch': 0.85}
{'loss': 0.9385, 'grad_norm': 1.0622931718826294, 'learning_rate': 1.3130197504371434e-05, 'epoch': 0.85}
{'loss': 0.717, 'grad_norm': 1.019562005996704, 'learning_rate': 1.3103608956352764e-05, 'epoch': 0.85}
{'loss': 1.1486, 'grad_norm': 1.0520899295806885, 'learning_rate': 1.3077045468834715e-05, 'epoch': 0.85}
{'loss': 1.0671, 'grad_norm': 1.1746344566345215, 'learning_rate': 1.30505070494781e-05, 'epoch': 0.85}
{'loss': 1.0092, 'grad_norm': 0.9866104125976562, 'learning_rate': 1.3023993705936444e-05, 'epoch': 0.85}
{'loss': 1.0822, 'grad_norm': 1.0442283153533936, 'learning_rate': 1.2997505445856084e-05, 'epoch': 0.85}
{'loss': 1.1903, 'grad_norm': 1.4046069383621216, 'learning_rate': 1.2971042276876088e-05, 'epoch': 0.85}
{'loss': 1.0536, 'grad_norm': 1.0425598621368408, 'learning_rate': 1.294460420662832e-05, 'epoch': 0.85}
{'loss': 0.87, 'grad_norm': 1.1735001802444458, 'learning_rate': 1.2918191242737366e-05, 'epoch': 0.85}
{'loss': 0.6941, 'grad_norm': 1.2537167072296143, 'learning_rate': 1.2891803392820668e-05, 'epoch': 0.85}
{'loss': 1.127, 'grad_norm': 1.2284976243972778, 'learning_rate': 1.2865440664488248e-05, 'epoch': 0.85}
{'loss': 0.956, 'grad_norm': 1.0658929347991943, 'learning_rate': 1.2839103065343083e-05, 'epoch': 0.85}
{'loss': 0.8577, 'grad_norm': 1.3154594898223877, 'learning_rate': 1.2812790602980773e-05, 'epoch': 0.85}
{'loss': 1.0997, 'grad_norm': 1.0938539505004883, 'learning_rate': 1.2786503284989704e-05, 'epoch': 0.85}
{'loss': 1.0645, 'grad_norm': 1.0789867639541626, 'learning_rate': 1.2760241118951011e-05, 'epoch': 0.85}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9478, 'grad_norm': 1.031783938407898, 'learning_rate': 1.2734004112438568e-05, 'epoch': 0.85}
{'loss': 0.8459, 'grad_norm': 1.4347188472747803, 'learning_rate': 1.2707792273019048e-05, 'epoch': 0.85}
{'loss': 1.0885, 'grad_norm': 1.1622647047042847, 'learning_rate': 1.2681605608251745e-05, 'epoch': 0.85}
{'loss': 0.7267, 'grad_norm': 1.0169765949249268, 'learning_rate': 1.2655444125688832e-05, 'epoch': 0.85}
{'loss': 1.1426, 'grad_norm': 1.1302987337112427, 'learning_rate': 1.2629307832875126e-05, 'epoch': 0.85}
{'loss': 1.0012, 'grad_norm': 0.961237907409668, 'learning_rate': 1.260319673734821e-05, 'epoch': 0.85}
{'loss': 0.9154, 'grad_norm': 1.615513563156128, 'learning_rate': 1.2577110846638407e-05, 'epoch': 0.85}
{'loss': 0.8999, 'grad_norm': 1.1379742622375488, 'learning_rate': 1.255105016826873e-05, 'epoch': 0.85}
{'loss': 0.9174, 'grad_norm': 1.459198236465454, 'learning_rate': 1.252501470975499e-05, 'epoch': 0.86}
{'loss': 1.3257, 'grad_norm': 1.065252661705017, 'learning_rate': 1.249900447860568e-05, 'epoch': 0.86}
{'loss': 0.7105, 'grad_norm': 1.0438939332962036, 'learning_rate': 1.2473019482322023e-05, 'epoch': 0.86}
{'loss': 0.9303, 'grad_norm': 1.2070327997207642, 'learning_rate': 1.244705972839797e-05, 'epoch': 0.86}
{'loss': 0.9633, 'grad_norm': 1.1369882822036743, 'learning_rate': 1.2421125224320185e-05, 'epoch': 0.86}
{'loss': 0.7611, 'grad_norm': 1.1796430349349976, 'learning_rate': 1.239521597756802e-05, 'epoch': 0.86}
{'loss': 1.0138, 'grad_norm': 1.2353382110595703, 'learning_rate': 1.2369331995613665e-05, 'epoch': 0.86}
{'loss': 1.0209, 'grad_norm': 1.354356288909912, 'learning_rate': 1.2343473285921825e-05, 'epoch': 0.86}
{'loss': 0.9881, 'grad_norm': 1.1982145309448242, 'learning_rate': 1.2317639855950114e-05, 'epoch': 0.86}
{'loss': 0.9295, 'grad_norm': 0.9935523867607117, 'learning_rate': 1.2291831713148727e-05, 'epoch': 0.86}
{'loss': 1.1329, 'grad_norm': 1.1074497699737549, 'learning_rate': 1.2266048864960634e-05, 'epoch': 0.86}
{'loss': 0.9273, 'grad_norm': 0.967077910900116, 'learning_rate': 1.2240291318821462e-05, 'epoch': 0.86}
{'loss': 1.0055, 'grad_norm': 1.065861463546753, 'learning_rate': 1.2214559082159537e-05, 'epoch': 0.86}
{'loss': 0.7575, 'grad_norm': 1.1132185459136963, 'learning_rate': 1.218885216239598e-05, 'epoch': 0.86}
{'loss': 1.0948, 'grad_norm': 1.032033920288086, 'learning_rate': 1.2163170566944504e-05, 'epoch': 0.86}
{'loss': 0.8696, 'grad_norm': 1.0006550550460815, 'learning_rate': 1.2137514303211561e-05, 'epoch': 0.86}
{'loss': 0.8607, 'grad_norm': 1.1307278871536255, 'learning_rate': 1.21118833785963e-05, 'epoch': 0.86}
{'loss': 0.9354, 'grad_norm': 1.0415819883346558, 'learning_rate': 1.2086277800490554e-05, 'epoch': 0.86}
{'loss': 0.8653, 'grad_norm': 1.550803780555725, 'learning_rate': 1.2060697576278813e-05, 'epoch': 0.86}
{'loss': 0.9981, 'grad_norm': 1.064995527267456, 'learning_rate': 1.2035142713338366e-05, 'epoch': 0.86}
{'loss': 0.7942, 'grad_norm': 1.2867759466171265, 'learning_rate': 1.2009613219039029e-05, 'epoch': 0.86}
{'loss': 0.9903, 'grad_norm': 1.2057044506072998, 'learning_rate': 1.1984109100743446e-05, 'epoch': 0.86}
{'loss': 1.0425, 'grad_norm': 1.2390916347503662, 'learning_rate': 1.1958630365806867e-05, 'epoch': 0.86}
{'loss': 0.931, 'grad_norm': 1.397170901298523, 'learning_rate': 1.1933177021577202e-05, 'epoch': 0.86}
{'loss': 0.8537, 'grad_norm': 1.4746124744415283, 'learning_rate': 1.1907749075395147e-05, 'epoch': 0.86}
{'loss': 1.0552, 'grad_norm': 1.4414997100830078, 'learning_rate': 1.1882346534593902e-05, 'epoch': 0.86}
{'loss': 1.1442, 'grad_norm': 1.1893707513809204, 'learning_rate': 1.1856969406499541e-05, 'epoch': 0.86}
{'loss': 0.7802, 'grad_norm': 1.2124269008636475, 'learning_rate': 1.1831617698430609e-05, 'epoch': 0.86}
{'loss': 0.9888, 'grad_norm': 1.0310763120651245, 'learning_rate': 1.180629141769849e-05, 'epoch': 0.86}
{'loss': 0.9879, 'grad_norm': 0.9101276397705078, 'learning_rate': 1.178099057160712e-05, 'epoch': 0.86}
{'loss': 1.115, 'grad_norm': 1.5021440982818604, 'learning_rate': 1.1755715167453151e-05, 'epoch': 0.86}
{'loss': 1.1459, 'grad_norm': 1.1638708114624023, 'learning_rate': 1.17304652125259e-05, 'epoch': 0.86}
{'loss': 1.1348, 'grad_norm': 1.3777363300323486, 'learning_rate': 1.1705240714107302e-05, 'epoch': 0.86}
{'loss': 0.7567, 'grad_norm': 0.9869682788848877, 'learning_rate': 1.1680041679472021e-05, 'epoch': 0.86}
{'loss': 0.9234, 'grad_norm': 1.110408067703247, 'learning_rate': 1.165486811588732e-05, 'epoch': 0.86}
{'loss': 0.9725, 'grad_norm': 0.985257089138031, 'learning_rate': 1.1629720030613122e-05, 'epoch': 0.86}
{'loss': 0.8614, 'grad_norm': 1.290805459022522, 'learning_rate': 1.160459743090203e-05, 'epoch': 0.86}
{'loss': 1.1237, 'grad_norm': 1.2350584268569946, 'learning_rate': 1.157950032399927e-05, 'epoch': 0.86}
{'loss': 1.0752, 'grad_norm': 1.1062216758728027, 'learning_rate': 1.1554428717142707e-05, 'epoch': 0.86}
{'loss': 0.8607, 'grad_norm': 1.1948670148849487, 'learning_rate': 1.1529382617562944e-05, 'epoch': 0.86}
{'loss': 0.8962, 'grad_norm': 1.0309232473373413, 'learning_rate': 1.1504362032483062e-05, 'epoch': 0.86}
{'loss': 0.7399, 'grad_norm': 1.3737553358078003, 'learning_rate': 1.1479366969118943e-05, 'epoch': 0.86}
{'loss': 0.691, 'grad_norm': 1.1183401346206665, 'learning_rate': 1.1454397434679021e-05, 'epoch': 0.86}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.845, 'grad_norm': 1.0458009243011475, 'learning_rate': 1.142945343636439e-05, 'epoch': 0.86}
{'loss': 0.8903, 'grad_norm': 1.243788242340088, 'learning_rate': 1.1404534981368775e-05, 'epoch': 0.86}
{'loss': 1.0615, 'grad_norm': 1.2006713151931763, 'learning_rate': 1.1379642076878527e-05, 'epoch': 0.86}
{'loss': 1.0479, 'grad_norm': 0.9192127585411072, 'learning_rate': 1.1354774730072692e-05, 'epoch': 0.86}
{'loss': 0.7817, 'grad_norm': 1.1032053232192993, 'learning_rate': 1.1329932948122823e-05, 'epoch': 0.86}
{'loss': 0.7561, 'grad_norm': 1.7705957889556885, 'learning_rate': 1.1305116738193212e-05, 'epoch': 0.86}
{'loss': 0.9076, 'grad_norm': 1.0730364322662354, 'learning_rate': 1.1280326107440731e-05, 'epoch': 0.86}
{'loss': 0.8381, 'grad_norm': 1.016406536102295, 'learning_rate': 1.1255561063014875e-05, 'epoch': 0.86}
{'loss': 0.9119, 'grad_norm': 1.2057417631149292, 'learning_rate': 1.1230821612057751e-05, 'epoch': 0.86}
{'loss': 0.6589, 'grad_norm': 1.087545394897461, 'learning_rate': 1.1206107761704132e-05, 'epoch': 0.86}
{'loss': 0.9874, 'grad_norm': 1.6041145324707031, 'learning_rate': 1.1181419519081316e-05, 'epoch': 0.86}
{'loss': 0.9264, 'grad_norm': 1.160454273223877, 'learning_rate': 1.1156756891309327e-05, 'epoch': 0.86}
{'loss': 1.1562, 'grad_norm': 1.104323148727417, 'learning_rate': 1.1132119885500735e-05, 'epoch': 0.86}
{'loss': 0.8874, 'grad_norm': 1.149577021598816, 'learning_rate': 1.11075085087607e-05, 'epoch': 0.86}
{'loss': 0.5832, 'grad_norm': 1.2245692014694214, 'learning_rate': 1.10829227681871e-05, 'epoch': 0.86}
{'loss': 1.3503, 'grad_norm': 0.8917898535728455, 'learning_rate': 1.1058362670870249e-05, 'epoch': 0.86}
{'loss': 1.1273, 'grad_norm': 1.0217044353485107, 'learning_rate': 1.1033828223893238e-05, 'epoch': 0.86}
{'loss': 0.8986, 'grad_norm': 1.519835352897644, 'learning_rate': 1.1009319434331622e-05, 'epoch': 0.86}
{'loss': 0.9403, 'grad_norm': 1.4175411462783813, 'learning_rate': 1.0984836309253666e-05, 'epoch': 0.86}
{'loss': 0.9299, 'grad_norm': 1.3067407608032227, 'learning_rate': 1.0960378855720177e-05, 'epoch': 0.86}
{'loss': 1.1749, 'grad_norm': 1.22669517993927, 'learning_rate': 1.0935947080784548e-05, 'epoch': 0.86}
{'loss': 0.7149, 'grad_norm': 1.189780354499817, 'learning_rate': 1.0911540991492797e-05, 'epoch': 0.86}
{'loss': 1.1139, 'grad_norm': 0.9384897351264954, 'learning_rate': 1.0887160594883506e-05, 'epoch': 0.87}
{'loss': 1.0366, 'grad_norm': 1.1443005800247192, 'learning_rate': 1.0862805897987894e-05, 'epoch': 0.87}
{'loss': 0.9452, 'grad_norm': 1.1509833335876465, 'learning_rate': 1.083847690782972e-05, 'epoch': 0.87}
{'loss': 1.1594, 'grad_norm': 1.1095554828643799, 'learning_rate': 1.081417363142535e-05, 'epoch': 0.87}
{'loss': 0.8983, 'grad_norm': 1.0764796733856201, 'learning_rate': 1.0789896075783733e-05, 'epoch': 0.87}
{'loss': 0.852, 'grad_norm': 1.1227948665618896, 'learning_rate': 1.0765644247906404e-05, 'epoch': 0.87}
{'loss': 0.8601, 'grad_norm': 1.0706186294555664, 'learning_rate': 1.0741418154787442e-05, 'epoch': 0.87}
{'loss': 0.9505, 'grad_norm': 1.0193150043487549, 'learning_rate': 1.0717217803413604e-05, 'epoch': 0.87}
{'loss': 0.9932, 'grad_norm': 1.152966856956482, 'learning_rate': 1.0693043200764074e-05, 'epoch': 0.87}
{'loss': 1.1996, 'grad_norm': 1.1421502828598022, 'learning_rate': 1.0668894353810743e-05, 'epoch': 0.87}
{'loss': 0.9257, 'grad_norm': 1.1396079063415527, 'learning_rate': 1.0644771269518017e-05, 'epoch': 0.87}
{'loss': 0.5857, 'grad_norm': 1.2393511533737183, 'learning_rate': 1.0620673954842841e-05, 'epoch': 0.87}
{'loss': 0.9177, 'grad_norm': 1.3016911745071411, 'learning_rate': 1.0596602416734835e-05, 'epoch': 0.87}
{'loss': 0.7271, 'grad_norm': 1.33933424949646, 'learning_rate': 1.0572556662136035e-05, 'epoch': 0.87}
{'loss': 0.8285, 'grad_norm': 1.112199068069458, 'learning_rate': 1.0548536697981192e-05, 'epoch': 0.87}
{'loss': 0.667, 'grad_norm': 1.020614743232727, 'learning_rate': 1.0524542531197467e-05, 'epoch': 0.87}
{'loss': 0.6947, 'grad_norm': 1.2116807699203491, 'learning_rate': 1.0500574168704746e-05, 'epoch': 0.87}
{'loss': 1.0364, 'grad_norm': 1.5674647092819214, 'learning_rate': 1.0476631617415333e-05, 'epoch': 0.87}
{'loss': 1.0323, 'grad_norm': 1.1495658159255981, 'learning_rate': 1.045271488423417e-05, 'epoch': 0.87}
{'loss': 0.8452, 'grad_norm': 3.2221648693084717, 'learning_rate': 1.042882397605871e-05, 'epoch': 0.87}
{'loss': 0.9368, 'grad_norm': 1.225663185119629, 'learning_rate': 1.0404958899778993e-05, 'epoch': 0.87}
{'loss': 1.2877, 'grad_norm': 1.122679352760315, 'learning_rate': 1.0381119662277594e-05, 'epoch': 0.87}
{'loss': 0.879, 'grad_norm': 0.9285066723823547, 'learning_rate': 1.0357306270429624e-05, 'epoch': 0.87}
{'loss': 0.8512, 'grad_norm': 1.1406134366989136, 'learning_rate': 1.0333518731102754e-05, 'epoch': 0.87}
{'loss': 0.7493, 'grad_norm': 1.4396334886550903, 'learning_rate': 1.0309757051157176e-05, 'epoch': 0.87}
{'loss': 0.793, 'grad_norm': 1.4806171655654907, 'learning_rate': 1.0286021237445697e-05, 'epoch': 0.87}
{'loss': 1.0109, 'grad_norm': 1.116033673286438, 'learning_rate': 1.0262311296813543e-05, 'epoch': 0.87}
{'loss': 0.9004, 'grad_norm': 1.1125432252883911, 'learning_rate': 1.0238627236098619e-05, 'epoch': 0.87}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9324, 'grad_norm': 1.146191954612732, 'learning_rate': 1.0214969062131208e-05, 'epoch': 0.87}
{'loss': 0.642, 'grad_norm': 1.054764747619629, 'learning_rate': 1.0191336781734285e-05, 'epoch': 0.87}
{'loss': 0.9115, 'grad_norm': 1.4023557901382446, 'learning_rate': 1.0167730401723263e-05, 'epoch': 0.87}
{'loss': 1.0732, 'grad_norm': 1.1984549760818481, 'learning_rate': 1.014414992890611e-05, 'epoch': 0.87}
{'loss': 0.9548, 'grad_norm': 1.0423475503921509, 'learning_rate': 1.0120595370083318e-05, 'epoch': 0.87}
{'loss': 0.8421, 'grad_norm': 1.202222466468811, 'learning_rate': 1.0097066732047878e-05, 'epoch': 0.87}
{'loss': 0.8878, 'grad_norm': 1.0093895196914673, 'learning_rate': 1.007356402158539e-05, 'epoch': 0.87}
{'loss': 0.9593, 'grad_norm': 1.4629842042922974, 'learning_rate': 1.0050087245473906e-05, 'epoch': 0.87}
{'loss': 1.2296, 'grad_norm': 1.2798328399658203, 'learning_rate': 1.0026636410484003e-05, 'epoch': 0.87}
{'loss': 0.9974, 'grad_norm': 1.3104236125946045, 'learning_rate': 1.0003211523378796e-05, 'epoch': 0.87}
{'loss': 1.4634, 'grad_norm': 1.3696500062942505, 'learning_rate': 9.979812590913906e-06, 'epoch': 0.87}
{'loss': 1.0606, 'grad_norm': 1.2081801891326904, 'learning_rate': 9.956439619837454e-06, 'epoch': 0.87}
{'loss': 0.8863, 'grad_norm': 0.9459824562072754, 'learning_rate': 9.93309261689015e-06, 'epoch': 0.87}
{'loss': 0.8345, 'grad_norm': 1.2254338264465332, 'learning_rate': 9.909771588805072e-06, 'epoch': 0.87}
{'loss': 0.986, 'grad_norm': 1.0395933389663696, 'learning_rate': 9.886476542307966e-06, 'epoch': 0.87}
{'loss': 0.7989, 'grad_norm': 1.321309208869934, 'learning_rate': 9.863207484116988e-06, 'epoch': 0.87}
{'loss': 1.1477, 'grad_norm': 0.898141622543335, 'learning_rate': 9.839964420942793e-06, 'epoch': 0.87}
{'loss': 0.7534, 'grad_norm': 1.220505714416504, 'learning_rate': 9.816747359488632e-06, 'epoch': 0.87}
{'loss': 0.9332, 'grad_norm': 0.900901198387146, 'learning_rate': 9.793556306450125e-06, 'epoch': 0.87}
{'loss': 0.9228, 'grad_norm': 1.0628100633621216, 'learning_rate': 9.770391268515522e-06, 'epoch': 0.87}
{'loss': 0.8028, 'grad_norm': 1.4598089456558228, 'learning_rate': 9.747252252365446e-06, 'epoch': 0.87}
{'loss': 0.879, 'grad_norm': 1.0241637229919434, 'learning_rate': 9.724139264673116e-06, 'epoch': 0.87}
{'loss': 0.8155, 'grad_norm': 1.0187345743179321, 'learning_rate': 9.701052312104209e-06, 'epoch': 0.87}
{'loss': 1.2349, 'grad_norm': 1.1134037971496582, 'learning_rate': 9.677991401316877e-06, 'epoch': 0.87}
{'loss': 0.9497, 'grad_norm': 1.0612614154815674, 'learning_rate': 9.65495653896179e-06, 'epoch': 0.87}
{'loss': 0.6602, 'grad_norm': 1.154441237449646, 'learning_rate': 9.631947731682056e-06, 'epoch': 0.87}
{'loss': 1.0769, 'grad_norm': 1.8712042570114136, 'learning_rate': 9.608964986113345e-06, 'epoch': 0.87}
{'loss': 0.8813, 'grad_norm': 1.1210070848464966, 'learning_rate': 9.586008308883754e-06, 'epoch': 0.87}
{'loss': 0.985, 'grad_norm': 1.3052176237106323, 'learning_rate': 9.563077706613877e-06, 'epoch': 0.87}
{'loss': 0.9096, 'grad_norm': 1.0446738004684448, 'learning_rate': 9.540173185916778e-06, 'epoch': 0.87}
{'loss': 0.9978, 'grad_norm': 0.859203577041626, 'learning_rate': 9.517294753398064e-06, 'epoch': 0.87}
{'loss': 0.729, 'grad_norm': 1.4366676807403564, 'learning_rate': 9.49444241565568e-06, 'epoch': 0.87}
{'loss': 1.0812, 'grad_norm': 1.4338419437408447, 'learning_rate': 9.471616179280207e-06, 'epoch': 0.87}
{'loss': 0.8082, 'grad_norm': 1.243969202041626, 'learning_rate': 9.44881605085456e-06, 'epoch': 0.87}
{'loss': 1.1128, 'grad_norm': 1.0890966653823853, 'learning_rate': 9.42604203695423e-06, 'epoch': 0.87}
{'loss': 0.927, 'grad_norm': 1.1350685358047485, 'learning_rate': 9.403294144147124e-06, 'epoch': 0.87}
{'loss': 0.867, 'grad_norm': 1.0661227703094482, 'learning_rate': 9.38057237899359e-06, 'epoch': 0.87}
{'loss': 0.9276, 'grad_norm': 0.9550747871398926, 'learning_rate': 9.357876748046546e-06, 'epoch': 0.88}
{'loss': 0.9292, 'grad_norm': 1.3607085943222046, 'learning_rate': 9.335207257851231e-06, 'epoch': 0.88}
{'loss': 1.1184, 'grad_norm': 1.1857830286026, 'learning_rate': 9.31256391494546e-06, 'epoch': 0.88}
{'loss': 1.028, 'grad_norm': 1.0893208980560303, 'learning_rate': 9.289946725859444e-06, 'epoch': 0.88}
{'loss': 1.0788, 'grad_norm': 1.0633318424224854, 'learning_rate': 9.267355697115887e-06, 'epoch': 0.88}
{'loss': 1.1342, 'grad_norm': 1.1103020906448364, 'learning_rate': 9.244790835229922e-06, 'epoch': 0.88}
{'loss': 1.0088, 'grad_norm': 0.9594208598136902, 'learning_rate': 9.222252146709142e-06, 'epoch': 0.88}
{'loss': 1.0337, 'grad_norm': 0.9831043481826782, 'learning_rate': 9.199739638053595e-06, 'epoch': 0.88}
{'loss': 0.9831, 'grad_norm': 1.1327658891677856, 'learning_rate': 9.177253315755796e-06, 'epoch': 0.88}
{'loss': 0.8857, 'grad_norm': 1.1653777360916138, 'learning_rate': 9.154793186300692e-06, 'epoch': 0.88}
{'loss': 0.7738, 'grad_norm': 0.8367586731910706, 'learning_rate': 9.132359256165667e-06, 'epoch': 0.88}
{'loss': 0.8543, 'grad_norm': 1.0199999809265137, 'learning_rate': 9.10995153182056e-06, 'epoch': 0.88}
{'loss': 1.0328, 'grad_norm': 1.0458309650421143, 'learning_rate': 9.08757001972762e-06, 'epoch': 0.88}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6536, 'grad_norm': 1.0449122190475464, 'learning_rate': 9.065214726341653e-06, 'epoch': 0.88}
{'loss': 1.1616, 'grad_norm': 1.7162271738052368, 'learning_rate': 9.042885658109712e-06, 'epoch': 0.88}
{'loss': 0.9759, 'grad_norm': 1.2964602708816528, 'learning_rate': 9.020582821471479e-06, 'epoch': 0.88}
{'loss': 0.8501, 'grad_norm': 1.1922389268875122, 'learning_rate': 8.998306222858922e-06, 'epoch': 0.88}
{'loss': 0.8262, 'grad_norm': 1.0362526178359985, 'learning_rate': 8.976055868696542e-06, 'epoch': 0.88}
{'loss': 0.7302, 'grad_norm': 1.2913793325424194, 'learning_rate': 8.953831765401233e-06, 'epoch': 0.88}
{'loss': 0.9779, 'grad_norm': 1.3277837038040161, 'learning_rate': 8.931633919382298e-06, 'epoch': 0.88}
{'loss': 0.9766, 'grad_norm': 1.0297210216522217, 'learning_rate': 8.909462337041507e-06, 'epoch': 0.88}
{'loss': 1.118, 'grad_norm': 1.3521641492843628, 'learning_rate': 8.887317024773012e-06, 'epoch': 0.88}
{'loss': 0.8215, 'grad_norm': 1.2236204147338867, 'learning_rate': 8.865197988963458e-06, 'epoch': 0.88}
{'loss': 1.0448, 'grad_norm': 1.1636102199554443, 'learning_rate': 8.843105235991833e-06, 'epoch': 0.88}
{'loss': 1.0328, 'grad_norm': 1.3152557611465454, 'learning_rate': 8.821038772229595e-06, 'epoch': 0.88}
{'loss': 0.6322, 'grad_norm': 1.9715124368667603, 'learning_rate': 8.798998604040609e-06, 'epoch': 0.88}
{'loss': 0.8959, 'grad_norm': 1.2686399221420288, 'learning_rate': 8.776984737781135e-06, 'epoch': 0.88}
{'loss': 0.7675, 'grad_norm': 0.9837713241577148, 'learning_rate': 8.754997179799874e-06, 'epoch': 0.88}
{'loss': 0.8322, 'grad_norm': 0.933929443359375, 'learning_rate': 8.733035936437961e-06, 'epoch': 0.88}
{'loss': 1.0108, 'grad_norm': 1.049421787261963, 'learning_rate': 8.711101014028856e-06, 'epoch': 0.88}
{'loss': 1.0457, 'grad_norm': 0.8733292818069458, 'learning_rate': 8.689192418898529e-06, 'epoch': 0.88}
{'loss': 0.9343, 'grad_norm': 1.0875052213668823, 'learning_rate': 8.667310157365304e-06, 'epoch': 0.88}
{'loss': 0.6764, 'grad_norm': 2.2117183208465576, 'learning_rate': 8.645454235739903e-06, 'epoch': 0.88}
{'loss': 0.7862, 'grad_norm': 1.410136103630066, 'learning_rate': 8.623624660325514e-06, 'epoch': 0.88}
{'loss': 1.1251, 'grad_norm': 1.2035112380981445, 'learning_rate': 8.601821437417613e-06, 'epoch': 0.88}
{'loss': 0.8087, 'grad_norm': 1.13240647315979, 'learning_rate': 8.580044573304202e-06, 'epoch': 0.88}
{'loss': 1.1072, 'grad_norm': 1.0369386672973633, 'learning_rate': 8.558294074265605e-06, 'epoch': 0.88}
{'loss': 0.8952, 'grad_norm': 1.072494387626648, 'learning_rate': 8.536569946574546e-06, 'epoch': 0.88}
{'loss': 0.9831, 'grad_norm': 0.9937965869903564, 'learning_rate': 8.514872196496183e-06, 'epoch': 0.88}
{'loss': 0.9239, 'grad_norm': 1.1985867023468018, 'learning_rate': 8.493200830288028e-06, 'epoch': 0.88}
{'loss': 1.061, 'grad_norm': 1.171795129776001, 'learning_rate': 8.471555854199987e-06, 'epoch': 0.88}
{'loss': 0.998, 'grad_norm': 1.1341854333877563, 'learning_rate': 8.449937274474396e-06, 'epoch': 0.88}
{'loss': 0.8029, 'grad_norm': 1.1160413026809692, 'learning_rate': 8.428345097345936e-06, 'epoch': 0.88}
{'loss': 1.0608, 'grad_norm': 1.0698668956756592, 'learning_rate': 8.406779329041681e-06, 'epoch': 0.88}
{'loss': 0.7955, 'grad_norm': 1.10454523563385, 'learning_rate': 8.385239975781112e-06, 'epoch': 0.88}
{'loss': 0.8329, 'grad_norm': 1.106615424156189, 'learning_rate': 8.363727043776038e-06, 'epoch': 0.88}
{'loss': 0.8007, 'grad_norm': 1.0446271896362305, 'learning_rate': 8.34224053923074e-06, 'epoch': 0.88}
{'loss': 1.082, 'grad_norm': 0.9987096190452576, 'learning_rate': 8.32078046834176e-06, 'epoch': 0.88}
{'loss': 0.9518, 'grad_norm': 1.2168325185775757, 'learning_rate': 8.29934683729814e-06, 'epoch': 0.88}
{'loss': 0.6506, 'grad_norm': 1.0130443572998047, 'learning_rate': 8.277939652281186e-06, 'epoch': 0.88}
{'loss': 0.9602, 'grad_norm': 1.1674962043762207, 'learning_rate': 8.256558919464652e-06, 'epoch': 0.88}
{'loss': 1.0646, 'grad_norm': 1.0983664989471436, 'learning_rate': 8.235204645014639e-06, 'epoch': 0.88}
{'loss': 0.9772, 'grad_norm': 1.1776043176651, 'learning_rate': 8.213876835089607e-06, 'epoch': 0.88}
{'loss': 1.0749, 'grad_norm': 1.2486402988433838, 'learning_rate': 8.192575495840404e-06, 'epoch': 0.88}
{'loss': 0.7598, 'grad_norm': 1.0267878770828247, 'learning_rate': 8.171300633410217e-06, 'epoch': 0.88}
{'loss': 1.0135, 'grad_norm': 1.144586443901062, 'learning_rate': 8.150052253934637e-06, 'epoch': 0.88}
{'loss': 0.9101, 'grad_norm': 1.2151384353637695, 'learning_rate': 8.128830363541574e-06, 'epoch': 0.88}
{'loss': 1.1203, 'grad_norm': 1.1606438159942627, 'learning_rate': 8.107634968351342e-06, 'epoch': 0.88}
{'loss': 0.7235, 'grad_norm': 1.4602214097976685, 'learning_rate': 8.086466074476563e-06, 'epoch': 0.88}
{'loss': 1.2145, 'grad_norm': 1.053605318069458, 'learning_rate': 8.065323688022264e-06, 'epoch': 0.88}
{'loss': 0.8627, 'grad_norm': 1.2899932861328125, 'learning_rate': 8.044207815085791e-06, 'epoch': 0.88}
{'loss': 1.1601, 'grad_norm': 1.0617412328720093, 'learning_rate': 8.023118461756872e-06, 'epoch': 0.88}
{'loss': 0.9446, 'grad_norm': 1.9074957370758057, 'learning_rate': 8.002055634117578e-06, 'epoch': 0.88}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7518, 'grad_norm': 1.3941659927368164, 'learning_rate': 7.981019338242323e-06, 'epoch': 0.88}
{'loss': 0.8768, 'grad_norm': 1.1443393230438232, 'learning_rate': 7.96000958019788e-06, 'epoch': 0.88}
{'loss': 0.9525, 'grad_norm': 1.0532686710357666, 'learning_rate': 7.939026366043322e-06, 'epoch': 0.89}
{'loss': 1.1227, 'grad_norm': 1.1709613800048828, 'learning_rate': 7.918069701830189e-06, 'epoch': 0.89}
{'loss': 1.0266, 'grad_norm': 1.3363608121871948, 'learning_rate': 7.897139593602188e-06, 'epoch': 0.89}
{'loss': 0.8258, 'grad_norm': 1.2222012281417847, 'learning_rate': 7.876236047395525e-06, 'epoch': 0.89}
{'loss': 0.8598, 'grad_norm': 1.1518453359603882, 'learning_rate': 7.855359069238666e-06, 'epoch': 0.89}
{'loss': 0.8109, 'grad_norm': 1.3319147825241089, 'learning_rate': 7.834508665152418e-06, 'epoch': 0.89}
{'loss': 0.6163, 'grad_norm': 1.5455563068389893, 'learning_rate': 7.81368484114996e-06, 'epoch': 0.89}
{'loss': 0.9419, 'grad_norm': 1.3936527967453003, 'learning_rate': 7.792887603236754e-06, 'epoch': 0.89}
{'loss': 0.933, 'grad_norm': 1.0146818161010742, 'learning_rate': 7.772116957410636e-06, 'epoch': 0.89}
{'loss': 0.8492, 'grad_norm': 1.5431631803512573, 'learning_rate': 7.751372909661769e-06, 'epoch': 0.89}
{'loss': 1.128, 'grad_norm': 1.1111067533493042, 'learning_rate': 7.730655465972635e-06, 'epoch': 0.89}
{'loss': 1.1304, 'grad_norm': 1.0866882801055908, 'learning_rate': 7.709964632318034e-06, 'epoch': 0.89}
{'loss': 1.0632, 'grad_norm': 1.3633524179458618, 'learning_rate': 7.689300414665124e-06, 'epoch': 0.89}
{'loss': 0.6257, 'grad_norm': 1.1422185897827148, 'learning_rate': 7.668662818973315e-06, 'epoch': 0.89}
{'loss': 0.8731, 'grad_norm': 1.045566201210022, 'learning_rate': 7.64805185119447e-06, 'epoch': 0.89}
{'loss': 1.0212, 'grad_norm': 1.1019887924194336, 'learning_rate': 7.6274675172726125e-06, 'epoch': 0.89}
{'loss': 1.1432, 'grad_norm': 1.0721583366394043, 'learning_rate': 7.606909823144237e-06, 'epoch': 0.89}
{'loss': 0.7383, 'grad_norm': 1.1315889358520508, 'learning_rate': 7.586378774738012e-06, 'epoch': 0.89}
{'loss': 0.9419, 'grad_norm': 1.0356929302215576, 'learning_rate': 7.565874377975046e-06, 'epoch': 0.89}
{'loss': 1.1993, 'grad_norm': 1.1941951513290405, 'learning_rate': 7.545396638768698e-06, 'epoch': 0.89}
{'loss': 0.9843, 'grad_norm': 1.0743850469589233, 'learning_rate': 7.5249455630246215e-06, 'epoch': 0.89}
{'loss': 0.7638, 'grad_norm': 1.2962441444396973, 'learning_rate': 7.504521156640853e-06, 'epoch': 0.89}
{'loss': 0.7589, 'grad_norm': 1.2592177391052246, 'learning_rate': 7.4841234255076495e-06, 'epoch': 0.89}
{'loss': 0.8818, 'grad_norm': 1.1080530881881714, 'learning_rate': 7.463752375507649e-06, 'epoch': 0.89}
{'loss': 0.9663, 'grad_norm': 1.2219915390014648, 'learning_rate': 7.443408012515751e-06, 'epoch': 0.89}
{'loss': 0.906, 'grad_norm': 1.0958017110824585, 'learning_rate': 7.4230903423991745e-06, 'epoch': 0.89}
{'loss': 0.6891, 'grad_norm': 1.125210165977478, 'learning_rate': 7.402799371017444e-06, 'epoch': 0.89}
{'loss': 1.0373, 'grad_norm': 1.2113436460494995, 'learning_rate': 7.382535104222366e-06, 'epoch': 0.89}
{'loss': 1.1284, 'grad_norm': 1.2269784212112427, 'learning_rate': 7.362297547858044e-06, 'epoch': 0.89}
{'loss': 1.011, 'grad_norm': 1.077460765838623, 'learning_rate': 7.342086707760931e-06, 'epoch': 0.89}
{'loss': 0.9428, 'grad_norm': 1.1852880716323853, 'learning_rate': 7.321902589759711e-06, 'epoch': 0.89}
{'loss': 0.9716, 'grad_norm': 1.0966638326644897, 'learning_rate': 7.301745199675392e-06, 'epoch': 0.89}
{'loss': 1.128, 'grad_norm': 1.1345572471618652, 'learning_rate': 7.281614543321269e-06, 'epoch': 0.89}
{'loss': 1.2247, 'grad_norm': 1.1338495016098022, 'learning_rate': 7.26151062650291e-06, 'epoch': 0.89}
{'loss': 0.7696, 'grad_norm': 1.3478566408157349, 'learning_rate': 7.241433455018231e-06, 'epoch': 0.89}
{'loss': 0.9833, 'grad_norm': 1.2088613510131836, 'learning_rate': 7.221383034657326e-06, 'epoch': 0.89}
{'loss': 0.9431, 'grad_norm': 1.5758789777755737, 'learning_rate': 7.201359371202699e-06, 'epoch': 0.89}
{'loss': 0.9823, 'grad_norm': 1.079525113105774, 'learning_rate': 7.1813624704290535e-06, 'epoch': 0.89}
{'loss': 1.02, 'grad_norm': 1.3439031839370728, 'learning_rate': 7.161392338103401e-06, 'epoch': 0.89}
{'loss': 0.9621, 'grad_norm': 1.269959807395935, 'learning_rate': 7.141448979985032e-06, 'epoch': 0.89}
{'loss': 1.1385, 'grad_norm': 1.087769627571106, 'learning_rate': 7.1215324018255145e-06, 'epoch': 0.89}
{'loss': 0.8242, 'grad_norm': 1.1653228998184204, 'learning_rate': 7.101642609368675e-06, 'epoch': 0.89}
{'loss': 0.9866, 'grad_norm': 1.4851765632629395, 'learning_rate': 7.08177960835068e-06, 'epoch': 0.89}
{'loss': 0.8717, 'grad_norm': 1.3277312517166138, 'learning_rate': 7.061943404499882e-06, 'epoch': 0.89}
{'loss': 0.7959, 'grad_norm': 1.6574769020080566, 'learning_rate': 7.042134003536971e-06, 'epoch': 0.89}
{'loss': 1.1236, 'grad_norm': 1.0044777393341064, 'learning_rate': 7.022351411174866e-06, 'epoch': 0.89}
{'loss': 0.9503, 'grad_norm': 1.4393305778503418, 'learning_rate': 7.0025956331187695e-06, 'epoch': 0.89}
{'loss': 0.9483, 'grad_norm': 1.225796103477478, 'learning_rate': 6.9828666750661795e-06, 'epoch': 0.89}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.9518, 'grad_norm': 1.157638669013977, 'learning_rate': 6.9631645427067906e-06, 'epoch': 0.89}
{'loss': 0.9261, 'grad_norm': 1.1812070608139038, 'learning_rate': 6.943489241722645e-06, 'epoch': 0.89}
{'loss': 0.9924, 'grad_norm': 1.2488532066345215, 'learning_rate': 6.923840777787982e-06, 'epoch': 0.89}
{'loss': 1.0183, 'grad_norm': 1.2033506631851196, 'learning_rate': 6.904219156569325e-06, 'epoch': 0.89}
{'loss': 0.9929, 'grad_norm': 1.1856790781021118, 'learning_rate': 6.884624383725469e-06, 'epoch': 0.89}
{'loss': 0.8275, 'grad_norm': 1.1525142192840576, 'learning_rate': 6.8650564649074156e-06, 'epoch': 0.89}
{'loss': 1.0126, 'grad_norm': 1.1236276626586914, 'learning_rate': 6.845515405758518e-06, 'epoch': 0.89}
{'loss': 0.9102, 'grad_norm': 1.0148578882217407, 'learning_rate': 6.826001211914257e-06, 'epoch': 0.89}
{'loss': 1.012, 'grad_norm': 1.2431554794311523, 'learning_rate': 6.806513889002486e-06, 'epoch': 0.89}
{'loss': 0.8434, 'grad_norm': 1.1790417432785034, 'learning_rate': 6.787053442643232e-06, 'epoch': 0.89}
{'loss': 0.8648, 'grad_norm': 1.0475292205810547, 'learning_rate': 6.767619878448783e-06, 'epoch': 0.89}
{'loss': 0.9511, 'grad_norm': 1.1839642524719238, 'learning_rate': 6.748213202023712e-06, 'epoch': 0.89}
{'loss': 1.1422, 'grad_norm': 1.0044139623641968, 'learning_rate': 6.728833418964797e-06, 'epoch': 0.89}
{'loss': 0.6632, 'grad_norm': 1.3878309726715088, 'learning_rate': 6.709480534861046e-06, 'epoch': 0.89}
{'loss': 0.9047, 'grad_norm': 1.0151768922805786, 'learning_rate': 6.690154555293793e-06, 'epoch': 0.89}
{'loss': 0.8496, 'grad_norm': 1.3514323234558105, 'learning_rate': 6.670855485836525e-06, 'epoch': 0.89}
{'loss': 0.8041, 'grad_norm': 1.160155177116394, 'learning_rate': 6.6515833320550115e-06, 'epoch': 0.89}
{'loss': 0.964, 'grad_norm': 1.3997385501861572, 'learning_rate': 6.63233809950724e-06, 'epoch': 0.9}
{'loss': 1.0052, 'grad_norm': 1.1878083944320679, 'learning_rate': 6.613119793743428e-06, 'epoch': 0.9}
{'loss': 0.9812, 'grad_norm': 1.2757055759429932, 'learning_rate': 6.593928420306084e-06, 'epoch': 0.9}
{'loss': 1.1584, 'grad_norm': 1.317713975906372, 'learning_rate': 6.5747639847298594e-06, 'epoch': 0.9}
{'loss': 0.8456, 'grad_norm': 1.5490751266479492, 'learning_rate': 6.555626492541744e-06, 'epoch': 0.9}
{'loss': 0.7378, 'grad_norm': 1.565751075744629, 'learning_rate': 6.5365159492608355e-06, 'epoch': 0.9}
{'loss': 0.7991, 'grad_norm': 1.7395312786102295, 'learning_rate': 6.517432360398556e-06, 'epoch': 0.9}
{'loss': 0.7428, 'grad_norm': 1.3553575277328491, 'learning_rate': 6.498375731458528e-06, 'epoch': 0.9}
{'loss': 0.9194, 'grad_norm': 1.1241788864135742, 'learning_rate': 6.479346067936554e-06, 'epoch': 0.9}
{'loss': 1.0208, 'grad_norm': 1.4227098226547241, 'learning_rate': 6.4603433753207435e-06, 'epoch': 0.9}
{'loss': 0.951, 'grad_norm': 1.1799005270004272, 'learning_rate': 6.441367659091357e-06, 'epoch': 0.9}
{'loss': 1.3732, 'grad_norm': 1.1511813402175903, 'learning_rate': 6.422418924720907e-06, 'epoch': 0.9}
{'loss': 0.7502, 'grad_norm': 1.188469648361206, 'learning_rate': 6.403497177674111e-06, 'epoch': 0.9}
{'loss': 0.7878, 'grad_norm': 1.0293818712234497, 'learning_rate': 6.384602423407904e-06, 'epoch': 0.9}
{'loss': 1.174, 'grad_norm': 0.9490359425544739, 'learning_rate': 6.365734667371448e-06, 'epoch': 0.9}
{'loss': 0.6168, 'grad_norm': 1.1465595960617065, 'learning_rate': 6.346893915006136e-06, 'epoch': 0.9}
{'loss': 0.9327, 'grad_norm': 1.0874700546264648, 'learning_rate': 6.32808017174551e-06, 'epoch': 0.9}
{'loss': 0.9434, 'grad_norm': 1.1834716796875, 'learning_rate': 6.309293443015385e-06, 'epoch': 0.9}
{'loss': 0.9964, 'grad_norm': 1.1547821760177612, 'learning_rate': 6.2905337342337615e-06, 'epoch': 0.9}
{'loss': 0.8025, 'grad_norm': 1.0656253099441528, 'learning_rate': 6.2718010508108545e-06, 'epoch': 0.9}
{'loss': 1.0995, 'grad_norm': 1.1558414697647095, 'learning_rate': 6.253095398149067e-06, 'epoch': 0.9}
{'loss': 1.0197, 'grad_norm': 1.4579616785049438, 'learning_rate': 6.2344167816430155e-06, 'epoch': 0.9}
{'loss': 0.9791, 'grad_norm': 1.5322822332382202, 'learning_rate': 6.215765206679569e-06, 'epoch': 0.9}
{'loss': 0.8436, 'grad_norm': 1.0913200378417969, 'learning_rate': 6.197140678637669e-06, 'epoch': 0.9}
{'loss': 0.7706, 'grad_norm': 1.1811589002609253, 'learning_rate': 6.178543202888621e-06, 'epoch': 0.9}
{'loss': 1.0049, 'grad_norm': 1.1450248956680298, 'learning_rate': 6.1599727847957975e-06, 'epoch': 0.9}
{'loss': 0.9326, 'grad_norm': 1.0149614810943604, 'learning_rate': 6.14142942971484e-06, 'epoch': 0.9}
{'loss': 1.0316, 'grad_norm': 1.3148999214172363, 'learning_rate': 6.1229131429935475e-06, 'epoch': 0.9}
{'loss': 0.8388, 'grad_norm': 1.0631979703903198, 'learning_rate': 6.104423929971948e-06, 'epoch': 0.9}
{'loss': 0.773, 'grad_norm': 1.2225978374481201, 'learning_rate': 6.085961795982209e-06, 'epoch': 0.9}
{'loss': 0.9197, 'grad_norm': 1.1467907428741455, 'learning_rate': 6.067526746348751e-06, 'epoch': 0.9}
{'loss': 1.0174, 'grad_norm': 1.3696279525756836, 'learning_rate': 6.049118786388152e-06, 'epoch': 0.9}
{'loss': 1.0211, 'grad_norm': 1.4070861339569092, 'learning_rate': 6.030737921409169e-06, 'epoch': 0.9}
{'loss': 1.136, 'grad_norm': 1.4155131578445435, 'learning_rate': 6.012384156712758e-06, 'epoch': 0.9}
{'loss': 0.858, 'grad_norm': 1.0268648862838745, 'learning_rate': 5.994057497592031e-06, 'epoch': 0.9}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1875, 'grad_norm': 1.0937557220458984, 'learning_rate': 5.975757949332373e-06, 'epoch': 0.9}
{'loss': 1.1024, 'grad_norm': 1.0624752044677734, 'learning_rate': 5.9574855172112145e-06, 'epoch': 0.9}
{'loss': 0.9932, 'grad_norm': 0.9762148857116699, 'learning_rate': 5.939240206498287e-06, 'epoch': 0.9}
{'loss': 1.0798, 'grad_norm': 1.0186761617660522, 'learning_rate': 5.921022022455424e-06, 'epoch': 0.9}
{'loss': 0.9624, 'grad_norm': 1.470807671546936, 'learning_rate': 5.90283097033667e-06, 'epoch': 0.9}
{'loss': 0.9293, 'grad_norm': 1.0770007371902466, 'learning_rate': 5.884667055388227e-06, 'epoch': 0.9}
{'loss': 1.0274, 'grad_norm': 1.3447465896606445, 'learning_rate': 5.866530282848492e-06, 'epoch': 0.9}
{'loss': 0.9141, 'grad_norm': 1.229238748550415, 'learning_rate': 5.8484206579480396e-06, 'epoch': 0.9}
{'loss': 0.9338, 'grad_norm': 1.0321593284606934, 'learning_rate': 5.8303381859095455e-06, 'epoch': 0.9}
{'loss': 0.9835, 'grad_norm': 1.1880024671554565, 'learning_rate': 5.812282871947961e-06, 'epoch': 0.9}
{'loss': 1.0127, 'grad_norm': 1.4035611152648926, 'learning_rate': 5.7942547212703315e-06, 'epoch': 0.9}
{'loss': 0.9298, 'grad_norm': 1.006584644317627, 'learning_rate': 5.7762537390758875e-06, 'epoch': 0.9}
{'loss': 1.0415, 'grad_norm': 1.432984709739685, 'learning_rate': 5.758279930556021e-06, 'epoch': 0.9}
{'loss': 0.7893, 'grad_norm': 1.021497368812561, 'learning_rate': 5.7403333008943186e-06, 'epoch': 0.9}
{'loss': 0.9241, 'grad_norm': 1.2887712717056274, 'learning_rate': 5.722413855266451e-06, 'epoch': 0.9}
{'loss': 0.6126, 'grad_norm': 1.030329942703247, 'learning_rate': 5.70452159884034e-06, 'epoch': 0.9}
{'loss': 1.066, 'grad_norm': 1.133481502532959, 'learning_rate': 5.686656536776025e-06, 'epoch': 0.9}
{'loss': 0.6285, 'grad_norm': 1.1178944110870361, 'learning_rate': 5.668818674225685e-06, 'epoch': 0.9}
{'loss': 0.9378, 'grad_norm': 1.0421605110168457, 'learning_rate': 5.651008016333703e-06, 'epoch': 0.9}
{'loss': 1.1177, 'grad_norm': 0.9261083006858826, 'learning_rate': 5.633224568236539e-06, 'epoch': 0.9}
{'loss': 1.0545, 'grad_norm': 1.1673027276992798, 'learning_rate': 5.615468335062912e-06, 'epoch': 0.9}
{'loss': 0.9439, 'grad_norm': 1.050140619277954, 'learning_rate': 5.5977393219335815e-06, 'epoch': 0.9}
{'loss': 0.8895, 'grad_norm': 1.323180079460144, 'learning_rate': 5.580037533961546e-06, 'epoch': 0.9}
{'loss': 0.9584, 'grad_norm': 1.0550457239151, 'learning_rate': 5.562362976251901e-06, 'epoch': 0.9}
{'loss': 0.9959, 'grad_norm': 1.4486290216445923, 'learning_rate': 5.5447156539019e-06, 'epoch': 0.9}
{'loss': 0.9515, 'grad_norm': 0.9832799434661865, 'learning_rate': 5.527095572000962e-06, 'epoch': 0.9}
{'loss': 1.0233, 'grad_norm': 1.1655936241149902, 'learning_rate': 5.509502735630601e-06, 'epoch': 0.9}
{'loss': 0.8887, 'grad_norm': 1.2526990175247192, 'learning_rate': 5.491937149864545e-06, 'epoch': 0.9}
{'loss': 1.0327, 'grad_norm': 0.9814407825469971, 'learning_rate': 5.4743988197686e-06, 'epoch': 0.9}
{'loss': 0.8726, 'grad_norm': 0.920509934425354, 'learning_rate': 5.456887750400752e-06, 'epoch': 0.9}
{'loss': 0.6488, 'grad_norm': 0.8121201992034912, 'learning_rate': 5.439403946811095e-06, 'epoch': 0.91}
{'loss': 0.6684, 'grad_norm': 1.1275010108947754, 'learning_rate': 5.421947414041883e-06, 'epoch': 0.91}
{'loss': 0.6659, 'grad_norm': 1.1823434829711914, 'learning_rate': 5.40451815712748e-06, 'epoch': 0.91}
{'loss': 0.8647, 'grad_norm': 1.2141697406768799, 'learning_rate': 5.387116181094431e-06, 'epoch': 0.91}
{'loss': 0.626, 'grad_norm': 0.9265093803405762, 'learning_rate': 5.369741490961344e-06, 'epoch': 0.91}
{'loss': 0.9246, 'grad_norm': 1.6460522413253784, 'learning_rate': 5.3523940917390215e-06, 'epoch': 0.91}
{'loss': 1.1033, 'grad_norm': 1.1896514892578125, 'learning_rate': 5.335073988430372e-06, 'epoch': 0.91}
{'loss': 0.6669, 'grad_norm': 1.0275297164916992, 'learning_rate': 5.317781186030413e-06, 'epoch': 0.91}
{'loss': 1.0871, 'grad_norm': 1.330947756767273, 'learning_rate': 5.30051568952632e-06, 'epoch': 0.91}
{'loss': 0.7857, 'grad_norm': 0.9946208596229553, 'learning_rate': 5.2832775038973544e-06, 'epoch': 0.91}
{'loss': 0.9829, 'grad_norm': 1.0703309774398804, 'learning_rate': 5.266066634114975e-06, 'epoch': 0.91}
{'loss': 0.7947, 'grad_norm': 0.8982098698616028, 'learning_rate': 5.248883085142653e-06, 'epoch': 0.91}
{'loss': 1.152, 'grad_norm': 0.9646344780921936, 'learning_rate': 5.231726861936082e-06, 'epoch': 0.91}
{'loss': 0.9197, 'grad_norm': 1.093780517578125, 'learning_rate': 5.214597969443025e-06, 'epoch': 0.91}
{'loss': 0.7745, 'grad_norm': 1.2332864999771118, 'learning_rate': 5.197496412603364e-06, 'epoch': 0.91}
{'loss': 1.0016, 'grad_norm': 1.1907469034194946, 'learning_rate': 5.180422196349111e-06, 'epoch': 0.91}
{'loss': 1.0403, 'grad_norm': 1.106067180633545, 'learning_rate': 5.163375325604414e-06, 'epoch': 0.91}
{'loss': 0.7045, 'grad_norm': 1.2825597524642944, 'learning_rate': 5.146355805285452e-06, 'epoch': 0.91}
{'loss': 1.1777, 'grad_norm': 1.0075081586837769, 'learning_rate': 5.129363640300611e-06, 'epoch': 0.91}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0049, 'grad_norm': 1.8150997161865234, 'learning_rate': 5.1123988355503475e-06, 'epoch': 0.91}
{'loss': 1.0371, 'grad_norm': 0.9675058126449585, 'learning_rate': 5.095461395927214e-06, 'epoch': 0.91}
{'loss': 0.9678, 'grad_norm': 1.160119652748108, 'learning_rate': 5.078551326315917e-06, 'epoch': 0.91}
{'loss': 0.6097, 'grad_norm': 1.0110487937927246, 'learning_rate': 5.061668631593197e-06, 'epoch': 0.91}
{'loss': 0.8767, 'grad_norm': 1.2466870546340942, 'learning_rate': 5.0448133166279944e-06, 'epoch': 0.91}
{'loss': 1.2572, 'grad_norm': 1.1212176084518433, 'learning_rate': 5.027985386281231e-06, 'epoch': 0.91}
{'loss': 0.7266, 'grad_norm': 1.2371104955673218, 'learning_rate': 5.01118484540607e-06, 'epoch': 0.91}
{'loss': 1.0004, 'grad_norm': 1.1111623048782349, 'learning_rate': 4.994411698847667e-06, 'epoch': 0.91}
{'loss': 1.2052, 'grad_norm': 1.1633745431900024, 'learning_rate': 4.977665951443322e-06, 'epoch': 0.91}
{'loss': 1.1238, 'grad_norm': 0.9762445688247681, 'learning_rate': 4.960947608022437e-06, 'epoch': 0.91}
{'loss': 0.9937, 'grad_norm': 1.2457045316696167, 'learning_rate': 4.944256673406478e-06, 'epoch': 0.91}
{'loss': 1.0787, 'grad_norm': 0.9306620359420776, 'learning_rate': 4.927593152409071e-06, 'epoch': 0.91}
{'loss': 0.6313, 'grad_norm': 1.0239958763122559, 'learning_rate': 4.910957049835863e-06, 'epoch': 0.91}
{'loss': 1.0213, 'grad_norm': 1.1461182832717896, 'learning_rate': 4.8943483704846475e-06, 'epoch': 0.91}
{'loss': 0.7324, 'grad_norm': 0.9994428753852844, 'learning_rate': 4.877767119145271e-06, 'epoch': 0.91}
{'loss': 0.6992, 'grad_norm': 1.1293549537658691, 'learning_rate': 4.861213300599687e-06, 'epoch': 0.91}
{'loss': 1.1512, 'grad_norm': 1.3205498456954956, 'learning_rate': 4.844686919621932e-06, 'epoch': 0.91}
{'loss': 0.8192, 'grad_norm': 1.1315467357635498, 'learning_rate': 4.828187980978171e-06, 'epoch': 0.91}
{'loss': 0.7615, 'grad_norm': 1.2256330251693726, 'learning_rate': 4.811716489426566e-06, 'epoch': 0.91}
{'loss': 0.9103, 'grad_norm': 1.1067613363265991, 'learning_rate': 4.795272449717458e-06, 'epoch': 0.91}
{'loss': 0.9468, 'grad_norm': 0.9915135502815247, 'learning_rate': 4.778855866593202e-06, 'epoch': 0.91}
{'loss': 0.9759, 'grad_norm': 1.1893515586853027, 'learning_rate': 4.7624667447882855e-06, 'epoch': 0.91}
{'loss': 1.0327, 'grad_norm': 1.188423991203308, 'learning_rate': 4.746105089029229e-06, 'epoch': 0.91}
{'loss': 0.992, 'grad_norm': 1.3427164554595947, 'learning_rate': 4.729770904034647e-06, 'epoch': 0.91}
{'loss': 0.701, 'grad_norm': 0.9092020392417908, 'learning_rate': 4.7134641945152935e-06, 'epoch': 0.91}
{'loss': 0.8472, 'grad_norm': 1.0024396181106567, 'learning_rate': 4.697184965173884e-06, 'epoch': 0.91}
{'loss': 1.0655, 'grad_norm': 1.2167465686798096, 'learning_rate': 4.680933220705308e-06, 'epoch': 0.91}
{'loss': 0.6314, 'grad_norm': 1.0260802507400513, 'learning_rate': 4.664708965796472e-06, 'epoch': 0.91}
{'loss': 0.8084, 'grad_norm': 1.1253074407577515, 'learning_rate': 4.648512205126376e-06, 'epoch': 0.91}
{'loss': 0.9676, 'grad_norm': 0.9821896553039551, 'learning_rate': 4.632342943366097e-06, 'epoch': 0.91}
{'loss': 0.9767, 'grad_norm': 1.4043186902999878, 'learning_rate': 4.616201185178748e-06, 'epoch': 0.91}
{'loss': 0.9408, 'grad_norm': 1.0445023775100708, 'learning_rate': 4.600086935219561e-06, 'epoch': 0.91}
{'loss': 0.7205, 'grad_norm': 1.087225079536438, 'learning_rate': 4.584000198135807e-06, 'epoch': 0.91}
{'loss': 0.7567, 'grad_norm': 1.0001307725906372, 'learning_rate': 4.567940978566809e-06, 'epoch': 0.91}
{'loss': 1.0014, 'grad_norm': 1.330970287322998, 'learning_rate': 4.551909281143962e-06, 'epoch': 0.91}
{'loss': 0.6963, 'grad_norm': 1.062907099723816, 'learning_rate': 4.535905110490779e-06, 'epoch': 0.91}
{'loss': 1.13, 'grad_norm': 1.161055564880371, 'learning_rate': 4.519928471222712e-06, 'epoch': 0.91}
{'loss': 0.9327, 'grad_norm': 1.1051816940307617, 'learning_rate': 4.50397936794742e-06, 'epoch': 0.91}
{'loss': 0.8974, 'grad_norm': 0.956953227519989, 'learning_rate': 4.488057805264478e-06, 'epoch': 0.91}
{'loss': 0.9319, 'grad_norm': 1.0108388662338257, 'learning_rate': 4.4721637877656375e-06, 'epoch': 0.91}
{'loss': 0.9976, 'grad_norm': 0.9920600652694702, 'learning_rate': 4.4562973200346416e-06, 'epoch': 0.91}
{'loss': 0.606, 'grad_norm': 0.9035683870315552, 'learning_rate': 4.440458406647297e-06, 'epoch': 0.91}
{'loss': 1.0701, 'grad_norm': 1.0358763933181763, 'learning_rate': 4.424647052171483e-06, 'epoch': 0.91}
{'loss': 0.98, 'grad_norm': 1.3123070001602173, 'learning_rate': 4.408863261167096e-06, 'epoch': 0.91}
{'loss': 1.0445, 'grad_norm': 0.993327260017395, 'learning_rate': 4.3931070381861396e-06, 'epoch': 0.91}
{'loss': 0.9002, 'grad_norm': 1.3026773929595947, 'learning_rate': 4.377378387772602e-06, 'epoch': 0.91}
{'loss': 0.8162, 'grad_norm': 1.1755518913269043, 'learning_rate': 4.3616773144625644e-06, 'epoch': 0.92}
{'loss': 0.9433, 'grad_norm': 1.1649609804153442, 'learning_rate': 4.346003822784139e-06, 'epoch': 0.92}
{'loss': 0.9723, 'grad_norm': 1.4318331480026245, 'learning_rate': 4.3303579172574885e-06, 'epoch': 0.92}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.7629, 'grad_norm': 1.250656247138977, 'learning_rate': 4.314739602394791e-06, 'epoch': 0.92}
{'loss': 0.8524, 'grad_norm': 1.0862962007522583, 'learning_rate': 4.299148882700355e-06, 'epoch': 0.92}
{'loss': 0.9262, 'grad_norm': 1.1715494394302368, 'learning_rate': 4.283585762670383e-06, 'epoch': 0.92}
{'loss': 0.8727, 'grad_norm': 1.0697053670883179, 'learning_rate': 4.268050246793276e-06, 'epoch': 0.92}
{'loss': 0.9288, 'grad_norm': 1.028184413909912, 'learning_rate': 4.25254233954937e-06, 'epoch': 0.92}
{'loss': 1.1003, 'grad_norm': 1.1533101797103882, 'learning_rate': 4.237062045411067e-06, 'epoch': 0.92}
{'loss': 0.955, 'grad_norm': 1.0432881116867065, 'learning_rate': 4.221609368842838e-06, 'epoch': 0.92}
{'loss': 1.2168, 'grad_norm': 1.076370358467102, 'learning_rate': 4.20618431430112e-06, 'epoch': 0.92}
{'loss': 1.1241, 'grad_norm': 1.4742724895477295, 'learning_rate': 4.190786886234465e-06, 'epoch': 0.92}
{'loss': 0.9062, 'grad_norm': 1.3145785331726074, 'learning_rate': 4.175417089083378e-06, 'epoch': 0.92}
{'loss': 1.0159, 'grad_norm': 0.9903188943862915, 'learning_rate': 4.1600749272804666e-06, 'epoch': 0.92}
{'loss': 0.8802, 'grad_norm': 1.1035583019256592, 'learning_rate': 4.144760405250325e-06, 'epoch': 0.92}
{'loss': 1.2496, 'grad_norm': 1.278457760810852, 'learning_rate': 4.129473527409577e-06, 'epoch': 0.92}
{'loss': 1.0702, 'grad_norm': 1.2504260540008545, 'learning_rate': 4.114214298166896e-06, 'epoch': 0.92}
{'loss': 0.9263, 'grad_norm': 1.0554122924804688, 'learning_rate': 4.098982721922961e-06, 'epoch': 0.92}
{'loss': 0.9934, 'grad_norm': 1.5234384536743164, 'learning_rate': 4.083778803070504e-06, 'epoch': 0.92}
{'loss': 1.1082, 'grad_norm': 1.083480954170227, 'learning_rate': 4.068602545994249e-06, 'epoch': 0.92}
{'loss': 0.7585, 'grad_norm': 0.9659656882286072, 'learning_rate': 4.053453955070952e-06, 'epoch': 0.92}
{'loss': 0.9164, 'grad_norm': 1.2109575271606445, 'learning_rate': 4.038333034669406e-06, 'epoch': 0.92}
{'loss': 1.0377, 'grad_norm': 1.2438961267471313, 'learning_rate': 4.023239789150402e-06, 'epoch': 0.92}
{'loss': 0.9026, 'grad_norm': 1.1487630605697632, 'learning_rate': 4.008174222866745e-06, 'epoch': 0.92}
{'loss': 0.8195, 'grad_norm': 1.3135900497436523, 'learning_rate': 3.993136340163317e-06, 'epoch': 0.92}
{'loss': 0.9786, 'grad_norm': 1.1412203311920166, 'learning_rate': 3.978126145376915e-06, 'epoch': 0.92}
{'loss': 0.9411, 'grad_norm': 1.3570736646652222, 'learning_rate': 3.963143642836442e-06, 'epoch': 0.92}
{'loss': 1.1846, 'grad_norm': 1.1625581979751587, 'learning_rate': 3.948188836862776e-06, 'epoch': 0.92}
{'loss': 0.8527, 'grad_norm': 1.4299284219741821, 'learning_rate': 3.9332617317688004e-06, 'epoch': 0.92}
{'loss': 1.2013, 'grad_norm': 1.2172905206680298, 'learning_rate': 3.918362331859437e-06, 'epoch': 0.92}
{'loss': 0.7562, 'grad_norm': 0.9968291521072388, 'learning_rate': 3.903490641431573e-06, 'epoch': 0.92}
{'loss': 0.9791, 'grad_norm': 1.2643259763717651, 'learning_rate': 3.888646664774154e-06, 'epoch': 0.92}
{'loss': 0.999, 'grad_norm': 1.0516458749771118, 'learning_rate': 3.873830406168111e-06, 'epoch': 0.92}
{'loss': 1.2405, 'grad_norm': 0.9769991636276245, 'learning_rate': 3.859041869886382e-06, 'epoch': 0.92}
{'loss': 1.1332, 'grad_norm': 1.307297706604004, 'learning_rate': 3.8442810601939105e-06, 'epoch': 0.92}
{'loss': 0.8687, 'grad_norm': 2.450498580932617, 'learning_rate': 3.8295479813476255e-06, 'epoch': 0.92}
{'loss': 0.9633, 'grad_norm': 1.5248215198516846, 'learning_rate': 3.814842637596483e-06, 'epoch': 0.92}
{'loss': 1.1124, 'grad_norm': 1.0651187896728516, 'learning_rate': 3.8001650331814465e-06, 'epoch': 0.92}
{'loss': 0.909, 'grad_norm': 1.0940110683441162, 'learning_rate': 3.785515172335463e-06, 'epoch': 0.92}
{'loss': 1.0105, 'grad_norm': 1.1349029541015625, 'learning_rate': 3.7708930592834648e-06, 'epoch': 0.92}
{'loss': 0.8977, 'grad_norm': 1.1936115026474, 'learning_rate': 3.756298698242422e-06, 'epoch': 0.92}
{'loss': 1.0077, 'grad_norm': 1.047428011894226, 'learning_rate': 3.7417320934212574e-06, 'epoch': 0.92}
{'loss': 0.9697, 'grad_norm': 0.9421303868293762, 'learning_rate': 3.7271932490209328e-06, 'epoch': 0.92}
{'loss': 0.7225, 'grad_norm': 1.291865348815918, 'learning_rate': 3.7126821692343605e-06, 'epoch': 0.92}
{'loss': 0.8849, 'grad_norm': 0.9938827753067017, 'learning_rate': 3.6981988582464822e-06, 'epoch': 0.92}
{'loss': 0.9819, 'grad_norm': 1.3117855787277222, 'learning_rate': 3.68374332023419e-06, 'epoch': 0.92}
{'loss': 1.0372, 'grad_norm': 1.1128236055374146, 'learning_rate': 3.6693155593664154e-06, 'epoch': 0.92}
{'loss': 0.9348, 'grad_norm': 1.3792293071746826, 'learning_rate': 3.654915579804041e-06, 'epoch': 0.92}
{'loss': 0.6852, 'grad_norm': 1.3295563459396362, 'learning_rate': 3.6405433856999684e-06, 'epoch': 0.92}
{'loss': 0.6346, 'grad_norm': 1.1863266229629517, 'learning_rate': 3.626198981199047e-06, 'epoch': 0.92}
{'loss': 0.5731, 'grad_norm': 1.1058266162872314, 'learning_rate': 3.6118823704381467e-06, 'epoch': 0.92}
{'loss': 0.7849, 'grad_norm': 1.0857620239257812, 'learning_rate': 3.5975935575461083e-06, 'epoch': 0.92}
{'loss': 0.834, 'grad_norm': 1.2204866409301758, 'learning_rate': 3.5833325466437694e-06, 'epoch': 0.92}
{'loss': 0.8712, 'grad_norm': 1.5254963636398315, 'learning_rate': 3.569099341843907e-06, 'epoch': 0.92}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.813, 'grad_norm': 1.007018804550171, 'learning_rate': 3.5548939472513365e-06, 'epoch': 0.92}
{'loss': 0.9804, 'grad_norm': 1.5674505233764648, 'learning_rate': 3.5407163669628153e-06, 'epoch': 0.92}
{'loss': 0.6947, 'grad_norm': 1.0641275644302368, 'learning_rate': 3.526566605067072e-06, 'epoch': 0.92}
{'loss': 1.0527, 'grad_norm': 0.9621402621269226, 'learning_rate': 3.512444665644865e-06, 'epoch': 0.92}
{'loss': 1.1087, 'grad_norm': 1.8712561130523682, 'learning_rate': 3.4983505527688586e-06, 'epoch': 0.92}
{'loss': 0.8777, 'grad_norm': 1.2646937370300293, 'learning_rate': 3.484284270503746e-06, 'epoch': 0.92}
{'loss': 0.8013, 'grad_norm': 1.0697293281555176, 'learning_rate': 3.470245822906182e-06, 'epoch': 0.92}
{'loss': 0.821, 'grad_norm': 1.389745831489563, 'learning_rate': 3.456235214024761e-06, 'epoch': 0.92}
{'loss': 0.7647, 'grad_norm': 1.4902362823486328, 'learning_rate': 3.4422524479001182e-06, 'epoch': 0.92}
{'loss': 0.838, 'grad_norm': 1.141983985900879, 'learning_rate': 3.428297528564761e-06, 'epoch': 0.92}
{'loss': 0.8518, 'grad_norm': 1.1732984781265259, 'learning_rate': 3.414370460043259e-06, 'epoch': 0.92}
{'loss': 0.8337, 'grad_norm': 0.8962636590003967, 'learning_rate': 3.4004712463521103e-06, 'epoch': 0.93}
{'loss': 0.844, 'grad_norm': 1.1041215658187866, 'learning_rate': 3.3865998914997643e-06, 'epoch': 0.93}
{'loss': 0.9947, 'grad_norm': 1.1004953384399414, 'learning_rate': 3.372756399486665e-06, 'epoch': 0.93}
{'loss': 0.6296, 'grad_norm': 1.1944807767868042, 'learning_rate': 3.358940774305208e-06, 'epoch': 0.93}
{'loss': 0.824, 'grad_norm': 1.0916391611099243, 'learning_rate': 3.3451530199397395e-06, 'epoch': 0.93}
{'loss': 0.7575, 'grad_norm': 1.4124280214309692, 'learning_rate': 3.331393140366601e-06, 'epoch': 0.93}
{'loss': 0.8488, 'grad_norm': 1.3101658821105957, 'learning_rate': 3.3176611395540626e-06, 'epoch': 0.93}
{'loss': 0.9291, 'grad_norm': 1.612747073173523, 'learning_rate': 3.3039570214623782e-06, 'epoch': 0.93}
{'loss': 0.9776, 'grad_norm': 1.0264443159103394, 'learning_rate': 3.2902807900437315e-06, 'epoch': 0.93}
{'loss': 0.8773, 'grad_norm': 0.9159240126609802, 'learning_rate': 3.2766324492422895e-06, 'epoch': 0.93}
{'loss': 1.0904, 'grad_norm': 1.2341792583465576, 'learning_rate': 3.2630120029942037e-06, 'epoch': 0.93}
{'loss': 0.9369, 'grad_norm': 1.5178147554397583, 'learning_rate': 3.249419455227476e-06, 'epoch': 0.93}
{'loss': 0.8874, 'grad_norm': 1.9034713506698608, 'learning_rate': 3.2358548098621932e-06, 'epoch': 0.93}
{'loss': 1.0568, 'grad_norm': 1.0184617042541504, 'learning_rate': 3.222318070810293e-06, 'epoch': 0.93}
{'loss': 0.7853, 'grad_norm': 1.2476603984832764, 'learning_rate': 3.20880924197573e-06, 'epoch': 0.93}
{'loss': 0.998, 'grad_norm': 1.1547023057937622, 'learning_rate': 3.1953283272543768e-06, 'epoch': 0.93}
{'loss': 0.6683, 'grad_norm': 1.370671033859253, 'learning_rate': 3.1818753305340565e-06, 'epoch': 0.93}
{'loss': 0.9109, 'grad_norm': 1.1392234563827515, 'learning_rate': 3.168450255694566e-06, 'epoch': 0.93}
{'loss': 0.8563, 'grad_norm': 1.0003175735473633, 'learning_rate': 3.1550531066076085e-06, 'epoch': 0.93}
{'loss': 0.9378, 'grad_norm': 1.1250442266464233, 'learning_rate': 3.1416838871368924e-06, 'epoch': 0.93}
{'loss': 0.9937, 'grad_norm': 1.186658501625061, 'learning_rate': 3.128342601138001e-06, 'epoch': 0.93}
{'loss': 0.9183, 'grad_norm': 1.4907523393630981, 'learning_rate': 3.1150292524585236e-06, 'epoch': 0.93}
{'loss': 0.8038, 'grad_norm': 1.1337766647338867, 'learning_rate': 3.1017438449379434e-06, 'epoch': 0.93}
{'loss': 1.12, 'grad_norm': 1.122283935546875, 'learning_rate': 3.088486382407718e-06, 'epoch': 0.93}
{'loss': 0.7375, 'grad_norm': 1.0177899599075317, 'learning_rate': 3.075256868691234e-06, 'epoch': 0.93}
{'loss': 0.8762, 'grad_norm': 1.2449041604995728, 'learning_rate': 3.0620553076038393e-06, 'epoch': 0.93}
{'loss': 0.6806, 'grad_norm': 1.4038286209106445, 'learning_rate': 3.048881702952755e-06, 'epoch': 0.93}
{'loss': 0.8594, 'grad_norm': 1.0099387168884277, 'learning_rate': 3.035736058537231e-06, 'epoch': 0.93}
{'loss': 1.027, 'grad_norm': 1.1760590076446533, 'learning_rate': 3.0226183781483896e-06, 'epoch': 0.93}
{'loss': 1.1063, 'grad_norm': 1.1607848405838013, 'learning_rate': 3.0095286655692945e-06, 'epoch': 0.93}
{'loss': 0.8806, 'grad_norm': 1.1470967531204224, 'learning_rate': 2.996466924574992e-06, 'epoch': 0.93}
{'loss': 0.8164, 'grad_norm': 1.1039432287216187, 'learning_rate': 2.9834331589323693e-06, 'epoch': 0.93}
{'loss': 1.1029, 'grad_norm': 1.2073522806167603, 'learning_rate': 2.970427372400353e-06, 'epoch': 0.93}
{'loss': 0.9505, 'grad_norm': 1.1941866874694824, 'learning_rate': 2.957449568729731e-06, 'epoch': 0.93}
{'loss': 0.8348, 'grad_norm': 0.9200169444084167, 'learning_rate': 2.9444997516632323e-06, 'epoch': 0.93}
{'loss': 0.8273, 'grad_norm': 1.4172431230545044, 'learning_rate': 2.9315779249355356e-06, 'epoch': 0.93}
{'loss': 0.9364, 'grad_norm': 1.0581406354904175, 'learning_rate': 2.918684092273216e-06, 'epoch': 0.93}
{'loss': 1.0399, 'grad_norm': 1.038926362991333, 'learning_rate': 2.905818257394799e-06, 'epoch': 0.93}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0245, 'grad_norm': 1.0222420692443848, 'learning_rate': 2.8929804240107385e-06, 'epoch': 0.93}
{'loss': 0.7994, 'grad_norm': 1.072835922241211, 'learning_rate': 2.880170595823395e-06, 'epoch': 0.93}
{'loss': 0.5958, 'grad_norm': 1.0444047451019287, 'learning_rate': 2.867388776527069e-06, 'epoch': 0.93}
{'loss': 0.713, 'grad_norm': 1.0538548231124878, 'learning_rate': 2.8546349698079787e-06, 'epoch': 0.93}
{'loss': 1.1649, 'grad_norm': 1.017541766166687, 'learning_rate': 2.8419091793442264e-06, 'epoch': 0.93}
{'loss': 1.1149, 'grad_norm': 0.9482582211494446, 'learning_rate': 2.8292114088059318e-06, 'epoch': 0.93}
{'loss': 0.9909, 'grad_norm': 1.1696635484695435, 'learning_rate': 2.8165416618549987e-06, 'epoch': 0.93}
{'loss': 0.7343, 'grad_norm': 1.2295714616775513, 'learning_rate': 2.8038999421453826e-06, 'epoch': 0.93}
{'loss': 0.7855, 'grad_norm': 1.0599316358566284, 'learning_rate': 2.7912862533228558e-06, 'epoch': 0.93}
{'loss': 0.8788, 'grad_norm': 1.1806800365447998, 'learning_rate': 2.7787005990251636e-06, 'epoch': 0.93}
{'loss': 0.7937, 'grad_norm': 1.1356192827224731, 'learning_rate': 2.766142982881936e-06, 'epoch': 0.93}
{'loss': 0.7311, 'grad_norm': 1.168296217918396, 'learning_rate': 2.7536134085147323e-06, 'epoch': 0.93}
{'loss': 0.8037, 'grad_norm': 1.3857629299163818, 'learning_rate': 2.7411118795370395e-06, 'epoch': 0.93}
{'loss': 0.9422, 'grad_norm': 1.474318027496338, 'learning_rate': 2.7286383995542065e-06, 'epoch': 0.93}
{'loss': 1.0038, 'grad_norm': 1.1620159149169922, 'learning_rate': 2.716192972163556e-06, 'epoch': 0.93}
{'loss': 1.2036, 'grad_norm': 1.3712904453277588, 'learning_rate': 2.7037756009542612e-06, 'epoch': 0.93}
{'loss': 0.7564, 'grad_norm': 1.0046732425689697, 'learning_rate': 2.691386289507458e-06, 'epoch': 0.93}
{'loss': 0.8918, 'grad_norm': 1.2633930444717407, 'learning_rate': 2.679025041396155e-06, 'epoch': 0.93}
{'loss': 0.6435, 'grad_norm': 0.9929576516151428, 'learning_rate': 2.6666918601852664e-06, 'epoch': 0.93}
{'loss': 0.9096, 'grad_norm': 1.223183274269104, 'learning_rate': 2.6543867494316256e-06, 'epoch': 0.93}
{'loss': 1.1858, 'grad_norm': 0.9857744574546814, 'learning_rate': 2.6421097126839712e-06, 'epoch': 0.93}
{'loss': 1.1145, 'grad_norm': 1.0506134033203125, 'learning_rate': 2.629860753482949e-06, 'epoch': 0.93}
{'loss': 0.7532, 'grad_norm': 1.0457634925842285, 'learning_rate': 2.6176398753610885e-06, 'epoch': 0.93}
{'loss': 1.1554, 'grad_norm': 1.616154670715332, 'learning_rate': 2.6054470818428377e-06, 'epoch': 0.93}
{'loss': 1.0898, 'grad_norm': 0.9987096190452576, 'learning_rate': 2.5932823764445392e-06, 'epoch': 0.93}
{'loss': 0.8896, 'grad_norm': 1.1230156421661377, 'learning_rate': 2.581145762674442e-06, 'epoch': 0.93}
{'loss': 1.1208, 'grad_norm': 0.9821019768714905, 'learning_rate': 2.5690372440326572e-06, 'epoch': 0.93}
{'loss': 0.6786, 'grad_norm': 1.2255181074142456, 'learning_rate': 2.5569568240112697e-06, 'epoch': 0.94}
{'loss': 1.1614, 'grad_norm': 0.9790927171707153, 'learning_rate': 2.54490450609417e-06, 'epoch': 0.94}
{'loss': 1.0534, 'grad_norm': 1.3598997592926025, 'learning_rate': 2.532880293757223e-06, 'epoch': 0.94}
{'loss': 1.0884, 'grad_norm': 1.2428004741668701, 'learning_rate': 2.5208841904681313e-06, 'epoch': 0.94}
{'loss': 1.0881, 'grad_norm': 1.2326241731643677, 'learning_rate': 2.5089161996865175e-06, 'epoch': 0.94}
{'loss': 1.0711, 'grad_norm': 1.1574112176895142, 'learning_rate': 2.496976324863898e-06, 'epoch': 0.94}
{'loss': 0.9518, 'grad_norm': 1.0849896669387817, 'learning_rate': 2.4850645694436736e-06, 'epoch': 0.94}
{'loss': 0.8819, 'grad_norm': 1.0960034132003784, 'learning_rate': 2.4731809368611413e-06, 'epoch': 0.94}
{'loss': 0.5741, 'grad_norm': 1.5371779203414917, 'learning_rate': 2.461325430543482e-06, 'epoch': 0.94}
{'loss': 0.7884, 'grad_norm': 1.0087922811508179, 'learning_rate': 2.44949805390976e-06, 'epoch': 0.94}
{'loss': 0.9327, 'grad_norm': 1.1303342580795288, 'learning_rate': 2.437698810370925e-06, 'epoch': 0.94}
{'loss': 0.74, 'grad_norm': 1.0673071146011353, 'learning_rate': 2.4259277033298555e-06, 'epoch': 0.94}
{'loss': 0.7765, 'grad_norm': 1.2963337898254395, 'learning_rate': 2.4141847361812462e-06, 'epoch': 0.94}
{'loss': 0.8338, 'grad_norm': 1.1542975902557373, 'learning_rate': 2.4024699123117445e-06, 'epoch': 0.94}
{'loss': 0.9001, 'grad_norm': 1.46328604221344, 'learning_rate': 2.390783235099814e-06, 'epoch': 0.94}
{'loss': 1.1696, 'grad_norm': 1.0909541845321655, 'learning_rate': 2.3791247079158584e-06, 'epoch': 0.94}
{'loss': 0.9853, 'grad_norm': 1.3190503120422363, 'learning_rate': 2.3674943341221445e-06, 'epoch': 0.94}
{'loss': 0.9314, 'grad_norm': 1.8078123331069946, 'learning_rate': 2.3558921170727888e-06, 'epoch': 0.94}
{'loss': 1.0579, 'grad_norm': 1.508662223815918, 'learning_rate': 2.344318060113859e-06, 'epoch': 0.94}
{'loss': 0.9636, 'grad_norm': 1.057974934577942, 'learning_rate': 2.332772166583208e-06, 'epoch': 0.94}
{'loss': 0.4935, 'grad_norm': 1.1696182489395142, 'learning_rate': 2.3212544398106496e-06, 'epoch': 0.94}
{'loss': 0.8569, 'grad_norm': 1.1408740282058716, 'learning_rate': 2.3097648831178376e-06, 'epoch': 0.94}
{'loss': 0.8128, 'grad_norm': 1.0547807216644287, 'learning_rate': 2.2983034998182997e-06, 'epoch': 0.94}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.6405, 'grad_norm': 0.9718446731567383, 'learning_rate': 2.2868702932174357e-06, 'epoch': 0.94}
{'loss': 0.934, 'grad_norm': 1.0875439643859863, 'learning_rate': 2.275465266612542e-06, 'epoch': 0.94}
{'loss': 0.9785, 'grad_norm': 1.009938359260559, 'learning_rate': 2.2640884232927427e-06, 'epoch': 0.94}
{'loss': 0.9266, 'grad_norm': 1.2511054277420044, 'learning_rate': 2.2527397665391027e-06, 'epoch': 0.94}
{'loss': 0.793, 'grad_norm': 1.1555389165878296, 'learning_rate': 2.241419299624514e-06, 'epoch': 0.94}
{'loss': 1.3124, 'grad_norm': 1.0818535089492798, 'learning_rate': 2.230127025813722e-06, 'epoch': 0.94}
{'loss': 1.0806, 'grad_norm': 1.159332513809204, 'learning_rate': 2.218862948363387e-06, 'epoch': 0.94}
{'loss': 0.747, 'grad_norm': 1.3590527772903442, 'learning_rate': 2.207627070521989e-06, 'epoch': 0.94}
{'loss': 0.726, 'grad_norm': 1.071718692779541, 'learning_rate': 2.196419395529936e-06, 'epoch': 0.94}
{'loss': 0.8674, 'grad_norm': 1.0273997783660889, 'learning_rate': 2.1852399266194314e-06, 'epoch': 0.94}
{'loss': 0.8704, 'grad_norm': 1.4557487964630127, 'learning_rate': 2.174088667014618e-06, 'epoch': 0.94}
{'loss': 0.8302, 'grad_norm': 1.0332640409469604, 'learning_rate': 2.162965619931423e-06, 'epoch': 0.94}
{'loss': 0.8667, 'grad_norm': 1.1797211170196533, 'learning_rate': 2.1518707885777146e-06, 'epoch': 0.94}
{'loss': 0.8541, 'grad_norm': 1.0796356201171875, 'learning_rate': 2.1408041761531772e-06, 'epoch': 0.94}
{'loss': 0.7383, 'grad_norm': 1.3525011539459229, 'learning_rate': 2.129765785849358e-06, 'epoch': 0.94}
{'loss': 0.9914, 'grad_norm': 1.313489317893982, 'learning_rate': 2.1187556208496883e-06, 'epoch': 0.94}
{'loss': 0.8673, 'grad_norm': 1.1829224824905396, 'learning_rate': 2.1077736843294393e-06, 'epoch': 0.94}
{'loss': 0.8217, 'grad_norm': 1.1230106353759766, 'learning_rate': 2.096819979455755e-06, 'epoch': 0.94}
{'loss': 0.999, 'grad_norm': 0.9914000630378723, 'learning_rate': 2.0858945093876316e-06, 'epoch': 0.94}
{'loss': 0.7086, 'grad_norm': 1.402530312538147, 'learning_rate': 2.074997277275914e-06, 'epoch': 0.94}
{'loss': 0.805, 'grad_norm': 1.606576681137085, 'learning_rate': 2.0641282862633114e-06, 'epoch': 0.94}
{'loss': 0.8569, 'grad_norm': 1.204783320426941, 'learning_rate': 2.053287539484405e-06, 'epoch': 0.94}
{'loss': 0.7324, 'grad_norm': 1.300275206565857, 'learning_rate': 2.0424750400655947e-06, 'epoch': 0.94}
{'loss': 0.6613, 'grad_norm': 0.9570481777191162, 'learning_rate': 2.031690791125163e-06, 'epoch': 0.94}
{'loss': 0.7036, 'grad_norm': 1.1638484001159668, 'learning_rate': 2.0209347957732328e-06, 'epoch': 0.94}
{'loss': 0.8175, 'grad_norm': 1.3941339254379272, 'learning_rate': 2.0102070571117795e-06, 'epoch': 0.94}
{'loss': 0.9504, 'grad_norm': 1.0240648984909058, 'learning_rate': 1.9995075782346385e-06, 'epoch': 0.94}
{'loss': 1.0016, 'grad_norm': 1.1137969493865967, 'learning_rate': 1.9888363622274642e-06, 'epoch': 0.94}
{'loss': 1.0665, 'grad_norm': 1.1713258028030396, 'learning_rate': 1.978193412167828e-06, 'epoch': 0.94}
{'loss': 0.8928, 'grad_norm': 1.1412699222564697, 'learning_rate': 1.9675787311250636e-06, 'epoch': 0.94}
{'loss': 0.7689, 'grad_norm': 1.1137019395828247, 'learning_rate': 1.9569923221604225e-06, 'epoch': 0.94}
{'loss': 1.0882, 'grad_norm': 1.4591387510299683, 'learning_rate': 1.946434188326951e-06, 'epoch': 0.94}
{'loss': 0.7568, 'grad_norm': 0.8578245043754578, 'learning_rate': 1.9359043326695804e-06, 'epoch': 0.94}
{'loss': 1.05, 'grad_norm': 1.3177005052566528, 'learning_rate': 1.925402758225059e-06, 'epoch': 0.94}
{'loss': 0.4948, 'grad_norm': 0.9896042346954346, 'learning_rate': 1.9149294680220087e-06, 'epoch': 0.94}
{'loss': 0.8828, 'grad_norm': 1.133410930633545, 'learning_rate': 1.904484465080847e-06, 'epoch': 0.94}
{'loss': 0.9054, 'grad_norm': 1.0600039958953857, 'learning_rate': 1.894067752413886e-06, 'epoch': 0.94}
{'loss': 0.7692, 'grad_norm': 1.105469822883606, 'learning_rate': 1.8836793330252456e-06, 'epoch': 0.94}
{'loss': 0.8823, 'grad_norm': 1.1538081169128418, 'learning_rate': 1.8733192099108955e-06, 'epoch': 0.94}
{'loss': 0.7796, 'grad_norm': 1.094718098640442, 'learning_rate': 1.8629873860586566e-06, 'epoch': 0.94}
{'loss': 1.0176, 'grad_norm': 1.1528189182281494, 'learning_rate': 1.852683864448157e-06, 'epoch': 0.94}
{'loss': 0.9955, 'grad_norm': 0.9615829586982727, 'learning_rate': 1.8424086480509196e-06, 'epoch': 0.94}
{'loss': 0.9121, 'grad_norm': 1.438381314277649, 'learning_rate': 1.8321617398302183e-06, 'epoch': 0.95}
{'loss': 1.0226, 'grad_norm': 1.5447320938110352, 'learning_rate': 1.821943142741256e-06, 'epoch': 0.95}
{'loss': 1.1991, 'grad_norm': 1.1340864896774292, 'learning_rate': 1.8117528597309862e-06, 'epoch': 0.95}
{'loss': 1.1191, 'grad_norm': 0.9617681503295898, 'learning_rate': 1.8015908937382586e-06, 'epoch': 0.95}
{'loss': 1.1135, 'grad_norm': 1.2592159509658813, 'learning_rate': 1.7914572476937508e-06, 'epoch': 0.95}
{'loss': 0.8303, 'grad_norm': 1.2738548517227173, 'learning_rate': 1.781351924519925e-06, 'epoch': 0.95}
{'loss': 0.9058, 'grad_norm': 1.2422540187835693, 'learning_rate': 1.771274927131139e-06, 'epoch': 0.95}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1817, 'grad_norm': 1.0204354524612427, 'learning_rate': 1.7612262584335237e-06, 'epoch': 0.95}
{'loss': 0.9951, 'grad_norm': 1.3126946687698364, 'learning_rate': 1.751205921325083e-06, 'epoch': 0.95}
{'loss': 0.904, 'grad_norm': 1.2121753692626953, 'learning_rate': 1.7412139186956278e-06, 'epoch': 0.95}
{'loss': 0.6172, 'grad_norm': 1.1510076522827148, 'learning_rate': 1.7312502534268082e-06, 'epoch': 0.95}
{'loss': 1.2685, 'grad_norm': 1.0775874853134155, 'learning_rate': 1.7213149283920927e-06, 'epoch': 0.95}
{'loss': 0.844, 'grad_norm': 1.0336331129074097, 'learning_rate': 1.7114079464567888e-06, 'epoch': 0.95}
{'loss': 0.6302, 'grad_norm': 1.2406805753707886, 'learning_rate': 1.7015293104780005e-06, 'epoch': 0.95}
{'loss': 1.0328, 'grad_norm': 1.179531455039978, 'learning_rate': 1.691679023304704e-06, 'epoch': 0.95}
{'loss': 1.0009, 'grad_norm': 1.1473605632781982, 'learning_rate': 1.6818570877776718e-06, 'epoch': 0.95}
{'loss': 1.0998, 'grad_norm': 1.3022680282592773, 'learning_rate': 1.6720635067294932e-06, 'epoch': 0.95}
{'loss': 0.9891, 'grad_norm': 0.9881719946861267, 'learning_rate': 1.6622982829845867e-06, 'epoch': 0.95}
{'loss': 0.7838, 'grad_norm': 1.0361534357070923, 'learning_rate': 1.6525614193591998e-06, 'epoch': 0.95}
{'loss': 1.145, 'grad_norm': 0.9499192237854004, 'learning_rate': 1.6428529186614195e-06, 'epoch': 0.95}
{'loss': 0.9894, 'grad_norm': 1.2578269243240356, 'learning_rate': 1.6331727836910837e-06, 'epoch': 0.95}
{'loss': 0.6888, 'grad_norm': 1.033052921295166, 'learning_rate': 1.6235210172399372e-06, 'epoch': 0.95}
{'loss': 0.9406, 'grad_norm': 1.0857748985290527, 'learning_rate': 1.613897622091487e-06, 'epoch': 0.95}
{'loss': 0.657, 'grad_norm': 0.9827181696891785, 'learning_rate': 1.6043026010210793e-06, 'epoch': 0.95}
{'loss': 0.8494, 'grad_norm': 1.1978726387023926, 'learning_rate': 1.5947359567958674e-06, 'epoch': 0.95}
{'loss': 0.9986, 'grad_norm': 1.0087698698043823, 'learning_rate': 1.5851976921748225e-06, 'epoch': 0.95}
{'loss': 0.7359, 'grad_norm': 0.9399533867835999, 'learning_rate': 1.5756878099087435e-06, 'epoch': 0.95}
{'loss': 0.8347, 'grad_norm': 1.0756916999816895, 'learning_rate': 1.566206312740226e-06, 'epoch': 0.95}
{'loss': 1.1036, 'grad_norm': 1.0550289154052734, 'learning_rate': 1.5567532034036936e-06, 'epoch': 0.95}
{'loss': 0.9998, 'grad_norm': 1.165816068649292, 'learning_rate': 1.5473284846253767e-06, 'epoch': 0.95}
{'loss': 0.8552, 'grad_norm': 1.1611250638961792, 'learning_rate': 1.537932159123312e-06, 'epoch': 0.95}
{'loss': 1.1878, 'grad_norm': 1.0554583072662354, 'learning_rate': 1.5285642296073654e-06, 'epoch': 0.95}
{'loss': 0.9038, 'grad_norm': 1.2800371646881104, 'learning_rate': 1.5192246987791981e-06, 'epoch': 0.95}
{'loss': 1.0404, 'grad_norm': 0.9942439198493958, 'learning_rate': 1.5099135693322774e-06, 'epoch': 0.95}
{'loss': 0.639, 'grad_norm': 0.8347936868667603, 'learning_rate': 1.5006308439519001e-06, 'epoch': 0.95}
{'loss': 1.086, 'grad_norm': 1.2280750274658203, 'learning_rate': 1.491376525315158e-06, 'epoch': 0.95}
{'loss': 0.7771, 'grad_norm': 0.9862122535705566, 'learning_rate': 1.4821506160909493e-06, 'epoch': 0.95}
{'loss': 0.8608, 'grad_norm': 1.3789950609207153, 'learning_rate': 1.4729531189399903e-06, 'epoch': 0.95}
{'loss': 1.0032, 'grad_norm': 1.7360185384750366, 'learning_rate': 1.4637840365147703e-06, 'epoch': 0.95}
{'loss': 0.7538, 'grad_norm': 1.1046003103256226, 'learning_rate': 1.4546433714596296e-06, 'epoch': 0.95}
{'loss': 1.0781, 'grad_norm': 2.0062360763549805, 'learning_rate': 1.4455311264106818e-06, 'epoch': 0.95}
{'loss': 1.1424, 'grad_norm': 1.0384361743927002, 'learning_rate': 1.4364473039958692e-06, 'epoch': 0.95}
{'loss': 1.0309, 'grad_norm': 1.1592639684677124, 'learning_rate': 1.4273919068349184e-06, 'epoch': 0.95}
{'loss': 0.9897, 'grad_norm': 1.1645121574401855, 'learning_rate': 1.4183649375393404e-06, 'epoch': 0.95}
{'loss': 0.6478, 'grad_norm': 1.5507439374923706, 'learning_rate': 1.409366398712497e-06, 'epoch': 0.95}
{'loss': 0.8027, 'grad_norm': 1.0899778604507446, 'learning_rate': 1.400396292949513e-06, 'epoch': 0.95}
{'loss': 0.7816, 'grad_norm': 0.8879754543304443, 'learning_rate': 1.3914546228373182e-06, 'epoch': 0.95}
{'loss': 0.9372, 'grad_norm': 1.108162522315979, 'learning_rate': 1.3825413909546503e-06, 'epoch': 0.95}
{'loss': 1.0028, 'grad_norm': 0.9628112316131592, 'learning_rate': 1.373656599872053e-06, 'epoch': 0.95}
{'loss': 1.0334, 'grad_norm': 1.3516881465911865, 'learning_rate': 1.3648002521518323e-06, 'epoch': 0.95}
{'loss': 0.7397, 'grad_norm': 0.9735799431800842, 'learning_rate': 1.3559723503481558e-06, 'epoch': 0.95}
{'loss': 1.1779, 'grad_norm': 1.2734917402267456, 'learning_rate': 1.3471728970068987e-06, 'epoch': 0.95}
{'loss': 0.8164, 'grad_norm': 1.7083075046539307, 'learning_rate': 1.3384018946658306e-06, 'epoch': 0.95}
{'loss': 1.0135, 'grad_norm': 1.2420700788497925, 'learning_rate': 1.3296593458544281e-06, 'epoch': 0.95}
{'loss': 0.8922, 'grad_norm': 1.054532766342163, 'learning_rate': 1.3209452530940192e-06, 'epoch': 0.95}
{'loss': 1.1991, 'grad_norm': 1.4929847717285156, 'learning_rate': 1.3122596188977043e-06, 'epoch': 0.95}
{'loss': 0.9293, 'grad_norm': 1.5448817014694214, 'learning_rate': 1.3036024457703798e-06, 'epoch': 0.95}
{'loss': 1.2419, 'grad_norm': 1.125234603881836, 'learning_rate': 1.2949737362087156e-06, 'epoch': 0.95}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.1854, 'grad_norm': 1.2035462856292725, 'learning_rate': 1.2863734927012095e-06, 'epoch': 0.95}
{'loss': 0.971, 'grad_norm': 1.0720981359481812, 'learning_rate': 1.277801717728122e-06, 'epoch': 0.95}
{'loss': 0.8846, 'grad_norm': 1.2083451747894287, 'learning_rate': 1.2692584137615204e-06, 'epoch': 0.95}
{'loss': 0.9697, 'grad_norm': 1.3867520093917847, 'learning_rate': 1.2607435832652448e-06, 'epoch': 0.95}
{'loss': 0.9541, 'grad_norm': 1.1710509061813354, 'learning_rate': 1.2522572286949197e-06, 'epoch': 0.95}
{'loss': 0.9092, 'grad_norm': 0.9783002734184265, 'learning_rate': 1.2437993524979984e-06, 'epoch': 0.95}
{'loss': 1.1427, 'grad_norm': 1.039882779121399, 'learning_rate': 1.2353699571136634e-06, 'epoch': 0.95}
{'loss': 0.9343, 'grad_norm': 1.020615816116333, 'learning_rate': 1.2269690449729253e-06, 'epoch': 0.96}
{'loss': 0.9043, 'grad_norm': 1.055773138999939, 'learning_rate': 1.2185966184985685e-06, 'epoch': 0.96}
{'loss': 0.929, 'grad_norm': 1.0762380361557007, 'learning_rate': 1.2102526801051506e-06, 'epoch': 0.96}
{'loss': 0.8754, 'grad_norm': 1.2537004947662354, 'learning_rate': 1.2019372321990352e-06, 'epoch': 0.96}
{'loss': 1.0094, 'grad_norm': 1.3812103271484375, 'learning_rate': 1.1936502771783486e-06, 'epoch': 0.96}
{'loss': 0.6777, 'grad_norm': 0.9425901174545288, 'learning_rate': 1.1853918174330125e-06, 'epoch': 0.96}
{'loss': 0.9119, 'grad_norm': 0.9473061561584473, 'learning_rate': 1.1771618553447216e-06, 'epoch': 0.96}
{'loss': 0.8083, 'grad_norm': 1.1074566841125488, 'learning_rate': 1.1689603932869665e-06, 'epoch': 0.96}
{'loss': 0.8765, 'grad_norm': 1.5300356149673462, 'learning_rate': 1.1607874336249992e-06, 'epoch': 0.96}
{'loss': 0.7875, 'grad_norm': 1.1632312536239624, 'learning_rate': 1.1526429787158676e-06, 'epoch': 0.96}
{'loss': 0.935, 'grad_norm': 1.2146453857421875, 'learning_rate': 1.1445270309083933e-06, 'epoch': 0.96}
{'loss': 0.9803, 'grad_norm': 0.9801303744316101, 'learning_rate': 1.1364395925431814e-06, 'epoch': 0.96}
{'loss': 0.94, 'grad_norm': 1.2844487428665161, 'learning_rate': 1.1283806659525996e-06, 'epoch': 0.96}
{'loss': 1.1897, 'grad_norm': 1.0321810245513916, 'learning_rate': 1.1203502534608113e-06, 'epoch': 0.96}
{'loss': 0.8961, 'grad_norm': 1.0945823192596436, 'learning_rate': 1.1123483573837412e-06, 'epoch': 0.96}
{'loss': 1.1479, 'grad_norm': 1.0512168407440186, 'learning_rate': 1.1043749800291102e-06, 'epoch': 0.96}
{'loss': 0.9351, 'grad_norm': 0.9823563694953918, 'learning_rate': 1.0964301236963904e-06, 'epoch': 0.96}
{'loss': 0.8312, 'grad_norm': 1.0039596557617188, 'learning_rate': 1.0885137906768372e-06, 'epoch': 0.96}
{'loss': 0.9522, 'grad_norm': 1.0489174127578735, 'learning_rate': 1.0806259832535026e-06, 'epoch': 0.96}
{'loss': 1.2789, 'grad_norm': 1.1021798849105835, 'learning_rate': 1.0727667037011668e-06, 'epoch': 0.96}
{'loss': 0.8789, 'grad_norm': 1.2099299430847168, 'learning_rate': 1.064935954286428e-06, 'epoch': 0.96}
{'loss': 0.9912, 'grad_norm': 1.595178246498108, 'learning_rate': 1.0571337372676238e-06, 'epoch': 0.96}
{'loss': 0.767, 'grad_norm': 0.9910616278648376, 'learning_rate': 1.0493600548948878e-06, 'epoch': 0.96}
{'loss': 0.6994, 'grad_norm': 1.0782839059829712, 'learning_rate': 1.0416149094101046e-06, 'epoch': 0.96}
{'loss': 0.8583, 'grad_norm': 1.3171473741531372, 'learning_rate': 1.0338983030469428e-06, 'epoch': 0.96}
{'loss': 0.9202, 'grad_norm': 1.2127131223678589, 'learning_rate': 1.0262102380308226e-06, 'epoch': 0.96}
{'loss': 1.0036, 'grad_norm': 1.1480085849761963, 'learning_rate': 1.0185507165789587e-06, 'epoch': 0.96}
{'loss': 0.9584, 'grad_norm': 1.5083544254302979, 'learning_rate': 1.0109197409003068e-06, 'epoch': 0.96}
{'loss': 0.8191, 'grad_norm': 1.2606960535049438, 'learning_rate': 1.0033173131956176e-06, 'epoch': 0.96}
{'loss': 0.9634, 'grad_norm': 1.0904432535171509, 'learning_rate': 9.957434356573814e-07, 'epoch': 0.96}
{'loss': 0.9039, 'grad_norm': 1.0121651887893677, 'learning_rate': 9.881981104698846e-07, 'epoch': 0.96}
{'loss': 0.7514, 'grad_norm': 1.0785040855407715, 'learning_rate': 9.80681339809142e-07, 'epoch': 0.96}
{'loss': 1.1111, 'grad_norm': 1.0066841840744019, 'learning_rate': 9.731931258429638e-07, 'epoch': 0.96}
{'loss': 1.0362, 'grad_norm': 1.141573190689087, 'learning_rate': 9.65733470730934e-07, 'epoch': 0.96}
{'loss': 0.7313, 'grad_norm': 1.2912991046905518, 'learning_rate': 9.583023766243426e-07, 'epoch': 0.96}
{'loss': 0.8602, 'grad_norm': 1.031935691833496, 'learning_rate': 9.508998456663088e-07, 'epoch': 0.96}
{'loss': 0.8307, 'grad_norm': 1.300714135169983, 'learning_rate': 9.435258799916801e-07, 'epoch': 0.96}
{'loss': 0.9166, 'grad_norm': 1.0398223400115967, 'learning_rate': 9.36180481727067e-07, 'epoch': 0.96}
{'loss': 1.0994, 'grad_norm': 1.0745662450790405, 'learning_rate': 9.288636529908635e-07, 'epoch': 0.96}
{'loss': 0.9591, 'grad_norm': 1.0869379043579102, 'learning_rate': 9.215753958931816e-07, 'epoch': 0.96}
{'loss': 0.6416, 'grad_norm': 1.1443217992782593, 'learning_rate': 9.143157125359514e-07, 'epoch': 0.96}
{'loss': 1.0143, 'grad_norm': 1.2495890855789185, 'learning_rate': 9.07084605012798e-07, 'epoch': 0.96}
{'loss': 1.0541, 'grad_norm': 1.1032390594482422, 'learning_rate': 8.998820754091531e-07, 'epoch': 0.96}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0682, 'grad_norm': 1.2428154945373535, 'learning_rate': 8.927081258021997e-07, 'epoch': 0.96}
{'loss': 1.0374, 'grad_norm': 1.0960654020309448, 'learning_rate': 8.855627582608606e-07, 'epoch': 0.96}
{'loss': 0.9109, 'grad_norm': 1.0735836029052734, 'learning_rate': 8.784459748458318e-07, 'epoch': 0.96}
{'loss': 0.7963, 'grad_norm': 1.3053081035614014, 'learning_rate': 8.713577776095494e-07, 'epoch': 0.96}
{'loss': 1.148, 'grad_norm': 0.9636727571487427, 'learning_rate': 8.642981685962226e-07, 'epoch': 0.96}
{'loss': 0.9357, 'grad_norm': 0.9643825888633728, 'learning_rate': 8.572671498418006e-07, 'epoch': 0.96}
{'loss': 1.188, 'grad_norm': 1.346437931060791, 'learning_rate': 8.502647233740169e-07, 'epoch': 0.96}
{'loss': 1.2486, 'grad_norm': 1.3759243488311768, 'learning_rate': 8.432908912123116e-07, 'epoch': 0.96}
{'loss': 0.8617, 'grad_norm': 1.5224956274032593, 'learning_rate': 8.363456553679206e-07, 'epoch': 0.96}
{'loss': 1.0445, 'grad_norm': 1.200549602508545, 'learning_rate': 8.294290178437969e-07, 'epoch': 0.96}
{'loss': 0.8109, 'grad_norm': 1.3305680751800537, 'learning_rate': 8.225409806347006e-07, 'epoch': 0.96}
{'loss': 1.1971, 'grad_norm': 0.9306028485298157, 'learning_rate': 8.15681545727065e-07, 'epoch': 0.96}
{'loss': 0.882, 'grad_norm': 1.3252780437469482, 'learning_rate': 8.08850715099152e-07, 'epoch': 0.96}
{'loss': 0.6513, 'grad_norm': 1.151509165763855, 'learning_rate': 8.020484907209302e-07, 'epoch': 0.96}
{'loss': 1.1058, 'grad_norm': 1.2607629299163818, 'learning_rate': 7.952748745541305e-07, 'epoch': 0.96}
{'loss': 1.0159, 'grad_norm': 1.0151985883712769, 'learning_rate': 7.885298685522235e-07, 'epoch': 0.96}
{'loss': 0.7852, 'grad_norm': 1.0194464921951294, 'learning_rate': 7.818134746604311e-07, 'epoch': 0.96}
{'loss': 0.9671, 'grad_norm': 1.1804918050765991, 'learning_rate': 7.751256948157481e-07, 'epoch': 0.96}
{'loss': 0.8198, 'grad_norm': 0.8852981925010681, 'learning_rate': 7.684665309468875e-07, 'epoch': 0.96}
{'loss': 0.9956, 'grad_norm': 1.0003050565719604, 'learning_rate': 7.618359849743128e-07, 'epoch': 0.96}
{'loss': 0.8628, 'grad_norm': 1.0524959564208984, 'learning_rate': 7.552340588102613e-07, 'epoch': 0.96}
{'loss': 1.4116, 'grad_norm': 1.3180862665176392, 'learning_rate': 7.486607543586766e-07, 'epoch': 0.96}
{'loss': 0.7985, 'grad_norm': 1.0052423477172852, 'learning_rate': 7.421160735152755e-07, 'epoch': 0.97}
{'loss': 1.0744, 'grad_norm': 1.0198404788970947, 'learning_rate': 7.35600018167526e-07, 'epoch': 0.97}
{'loss': 1.0274, 'grad_norm': 1.07500159740448, 'learning_rate': 7.291125901946027e-07, 'epoch': 0.97}
{'loss': 0.989, 'grad_norm': 1.0354440212249756, 'learning_rate': 7.226537914674647e-07, 'epoch': 0.97}
{'loss': 1.0419, 'grad_norm': 1.0618364810943604, 'learning_rate': 7.162236238487885e-07, 'epoch': 0.97}
{'loss': 0.7992, 'grad_norm': 1.2009029388427734, 'learning_rate': 7.098220891930129e-07, 'epoch': 0.97}
{'loss': 0.8679, 'grad_norm': 1.1437703371047974, 'learning_rate': 7.034491893463058e-07, 'epoch': 0.97}
{'loss': 0.9612, 'grad_norm': 1.2437033653259277, 'learning_rate': 6.971049261465745e-07, 'epoch': 0.97}
{'loss': 1.0569, 'grad_norm': 1.0981664657592773, 'learning_rate': 6.90789301423489e-07, 'epoch': 0.97}
{'loss': 0.8314, 'grad_norm': 1.2049371004104614, 'learning_rate': 6.845023169984366e-07, 'epoch': 0.97}
{'loss': 0.9389, 'grad_norm': 1.0266891717910767, 'learning_rate': 6.782439746845448e-07, 'epoch': 0.97}
{'loss': 1.2586, 'grad_norm': 1.2706416845321655, 'learning_rate': 6.720142762867032e-07, 'epoch': 0.97}
{'loss': 0.8866, 'grad_norm': 1.1994003057479858, 'learning_rate': 6.658132236015191e-07, 'epoch': 0.97}
{'loss': 0.8479, 'grad_norm': 1.290988802909851, 'learning_rate': 6.596408184173509e-07, 'epoch': 0.97}
{'loss': 0.8806, 'grad_norm': 1.4015613794326782, 'learning_rate': 6.534970625142856e-07, 'epoch': 0.97}
{'loss': 0.8697, 'grad_norm': 1.0865918397903442, 'learning_rate': 6.473819576641504e-07, 'epoch': 0.97}
{'loss': 0.9735, 'grad_norm': 1.256603717803955, 'learning_rate': 6.412955056305236e-07, 'epoch': 0.97}
{'loss': 1.1377, 'grad_norm': 1.1735769510269165, 'learning_rate': 6.352377081687011e-07, 'epoch': 0.97}
{'loss': 0.7291, 'grad_norm': 1.0227395296096802, 'learning_rate': 6.292085670257187e-07, 'epoch': 0.97}
{'loss': 0.9578, 'grad_norm': 1.0441147089004517, 'learning_rate': 6.232080839403631e-07, 'epoch': 0.97}
{'loss': 1.1745, 'grad_norm': 1.1719446182250977, 'learning_rate': 6.172362606431281e-07, 'epoch': 0.97}
{'loss': 0.8664, 'grad_norm': 1.1267491579055786, 'learning_rate': 6.112930988562804e-07, 'epoch': 0.97}
{'loss': 0.6645, 'grad_norm': 0.9776591658592224, 'learning_rate': 6.053786002937822e-07, 'epoch': 0.97}
{'loss': 0.9083, 'grad_norm': 1.3494056463241577, 'learning_rate': 5.99492766661347e-07, 'epoch': 0.97}
{'loss': 0.756, 'grad_norm': 1.1597683429718018, 'learning_rate': 5.93635599656428e-07, 'epoch': 0.97}
{'loss': 0.9685, 'grad_norm': 1.4706432819366455, 'learning_rate': 5.878071009682074e-07, 'epoch': 0.97}
{'loss': 1.1137, 'grad_norm': 1.0616260766983032, 'learning_rate': 5.820072722775849e-07, 'epoch': 0.97}
{'loss': 0.8291, 'grad_norm': 1.1293548345565796, 'learning_rate': 5.762361152572115e-07, 'epoch': 0.97}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0207, 'grad_norm': 0.9713089466094971, 'learning_rate': 5.704936315714449e-07, 'epoch': 0.97}
{'loss': 0.7671, 'grad_norm': 0.9634161591529846, 'learning_rate': 5.647798228764156e-07, 'epoch': 0.97}
{'loss': 1.1699, 'grad_norm': 1.2487165927886963, 'learning_rate': 5.590946908199501e-07, 'epoch': 0.97}
{'loss': 1.0584, 'grad_norm': 1.0811216831207275, 'learning_rate': 5.534382370416036e-07, 'epoch': 0.97}
{'loss': 0.7128, 'grad_norm': 1.1246311664581299, 'learning_rate': 5.478104631726711e-07, 'epoch': 0.97}
{'loss': 0.855, 'grad_norm': 1.1995761394500732, 'learning_rate': 5.422113708361764e-07, 'epoch': 0.97}
{'loss': 0.7027, 'grad_norm': 1.110763430595398, 'learning_rate': 5.366409616468837e-07, 'epoch': 0.97}
{'loss': 0.9497, 'grad_norm': 1.20602285861969, 'learning_rate': 5.310992372112522e-07, 'epoch': 0.97}
{'loss': 0.9048, 'grad_norm': 1.3617559671401978, 'learning_rate': 5.255861991275035e-07, 'epoch': 0.97}
{'loss': 1.2016, 'grad_norm': 1.167037844657898, 'learning_rate': 5.201018489855658e-07, 'epoch': 0.97}
{'loss': 1.0961, 'grad_norm': 1.4122798442840576, 'learning_rate': 5.146461883671072e-07, 'epoch': 0.97}
{'loss': 0.9947, 'grad_norm': 1.071852207183838, 'learning_rate': 5.092192188455025e-07, 'epoch': 0.97}
{'loss': 0.9872, 'grad_norm': 1.0324091911315918, 'learning_rate': 5.038209419858664e-07, 'epoch': 0.97}
{'loss': 0.8641, 'grad_norm': 1.1286990642547607, 'learning_rate': 4.984513593450424e-07, 'epoch': 0.97}
{'loss': 0.8666, 'grad_norm': 1.2228553295135498, 'learning_rate': 4.931104724715696e-07, 'epoch': 0.97}
{'loss': 0.8168, 'grad_norm': 1.1494938135147095, 'learning_rate': 4.877982829057714e-07, 'epoch': 0.97}
{'loss': 1.1738, 'grad_norm': 0.9109176993370056, 'learning_rate': 4.825147921796225e-07, 'epoch': 0.97}
{'loss': 1.1691, 'grad_norm': 1.0720109939575195, 'learning_rate': 4.772600018168816e-07, 'epoch': 0.97}
{'loss': 0.8053, 'grad_norm': 1.0597119331359863, 'learning_rate': 4.720339133329921e-07, 'epoch': 0.97}
{'loss': 1.024, 'grad_norm': 1.217164397239685, 'learning_rate': 4.668365282351372e-07, 'epoch': 0.97}
{'loss': 0.9386, 'grad_norm': 1.204410433769226, 'learning_rate': 4.6166784802221805e-07, 'epoch': 0.97}
{'loss': 1.0819, 'grad_norm': 1.088323712348938, 'learning_rate': 4.5652787418485334e-07, 'epoch': 0.97}
{'loss': 1.0845, 'grad_norm': 1.156693458557129, 'learning_rate': 4.514166082053795e-07, 'epoch': 0.97}
{'loss': 1.1048, 'grad_norm': 1.0908379554748535, 'learning_rate': 4.46334051557884e-07, 'epoch': 0.97}
{'loss': 1.0291, 'grad_norm': 1.1107096672058105, 'learning_rate': 4.412802057081278e-07, 'epoch': 0.97}
{'loss': 0.8985, 'grad_norm': 1.510326623916626, 'learning_rate': 4.362550721136338e-07, 'epoch': 0.97}
{'loss': 1.132, 'grad_norm': 1.0842469930648804, 'learning_rate': 4.312586522236206e-07, 'epoch': 0.97}
{'loss': 1.0387, 'grad_norm': 1.0883184671401978, 'learning_rate': 4.262909474790133e-07, 'epoch': 0.97}
{'loss': 0.886, 'grad_norm': 0.947679340839386, 'learning_rate': 4.2135195931249926e-07, 'epoch': 0.97}
{'loss': 0.6458, 'grad_norm': 1.0046401023864746, 'learning_rate': 4.164416891484502e-07, 'epoch': 0.97}
{'loss': 1.5826, 'grad_norm': 1.0959715843200684, 'learning_rate': 4.115601384029666e-07, 'epoch': 0.97}
{'loss': 0.9052, 'grad_norm': 1.0820330381393433, 'learning_rate': 4.0670730848385576e-07, 'epoch': 0.97}
{'loss': 0.7042, 'grad_norm': 1.2362607717514038, 'learning_rate': 4.0188320079065365e-07, 'epoch': 0.97}
{'loss': 0.7723, 'grad_norm': 1.7309727668762207, 'learning_rate': 3.9708781671462527e-07, 'epoch': 0.97}
{'loss': 0.727, 'grad_norm': 1.0185832977294922, 'learning_rate': 3.923211576387087e-07, 'epoch': 0.97}
{'loss': 0.7727, 'grad_norm': 1.0165083408355713, 'learning_rate': 3.8758322493760436e-07, 'epoch': 0.97}
{'loss': 1.0848, 'grad_norm': 1.1118587255477905, 'learning_rate': 3.828740199777081e-07, 'epoch': 0.97}
{'loss': 0.8808, 'grad_norm': 1.11808180809021, 'learning_rate': 3.781935441171336e-07, 'epoch': 0.98}
{'loss': 1.2153, 'grad_norm': 1.9108117818832397, 'learning_rate': 3.7354179870568994e-07, 'epoch': 0.98}
{'loss': 0.9526, 'grad_norm': 1.3844159841537476, 'learning_rate': 3.689187850849374e-07, 'epoch': 0.98}
{'loss': 0.9808, 'grad_norm': 1.341610074043274, 'learning_rate': 3.643245045881205e-07, 'epoch': 0.98}
{'loss': 0.9178, 'grad_norm': 1.3265305757522583, 'learning_rate': 3.597589585402128e-07, 'epoch': 0.98}
{'loss': 1.2077, 'grad_norm': 0.9415509104728699, 'learning_rate': 3.552221482579055e-07, 'epoch': 0.98}
{'loss': 0.9342, 'grad_norm': 1.1293964385986328, 'learning_rate': 3.50714075049563e-07, 'epoch': 0.98}
{'loss': 0.9169, 'grad_norm': 1.008580207824707, 'learning_rate': 3.462347402153232e-07, 'epoch': 0.98}
{'loss': 1.0479, 'grad_norm': 1.1240482330322266, 'learning_rate': 3.4178414504698606e-07, 'epoch': 0.98}
{'loss': 0.8119, 'grad_norm': 1.2666279077529907, 'learning_rate': 3.373622908280916e-07, 'epoch': 0.98}
{'loss': 0.8391, 'grad_norm': 0.9786645174026489, 'learning_rate': 3.329691788338751e-07, 'epoch': 0.98}
{'loss': 0.7915, 'grad_norm': 0.9404112696647644, 'learning_rate': 3.2860481033129e-07, 'epoch': 0.98}
{'loss': 0.9717, 'grad_norm': 1.0942939519882202, 'learning_rate': 3.2426918657900704e-07, 'epoch': 0.98}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0199, 'grad_norm': 1.293654441833496, 'learning_rate': 3.1996230882738175e-07, 'epoch': 0.98}
{'loss': 1.0118, 'grad_norm': 1.07817804813385, 'learning_rate': 3.156841783185205e-07, 'epoch': 0.98}
{'loss': 0.991, 'grad_norm': 1.0135594606399536, 'learning_rate': 3.114347962861919e-07, 'epoch': 0.98}
{'loss': 1.4982, 'grad_norm': 1.0214229822158813, 'learning_rate': 3.072141639559045e-07, 'epoch': 0.98}
{'loss': 0.7639, 'grad_norm': 1.0920414924621582, 'learning_rate': 3.0302228254488474e-07, 'epoch': 0.98}
{'loss': 0.9757, 'grad_norm': 1.232418179512024, 'learning_rate': 2.988591532620322e-07, 'epoch': 0.98}
{'loss': 0.9696, 'grad_norm': 1.1705752611160278, 'learning_rate': 2.947247773079753e-07, 'epoch': 0.98}
{'loss': 0.8268, 'grad_norm': 1.1822320222854614, 'learning_rate': 2.9061915587506036e-07, 'epoch': 0.98}
{'loss': 0.913, 'grad_norm': 1.0385892391204834, 'learning_rate': 2.8654229014730694e-07, 'epoch': 0.98}
{'loss': 0.8732, 'grad_norm': 1.1987519264221191, 'learning_rate': 2.8249418130049665e-07, 'epoch': 0.98}
{'loss': 1.4007, 'grad_norm': 1.391554832458496, 'learning_rate': 2.784748305020513e-07, 'epoch': 0.98}
{'loss': 1.0254, 'grad_norm': 1.0217257738113403, 'learning_rate': 2.744842389111546e-07, 'epoch': 0.98}
{'loss': 0.8647, 'grad_norm': 1.0965584516525269, 'learning_rate': 2.7052240767865276e-07, 'epoch': 0.98}
{'loss': 1.0464, 'grad_norm': 1.225111722946167, 'learning_rate': 2.665893379471429e-07, 'epoch': 0.98}
{'loss': 0.8618, 'grad_norm': 1.3842439651489258, 'learning_rate': 2.6268503085089547e-07, 'epoch': 0.98}
{'loss': 0.6519, 'grad_norm': 0.9474925398826599, 'learning_rate': 2.5880948751587643e-07, 'epoch': 0.98}
{'loss': 1.0771, 'grad_norm': 1.2526921033859253, 'learning_rate': 2.549627090598028e-07, 'epoch': 0.98}
{'loss': 0.7806, 'grad_norm': 1.151167392730713, 'learning_rate': 2.5114469659203167e-07, 'epoch': 0.98}
{'loss': 0.7691, 'grad_norm': 1.0976710319519043, 'learning_rate': 2.473554512136933e-07, 'epoch': 0.98}
{'loss': 0.824, 'grad_norm': 1.2133986949920654, 'learning_rate': 2.4359497401758024e-07, 'epoch': 0.98}
{'loss': 0.8695, 'grad_norm': 1.0039209127426147, 'learning_rate': 2.3986326608818054e-07, 'epoch': 0.98}
{'loss': 0.9568, 'grad_norm': 1.2634028196334839, 'learning_rate': 2.361603285017111e-07, 'epoch': 0.98}
{'loss': 1.014, 'grad_norm': 1.1138190031051636, 'learning_rate': 2.3248616232608433e-07, 'epoch': 0.98}
{'loss': 0.9211, 'grad_norm': 1.1435914039611816, 'learning_rate': 2.288407686208971e-07, 'epoch': 0.98}
{'loss': 1.142, 'grad_norm': 1.2472330331802368, 'learning_rate': 2.2522414843748618e-07, 'epoch': 0.98}
{'loss': 0.8678, 'grad_norm': 1.0080487728118896, 'learning_rate': 2.2163630281885062e-07, 'epoch': 0.98}
{'loss': 0.7623, 'grad_norm': 1.2458217144012451, 'learning_rate': 2.1807723279971826e-07, 'epoch': 0.98}
{'loss': 0.8424, 'grad_norm': 1.0239325761795044, 'learning_rate': 2.145469394065014e-07, 'epoch': 0.98}
{'loss': 0.6872, 'grad_norm': 1.039662480354309, 'learning_rate': 2.1104542365730783e-07, 'epoch': 0.98}
{'loss': 0.7753, 'grad_norm': 1.153082251548767, 'learning_rate': 2.0757268656198537e-07, 'epoch': 0.98}
{'loss': 0.8342, 'grad_norm': 1.0751127004623413, 'learning_rate': 2.0412872912203284e-07, 'epoch': 0.98}
{'loss': 1.0724, 'grad_norm': 0.9975805878639221, 'learning_rate': 2.00713552330678e-07, 'epoch': 0.98}
{'loss': 1.0019, 'grad_norm': 1.097998023033142, 'learning_rate': 1.973271571728441e-07, 'epoch': 0.98}
{'loss': 1.1888, 'grad_norm': 1.0702052116394043, 'learning_rate': 1.9396954462514994e-07, 'epoch': 0.98}
{'loss': 1.2774, 'grad_norm': 1.089975118637085, 'learning_rate': 1.906407156559098e-07, 'epoch': 0.98}
{'loss': 0.8904, 'grad_norm': 1.1971133947372437, 'learning_rate': 1.8734067122514465e-07, 'epoch': 0.98}
{'loss': 1.1123, 'grad_norm': 1.0515928268432617, 'learning_rate': 1.8406941228457098e-07, 'epoch': 0.98}
{'loss': 1.2437, 'grad_norm': 1.0032799243927002, 'learning_rate': 1.808269397776119e-07, 'epoch': 0.98}
{'loss': 1.0454, 'grad_norm': 1.2154618501663208, 'learning_rate': 1.7761325463937494e-07, 'epoch': 0.98}
{'loss': 0.9865, 'grad_norm': 1.210555076599121, 'learning_rate': 1.744283577966632e-07, 'epoch': 0.98}
{'loss': 0.6168, 'grad_norm': 1.0916780233383179, 'learning_rate': 1.7127225016799753e-07, 'epoch': 0.98}
{'loss': 0.6898, 'grad_norm': 1.0693262815475464, 'learning_rate': 1.68144932663572e-07, 'epoch': 0.98}
{'loss': 0.7909, 'grad_norm': 1.224546194076538, 'learning_rate': 1.6504640618530963e-07, 'epoch': 0.98}
{'loss': 0.8093, 'grad_norm': 1.1891802549362183, 'learning_rate': 1.6197667162679564e-07, 'epoch': 0.98}
{'loss': 0.8138, 'grad_norm': 0.9987505078315735, 'learning_rate': 1.5893572987333293e-07, 'epoch': 0.98}
{'loss': 0.9059, 'grad_norm': 1.1018162965774536, 'learning_rate': 1.5592358180189782e-07, 'epoch': 0.98}
{'loss': 0.9218, 'grad_norm': 0.9214147329330444, 'learning_rate': 1.5294022828120647e-07, 'epoch': 0.98}
{'loss': 0.8784, 'grad_norm': 1.0654230117797852, 'learning_rate': 1.4998567017162624e-07, 'epoch': 0.98}
{'loss': 1.0989, 'grad_norm': 1.0764223337173462, 'learning_rate': 1.470599083252311e-07, 'epoch': 0.98}
{'loss': 1.1394, 'grad_norm': 1.7858065366744995, 'learning_rate': 1.4416294358582384e-07, 'epoch': 0.98}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 0.8186, 'grad_norm': 1.44539475440979, 'learning_rate': 1.412947767888473e-07, 'epoch': 0.98}
{'loss': 0.9482, 'grad_norm': 1.5030885934829712, 'learning_rate': 1.3845540876147312e-07, 'epoch': 0.98}
{'loss': 0.7285, 'grad_norm': 1.20796537399292, 'learning_rate': 1.3564484032257963e-07, 'epoch': 0.99}
{'loss': 0.9663, 'grad_norm': 1.4428735971450806, 'learning_rate': 1.3286307228269623e-07, 'epoch': 0.99}
{'loss': 0.9813, 'grad_norm': 1.0480828285217285, 'learning_rate': 1.301101054440923e-07, 'epoch': 0.99}
{'loss': 0.8967, 'grad_norm': 1.072501540184021, 'learning_rate': 1.273859406006883e-07, 'epoch': 0.99}
{'loss': 1.1705, 'grad_norm': 1.14412522315979, 'learning_rate': 1.246905785381447e-07, 'epoch': 0.99}
{'loss': 1.177, 'grad_norm': 1.2648640871047974, 'learning_rate': 1.22024020033773e-07, 'epoch': 0.99}
{'loss': 1.0987, 'grad_norm': 1.0344055891036987, 'learning_rate': 1.193862658566025e-07, 'epoch': 0.99}
{'loss': 0.631, 'grad_norm': 1.0579670667648315, 'learning_rate': 1.1677731676733584e-07, 'epoch': 0.99}
{'loss': 0.6631, 'grad_norm': 1.371724247932434, 'learning_rate': 1.1419717351841552e-07, 'epoch': 0.99}
{'loss': 0.8279, 'grad_norm': 0.9726997017860413, 'learning_rate': 1.1164583685390195e-07, 'epoch': 0.99}
{'loss': 0.9205, 'grad_norm': 1.4617058038711548, 'learning_rate': 1.0912330750961763e-07, 'epoch': 0.99}
{'loss': 0.8765, 'grad_norm': 1.20173978805542, 'learning_rate': 1.066295862130362e-07, 'epoch': 0.99}
{'loss': 0.6157, 'grad_norm': 1.0708943605422974, 'learning_rate': 1.0416467368332683e-07, 'epoch': 0.99}
{'loss': 1.0355, 'grad_norm': 1.0766230821609497, 'learning_rate': 1.0172857063137641e-07, 'epoch': 0.99}
{'loss': 0.8458, 'grad_norm': 1.3302785158157349, 'learning_rate': 9.93212777597341e-08, 'epoch': 0.99}
{'loss': 0.8087, 'grad_norm': 1.0413743257522583, 'learning_rate': 9.694279576265564e-08, 'epoch': 0.99}
{'loss': 0.9413, 'grad_norm': 1.0130023956298828, 'learning_rate': 9.459312532608122e-08, 'epoch': 0.99}
{'loss': 0.8424, 'grad_norm': 1.1293538808822632, 'learning_rate': 9.227226712763549e-08, 'epoch': 0.99}
{'loss': 0.9609, 'grad_norm': 1.1883435249328613, 'learning_rate': 8.998022183666077e-08, 'epoch': 0.99}
{'loss': 0.9771, 'grad_norm': 0.9918035268783569, 'learning_rate': 8.771699011416168e-08, 'epoch': 0.99}
{'loss': 0.7918, 'grad_norm': 1.161520004272461, 'learning_rate': 8.548257261284941e-08, 'epoch': 0.99}
{'loss': 0.8724, 'grad_norm': 1.1664830446243286, 'learning_rate': 8.327696997710855e-08, 'epoch': 0.99}
{'loss': 0.7256, 'grad_norm': 1.3806253671646118, 'learning_rate': 8.110018284304133e-08, 'epoch': 0.99}
{'loss': 0.9771, 'grad_norm': 1.1160900592803955, 'learning_rate': 7.895221183840118e-08, 'epoch': 0.99}
{'loss': 0.8108, 'grad_norm': 0.9475555419921875, 'learning_rate': 7.683305758265924e-08, 'epoch': 0.99}
{'loss': 0.8645, 'grad_norm': 1.216929316520691, 'learning_rate': 7.474272068698218e-08, 'epoch': 0.99}
{'loss': 1.0393, 'grad_norm': 1.4192407131195068, 'learning_rate': 7.268120175421001e-08, 'epoch': 0.99}
{'loss': 1.1252, 'grad_norm': 1.767665147781372, 'learning_rate': 7.064850137885604e-08, 'epoch': 0.99}
{'loss': 1.079, 'grad_norm': 1.1992217302322388, 'learning_rate': 6.864462014716245e-08, 'epoch': 0.99}
{'loss': 0.7397, 'grad_norm': 1.1454674005508423, 'learning_rate': 6.666955863703361e-08, 'epoch': 0.99}
{'loss': 0.7847, 'grad_norm': 1.1700820922851562, 'learning_rate': 6.472331741805837e-08, 'epoch': 0.99}
{'loss': 0.7363, 'grad_norm': 1.1719233989715576, 'learning_rate': 6.280589705153217e-08, 'epoch': 0.99}
{'loss': 0.8009, 'grad_norm': 1.0431272983551025, 'learning_rate': 6.09172980904238e-08, 'epoch': 0.99}
{'loss': 0.8399, 'grad_norm': 1.2107163667678833, 'learning_rate': 5.905752107940865e-08, 'epoch': 0.99}
{'loss': 0.9458, 'grad_norm': 1.1817879676818848, 'learning_rate': 5.722656655482439e-08, 'epoch': 0.99}
{'loss': 0.9688, 'grad_norm': 1.1792089939117432, 'learning_rate': 5.542443504471528e-08, 'epoch': 0.99}
{'loss': 1.0321, 'grad_norm': 1.130786657333374, 'learning_rate': 5.3651127068798934e-08, 'epoch': 0.99}
{'loss': 0.956, 'grad_norm': 1.2416648864746094, 'learning_rate': 5.190664313851068e-08, 'epoch': 0.99}
{'loss': 0.7876, 'grad_norm': 1.1932716369628906, 'learning_rate': 5.019098375692588e-08, 'epoch': 0.99}
{'loss': 0.8815, 'grad_norm': 0.9069580435752869, 'learning_rate': 4.850414941883763e-08, 'epoch': 0.99}
{'loss': 1.0619, 'grad_norm': 1.1232177019119263, 'learning_rate': 4.684614061073456e-08, 'epoch': 0.99}
{'loss': 1.1878, 'grad_norm': 1.1609395742416382, 'learning_rate': 4.521695781076751e-08, 'epoch': 0.99}
{'loss': 0.9589, 'grad_norm': 1.1950019598007202, 'learning_rate': 4.361660148879398e-08, 'epoch': 0.99}
{'loss': 1.0915, 'grad_norm': 1.1465415954589844, 'learning_rate': 4.2045072106333684e-08, 'epoch': 0.99}
{'loss': 1.0022, 'grad_norm': 1.1107301712036133, 'learning_rate': 4.0502370116624055e-08, 'epoch': 0.99}
{'loss': 0.8302, 'grad_norm': 1.0632096529006958, 'learning_rate': 3.898849596456478e-08, 'epoch': 0.99}
{'loss': 0.9671, 'grad_norm': 1.1001064777374268, 'learning_rate': 3.750345008675105e-08, 'epoch': 0.99}
{'loss': 0.9529, 'grad_norm': 1.5115082263946533, 'learning_rate': 3.60472329114625e-08, 'epoch': 0.99}
/home/ostrich/anaconda3/envs/my_master/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
{'loss': 1.0671, 'grad_norm': 1.0701971054077148, 'learning_rate': 3.461984485866321e-08, 'epoch': 0.99}
{'loss': 0.9913, 'grad_norm': 1.077005386352539, 'learning_rate': 3.3221286340001654e-08, 'epoch': 0.99}
{'loss': 0.9353, 'grad_norm': 0.99635910987854, 'learning_rate': 3.1851557758832975e-08, 'epoch': 0.99}
{'loss': 0.8513, 'grad_norm': 1.1780848503112793, 'learning_rate': 3.0510659510163406e-08, 'epoch': 0.99}
{'loss': 1.0763, 'grad_norm': 1.2811356782913208, 'learning_rate': 2.9198591980705848e-08, 'epoch': 0.99}
{'loss': 0.9426, 'grad_norm': 0.9664118885993958, 'learning_rate': 2.7915355548857603e-08, 'epoch': 0.99}
{'loss': 0.687, 'grad_norm': 1.2808040380477905, 'learning_rate': 2.6660950584689314e-08, 'epoch': 0.99}
{'loss': 0.8618, 'grad_norm': 1.1413928270339966, 'learning_rate': 2.543537744997826e-08, 'epoch': 0.99}
{'loss': 1.0518, 'grad_norm': 1.077817440032959, 'learning_rate': 2.4238636498163935e-08, 'epoch': 0.99}
{'loss': 0.7443, 'grad_norm': 0.8864713907241821, 'learning_rate': 2.3070728074381376e-08, 'epoch': 0.99}
{'loss': 0.6722, 'grad_norm': 1.027719259262085, 'learning_rate': 2.193165251545004e-08, 'epoch': 0.99}
{'loss': 0.8235, 'grad_norm': 1.1808347702026367, 'learning_rate': 2.0821410149884922e-08, 'epoch': 0.99}
{'loss': 0.6, 'grad_norm': 0.9361879825592041, 'learning_rate': 1.974000129785214e-08, 'epoch': 0.99}
{'loss': 0.8281, 'grad_norm': 1.248989462852478, 'learning_rate': 1.8687426271246645e-08, 'epoch': 0.99}
{'loss': 1.0837, 'grad_norm': 1.254574179649353, 'learning_rate': 1.7663685373614514e-08, 'epoch': 0.99}
{'loss': 1.1444, 'grad_norm': 1.271748423576355, 'learning_rate': 1.666877890020846e-08, 'epoch': 0.99}
{'loss': 1.0984, 'grad_norm': 1.0772054195404053, 'learning_rate': 1.570270713794342e-08, 'epoch': 0.99}
{'loss': 1.2048, 'grad_norm': 1.284195065498352, 'learning_rate': 1.4765470365429857e-08, 'epoch': 1.0}
{'loss': 0.8822, 'grad_norm': 0.9631016850471497, 'learning_rate': 1.3857068852962674e-08, 'epoch': 1.0}
{'loss': 1.0916, 'grad_norm': 0.9480567574501038, 'learning_rate': 1.2977502862532297e-08, 'epoch': 1.0}
{'loss': 0.7913, 'grad_norm': 1.0280276536941528, 'learning_rate': 1.2126772647780282e-08, 'epoch': 1.0}
{'loss': 0.9189, 'grad_norm': 1.2603685855865479, 'learning_rate': 1.1304878454077017e-08, 'epoch': 1.0}
{'loss': 0.7458, 'grad_norm': 0.9009056091308594, 'learning_rate': 1.0511820518432913e-08, 'epoch': 1.0}
{'loss': 0.9138, 'grad_norm': 1.116525411605835, 'learning_rate': 9.747599069576119e-09, 'epoch': 1.0}
{'loss': 0.9338, 'grad_norm': 1.0388926267623901, 'learning_rate': 9.012214327897006e-09, 'epoch': 1.0}
{'loss': 0.7391, 'grad_norm': 0.9556406736373901, 'learning_rate': 8.305666505481479e-09, 'epoch': 1.0}
{'loss': 0.6713, 'grad_norm': 1.255623698234558, 'learning_rate': 7.627955806088772e-09, 'epoch': 1.0}
{'loss': 1.1184, 'grad_norm': 1.3572243452072144, 'learning_rate': 6.9790824251736444e-09, 'epoch': 1.0}
{'loss': 0.5031, 'grad_norm': 0.7943048477172852, 'learning_rate': 6.359046549864189e-09, 'epoch': 1.0}
{'loss': 0.8774, 'grad_norm': 1.0953896045684814, 'learning_rate': 5.767848358972927e-09, 'epoch': 1.0}
{'loss': 1.1448, 'grad_norm': 0.9102027416229248, 'learning_rate': 5.205488022996807e-09, 'epoch': 1.0}
{'loss': 1.2697, 'grad_norm': 1.173194169998169, 'learning_rate': 4.6719657041283114e-09, 'epoch': 1.0}
{'loss': 1.2915, 'grad_norm': 1.1258288621902466, 'learning_rate': 4.167281556233249e-09, 'epoch': 1.0}
{'loss': 0.9629, 'grad_norm': 1.0075249671936035, 'learning_rate': 3.6914357248507556e-09, 'epoch': 1.0}
{'loss': 0.7491, 'grad_norm': 1.1317287683486938, 'learning_rate': 3.244428347204398e-09, 'epoch': 1.0}
{'loss': 1.1368, 'grad_norm': 1.0136710405349731, 'learning_rate': 2.8262595522354775e-09, 'epoch': 1.0}
{'loss': 0.7406, 'grad_norm': 1.045649766921997, 'learning_rate': 2.4369294605253166e-09, 'epoch': 1.0}
{'loss': 1.154, 'grad_norm': 0.9701056480407715, 'learning_rate': 2.0764381843507708e-09, 'epoch': 1.0}
{'loss': 0.8824, 'grad_norm': 1.319975733757019, 'learning_rate': 1.7447858276842255e-09, 'epoch': 1.0}
{'loss': 0.815, 'grad_norm': 0.997291088104248, 'learning_rate': 1.4419724861602924e-09, 'epoch': 1.0}
{'loss': 0.8322, 'grad_norm': 1.3392467498779297, 'learning_rate': 1.167998247131319e-09, 'epoch': 1.0}
{'loss': 1.1456, 'grad_norm': 1.1727887392044067, 'learning_rate': 9.228631895896733e-10, 'epoch': 1.0}
{'loss': 0.9692, 'grad_norm': 1.2216781377792358, 'learning_rate': 7.065673842454601e-10, 'epoch': 1.0}
{'loss': 1.0728, 'grad_norm': 1.2782524824142456, 'learning_rate': 5.191108934710087e-10, 'epoch': 1.0}
{'loss': 1.1332, 'grad_norm': 1.2574446201324463, 'learning_rate': 3.604937713230783e-10, 'epoch': 1.0}
{'loss': 0.8766, 'grad_norm': 1.1591815948486328, 'learning_rate': 2.3071606354285735e-10, 'epoch': 1.0}
{'loss': 1.0211, 'grad_norm': 1.2462477684020996, 'learning_rate': 1.297778075781686e-10, 'epoch': 1.0}
{'loss': 1.0379, 'grad_norm': 0.9535603523254395, 'learning_rate': 5.767903251685525e-11, 'epoch': 1.0}
{'loss': 0.8357, 'grad_norm': 1.0342227220535278, 'learning_rate': 1.4419759164496782e-11, 'epoch': 1.0}
{'loss': 1.1714, 'grad_norm': 0.9238348603248596, 'learning_rate': 0.0, 'epoch': 1.0}
