{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SFT finetune the `phi-2` model\n",
    "\n",
    "- Fine-tune the `phi-2` model from Microsoft to better align with human preferences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -q datasets transformers bitsandbytes sentencepiece wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import AutoPeftModelForCausalLM, PeftConfig\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, PeftModelForCausalLM\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "\n",
    "# import bitsandbytes as bnb\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "#### basic config\n",
    "base_model_name = \"microsoft/phi-2\"\n",
    "modified_model_name = \"phi2-sft-alpaca\"\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "# max_steps = 1000\n",
    "\n",
    "secrets_path = \"./secrets/secrets.json\"\n",
    "output_dir = f\".models/adapters/{modified_model_name}_alignment-handbook\"\n",
    "\n",
    "\n",
    "run_name = f\"{modified_model_name}-{num_epochs}_alignment-handbook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ostrich/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the secrets\n",
    "\n",
    "with open(secrets_path, \"r\") as f:\n",
    "    secrets = json.load(f)\n",
    "\n",
    "HF_TOKEN = secrets[\"HF_TOKEN\"]\n",
    "WANDB_TOKEN = secrets[\"WANDB_TOKEN\"]\n",
    "\n",
    "wandb.login(key=WANDB_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised fine-tune the model on the distillaplaca dataset\n",
    "\n",
    "#### Download the alpaca data using wget\n",
    "# ! wget https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json -O ./data/alpaca_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the downloaded dataset into hugging face\n",
    "train_dataset = load_dataset(\"data\", data_files=\"alpaca_data.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'input', 'instruction'],\n",
       "    num_rows: 52002\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.',\n",
       " 'input': '',\n",
       " 'instruction': 'Give three tips for staying healthy.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one example\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the chatml template since we will be utilizing the chatml format primarily\n",
    "\n",
    "\n",
    "def convert_chatml(example, tokenizer: AutoTokenizer, add_eos: bool = False):\n",
    "\n",
    "    # system prompt\n",
    "    ## Prepare the chatml message\n",
    "\n",
    "    system_prompt = (\n",
    "        {\"role\": \"system\", \"content\": example[\"input\"]}\n",
    "        if len(example[\"input\"]) > 0\n",
    "        else {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are Phi a friendly chat assistant that follows user instructions.\",\n",
    "        }\n",
    "    )\n",
    "    inst_prompt = {\"role\": \"user\", \"content\": example[\"instruction\"]}\n",
    "    assistant_prompt = {\"role\": \"assistant\", \"content\": example[\"output\"]}\n",
    "\n",
    "    if system_prompt is not None:\n",
    "        message = [system_prompt, inst_prompt, assistant_prompt]\n",
    "    else:\n",
    "        message = [inst_prompt, assistant_prompt]\n",
    "\n",
    "    return {\"text\": tokenizer.apply_chat_template(message, tokenize=False)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_name,\n",
    "    # add_eos_token=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Look at a tokenization example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[40, 550], [257, 1049], [4320, 50296]], 'attention_mask': [[1, 1], [1, 1], [1, 1]], 'overflow_to_sample_mapping': [0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "ex = \"I had a great dream<|im_end|>\"\n",
    "\n",
    "encoded_ex = tokenizer(\n",
    "    ex,\n",
    "    # padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    add_special_tokens=True,\n",
    "    max_length=2,\n",
    "    return_overflowing_tokens=True\n",
    "\n",
    ")\n",
    "print(encoded_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4666ace258e544739026b097495e5064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(base_model_name, max_shard_size=\"2GB\", safe_serialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2560)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x PhiDecoderLayer(\n",
       "        (self_attn): PhiFlashAttention2(\n",
       "          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "          (rotary_emb): PhiRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51200"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head.out_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand the vocabulary, to fit the chat tokens [Optional]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50298, 2560)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class SpecialTokens:\n",
    "    conversation_start_token: str = \"<|im_start|>\"\n",
    "    conversation_end_token: str = \"<|im_end|>\"\n",
    "    pad_token: str = \"<|pad|>\"\n",
    "\n",
    "\n",
    "chat_format_tokens = SpecialTokens\n",
    "\n",
    "# Add special tokens to the tokenizer\n",
    "tokenizer.add_special_tokens(\n",
    "    {\n",
    "        \"additional_special_tokens\": [\n",
    "            chat_format_tokens.conversation_start_token,\n",
    "            chat_format_tokens.conversation_end_token,\n",
    "            chat_format_tokens.pad_token,\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "# resize the model embedding layers ???? or do we ?? .. Its already expanded\n",
    "# if model.lm_head.out_features < len(tokenizer):\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the peft model config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        # \"embed_tokens.weight\",\n",
    "        # \"lm_head.weight\",\n",
    "    ],\n",
    "    modules_to_save=[\"embed_tokens\", \"lm_head\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|pad|>'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_format_tokens.pad_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data and the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CHAT_TEMPLATE = \n",
    "    {% if not add_generation_prompt is defined %}\n",
    "        {% set add_generation_prompt=false %}\n",
    "    {% endif %}\n",
    "    {% for message in messages %}\n",
    "        {{'<|im_start|>' + message['role'] + '\\n' + message['content'] +  + '<|im_end|>' + '\\n'}}\n",
    "    {% endfor %}\n",
    "    {% if add_generation_prompt %}\n",
    "        {{'<|im_start|>assistant' + '\\n'}}\n",
    "    {% elif not add_generation_prompt %}\n",
    "        {{eos_token}}\n",
    "    {% endif %}\n",
    "\"\"\"\n",
    "\n",
    "CHAT_TEMPLATE = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{'<|im_start|>assistant' + '\\n'}}{% elif not add_generation_prompt %}{{eos_token}}{% endif %}\"\n",
    "\n",
    "tokenizer.chat_template = (\n",
    "    CHAT_TEMPLATE  # set a new chat template (slightly different chatml template)\n",
    ")\n",
    "tokenizer.pad_token = chat_format_tokens.pad_token  # set the new pad token\n",
    "tokenizer.padding_side = (\n",
    "    \"right\"  # should ideally set to left but it causes some overflow issue\n",
    ")\n",
    "# tokenizer.truncation_side = \"left\"  # to avoid truncating latest generation\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    convert_chatml,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<|im_start|>system\\nYou are Phi a friendly chat assistant that follows user instructions.<|im_end|>\\n<|im_start|>user\\nDescribe the structure of an atom.<|im_end|>\\n<|im_start|>assistant\\nAn atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.<|im_end|>\\n'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### Decide on the max seq len to use\n",
    "import numpy as np\n",
    "\n",
    "text_max_len = int(\n",
    "    np.percentile(\n",
    "        [len(tokenizer(example[\"text\"])[\"input_ids\"]) for example in train_dataset],\n",
    "        99,\n",
    "    ),\n",
    ")\n",
    "text_max_len\n",
    "\n",
    "\n",
    "### The length for most of the examples in the alpaca dataset are less than 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 273,304,698 || all params: 3,048,369,396 || trainable%: 8.965602999381378\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ostrich/anaconda3/envs/my_master/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:317: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/ostrich/anaconda3/envs/my_master/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### Trainer Args\n",
    "\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\n",
    "        \"use_reentrant\": False\n",
    "    },  # Avoids saving the whole graphs, computes the required activations.\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-5,\n",
    "    max_grad_norm=0.3,  # Clips the gradnorm value to 0.3\n",
    "    num_train_epochs=num_epochs,\n",
    "    # max_steps=max_steps,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,  # Around 10% warmup Zephyr Recipie\n",
    "    log_level=\"error\",  # Avoid overthinking due to unnecessary logs.\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    # eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,  # Save every evalutaion steps\n",
    "    save_total_limit=3,\n",
    "    tf32=True,  # TensorFloat32 dtype supported for GeForce 40 series\n",
    "    bf16=True,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=run_name,\n",
    "    disable_tqdm=True,\n",
    "    # groupby_length=True,   # Groups examples of similar lengths together to save padding\n",
    ")\n",
    "\n",
    "\n",
    "### SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=trainer_args,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset_text_field=\"text\",\n",
    "    peft_config=lora_config,\n",
    "    packing=True,\n",
    "    max_seq_length=1024,  # max len of the phi2 model\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,\n",
    "        \"append_concat_token\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "print(trainer.model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/ostrich/ubuntu/my_projects/llm-notebooks/wandb/run-20240407_083733-dj3r7dap</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/parth-shastri/huggingface/runs/dj3r7dap' target=\"_blank\">phi2-sft-alpaca-1_alignment-handbook</a></strong> to <a href='https://wandb.ai/parth-shastri/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/parth-shastri/huggingface' target=\"_blank\">https://wandb.ai/parth-shastri/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/parth-shastri/huggingface/runs/dj3r7dap' target=\"_blank\">https://wandb.ai/parth-shastri/huggingface/runs/dj3r7dap</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.895, 'grad_norm': 492.0, 'learning_rate': 6.25e-07, 'epoch': 0.0}\n",
      "{'loss': 3.0245, 'grad_norm': 386.0, 'learning_rate': 1.25e-06, 'epoch': 0.01}\n",
      "{'loss': 3.0733, 'grad_norm': 358.0, 'learning_rate': 1.8750000000000003e-06, 'epoch': 0.01}\n",
      "{'loss': 3.128, 'grad_norm': 352.0, 'learning_rate': 2.5e-06, 'epoch': 0.01}\n",
      "{'loss': 3.186, 'grad_norm': 292.0, 'learning_rate': 3.125e-06, 'epoch': 0.02}\n",
      "{'loss': 3.1209, 'grad_norm': 406.0, 'learning_rate': 3.7500000000000005e-06, 'epoch': 0.02}\n",
      "{'loss': 3.2977, 'grad_norm': 316.0, 'learning_rate': 4.3750000000000005e-06, 'epoch': 0.02}\n",
      "{'loss': 3.1761, 'grad_norm': 282.0, 'learning_rate': 5e-06, 'epoch': 0.03}\n",
      "{'loss': 3.1725, 'grad_norm': 248.0, 'learning_rate': 5.625e-06, 'epoch': 0.03}\n",
      "{'loss': 3.1725, 'grad_norm': 368.0, 'learning_rate': 6.25e-06, 'epoch': 0.03}\n",
      "{'loss': 2.8646, 'grad_norm': 310.0, 'learning_rate': 6.875e-06, 'epoch': 0.04}\n",
      "{'loss': 2.945, 'grad_norm': 636.0, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.04}\n",
      "{'loss': 2.8084, 'grad_norm': 292.0, 'learning_rate': 8.125000000000001e-06, 'epoch': 0.04}\n",
      "{'loss': 2.966, 'grad_norm': 266.0, 'learning_rate': 8.750000000000001e-06, 'epoch': 0.04}\n",
      "{'loss': 2.9728, 'grad_norm': 474.0, 'learning_rate': 9.375000000000001e-06, 'epoch': 0.05}\n",
      "{'loss': 2.8747, 'grad_norm': 384.0, 'learning_rate': 1e-05, 'epoch': 0.05}\n",
      "{'loss': 3.0196, 'grad_norm': 496.0, 'learning_rate': 1.0625e-05, 'epoch': 0.05}\n",
      "{'loss': 2.8977, 'grad_norm': 288.0, 'learning_rate': 1.125e-05, 'epoch': 0.06}\n",
      "{'loss': 2.7115, 'grad_norm': 552.0, 'learning_rate': 1.1875e-05, 'epoch': 0.06}\n",
      "{'loss': 2.905, 'grad_norm': 272.0, 'learning_rate': 1.25e-05, 'epoch': 0.06}\n",
      "{'loss': 2.7612, 'grad_norm': 260.0, 'learning_rate': 1.3125e-05, 'epoch': 0.07}\n",
      "{'loss': 2.926, 'grad_norm': 298.0, 'learning_rate': 1.375e-05, 'epoch': 0.07}\n",
      "{'loss': 2.5952, 'grad_norm': 211.0, 'learning_rate': 1.4375e-05, 'epoch': 0.07}\n",
      "{'loss': 2.6169, 'grad_norm': 236.0, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.08}\n",
      "{'loss': 2.5385, 'grad_norm': 360.0, 'learning_rate': 1.5625e-05, 'epoch': 0.08}\n",
      "{'loss': 2.4679, 'grad_norm': 241.0, 'learning_rate': 1.6250000000000002e-05, 'epoch': 0.08}\n",
      "{'loss': 2.6208, 'grad_norm': 225.0, 'learning_rate': 1.6875e-05, 'epoch': 0.09}\n",
      "{'loss': 2.5198, 'grad_norm': 246.0, 'learning_rate': 1.7500000000000002e-05, 'epoch': 0.09}\n",
      "{'loss': 2.606, 'grad_norm': 146.0, 'learning_rate': 1.8125e-05, 'epoch': 0.09}\n",
      "{'loss': 2.4434, 'grad_norm': 150.0, 'learning_rate': 1.8750000000000002e-05, 'epoch': 0.1}\n",
      "{'loss': 2.5192, 'grad_norm': 390.0, 'learning_rate': 1.9375e-05, 'epoch': 0.1}\n",
      "{'loss': 2.5137, 'grad_norm': 308.0, 'learning_rate': 2e-05, 'epoch': 0.1}\n",
      "{'loss': 2.3899, 'grad_norm': 322.0, 'learning_rate': 1.9999375039475278e-05, 'epoch': 0.11}\n",
      "{'loss': 2.507, 'grad_norm': 243.0, 'learning_rate': 1.9997500236016233e-05, 'epoch': 0.11}\n",
      "{'loss': 2.4771, 'grad_norm': 140.0, 'learning_rate': 1.9994375823958504e-05, 'epoch': 0.11}\n",
      "{'loss': 2.346, 'grad_norm': 352.0, 'learning_rate': 1.9990002193828923e-05, 'epoch': 0.11}\n",
      "{'loss': 2.2413, 'grad_norm': 548.0, 'learning_rate': 1.998437989229673e-05, 'epoch': 0.12}\n",
      "{'loss': 2.3727, 'grad_norm': 107.5, 'learning_rate': 1.9977509622105233e-05, 'epoch': 0.12}\n",
      "{'loss': 2.1825, 'grad_norm': 92.5, 'learning_rate': 1.9969392241983957e-05, 'epoch': 0.12}\n",
      "{'loss': 2.2891, 'grad_norm': 77.5, 'learning_rate': 1.9960028766541336e-05, 'epoch': 0.13}\n",
      "{'loss': 2.3051, 'grad_norm': 82.5, 'learning_rate': 1.994942036613787e-05, 'epoch': 0.13}\n",
      "{'loss': 2.316, 'grad_norm': 133.0, 'learning_rate': 1.9937568366739858e-05, 'epoch': 0.13}\n",
      "{'loss': 2.1927, 'grad_norm': 106.0, 'learning_rate': 1.9924474249753656e-05, 'epoch': 0.14}\n",
      "{'loss': 2.1969, 'grad_norm': 95.0, 'learning_rate': 1.9910139651840497e-05, 'epoch': 0.14}\n",
      "{'loss': 2.2682, 'grad_norm': 89.5, 'learning_rate': 1.9894566364711965e-05, 'epoch': 0.14}\n",
      "{'loss': 2.2858, 'grad_norm': 89.0, 'learning_rate': 1.9877756334905983e-05, 'epoch': 0.15}\n",
      "{'loss': 2.2103, 'grad_norm': 119.0, 'learning_rate': 1.9859711663543573e-05, 'epoch': 0.15}\n",
      "{'loss': 2.1382, 'grad_norm': 64.5, 'learning_rate': 1.9840434606066182e-05, 'epoch': 0.15}\n",
      "{'loss': 2.2332, 'grad_norm': 125.5, 'learning_rate': 1.9819927571953804e-05, 'epoch': 0.16}\n",
      "{'loss': 2.157, 'grad_norm': 109.0, 'learning_rate': 1.9798193124423804e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ostrich/anaconda3/envs/my_master/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1144, 'grad_norm': 144.0, 'learning_rate': 1.9775233980110524e-05, 'epoch': 0.16}\n",
      "{'loss': 2.2385, 'grad_norm': 84.0, 'learning_rate': 1.9751053008725736e-05, 'epoch': 0.17}\n",
      "{'loss': 2.1322, 'grad_norm': 66.0, 'learning_rate': 1.9725653232699962e-05, 'epoch': 0.17}\n",
      "{'loss': 2.1009, 'grad_norm': 85.0, 'learning_rate': 1.969903782680467e-05, 'epoch': 0.17}\n",
      "{'loss': 2.2009, 'grad_norm': 61.25, 'learning_rate': 1.967121011775546e-05, 'epoch': 0.18}\n",
      "{'loss': 2.1498, 'grad_norm': 51.0, 'learning_rate': 1.9642173583796265e-05, 'epoch': 0.18}\n",
      "{'loss': 2.2042, 'grad_norm': 58.0, 'learning_rate': 1.961193185426459e-05, 'epoch': 0.18}\n",
      "{'loss': 2.1578, 'grad_norm': 53.75, 'learning_rate': 1.958048870913786e-05, 'epoch': 0.19}\n",
      "{'loss': 2.2291, 'grad_norm': 61.0, 'learning_rate': 1.9547848078560975e-05, 'epoch': 0.19}\n",
      "{'loss': 1.9899, 'grad_norm': 48.0, 'learning_rate': 1.9514014042355057e-05, 'epoch': 0.19}\n",
      "{'loss': 2.0433, 'grad_norm': 51.5, 'learning_rate': 1.9478990829507507e-05, 'epoch': 0.19}\n",
      "{'loss': 2.0615, 'grad_norm': 62.25, 'learning_rate': 1.9442782817643425e-05, 'epoch': 0.2}\n",
      "{'loss': 2.0284, 'grad_norm': 65.5, 'learning_rate': 1.9405394532478422e-05, 'epoch': 0.2}\n",
      "{'loss': 2.0776, 'grad_norm': 74.5, 'learning_rate': 1.9366830647252974e-05, 'epoch': 0.2}\n",
      "{'loss': 2.0606, 'grad_norm': 74.0, 'learning_rate': 1.9327095982148258e-05, 'epoch': 0.21}\n",
      "{'loss': 2.0377, 'grad_norm': 45.0, 'learning_rate': 1.928619550368371e-05, 'epoch': 0.21}\n",
      "{'loss': 2.0789, 'grad_norm': 79.0, 'learning_rate': 1.9244134324096223e-05, 'epoch': 0.21}\n",
      "{'loss': 1.9742, 'grad_norm': 48.5, 'learning_rate': 1.9200917700701176e-05, 'epoch': 0.22}\n",
      "{'loss': 1.9532, 'grad_norm': 43.75, 'learning_rate': 1.915655103523529e-05, 'epoch': 0.22}\n",
      "{'loss': 1.9647, 'grad_norm': 39.5, 'learning_rate': 1.9111039873181478e-05, 'epoch': 0.22}\n",
      "{'loss': 2.0003, 'grad_norm': 37.75, 'learning_rate': 1.9064389903075676e-05, 'epoch': 0.23}\n",
      "{'loss': 2.0467, 'grad_norm': 49.5, 'learning_rate': 1.901660695579585e-05, 'epoch': 0.23}\n",
      "{'loss': 1.9315, 'grad_norm': 42.0, 'learning_rate': 1.8967697003833156e-05, 'epoch': 0.23}\n",
      "{'loss': 1.9529, 'grad_norm': 39.75, 'learning_rate': 1.8917666160545446e-05, 'epoch': 0.24}\n",
      "{'loss': 1.9343, 'grad_norm': 46.5, 'learning_rate': 1.8866520679393127e-05, 'epoch': 0.24}\n",
      "{'loss': 1.9326, 'grad_norm': 51.75, 'learning_rate': 1.8814266953157557e-05, 'epoch': 0.24}\n",
      "{'loss': 1.9767, 'grad_norm': 46.0, 'learning_rate': 1.876091151314196e-05, 'epoch': 0.25}\n",
      "{'loss': 1.9807, 'grad_norm': 53.5, 'learning_rate': 1.8706461028355107e-05, 'epoch': 0.25}\n",
      "{'loss': 2.0785, 'grad_norm': 42.25, 'learning_rate': 1.865092230467769e-05, 'epoch': 0.25}\n",
      "{'loss': 1.9175, 'grad_norm': 31.5, 'learning_rate': 1.8594302284011704e-05, 'epoch': 0.26}\n",
      "{'loss': 1.8356, 'grad_norm': 39.25, 'learning_rate': 1.85366080434127e-05, 'epoch': 0.26}\n",
      "{'loss': 1.9461, 'grad_norm': 50.25, 'learning_rate': 1.8477846794205258e-05, 'epoch': 0.26}\n",
      "{'loss': 1.8695, 'grad_norm': 62.25, 'learning_rate': 1.8418025881081612e-05, 'epoch': 0.26}\n",
      "{'loss': 1.8076, 'grad_norm': 41.5, 'learning_rate': 1.8357152781183606e-05, 'epoch': 0.27}\n",
      "{'loss': 1.8593, 'grad_norm': 53.5, 'learning_rate': 1.829523510316813e-05, 'epoch': 0.27}\n",
      "{'loss': 1.8665, 'grad_norm': 52.5, 'learning_rate': 1.82322805862561e-05, 'epoch': 0.27}\n",
      "{'loss': 1.7466, 'grad_norm': 37.25, 'learning_rate': 1.8168297099265094e-05, 'epoch': 0.28}\n",
      "{'loss': 1.9329, 'grad_norm': 36.5, 'learning_rate': 1.810329263962584e-05, 'epoch': 0.28}\n",
      "{'loss': 1.778, 'grad_norm': 31.125, 'learning_rate': 1.803727533238257e-05, 'epoch': 0.28}\n",
      "{'loss': 1.8011, 'grad_norm': 39.5, 'learning_rate': 1.7970253429177477e-05, 'epoch': 0.29}\n",
      "{'loss': 1.8007, 'grad_norm': 31.875, 'learning_rate': 1.7902235307219333e-05, 'epoch': 0.29}\n",
      "{'loss': 1.773, 'grad_norm': 37.75, 'learning_rate': 1.7833229468236367e-05, 'epoch': 0.29}\n",
      "{'loss': 1.8195, 'grad_norm': 110.5, 'learning_rate': 1.776324453741365e-05, 'epoch': 0.3}\n",
      "{'loss': 1.8242, 'grad_norm': 28.125, 'learning_rate': 1.7692289262315e-05, 'epoch': 0.3}\n",
      "{'loss': 1.7246, 'grad_norm': 29.625, 'learning_rate': 1.7620372511789607e-05, 'epoch': 0.3}\n",
      "{'loss': 1.7223, 'grad_norm': 30.125, 'learning_rate': 1.75475032748635e-05, 'epoch': 0.31}\n",
      "{'loss': 1.8081, 'grad_norm': 42.75, 'learning_rate': 1.747369065961599e-05, 'epoch': 0.31}\n",
      "{'loss': 1.7432, 'grad_norm': 27.0, 'learning_rate': 1.7398943892041223e-05, 'epoch': 0.31}\n",
      "{'loss': 1.7574, 'grad_norm': 34.0, 'learning_rate': 1.7323272314895022e-05, 'epoch': 0.32}\n",
      "{'loss': 1.7842, 'grad_norm': 40.0, 'learning_rate': 1.7246685386527098e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ostrich/anaconda3/envs/my_master/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7384, 'grad_norm': 51.25, 'learning_rate': 1.7169192679698837e-05, 'epoch': 0.32}\n",
      "{'loss': 1.7036, 'grad_norm': 25.0, 'learning_rate': 1.7090803880386784e-05, 'epoch': 0.33}\n",
      "{'loss': 1.7882, 'grad_norm': 42.0, 'learning_rate': 1.701152878657197e-05, 'epoch': 0.33}\n",
      "{'loss': 1.7328, 'grad_norm': 25.625, 'learning_rate': 1.693137730701524e-05, 'epoch': 0.33}\n",
      "{'loss': 1.749, 'grad_norm': 39.5, 'learning_rate': 1.6850359460018737e-05, 'epoch': 0.33}\n",
      "{'loss': 1.7743, 'grad_norm': 41.5, 'learning_rate': 1.6768485372173696e-05, 'epoch': 0.34}\n",
      "{'loss': 1.7444, 'grad_norm': 41.0, 'learning_rate': 1.6685765277094702e-05, 'epoch': 0.34}\n",
      "{'loss': 1.6448, 'grad_norm': 26.875, 'learning_rate': 1.6602209514140552e-05, 'epoch': 0.34}\n",
      "{'loss': 1.7149, 'grad_norm': 30.625, 'learning_rate': 1.6517828527121942e-05, 'epoch': 0.35}\n",
      "{'loss': 1.6457, 'grad_norm': 41.75, 'learning_rate': 1.6432632862996056e-05, 'epoch': 0.35}\n",
      "{'loss': 1.6901, 'grad_norm': 30.0, 'learning_rate': 1.634663317054829e-05, 'epoch': 0.35}\n",
      "{'loss': 1.6834, 'grad_norm': 26.875, 'learning_rate': 1.6259840199061215e-05, 'epoch': 0.36}\n",
      "{'loss': 1.6751, 'grad_norm': 34.25, 'learning_rate': 1.617226479697105e-05, 'epoch': 0.36}\n",
      "{'loss': 1.6081, 'grad_norm': 33.75, 'learning_rate': 1.608391791051163e-05, 'epoch': 0.36}\n",
      "{'loss': 1.6906, 'grad_norm': 26.75, 'learning_rate': 1.599481058234626e-05, 'epoch': 0.37}\n",
      "{'loss': 1.6907, 'grad_norm': 40.75, 'learning_rate': 1.5904953950187458e-05, 'epoch': 0.37}\n",
      "{'loss': 1.6412, 'grad_norm': 35.0, 'learning_rate': 1.5814359245404818e-05, 'epoch': 0.37}\n",
      "{'loss': 1.6548, 'grad_norm': 30.625, 'learning_rate': 1.5723037791621193e-05, 'epoch': 0.38}\n",
      "{'loss': 1.7316, 'grad_norm': 38.5, 'learning_rate': 1.563100100329731e-05, 'epoch': 0.38}\n",
      "{'loss': 1.645, 'grad_norm': 29.75, 'learning_rate': 1.5538260384305076e-05, 'epoch': 0.38}\n",
      "{'loss': 1.6413, 'grad_norm': 29.625, 'learning_rate': 1.5444827526489675e-05, 'epoch': 0.39}\n",
      "{'loss': 1.541, 'grad_norm': 29.375, 'learning_rate': 1.5350714108220673e-05, 'epoch': 0.39}\n",
      "{'loss': 1.6612, 'grad_norm': 23.0, 'learning_rate': 1.5255931892932333e-05, 'epoch': 0.39}\n",
      "{'loss': 1.5767, 'grad_norm': 27.0, 'learning_rate': 1.5160492727653241e-05, 'epoch': 0.4}\n",
      "{'loss': 1.6317, 'grad_norm': 26.625, 'learning_rate': 1.5064408541525573e-05, 'epoch': 0.4}\n",
      "{'loss': 1.594, 'grad_norm': 23.75, 'learning_rate': 1.4967691344313995e-05, 'epoch': 0.4}\n",
      "{'loss': 1.5997, 'grad_norm': 29.0, 'learning_rate': 1.4870353224904572e-05, 'epoch': 0.41}\n",
      "{'loss': 1.6051, 'grad_norm': 32.75, 'learning_rate': 1.4772406349793744e-05, 'epoch': 0.41}\n",
      "{'loss': 1.6048, 'grad_norm': 31.75, 'learning_rate': 1.4673862961567602e-05, 'epoch': 0.41}\n",
      "{'loss': 1.5948, 'grad_norm': 22.125, 'learning_rate': 1.457473537737167e-05, 'epoch': 0.41}\n",
      "{'loss': 1.5924, 'grad_norm': 27.25, 'learning_rate': 1.4475035987371355e-05, 'epoch': 0.42}\n",
      "{'loss': 1.5764, 'grad_norm': 21.75, 'learning_rate': 1.4374777253203273e-05, 'epoch': 0.42}\n",
      "{'loss': 1.5535, 'grad_norm': 28.0, 'learning_rate': 1.4273971706417648e-05, 'epoch': 0.42}\n",
      "{'loss': 1.571, 'grad_norm': 28.5, 'learning_rate': 1.4172631946911964e-05, 'epoch': 0.43}\n",
      "{'loss': 1.5885, 'grad_norm': 26.25, 'learning_rate': 1.407077064135607e-05, 'epoch': 0.43}\n",
      "{'loss': 1.6443, 'grad_norm': 22.125, 'learning_rate': 1.3968400521608969e-05, 'epoch': 0.43}\n",
      "{'loss': 1.6053, 'grad_norm': 29.375, 'learning_rate': 1.3865534383127406e-05, 'epoch': 0.44}\n",
      "{'loss': 1.5922, 'grad_norm': 26.375, 'learning_rate': 1.3762185083366557e-05, 'epoch': 0.44}\n",
      "{'loss': 1.5198, 'grad_norm': 25.375, 'learning_rate': 1.3658365540172948e-05, 'epoch': 0.44}\n",
      "{'loss': 1.5857, 'grad_norm': 36.0, 'learning_rate': 1.3554088730169814e-05, 'epoch': 0.45}\n",
      "{'loss': 1.5283, 'grad_norm': 27.25, 'learning_rate': 1.3449367687135134e-05, 'epoch': 0.45}\n",
      "{'loss': 1.5013, 'grad_norm': 38.0, 'learning_rate': 1.334421550037251e-05, 'epoch': 0.45}\n",
      "{'loss': 1.5943, 'grad_norm': 22.125, 'learning_rate': 1.3238645313075104e-05, 'epoch': 0.46}\n",
      "{'loss': 1.5577, 'grad_norm': 34.25, 'learning_rate': 1.313267032068285e-05, 'epoch': 0.46}\n",
      "{'loss': 1.543, 'grad_norm': 34.5, 'learning_rate': 1.3026303769233112e-05, 'epoch': 0.46}\n",
      "{'loss': 1.5331, 'grad_norm': 25.875, 'learning_rate': 1.2919558953705055e-05, 'epoch': 0.47}\n",
      "{'loss': 1.5059, 'grad_norm': 26.125, 'learning_rate': 1.2812449216357863e-05, 'epoch': 0.47}\n",
      "{'loss': 1.6059, 'grad_norm': 29.75, 'learning_rate': 1.270498794506307e-05, 'epoch': 0.47}\n",
      "{'loss': 1.5332, 'grad_norm': 17.625, 'learning_rate': 1.259718857163117e-05, 'epoch': 0.48}\n",
      "{'loss': 1.5106, 'grad_norm': 32.75, 'learning_rate': 1.2489064570132764e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ostrich/anaconda3/envs/my_master/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5694, 'grad_norm': 28.625, 'learning_rate': 1.2380629455214392e-05, 'epoch': 0.48}\n",
      "{'loss': 1.5206, 'grad_norm': 39.75, 'learning_rate': 1.2271896780409321e-05, 'epoch': 0.48}\n",
      "{'loss': 1.5593, 'grad_norm': 25.5, 'learning_rate': 1.2162880136443447e-05, 'epoch': 0.49}\n",
      "{'loss': 1.5353, 'grad_norm': 19.75, 'learning_rate': 1.2053593149536576e-05, 'epoch': 0.49}\n",
      "{'loss': 1.4984, 'grad_norm': 25.375, 'learning_rate': 1.1944049479699244e-05, 'epoch': 0.49}\n",
      "{'loss': 1.4339, 'grad_norm': 40.0, 'learning_rate': 1.1834262819025326e-05, 'epoch': 0.5}\n",
      "{'loss': 1.516, 'grad_norm': 25.0, 'learning_rate': 1.1724246889980638e-05, 'epoch': 0.5}\n",
      "{'loss': 1.5066, 'grad_norm': 30.0, 'learning_rate': 1.1614015443687723e-05, 'epoch': 0.5}\n",
      "{'loss': 1.5195, 'grad_norm': 25.625, 'learning_rate': 1.150358225820709e-05, 'epoch': 0.51}\n",
      "{'loss': 1.5166, 'grad_norm': 38.75, 'learning_rate': 1.1392961136815046e-05, 'epoch': 0.51}\n",
      "{'loss': 1.4906, 'grad_norm': 30.125, 'learning_rate': 1.1282165906278402e-05, 'epoch': 0.51}\n",
      "{'loss': 1.5249, 'grad_norm': 20.0, 'learning_rate': 1.1171210415126248e-05, 'epoch': 0.52}\n",
      "{'loss': 1.4269, 'grad_norm': 31.375, 'learning_rate': 1.1060108531918972e-05, 'epoch': 0.52}\n",
      "{'loss': 1.4791, 'grad_norm': 24.75, 'learning_rate': 1.094887414351482e-05, 'epoch': 0.52}\n",
      "{'loss': 1.5215, 'grad_norm': 33.25, 'learning_rate': 1.0837521153334143e-05, 'epoch': 0.53}\n",
      "{'loss': 1.4672, 'grad_norm': 26.625, 'learning_rate': 1.0726063479621574e-05, 'epoch': 0.53}\n",
      "{'loss': 1.485, 'grad_norm': 27.0, 'learning_rate': 1.0614515053706367e-05, 'epoch': 0.53}\n",
      "{'loss': 1.458, 'grad_norm': 24.875, 'learning_rate': 1.0502889818261075e-05, 'epoch': 0.54}\n",
      "{'loss': 1.4905, 'grad_norm': 32.5, 'learning_rate': 1.0391201725558842e-05, 'epoch': 0.54}\n",
      "{'loss': 1.468, 'grad_norm': 25.0, 'learning_rate': 1.0279464735729472e-05, 'epoch': 0.54}\n",
      "{'loss': 1.4344, 'grad_norm': 25.125, 'learning_rate': 1.0167692815014527e-05, 'epoch': 0.55}\n",
      "{'loss': 1.4482, 'grad_norm': 25.75, 'learning_rate': 1.0055899934021649e-05, 'epoch': 0.55}\n",
      "{'loss': 1.4549, 'grad_norm': 63.25, 'learning_rate': 9.944100065978351e-06, 'epoch': 0.55}\n",
      "{'loss': 1.545, 'grad_norm': 24.875, 'learning_rate': 9.832307184985475e-06, 'epoch': 0.56}\n",
      "{'loss': 1.4314, 'grad_norm': 22.125, 'learning_rate': 9.720535264270529e-06, 'epoch': 0.56}\n",
      "{'loss': 1.4879, 'grad_norm': 25.375, 'learning_rate': 9.60879827444116e-06, 'epoch': 0.56}\n",
      "{'loss': 1.362, 'grad_norm': 26.0, 'learning_rate': 9.497110181738928e-06, 'epoch': 0.56}\n",
      "{'loss': 1.4793, 'grad_norm': 24.375, 'learning_rate': 9.385484946293636e-06, 'epoch': 0.57}\n",
      "{'loss': 1.4401, 'grad_norm': 26.25, 'learning_rate': 9.273936520378428e-06, 'epoch': 0.57}\n",
      "{'loss': 1.5434, 'grad_norm': 27.125, 'learning_rate': 9.16247884666586e-06, 'epoch': 0.57}\n",
      "{'loss': 1.5269, 'grad_norm': 24.75, 'learning_rate': 9.051125856485183e-06, 'epoch': 0.58}\n",
      "{'loss': 1.4554, 'grad_norm': 31.25, 'learning_rate': 8.939891468081033e-06, 'epoch': 0.58}\n",
      "{'loss': 1.4606, 'grad_norm': 23.875, 'learning_rate': 8.828789584873754e-06, 'epoch': 0.58}\n",
      "{'loss': 1.4056, 'grad_norm': 20.25, 'learning_rate': 8.717834093721598e-06, 'epoch': 0.59}\n",
      "{'loss': 1.5007, 'grad_norm': 28.125, 'learning_rate': 8.607038863184957e-06, 'epoch': 0.59}\n",
      "{'loss': 1.4562, 'grad_norm': 22.375, 'learning_rate': 8.496417741792912e-06, 'epoch': 0.59}\n",
      "{'loss': 1.4275, 'grad_norm': 27.5, 'learning_rate': 8.385984556312282e-06, 'epoch': 0.6}\n",
      "{'loss': 1.4559, 'grad_norm': 25.625, 'learning_rate': 8.275753110019367e-06, 'epoch': 0.6}\n",
      "{'loss': 1.507, 'grad_norm': 20.5, 'learning_rate': 8.165737180974678e-06, 'epoch': 0.6}\n",
      "{'loss': 1.3744, 'grad_norm': 27.875, 'learning_rate': 8.05595052030076e-06, 'epoch': 0.61}\n",
      "{'loss': 1.3631, 'grad_norm': 24.25, 'learning_rate': 7.94640685046343e-06, 'epoch': 0.61}\n",
      "{'loss': 1.4572, 'grad_norm': 21.25, 'learning_rate': 7.837119863556554e-06, 'epoch': 0.61}\n",
      "{'loss': 1.4481, 'grad_norm': 20.875, 'learning_rate': 7.72810321959068e-06, 'epoch': 0.62}\n",
      "{'loss': 1.4251, 'grad_norm': 35.25, 'learning_rate': 7.619370544785608e-06, 'epoch': 0.62}\n",
      "{'loss': 1.4959, 'grad_norm': 25.125, 'learning_rate': 7.510935429867237e-06, 'epoch': 0.62}\n",
      "{'loss': 1.3544, 'grad_norm': 36.5, 'learning_rate': 7.402811428368832e-06, 'epoch': 0.63}\n",
      "{'loss': 1.4365, 'grad_norm': 31.375, 'learning_rate': 7.295012054936934e-06, 'epoch': 0.63}\n",
      "{'loss': 1.4414, 'grad_norm': 25.75, 'learning_rate': 7.187550783642141e-06, 'epoch': 0.63}\n",
      "{'loss': 1.4442, 'grad_norm': 20.875, 'learning_rate': 7.080441046294948e-06, 'epoch': 0.63}\n",
      "{'loss': 1.4283, 'grad_norm': 23.5, 'learning_rate': 6.973696230766891e-06, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ostrich/anaconda3/envs/my_master/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4696, 'grad_norm': 22.75, 'learning_rate': 6.8673296793171555e-06, 'epoch': 0.64}\n",
      "{'loss': 1.501, 'grad_norm': 34.0, 'learning_rate': 6.761354686924895e-06, 'epoch': 0.64}\n",
      "{'loss': 1.4627, 'grad_norm': 33.0, 'learning_rate': 6.655784499627491e-06, 'epoch': 0.65}\n",
      "{'loss': 1.4393, 'grad_norm': 25.25, 'learning_rate': 6.550632312864869e-06, 'epoch': 0.65}\n",
      "{'loss': 1.4318, 'grad_norm': 22.375, 'learning_rate': 6.445911269830189e-06, 'epoch': 0.65}\n",
      "{'loss': 1.4532, 'grad_norm': 27.125, 'learning_rate': 6.341634459827053e-06, 'epoch': 0.66}\n",
      "{'loss': 1.4541, 'grad_norm': 21.75, 'learning_rate': 6.237814916633444e-06, 'epoch': 0.66}\n",
      "{'loss': 1.436, 'grad_norm': 27.75, 'learning_rate': 6.134465616872598e-06, 'epoch': 0.66}\n",
      "{'loss': 1.4565, 'grad_norm': 27.5, 'learning_rate': 6.0315994783910345e-06, 'epoch': 0.67}\n",
      "{'loss': 1.485, 'grad_norm': 52.25, 'learning_rate': 5.929229358643932e-06, 'epoch': 0.67}\n",
      "{'loss': 1.4714, 'grad_norm': 22.75, 'learning_rate': 5.827368053088043e-06, 'epoch': 0.67}\n",
      "{'loss': 1.4666, 'grad_norm': 22.75, 'learning_rate': 5.726028293582355e-06, 'epoch': 0.68}\n",
      "{'loss': 1.468, 'grad_norm': 32.75, 'learning_rate': 5.62522274679673e-06, 'epoch': 0.68}\n",
      "{'loss': 1.4463, 'grad_norm': 28.125, 'learning_rate': 5.524964012628648e-06, 'epoch': 0.68}\n",
      "{'loss': 1.4951, 'grad_norm': 29.0, 'learning_rate': 5.42526462262833e-06, 'epoch': 0.69}\n",
      "{'loss': 1.4039, 'grad_norm': 29.125, 'learning_rate': 5.326137038432399e-06, 'epoch': 0.69}\n",
      "{'loss': 1.4477, 'grad_norm': 32.75, 'learning_rate': 5.227593650206258e-06, 'epoch': 0.69}\n",
      "{'loss': 1.4181, 'grad_norm': 33.25, 'learning_rate': 5.129646775095432e-06, 'epoch': 0.7}\n",
      "{'loss': 1.4137, 'grad_norm': 25.75, 'learning_rate': 5.032308655686011e-06, 'epoch': 0.7}\n",
      "{'loss': 1.3548, 'grad_norm': 26.375, 'learning_rate': 4.935591458474433e-06, 'epoch': 0.7}\n",
      "{'loss': 1.4649, 'grad_norm': 23.75, 'learning_rate': 4.8395072723467585e-06, 'epoch': 0.7}\n",
      "{'loss': 1.4948, 'grad_norm': 26.5, 'learning_rate': 4.74406810706767e-06, 'epoch': 0.71}\n",
      "{'loss': 1.4168, 'grad_norm': 29.875, 'learning_rate': 4.649285891779327e-06, 'epoch': 0.71}\n",
      "{'loss': 1.4385, 'grad_norm': 24.125, 'learning_rate': 4.5551724735103285e-06, 'epoch': 0.71}\n",
      "{'loss': 1.4032, 'grad_norm': 26.875, 'learning_rate': 4.461739615694929e-06, 'epoch': 0.72}\n",
      "{'loss': 1.4537, 'grad_norm': 36.0, 'learning_rate': 4.368998996702694e-06, 'epoch': 0.72}\n",
      "{'loss': 1.463, 'grad_norm': 31.0, 'learning_rate': 4.276962208378811e-06, 'epoch': 0.72}\n",
      "{'loss': 1.4691, 'grad_norm': 40.5, 'learning_rate': 4.185640754595183e-06, 'epoch': 0.73}\n",
      "{'loss': 1.4127, 'grad_norm': 39.0, 'learning_rate': 4.095046049812545e-06, 'epoch': 0.73}\n",
      "{'loss': 1.4199, 'grad_norm': 24.0, 'learning_rate': 4.005189417653743e-06, 'epoch': 0.73}\n",
      "{'loss': 1.5101, 'grad_norm': 41.25, 'learning_rate': 3.916082089488372e-06, 'epoch': 0.74}\n",
      "{'loss': 1.4257, 'grad_norm': 32.75, 'learning_rate': 3.827735203028953e-06, 'epoch': 0.74}\n",
      "{'loss': 1.4258, 'grad_norm': 37.5, 'learning_rate': 3.740159800938784e-06, 'epoch': 0.74}\n",
      "{'loss': 1.4258, 'grad_norm': 20.75, 'learning_rate': 3.6533668294517154e-06, 'epoch': 0.75}\n",
      "{'loss': 1.4304, 'grad_norm': 30.75, 'learning_rate': 3.5673671370039464e-06, 'epoch': 0.75}\n",
      "{'loss': 1.4491, 'grad_norm': 28.375, 'learning_rate': 3.482171472878062e-06, 'epoch': 0.75}\n",
      "{'loss': 1.4217, 'grad_norm': 52.25, 'learning_rate': 3.39779048585945e-06, 'epoch': 0.76}\n",
      "{'loss': 1.4393, 'grad_norm': 33.0, 'learning_rate': 3.314234722905302e-06, 'epoch': 0.76}\n",
      "{'loss': 1.3963, 'grad_norm': 22.5, 'learning_rate': 3.2315146278263053e-06, 'epoch': 0.76}\n",
      "{'loss': 1.4176, 'grad_norm': 33.25, 'learning_rate': 3.149640539981267e-06, 'epoch': 0.77}\n",
      "{'loss': 1.467, 'grad_norm': 40.5, 'learning_rate': 3.0686226929847617e-06, 'epoch': 0.77}\n",
      "{'loss': 1.3691, 'grad_norm': 25.625, 'learning_rate': 2.9884712134280324e-06, 'epoch': 0.77}\n",
      "{'loss': 1.4487, 'grad_norm': 38.25, 'learning_rate': 2.909196119613218e-06, 'epoch': 0.78}\n",
      "{'loss': 1.4736, 'grad_norm': 25.125, 'learning_rate': 2.8308073203011667e-06, 'epoch': 0.78}\n",
      "{'loss': 1.4578, 'grad_norm': 27.375, 'learning_rate': 2.753314613472906e-06, 'epoch': 0.78}\n",
      "{'loss': 1.4363, 'grad_norm': 48.0, 'learning_rate': 2.6767276851049818e-06, 'epoch': 0.78}\n",
      "{'loss': 1.4034, 'grad_norm': 26.25, 'learning_rate': 2.6010561079587817e-06, 'epoch': 0.79}\n",
      "{'loss': 1.4321, 'grad_norm': 51.0, 'learning_rate': 2.5263093403840145e-06, 'epoch': 0.79}\n",
      "{'loss': 1.366, 'grad_norm': 35.25, 'learning_rate': 2.452496725136503e-06, 'epoch': 0.79}\n",
      "{'loss': 1.4979, 'grad_norm': 32.5, 'learning_rate': 2.3796274882103964e-06, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ostrich/anaconda3/envs/my_master/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4448, 'grad_norm': 35.0, 'learning_rate': 2.3077107376850005e-06, 'epoch': 0.8}\n",
      "{'loss': 1.4652, 'grad_norm': 40.5, 'learning_rate': 2.2367554625863496e-06, 'epoch': 0.8}\n",
      "{'loss': 1.501, 'grad_norm': 26.875, 'learning_rate': 2.1667705317636333e-06, 'epoch': 0.81}\n",
      "{'loss': 1.4949, 'grad_norm': 49.0, 'learning_rate': 2.0977646927806682e-06, 'epoch': 0.81}\n",
      "{'loss': 1.4546, 'grad_norm': 25.5, 'learning_rate': 2.029746570822524e-06, 'epoch': 0.81}\n",
      "{'loss': 1.3918, 'grad_norm': 27.375, 'learning_rate': 1.9627246676174363e-06, 'epoch': 0.82}\n",
      "{'loss': 1.4535, 'grad_norm': 31.125, 'learning_rate': 1.896707360374167e-06, 'epoch': 0.82}\n",
      "{'loss': 1.4109, 'grad_norm': 33.75, 'learning_rate': 1.8317029007349086e-06, 'epoch': 0.82}\n",
      "{'loss': 1.367, 'grad_norm': 32.5, 'learning_rate': 1.7677194137439036e-06, 'epoch': 0.83}\n",
      "{'loss': 1.4498, 'grad_norm': 25.875, 'learning_rate': 1.7047648968318697e-06, 'epoch': 0.83}\n",
      "{'loss': 1.4274, 'grad_norm': 24.625, 'learning_rate': 1.642847218816398e-06, 'epoch': 0.83}\n",
      "{'loss': 1.4422, 'grad_norm': 23.75, 'learning_rate': 1.5819741189183902e-06, 'epoch': 0.84}\n",
      "{'loss': 1.5079, 'grad_norm': 36.5, 'learning_rate': 1.522153205794742e-06, 'epoch': 0.84}\n",
      "{'loss': 1.3703, 'grad_norm': 26.25, 'learning_rate': 1.4633919565873033e-06, 'epoch': 0.84}\n",
      "{'loss': 1.4914, 'grad_norm': 21.25, 'learning_rate': 1.4056977159883011e-06, 'epoch': 0.85}\n",
      "{'loss': 1.4384, 'grad_norm': 23.25, 'learning_rate': 1.3490776953223107e-06, 'epoch': 0.85}\n",
      "{'loss': 1.4381, 'grad_norm': 26.0, 'learning_rate': 1.2935389716448976e-06, 'epoch': 0.85}\n",
      "{'loss': 1.3793, 'grad_norm': 23.125, 'learning_rate': 1.23908848685804e-06, 'epoch': 0.85}\n",
      "{'loss': 1.4502, 'grad_norm': 33.5, 'learning_rate': 1.1857330468424466e-06, 'epoch': 0.86}\n",
      "{'loss': 1.4362, 'grad_norm': 25.125, 'learning_rate': 1.1334793206068739e-06, 'epoch': 0.86}\n",
      "{'loss': 1.5225, 'grad_norm': 23.375, 'learning_rate': 1.082333839454559e-06, 'epoch': 0.86}\n",
      "{'loss': 1.4343, 'grad_norm': 42.5, 'learning_rate': 1.0323029961668463e-06, 'epoch': 0.87}\n",
      "{'loss': 1.4325, 'grad_norm': 50.25, 'learning_rate': 9.833930442041506e-07, 'epoch': 0.87}\n",
      "{'loss': 1.3859, 'grad_norm': 37.75, 'learning_rate': 9.356100969243231e-07, 'epoch': 0.87}\n",
      "{'loss': 1.492, 'grad_norm': 28.25, 'learning_rate': 8.889601268185233e-07, 'epoch': 0.88}\n",
      "{'loss': 1.4765, 'grad_norm': 27.125, 'learning_rate': 8.434489647647093e-07, 'epoch': 0.88}\n",
      "{'loss': 1.4568, 'grad_norm': 35.5, 'learning_rate': 7.990822992988267e-07, 'epoch': 0.88}\n",
      "{'loss': 1.3406, 'grad_norm': 26.375, 'learning_rate': 7.558656759037796e-07, 'epoch': 0.89}\n",
      "{'loss': 1.3936, 'grad_norm': 29.875, 'learning_rate': 7.13804496316296e-07, 'epoch': 0.89}\n",
      "{'loss': 1.4277, 'grad_norm': 24.625, 'learning_rate': 6.729040178517454e-07, 'epoch': 0.89}\n",
      "{'loss': 1.4224, 'grad_norm': 30.75, 'learning_rate': 6.331693527470306e-07, 'epoch': 0.9}\n",
      "{'loss': 1.3948, 'grad_norm': 26.875, 'learning_rate': 5.946054675215785e-07, 'epoch': 0.9}\n",
      "{'loss': 1.4314, 'grad_norm': 27.375, 'learning_rate': 5.572171823565797e-07, 'epoch': 0.9}\n",
      "{'loss': 1.4323, 'grad_norm': 24.75, 'learning_rate': 5.210091704924947e-07, 'epoch': 0.91}\n",
      "{'loss': 1.4662, 'grad_norm': 27.125, 'learning_rate': 4.859859576449444e-07, 'epoch': 0.91}\n",
      "{'loss': 1.5409, 'grad_norm': 30.125, 'learning_rate': 4.5215192143902577e-07, 'epoch': 0.91}\n",
      "{'loss': 1.5206, 'grad_norm': 25.5, 'learning_rate': 4.1951129086214015e-07, 'epoch': 0.92}\n",
      "{'loss': 1.4937, 'grad_norm': 41.0, 'learning_rate': 3.8806814573541185e-07, 'epoch': 0.92}\n",
      "{'loss': 1.4362, 'grad_norm': 30.0, 'learning_rate': 3.578264162037348e-07, 'epoch': 0.92}\n",
      "{'loss': 1.4266, 'grad_norm': 33.0, 'learning_rate': 3.2878988224454346e-07, 'epoch': 0.93}\n",
      "{'loss': 1.3681, 'grad_norm': 36.25, 'learning_rate': 3.0096217319533386e-07, 'epoch': 0.93}\n",
      "{'loss': 1.4375, 'grad_norm': 37.0, 'learning_rate': 2.7434676730003886e-07, 'epoch': 0.93}\n",
      "{'loss': 1.4397, 'grad_norm': 23.25, 'learning_rate': 2.489469912742637e-07, 'epoch': 0.93}\n",
      "{'loss': 1.4483, 'grad_norm': 44.25, 'learning_rate': 2.2476601988947965e-07, 'epoch': 0.94}\n",
      "{'loss': 1.4349, 'grad_norm': 28.25, 'learning_rate': 2.0180687557619816e-07, 'epoch': 0.94}\n",
      "{'loss': 1.4204, 'grad_norm': 47.0, 'learning_rate': 1.800724280461963e-07, 'epoch': 0.94}\n",
      "{'loss': 1.443, 'grad_norm': 25.5, 'learning_rate': 1.5956539393382043e-07, 'epoch': 0.95}\n",
      "{'loss': 1.4085, 'grad_norm': 39.25, 'learning_rate': 1.4028833645643113e-07, 'epoch': 0.95}\n",
      "{'loss': 1.4731, 'grad_norm': 31.75, 'learning_rate': 1.2224366509401732e-07, 'epoch': 0.95}\n",
      "{'loss': 1.3936, 'grad_norm': 38.75, 'learning_rate': 1.0543363528803696e-07, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ostrich/anaconda3/envs/my_master/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5046, 'grad_norm': 46.25, 'learning_rate': 8.986034815950173e-08, 'epoch': 0.96}\n",
      "{'loss': 1.4379, 'grad_norm': 39.0, 'learning_rate': 7.55257502463469e-08, 'epoch': 0.96}\n",
      "{'loss': 1.4748, 'grad_norm': 38.25, 'learning_rate': 6.243163326014268e-08, 'epoch': 0.97}\n",
      "{'loss': 1.4439, 'grad_norm': 53.25, 'learning_rate': 5.057963386213116e-08, 'epoch': 0.97}\n",
      "{'loss': 1.4482, 'grad_norm': 23.875, 'learning_rate': 3.9971233458665495e-08, 'epoch': 0.97}\n",
      "{'loss': 1.406, 'grad_norm': 53.75, 'learning_rate': 3.0607758016043546e-08, 'epoch': 0.98}\n",
      "{'loss': 1.4179, 'grad_norm': 20.0, 'learning_rate': 2.2490377894768266e-08, 'epoch': 0.98}\n",
      "{'loss': 1.4311, 'grad_norm': 111.0, 'learning_rate': 1.562010770326916e-08, 'epoch': 0.98}\n",
      "{'loss': 1.4804, 'grad_norm': 51.0, 'learning_rate': 9.99780617107815e-09, 'epoch': 0.99}\n",
      "{'loss': 1.41, 'grad_norm': 24.625, 'learning_rate': 5.6241760414987856e-09, 'epoch': 0.99}\n",
      "{'loss': 1.3936, 'grad_norm': 35.25, 'learning_rate': 2.4997639837687217e-09, 'epoch': 0.99}\n",
      "{'loss': 1.4486, 'grad_norm': 41.25, 'learning_rate': 6.24960524725493e-10, 'epoch': 1.0}\n",
      "{'loss': 1.4412, 'grad_norm': 46.5, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'train_runtime': 2721.7709, 'train_samples_per_second': 0.921, 'train_steps_per_second': 0.115, 'train_loss': 1.750066633803395, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=313, training_loss=1.750066633803395, metrics={'train_runtime': 2721.7709, 'train_samples_per_second': 0.921, 'train_steps_per_second': 0.115, 'train_loss': 1.750066633803395, 'epoch': 1.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = trainer.train()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ostrich/anaconda3/envs/my_master/lib/python3.10/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### delete the model state and the trainer\n",
    "del trainer\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the Peft Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03bd1b6371a437888b917243987bfdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# redefine the output dir\n",
    "\n",
    "## Load PeftModel on CPU\n",
    "\n",
    "adapter_path = f\"models/adapters/{modified_model_name}_alignment-handbook\"\n",
    "\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     'microsoft/phi-2', low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\n",
    "# )\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    adapter_path, adapter_name=\"sft\", low_cpu_mem_usage=True, torch_dtype=torch.bfloat16\n",
    ")\n",
    "## Merge Adapter and the base model\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\n",
    "    f\"models/{modified_model_name}_alignment-handbook\",\n",
    "    safe_serialization=True,\n",
    "    max_shard_size=\"2GB\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vibe check the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bacbfc4967a4a1682800457e5e9894a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "### Reload the model\n",
    "ft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    f\"models/{modified_model_name}_alignment-handbook\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"models/adapters/{modified_model_name}_alignment-handbook\")\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# With or without adapter\n",
    "# ft_model.disable_adapters()\n",
    "# define the pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=ft_model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\",\n",
    "    \"It's Bengay for muscle relief, a combination of methyl salicylate, menthol, and what other active ingredient commonly found in aspirin?\",\n",
    "    \"How can i get rid of llamas in my backyard?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**PROMPT**: \n",
      "A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\n",
      "\n",
      "**GENERATED ANS**: \n",
      "<|im_start|>system\n",
      "Follow user instructions<|im_end|>\n",
      "<|im_start|>user\n",
      "A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "A rectangular garden has a length of 25 feet and a width of 15 feet. If you want to build a fence around the entire garden, how many feet of fencing will you need?\n",
      "\n",
      "==============================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**PROMPT**: \n",
      "It's Bengay for muscle relief, a combination of methyl salicylate, menthol, and what other active ingredient commonly found in aspirin?\n",
      "\n",
      "**GENERATED ANS**: \n",
      "<|im_start|>system\n",
      "Follow user instructions<|im_end|>\n",
      "<|im_start|>user\n",
      "It's Bengay for muscle relief, a combination of methyl salicylate, menthol, and what other active ingredient commonly found in aspirin?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "It's aspirin, a pain reliever and anti-inflammatory agent. It's used to treat headaches, muscle aches, and fever. It's also used to prevent heart attacks and strokes. It's available in tablet, capsule, and liquid form. It's important to follow the instructions on the label and to talk to a doctor before taking aspirin if you have any medical conditions.\n",
      " Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan Azerbijan\n",
      "==============================\n",
      "**PROMPT**: \n",
      "How can i get rid of llamas in my backyard?\n",
      "\n",
      "**GENERATED ANS**: \n",
      "<|im_start|>system\n",
      "Follow user instructions<|im_end|>\n",
      "<|im_start|>user\n",
      "How can i get rid of llamas in my backyard?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "I'm not sure what you mean by \"get rid of llamas in my backyard\". Are you asking how to remove them or how to prevent them from entering your yard?\n",
      "\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    messages = pipe.tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"Follow user instructions\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "    # print(messages)\n",
    "    outputs = pipe(\n",
    "        messages,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True,\n",
    "        temperature=0.4,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    print(f\"**PROMPT**: \\n{prompt}\\n\")\n",
    "    print(f\"**GENERATED ANS**: \\n{outputs[0]['generated_text']}\")\n",
    "    print(\"===\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del ft_model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: Vibe check the not finetuned phi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets have a look at what the `packing=True` argument does..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl.trainer import ConstantLengthDataset\n",
    "\n",
    "cdl = ConstantLengthDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    seq_length=2048,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for data in tqdm(cdl):\n",
    "    print(data)\n",
    "    print(data[\"input_ids\"].shape, data[\"labels\"].shape)\n",
    "\n",
    "    token_ids = data[\"input_ids\"].tolist()\n",
    "    print(tokenizer.decode(token_ids))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Checking out the behaviour of the llama tokenizer\n",
    "\n",
    "# llama_tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     \"meta-llama/Llama-2-7b-hf\", add_eos_token=True\n",
    "# )\n",
    "# llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "\n",
    "# llama_tokenizer.padding_side = \"left\"\n",
    "\n",
    "# llama_tokenizer(\n",
    "#     ex, add_special_tokens=True, padding=\"max_length\", truncation=True, max_length=25\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_embeddings = ft_model.get_input_embeddings()\n",
    "# print(trained_embeddings.weight[tokenizer.pad_token_id])\n",
    "trained_pad_token = trained_embeddings.modules_to_save.default.weight[\n",
    "    tokenizer.eos_token_id\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_embeddings = model.get_input_embeddings()\n",
    "\n",
    "original_pad_token = original_embeddings.weight[tokenizer.eos_token_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_embeddings.modules_to_save.default.weight[50295].equal(\n",
    "    original_embeddings.weight[50295]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_pad_token.equal(trained_pad_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
